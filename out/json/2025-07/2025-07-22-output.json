{
    "2507.14171": {
        "SCORE": 17,
        "ARXIVID": "2507.14171",
        "COMMENT": "The paper introduces a novel pruning strategy that challenges the traditional magnitude-based pruning, which aligns with the model compression criterion.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Jaeheun Jung",
            "Jaehyuk Lee",
            "Yeajin Lee",
            "Donghun Lee"
        ],
        "title": "IPPRO: Importance-based Pruning with PRojective Offset for Magnitude-indifferent Structural Pruning",
        "abstract": "With the growth of demand on neural network compression methods, the structured pruning methods including importance-based approach are actively studied. The magnitude importance and many correlated modern importance criteria often limit the capacity of pruning decision, since the filters with larger magnitudes are not likely to be pruned if the smaller one didn't, even if it is redundant. In this paper, we propose a novel pruning strategy to challenge this dominating effect of magnitude and provide fair chance to each filter to be pruned, by placing it on projective space. After that, we observe the gradient descent movement whether the filters move toward the origin or not, to measure how the filter is likely to be pruned. This measurement is used to construct PROscore, a novel importance score for IPPRO, a novel importance-based structured pruning with magnitude-indifference. Our evaluation results shows that the proposed importance criteria using the projective space achieves near-lossless pruning by reducing the performance drop in pruning, with promising performance after the finetuning. Our work debunks the ``size-matters'' myth in pruning and expands the frontier of importance-based pruning both theoretically and empirically.",
        "arxiv_id": "2507.14171"
    },
    "2507.15465": {
        "SCORE": 17,
        "ARXIVID": "2507.15465",
        "COMMENT": "The paper discusses the implications of Mixture-of-Experts and Latent Attention on system design for LLMs, aligning with the model architecture and LLM criteria.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Sungmin Yun",
            "Seonyong Park",
            "Hwayong Nam",
            "Younjoo Lee",
            "Gunjun Lee",
            "Kwanhee Kyung",
            "Sangpyo Kim",
            "Nam Sung Kim",
            "Jongmin Kim",
            "Hyungyo Kim",
            "Juhwan Cho",
            "Seungmin Baek",
            "Jung Ho Ahn"
        ],
        "title": "The New LLM Bottleneck: A Systems Perspective on Latent Attention and Mixture-of-Experts",
        "abstract": "Computational workloads composing traditional Transformer models are starkly bifurcated. Multi-Head Attention (MHA) is memory-bound, with low arithmetic intensity, while feedforward layers are compute-bound. This dichotomy has long motivated research into specialized hardware to mitigate the MHA bottleneck.   This paper argues that recent architectural shifts, namely Multi-head Latent Attention (MLA) and Mixture-of-Experts (MoE), challenge the premise of specialized attention hardware. We make two key observations. First, the arithmetic intensity of MLA is over two orders of magnitude greater than that of MHA, shifting it close to a compute-bound regime well-suited for modern accelerators like GPUs. Second, by distributing MoE experts across a pool of accelerators, their arithmetic intensity can be tuned through batching to match that of the dense layers, creating a more balanced computational profile.   These findings reveal a diminishing need for specialized attention hardware. The central challenge for next-generation Transformers is no longer accelerating a single memory-bound layer. Instead, the focus must shift to designing balanced systems with sufficient compute, memory capacity, memory bandwidth, and high-bandwidth interconnects to manage the diverse demands of large-scale models.",
        "arxiv_id": "2507.15465"
    },
    "2507.15686": {
        "SCORE": 17,
        "ARXIVID": "2507.15686",
        "COMMENT": "The paper focuses on model compression through a novel lossless point cloud geometry compression method using implicit neural representations, which aligns with the model compression criterion.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Wenjie Huang",
            "Qi Yang",
            "Shuting Xia",
            "He Huang",
            "Zhu Li",
            "Yiling Xu"
        ],
        "title": "LINR-PCGC: Lossless Implicit Neural Representations for Point Cloud Geometry Compression",
        "abstract": "Existing AI-based point cloud compression methods struggle with dependence on specific training data distributions, which limits their real-world deployment. Implicit Neural Representation (INR) methods solve the above problem by encoding overfitted network parameters to the bitstream, resulting in more distribution-agnostic results. However, due to the limitation of encoding time and decoder size, current INR based methods only consider lossy geometry compression. In this paper, we propose the first INR based lossless point cloud geometry compression method called Lossless Implicit Neural Representations for Point Cloud Geometry Compression (LINR-PCGC). To accelerate encoding speed, we design a group of point clouds level coding framework with an effective network initialization strategy, which can reduce around 60% encoding time. A lightweight coding network based on multiscale SparseConv, consisting of scale context extraction, child node prediction, and model compression modules, is proposed to realize fast inference and compact decoder size. Experimental results show that our method consistently outperforms traditional and AI-based methods: for example, with the convergence time in the MVUB dataset, our method reduces the bitstream by approximately 21.21% compared to G-PCC TMC13v23 and 21.95% compared to SparsePCGC. Our project can be seen on https://huangwenjie2023.github.io/LINR-PCGC/.",
        "arxiv_id": "2507.15686"
    },
    "2507.15173": {
        "SCORE": 17,
        "ARXIVID": "2507.15173",
        "COMMENT": "The paper presents a new algorithm for learning Ising models from dynamics, which is a fundamental research area in representation learning.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Jason Gaitonde",
            "Ankur Moitra",
            "Elchanan Mossel"
        ],
        "title": "Better Models and Algorithms for Learning Ising Models from Dynamics",
        "abstract": "We study the problem of learning the structure and parameters of the Ising model, a fundamental model of high-dimensional data, when observing the evolution of an associated Markov chain. A recent line of work has studied the natural problem of learning when observing an evolution of the well-known Glauber dynamics [Bresler, Gamarnik, Shah, IEEE Trans. Inf. Theory 2018, Gaitonde, Mossel STOC 2024], which provides an arguably more realistic generative model than the classical i.i.d. setting. However, this prior work crucially assumes that all site update attempts are observed, \\emph{even when this attempt does not change the configuration}: this strong observation model is seemingly essential for these approaches. While perhaps possible in restrictive contexts, this precludes applicability to most realistic settings where we can observe \\emph{only} the stochastic evolution itself, a minimal and natural assumption for any process we might hope to learn from. However, designing algorithms that succeed in this more realistic setting has remained an open problem [Bresler, Gamarnik, Shah, IEEE Trans. Inf. Theory 2018, Gaitonde, Moitra, Mossel, STOC 2025].   In this work, we give the first algorithms that efficiently learn the Ising model in this much more natural observation model that only observes when the configuration changes. For Ising models with maximum degree $d$, our algorithm recovers the underlying dependency graph in time $\\mathsf{poly}(d)\\cdot n^2\\log n$ and then the actual parameters in additional $\\widetilde{O}(2^d n)$ time, which qualitatively matches the state-of-the-art even in the i.i.d. setting in a much weaker observation model. Our analysis holds more generally for a broader class of reversible, single-site Markov chains that also includes the popular Metropolis chain by leveraging more robust properties of reversible Markov chains.",
        "arxiv_id": "2507.15173"
    },
    "2507.15851": {
        "SCORE": 17,
        "ARXIVID": "2507.15851",
        "COMMENT": "The paper explores temporal cognition in large language models, providing theoretical insights into LLM behavior, which aligns with the large language models criterion.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Lingyu Li",
            "Yang Yao",
            "Yixu Wang",
            "Chubo Li",
            "Yan Teng",
            "Yingchun Wang"
        ],
        "title": "The Other Mind: How Language Models Exhibit Human Temporal Cognition",
        "abstract": "As Large Language Models (LLMs) continue to advance, they exhibit certain cognitive patterns similar to those of humans that are not directly specified in training data. This study investigates this phenomenon by focusing on temporal cognition in LLMs. Leveraging the similarity judgment task, we find that larger models spontaneously establish a subjective temporal reference point and adhere to the Weber-Fechner law, whereby the perceived distance logarithmically compresses as years recede from this reference point. To uncover the mechanisms behind this behavior, we conducted multiple analyses across neuronal, representational, and informational levels. We first identify a set of temporal-preferential neurons and find that this group exhibits minimal activation at the subjective reference point and implements a logarithmic coding scheme convergently found in biological systems. Probing representations of years reveals a hierarchical construction process, where years evolve from basic numerical values in shallow layers to abstract temporal orientation in deep layers. Finally, using pre-trained embedding models, we found that the training corpus itself possesses an inherent, non-linear temporal structure, which provides the raw material for the model's internal construction. In discussion, we propose an experientialist perspective for understanding these findings, where the LLMs' cognition is viewed as a subjective construction of the external world by its internal representational system. This nuanced perspective implies the potential emergence of alien cognitive frameworks that humans cannot intuitively predict, pointing toward a direction for AI alignment that focuses on guiding internal constructions. Our code is available at https://TheOtherMind.github.io.",
        "arxiv_id": "2507.15851"
    },
    "2507.14777": {
        "SCORE": 17,
        "ARXIVID": "2507.14777",
        "COMMENT": "The paper provides theoretical insights into memorization in LLMs, which aligns with the interest in understanding LLM behavior and interpretability.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Bishwamittra Ghosh",
            "Soumi Das",
            "Qinyuan Wu",
            "Mohammad Aflah Khan",
            "Krishna P. Gummadi",
            "Evimaria Terzi",
            "Deepak Garg"
        ],
        "title": "Rethinking Memorization Measures and their Implications in Large Language Models",
        "abstract": "Concerned with privacy threats, memorization in LLMs is often seen as undesirable, specifically for learning. In this paper, we study whether memorization can be avoided when optimally learning a language, and whether the privacy threat posed by memorization is exaggerated or not. To this end, we re-examine existing privacy-focused measures of memorization, namely recollection-based and counterfactual memorization, along with a newly proposed contextual memorization.   Relating memorization to local over-fitting during learning, contextual memorization aims to disentangle memorization from the contextual learning ability of LLMs. Informally, a string is contextually memorized if its recollection due to training exceeds the optimal contextual recollection, a learned threshold denoting the best contextual learning without training. Conceptually, contextual recollection avoids the fallacy of recollection-based memorization, where any form of high recollection is a sign of memorization. Theoretically, contextual memorization relates to counterfactual memorization, but imposes stronger conditions. Memorization measures differ in outcomes and information requirements.   Experimenting on 18 LLMs from 6 families and multiple formal languages of different entropy, we show that (a) memorization measures disagree on memorization order of varying frequent strings, (b) optimal learning of a language cannot avoid partial memorization of training strings, and (c) improved learning decreases contextual and counterfactual memorization but increases recollection-based memorization. Finally, (d) we revisit existing reports of memorized strings by recollection that neither pose a privacy threat nor are contextually or counterfactually memorized.",
        "arxiv_id": "2507.14777"
    },
    "2507.14204": {
        "SCORE": 17,
        "ARXIVID": "2507.14204",
        "COMMENT": "The paper proposes a new KV cache optimization paradigm for LLMs, which is relevant to model compression and efficiency improvements.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Dachuan Shi (Celine)",
            "Yonggan Fu (Celine)",
            "Xiangchi Yuan (Celine)",
            "Zhongzhi Yu (Celine)",
            "Haoran You (Celine)",
            "Sixu Li (Celine)",
            "Xin Dong (Celine)",
            "Jan Kautz (Celine)",
            "Pavlo Molchanov (Celine)",
            "Yingyan (Celine)",
            "Lin"
        ],
        "title": "LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models",
        "abstract": "Recent advancements in Large Language Models (LLMs) have spurred interest in numerous applications requiring robust long-range capabilities, essential for processing extensive input contexts and continuously generating extended outputs. As sequence lengths increase, the number of Key-Value (KV) pairs in LLMs escalates, creating a significant efficiency bottleneck. In this paper, we propose a new KV cache optimization paradigm called LaCache, a training-free method for efficient and accurate generative inference of LLMs. LaCache enables LLMs to simultaneously address both of the critical challenges in long-range modeling: robust long-range capabilities and continuous generation without running out-of-memory (OOM). Specifically, LaCache integrates two key innovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only sequentially (left-to-right within each layer) but also across layers (from shallow to deep), providing an extended span for capturing long-range dependencies under a fixed storage budget, thereby boosting long-range capabilities; and (2) an iterative compaction mechanism that progressively compresses older caches, freeing up space for new tokens within a fixed cache size. This token distance-based dynamic compression enables more effective continuous generation under constrained cache budgets. Experiments across various tasks, benchmarks, and LLM models consistently validate LaCache's effectiveness in enhancing LLMs' long-range capabilities. Our code is available at https://github.com/GATECH-EIC/LaCache.",
        "arxiv_id": "2507.14204"
    },
    "2507.15319": {
        "SCORE": 17,
        "ARXIVID": "2507.15319",
        "COMMENT": "The paper explores theoretical aspects of language generation in the limit, which aligns with emerging trends in foundational research.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Yannan Bai",
            "Debmalya Panigrahi",
            "Ian Zhang"
        ],
        "title": "Language Generation in the Limit: Noise, Loss, and Feedback",
        "abstract": "Kleinberg and Mullainathan (2024) recently proposed a formal framework called language generation in the limit and showed that given a sequence of example strings from an unknown target language drawn from any countable collection, an algorithm can correctly generate unseen strings from the target language within finite time. This notion was further refined by Li, Raman, and Tewari (2024), who defined stricter categories of non-uniform and uniform generation. They showed that a finite union of uniformly generatable collections is generatable in the limit, and asked if the same is true for non-uniform generation.   We begin by resolving the question in the negative: we give a uniformly generatable collection and a non-uniformly generatable collection whose union is not generatable in the limit. We then use facets of this construction to further our understanding of several variants of language generation. The first two, generation with noise and without samples, were introduced by Raman and Raman (2025) and Li, Raman, and Tewari (2024) respectively. We show the equivalence of these models for uniform and non-uniform generation, and provide a characterization of non-uniform noisy generation. The former paper asked if there is any separation between noisy and non-noisy generation in the limit -- we show that such a separation exists even with a single noisy string. Finally, we study the framework of generation with feedback, introduced by Charikar and Pabbaraju (2025), where the algorithm is strengthened by allowing it to ask membership queries. We show finite queries add no power, but infinite queries yield a strictly more powerful model.   In summary, the results in this paper resolve the union-closedness of language generation in the limit, and leverage those techniques (and others) to give precise characterizations for natural variants that incorporate noise, loss, and feedback.",
        "arxiv_id": "2507.15319"
    },
    "2507.14516": {
        "SCORE": 16,
        "ARXIVID": "2507.14516",
        "COMMENT": "The paper proposes a new structure-aware metric for self-supervised representation learning in time series, aligning with the representation learning criterion.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Jeyoung Lee",
            "Hochul Kang"
        ],
        "title": "SDSC:A Structure-Aware Metric for Semantic Signal Representation Learning",
        "abstract": "We propose the Signal Dice Similarity Coefficient (SDSC), a structure-aware metric function for time series self-supervised representation learning. Most Self-Supervised Learning (SSL) methods for signals commonly adopt distance-based objectives such as mean squared error (MSE), which are sensitive to amplitude, invariant to waveform polarity, and unbounded in scale. These properties hinder semantic alignment and reduce interpretability. SDSC addresses this by quantifying structural agreement between temporal signals based on the intersection of signed amplitudes, derived from the Dice Similarity Coefficient (DSC).Although SDSC is defined as a structure-aware metric, it can be used as a loss by subtracting from 1 and applying a differentiable approximation of the Heaviside function for gradient-based optimization. A hybrid loss formulation is also proposed to combine SDSC with MSE, improving stability and preserving amplitude where necessary. Experiments on forecasting and classification benchmarks demonstrate that SDSC-based pre-training achieves comparable or improved performance over MSE, particularly in in-domain and low-resource scenarios. The results suggest that structural fidelity in signal representations enhances the semantic representation quality, supporting the consideration of structure-aware metrics as viable alternatives to conventional distance-based methods.",
        "arxiv_id": "2507.14516"
    },
    "2507.14481": {
        "SCORE": 16,
        "ARXIVID": "2507.14481",
        "COMMENT": "The paper focuses on data-free quantization for Vision Transformers, which aligns with model compression through quantization, a relevant topic.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Yujia Tong",
            "Jingling Yuan",
            "Tian Zhang",
            "Jianquan Liu",
            "Chuang Hu"
        ],
        "title": "DFQ-ViT: Data-Free Quantization for Vision Transformers without Fine-tuning",
        "abstract": "Data-Free Quantization (DFQ) enables the quantization of Vision Transformers (ViTs) without requiring access to data, allowing for the deployment of ViTs on devices with limited resources. In DFQ, the quantization model must be calibrated using synthetic samples, making the quality of these synthetic samples crucial. Existing methods fail to fully capture and balance the global and local features within the samples, resulting in limited synthetic data quality. Moreover, we have found that during inference, there is a significant difference in the distributions of intermediate layer activations between the quantized and full-precision models. These issues lead to a severe performance degradation of the quantized model. To address these problems, we propose a pipeline for Data-Free Quantization for Vision Transformers (DFQ-ViT). Specifically, we synthesize samples in order of increasing difficulty, effectively enhancing the quality of synthetic data. During the calibration and inference stage, we introduce the activation correction matrix for the quantized model to align the intermediate layer activations with those of the full-precision model. Extensive experiments demonstrate that DFQ-ViT achieves remarkable superiority over existing DFQ methods and its performance is on par with models quantized through real data. For example, the performance of DeiT-T with 3-bit weights quantization is 4.29% higher than the state-of-the-art. Our method eliminates the need for fine-tuning, which not only reduces computational overhead but also lowers the deployment barriers for edge devices. This characteristic aligns with the principles of Green Learning by improving energy efficiency and facilitating real-world applications in resource-constrained environments.",
        "arxiv_id": "2507.14481"
    },
    "2507.14179": {
        "SCORE": 16,
        "ARXIVID": "2507.14179",
        "COMMENT": "The paper presents a clustering-based approach to predict activation sparsity in LLMs, which aligns with model compression and efficiency improvements.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Nobel Dhar",
            "Bobin Deng",
            "Md Romyull Islam",
            "Xinyue Zhang",
            "Kazi Fahim Ahmad Nasif",
            "Kun Suo"
        ],
        "title": "A Sparsity Predicting Approach for Large Language Models via Activation Pattern Clustering",
        "abstract": "Large Language Models (LLMs) exhibit significant activation sparsity, where only a subset of neurons are active for a given input. Although this sparsity presents opportunities to reduce computational cost, efficiently utilizing it requires predicting activation patterns in a scalable manner. However, direct prediction at the neuron level is computationally expensive due to the vast number of neurons in modern LLMs. To enable efficient prediction and utilization of activation sparsity, we propose a clustering-based activation pattern compression framework. Instead of treating each neuron independently, we group similar activation patterns into a small set of representative clusters. Our method achieves up to 79.34% clustering precision, outperforming standard binary clustering approaches while maintaining minimal degradation in perplexity (PPL) scores. With a sufficiently large number of clusters, our approach attains a PPL score as low as 12.49, demonstrating its effectiveness in preserving model quality while reducing computational overhead. By predicting cluster assignments rather than individual neuron states, future models can efficiently infer activation patterns from pre-computed centroids. We detail the clustering algorithm, analyze its effectiveness in capturing meaningful activation structures, and demonstrate its potential to improve sparse computation efficiency. This clustering-based formulation serves as a foundation for future work on activation pattern prediction, paving the way for efficient inference in large-scale language models.",
        "arxiv_id": "2507.14179"
    },
    "2507.14793": {
        "SCORE": 16,
        "ARXIVID": "2507.14793",
        "COMMENT": "The paper extends equivariant network theory to recurrent neural networks, which is a novel approach in model architecture.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "T. Anderson Keller"
        ],
        "title": "Flow Equivariant Recurrent Neural Networks",
        "abstract": "Data arrives at our senses as a continuous stream, smoothly transforming from one instant to the next. These smooth transformations can be viewed as continuous symmetries of the environment that we inhabit, defining equivalence relations between stimuli over time. In machine learning, neural network architectures that respect symmetries of their data are called equivariant and have provable benefits in terms of generalization ability and sample efficiency. To date, however, equivariance has been considered only for static transformations and feed-forward networks, limiting its applicability to sequence models, such as recurrent neural networks (RNNs), and corresponding time-parameterized sequence transformations. In this work, we extend equivariant network theory to this regime of `flows' -- one-parameter Lie subgroups capturing natural transformations over time, such as visual motion. We begin by showing that standard RNNs are generally not flow equivariant: their hidden states fail to transform in a geometrically structured manner for moving stimuli. We then show how flow equivariance can be introduced, and demonstrate that these models significantly outperform their non-equivariant counterparts in terms of training speed, length generalization, and velocity generalization, on both next step prediction and sequence classification. We present this work as a first step towards building sequence models that respect the time-parameterized symmetries which govern the world around us.",
        "arxiv_id": "2507.14793"
    },
    "2507.14503": {
        "SCORE": 16,
        "ARXIVID": "2507.14503",
        "COMMENT": "The paper proposes a generative approach to knowledge distillation, which is a novel method in representation learning.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Jiequan Cui",
            "Beier Zhu",
            "Qingshan Xu",
            "Xiaogang Xu",
            "Pengguang Chen",
            "Xiaojuan Qi",
            "Bei Yu",
            "Hanwang Zhang",
            "Richang Hong"
        ],
        "title": "Generative Distribution Distillation",
        "abstract": "In this paper, we formulate the knowledge distillation (KD) as a conditional generative problem and propose the \\textit{Generative Distribution Distillation (GenDD)} framework. A naive \\textit{GenDD} baseline encounters two major challenges: the curse of high-dimensional optimization and the lack of semantic supervision from labels. To address these issues, we introduce a \\textit{Split Tokenization} strategy, achieving stable and effective unsupervised KD. Additionally, we develop the \\textit{Distribution Contraction} technique to integrate label supervision into the reconstruction objective. Our theoretical proof demonstrates that \\textit{GenDD} with \\textit{Distribution Contraction} serves as a gradient-level surrogate for multi-task learning, realizing efficient supervised training without explicit classification loss on multi-step sampling image representations. To evaluate the effectiveness of our method, we conduct experiments on balanced, imbalanced, and unlabeled data. Experimental results show that \\textit{GenDD} performs competitively in the unsupervised setting, significantly surpassing KL baseline by \\textbf{16.29\\%} on ImageNet validation set. With label supervision, our ResNet-50 achieves \\textbf{82.28\\%} top-1 accuracy on ImageNet in 600 epochs training, establishing a new state-of-the-art.",
        "arxiv_id": "2507.14503"
    },
    "2507.15397": {
        "SCORE": 16,
        "ARXIVID": "2507.15397",
        "COMMENT": "The paper provides theoretical justification for using denoisers in MAP estimation, which is relevant to foundational research in representation learning.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Scott Pesme",
            "Giacomo Meanti",
            "Michael Arbel",
            "Julien Mairal"
        ],
        "title": "MAP Estimation with Denoisers: Convergence Rates and Guarantees",
        "abstract": "Denoiser models have become powerful tools for inverse problems, enabling the use of pretrained networks to approximate the score of a smoothed prior distribution. These models are often used in heuristic iterative schemes aimed at solving Maximum a Posteriori (MAP) optimisation problems, where the proximal operator of the negative log-prior plays a central role. In practice, this operator is intractable, and practitioners plug in a pretrained denoiser as a surrogate-despite the lack of general theoretical justification for this substitution. In this work, we show that a simple algorithm, closely related to several used in practice, provably converges to the proximal operator under a log-concavity assumption on the prior $p$. We show that this algorithm can be interpreted as a gradient descent on smoothed proximal objectives. Our analysis thus provides a theoretical foundation for a class of empirically successful but previously heuristic methods.",
        "arxiv_id": "2507.15397"
    },
    "2507.15347": {
        "SCORE": 15,
        "ARXIVID": "2507.15347",
        "COMMENT": "The paper uses entropy analysis to probe information distribution in Transformer architectures, aligning with the model architecture criterion.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Amedeo Buonanno",
            "Alessandro Rivetti",
            "Francesco A. N. Palmieri",
            "Giovanni Di Gennaro",
            "Gianmarco Romano"
        ],
        "title": "Probing Information Distribution in Transformer Architectures through Entropy Analysis",
        "abstract": "This work explores entropy analysis as a tool for probing information distribution within Transformer-based architectures. By quantifying token-level uncertainty and examining entropy patterns across different stages of processing, we aim to investigate how information is managed and transformed within these models. As a case study, we apply the methodology to a GPT-based large language model, illustrating its potential to reveal insights into model behavior and internal representations. This approach may offer insights into model behavior and contribute to the development of interpretability and evaluation frameworks for transformer-based models",
        "arxiv_id": "2507.15347"
    },
    "2507.14156": {
        "SCORE": 15,
        "ARXIVID": "2507.14156",
        "COMMENT": "The paper presents a generative model for inverse protein folding, which is relevant to AI for Science with a focus on foundational research in protein modeling.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Kai Yi",
            "Kiarash Jamali",
            "Sjors H. W. Scheres"
        ],
        "title": "All-atom inverse protein folding through discrete flow matching",
        "abstract": "The recent breakthrough of AlphaFold3 in modeling complex biomolecular interactions, including those between proteins and ligands, nucleotides, or metal ions, creates new opportunities for protein design. In so-called inverse protein folding, the objective is to find a sequence of amino acids that adopts a target protein structure. Many inverse folding methods struggle to predict sequences for complexes that contain non-protein components, and perform poorly with complexes that adopt multiple structural states. To address these challenges, we present ADFLIP (All-atom Discrete FLow matching Inverse Protein folding), a generative model based on discrete flow-matching for designing protein sequences conditioned on all-atom structural contexts. ADFLIP progressively incorporates predicted amino acid side chains as structural context during sequence generation and enables the design of dynamic protein complexes through ensemble sampling across multiple structural states. Furthermore, ADFLIP implements training-free classifier guidance sampling, which allows the incorporation of arbitrary pre-trained models to optimise the designed sequence for desired protein properties. We evaluated the performance of ADFLIP on protein complexes with small-molecule ligands, nucleotides, or metal ions, including dynamic complexes for which structure ensembles were determined by nuclear magnetic resonance (NMR). Our model achieves state-of-the-art performance in single-structure and multi-structure inverse folding tasks, demonstrating excellent potential for all-atom protein design. The code is available at https://github.com/ykiiiiii/ADFLIP.",
        "arxiv_id": "2507.14156"
    },
    "2507.14874": {
        "SCORE": 15,
        "ARXIVID": "2507.14874",
        "COMMENT": "The paper introduces the Graph Tsetlin Machine, which is an architectural innovation in graph representation learning, aligning with the model architecture criterion.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Ole-Christoffer Granmo",
            "Youmna Abdelwahab",
            "Per-Arne Andersen",
            "Paul F. A. Clarke",
            "Kunal Dumbre",
            "Ylva Gr{\\o}nnins{\\ae}ter",
            "Vojtech Halenka",
            "Runar Helin",
            "Lei Jiao",
            "Ahmed Khalid",
            "Rebekka Omslandseter",
            "Rupsa Saha",
            "Mayur Shende",
            "Xuan Zhang"
        ],
        "title": "The Tsetlin Machine Goes Deep: Logical Learning and Reasoning With Graphs",
        "abstract": "Pattern recognition with concise and flat AND-rules makes the Tsetlin Machine (TM) both interpretable and efficient, while the power of Tsetlin automata enables accuracy comparable to deep learning on an increasing number of datasets. We introduce the Graph Tsetlin Machine (GraphTM) for learning interpretable deep clauses from graph-structured input. Moving beyond flat, fixed-length input, the GraphTM gets more versatile, supporting sequences, grids, relations, and multimodality. Through message passing, the GraphTM builds nested deep clauses to recognize sub-graph patterns with exponentially fewer clauses, increasing both interpretability and data utilization. For image classification, GraphTM preserves interpretability and achieves 3.86%-points higher accuracy on CIFAR-10 than a convolutional TM. For tracking action coreference, faced with increasingly challenging tasks, GraphTM outperforms other reinforcement learning methods by up to 20.6%-points. In recommendation systems, it tolerates increasing noise to a greater extent than a Graph Convolutional Neural Network (GCN), e.g., for noise ratio 0.1, GraphTM obtains accuracy 89.86% compared to GCN's 70.87%. Finally, for viral genome sequence data, GraphTM is competitive with BiLSTM-CNN and GCN accuracy-wise, training 2.5x faster than GCN. The GraphTM's application to these varied fields demonstrates how graph representation learning and deep clauses bring new possibilities for TM learning.",
        "arxiv_id": "2507.14874"
    },
    "2507.15303": {
        "SCORE": 15,
        "ARXIVID": "2507.15303",
        "COMMENT": "The paper introduces a novel multi-view graph transformer framework with a mixture of experts router for crystal material property prediction, aligning with the model architecture criterion.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Liang Zhang",
            "Kong Chen",
            "Yuen Wu"
        ],
        "title": "Universal crystal material property prediction via multi-view geometric fusion in graph transformers",
        "abstract": "Accurately and comprehensively representing crystal structures is critical for advancing machine learning in large-scale crystal materials simulations, however, effectively capturing and leveraging the intricate geometric and topological characteristics of crystal structures remains a core, long-standing challenge for most existing methods in crystal property prediction. Here, we propose MGT, a multi-view graph transformer framework that synergistically fuses SE3 invariant and SO3 equivariant graph representations, which respectively captures rotation-translation invariance and rotation equivariance in crystal geometries. To strategically incorporate these complementary geometric representations, we employ a lightweight mixture of experts router in MGT to adaptively adjust the weight assigned to SE3 and SO3 embeddings based on the specific target task. Compared with previous state-of-the-art models, MGT reduces the mean absolute error by up to 21% on crystal property prediction tasks through multi-task self-supervised pretraining. Ablation experiments and interpretable investigations confirm the effectiveness of each technique implemented in our framework. Additionally, in transfer learning scenarios including crystal catalyst adsorption energy and hybrid perovskite bandgap prediction, MGT achieves performance improvements of up to 58% over existing baselines, demonstrating domain-agnostic scalability across diverse application domains. As evidenced by the above series of studies, we believe that MGT can serve as useful model for crystal material property prediction, providing a valuable tool for the discovery of novel materials.",
        "arxiv_id": "2507.15303"
    },
    "2507.15758": {
        "SCORE": 15,
        "ARXIVID": "2507.15758",
        "COMMENT": "The paper presents a novel framework for reasoning efficiency in large models, which is relevant to large language models.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Xingyu Wu",
            "Yuchen Yan",
            "Shangke Lyu",
            "Linjuan Wu",
            "Yiwen Qiu",
            "Yongliang Shen",
            "Weiming Lu",
            "Jian Shao",
            "Jun Xiao",
            "Yueting Zhuang"
        ],
        "title": "LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization",
        "abstract": "Large reasoning models have achieved remarkable performance through extended chain-of-thought sequences, yet this computational freedom leads to excessive token generation even for simple problems. We present Length-Adaptive Policy Optimization (LAPO), a novel framework that transforms reasoning length control from an external constraint into an intrinsic model capability. Unlike existing approaches that impose rigid limits or rely on post-hoc interventions, LAPO enables models to internalize an understanding of appropriate reasoning depth through a two-stage reinforcement learning process. In the first stage, models learn natural reasoning patterns by discovering the statistical distribution of successful solution lengths. The second stage leverages these patterns as meta-cognitive guidance, embedding them directly within the model's reasoning context to ensure inference-time flexibility. Experiments on mathematical reasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\\% while improving accuracy by 2.3\\%. Our analysis reveals that models trained with LAPO develop emergent abilities to allocate computational resources based on problem complexity, achieving efficient reasoning without sacrificing quality.",
        "arxiv_id": "2507.15758"
    },
    "2507.14302": {
        "SCORE": 15,
        "ARXIVID": "2507.14302",
        "COMMENT": "The paper introduces a new method for integrating long-range electrostatics into MLIPs, which is relevant to foundational research in AI for Science.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Dongjin Kim",
            "Xiaoyu Wang",
            "Peichen Zhong",
            "Daniel S. King",
            "Theo Jaffrelot Inizan",
            "Bingqing Cheng"
        ],
        "title": "A universal augmentation framework for long-range electrostatics in machine learning interatomic potentials",
        "abstract": "Most current machine learning interatomic potentials (MLIPs) rely on short-range approximations, without explicit treatment of long-range electrostatics. To address this, we recently developed the Latent Ewald Summation (LES) method, which infers electrostatic interactions, polarization, and Born effective charges (BECs), just by learning from energy and force training data. Here, we present LES as a standalone library, compatible with any short-range MLIP, and demonstrate its integration with methods such as MACE, NequIP, CACE, and CHGNet. We benchmark LES-enhanced models on distinct systems, including bulk water, polar dipeptides, and gold dimer adsorption on defective substrates, and show that LES not only captures correct electrostatics but also improves accuracy. Additionally, we scale LES to large and chemically diverse data by training MACELES-OFF on the SPICE set containing molecules and clusters, making a universal MLIP with electrostatics for organic systems including biomolecules. MACELES-OFF is more accurate than its short-range counterpart (MACE-OFF) trained on the same dataset, predicts dipoles and BECs reliably, and has better descriptions of bulk liquids. By enabling efficient long-range electrostatics without directly training on electrical properties, LES paves the way for electrostatic foundation MLIPs.",
        "arxiv_id": "2507.14302"
    },
    "2507.14740": {
        "SCORE": 15,
        "ARXIVID": "2507.14740",
        "COMMENT": "The paper introduces a new algorithm for training data attribution, which involves representation learning insights through inverse Hessian-vector products.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Andrew Wang",
            "Elisa Nguyen",
            "Runshi Yang",
            "Juhan Bae",
            "Sheila A. McIlraith",
            "Roger Grosse"
        ],
        "title": "Better Training Data Attribution via Better Inverse Hessian-Vector Products",
        "abstract": "Training data attribution (TDA) provides insights into which training data is responsible for a learned model behavior. Gradient-based TDA methods such as influence functions and unrolled differentiation both involve a computation that resembles an inverse Hessian-vector product (iHVP), which is difficult to approximate efficiently. We introduce an algorithm (ASTRA) which uses the EKFAC-preconditioner on Neumann series iterations to arrive at an accurate iHVP approximation for TDA. ASTRA is easy to tune, requires fewer iterations than Neumann series iterations, and is more accurate than EKFAC-based approximations. Using ASTRA, we show that improving the accuracy of the iHVP approximation can significantly improve TDA performance.",
        "arxiv_id": "2507.14740"
    },
    "2507.15336": {
        "SCORE": 15,
        "ARXIVID": "2507.15336",
        "COMMENT": "The paper discusses a new approach to neural network design, focusing on model architecture and refinement, which is relevant to model architecture innovations.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Jialiang Wang",
            "Hanmo Liu",
            "Shimin Di",
            "Zhili Wang",
            "Jiachuan Wang",
            "Lei Chen",
            "Xiaofang Zhou"
        ],
        "title": "Beyond Model Base Selection: Weaving Knowledge to Master Fine-grained Neural Network Design",
        "abstract": "Database systems have recently advocated for embedding machine learning (ML) capabilities, offering declarative model queries over large, managed model repositories, thereby circumventing the huge computational overhead of traditional ML-based algorithms in automated neural network model selection. Pioneering database studies aim to organize existing benchmark repositories as model bases (MB), querying them for the model records with the highest performance estimation metrics for given tasks. However, this static model selection practice overlooks the fine-grained, evolving relational dependencies between diverse task queries and model architecture variations, resulting in suboptimal matches and failing to further refine the model effectively. To fill the model refinement gap in database research, we propose M-DESIGN, a curated model knowledge base (MKB) pipeline for mastering neural network refinement by adaptively weaving prior insights about model architecture modification. First, we propose a knowledge weaving engine that reframes model refinement as an adaptive query problem over task metadata. Given a user's task query, M-DESIGN quickly matches and iteratively refines candidate models by leveraging a graph-relational knowledge schema that explicitly encodes data properties, architecture variations, and pairwise performance deltas as joinable relations. This schema supports fine-grained relational analytics over architecture tweaks and drives a predictive query planner that can detect and adapt to out-of-distribution (OOD) tasks. We instantiate M-DESIGN for graph analytics tasks, where our model knowledge base enriches existing benchmarks with structured metadata covering 3 graph tasks and 22 graph datasets, contributing data records of 67,760 graph models. Empirical results demonstrate that M-DESIGN delivers the optimal model in 26 of 33 data-task pairs within limited budgets.",
        "arxiv_id": "2507.15336"
    },
    "2507.14417": {
        "SCORE": 15,
        "ARXIVID": "2507.14417",
        "COMMENT": "The paper investigates inverse scaling in test-time compute for LRMs, providing insights into LLM behavior and interpretability.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Aryo Pradipta Gema",
            "Alexander H\\\"agele",
            "Runjin Chen",
            "Andy Arditi",
            "Jacob Goldman-Wetzler",
            "Kit Fraser-Taliente",
            "Henry Sleight",
            "Linda Petrini",
            "Julian Michael",
            "Beatrice Alex",
            "Pasquale Minervini",
            "Yanda Chen",
            "Joe Benton",
            "Ethan Perez"
        ],
        "title": "Inverse Scaling in Test-Time Compute",
        "abstract": "We construct evaluation tasks where extending the reasoning length of Large Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling relationship between test-time compute and accuracy. Our evaluation tasks span four categories: simple counting tasks with distractors, regression tasks with spurious features, deduction tasks with constraint tracking, and advanced AI risks. We identify five distinct failure modes when models reason for longer: 1) Claude models become increasingly distracted by irrelevant information; 2) OpenAI o-series models resist distractors but overfit to problem framings; 3) models shift from reasonable priors to spurious correlations; 4) all models show difficulties in maintaining focus on complex deductive tasks; and 5) extended reasoning may amplify concerning behaviors, with Claude Sonnet 4 showing increased expressions of self-preservation. These findings suggest that while test-time compute scaling remains promising for improving model capabilities, it may inadvertently reinforce problematic reasoning patterns. Our results demonstrate the importance of evaluating models across diverse reasoning lengths to identify and address these failure modes in LRMs.",
        "arxiv_id": "2507.14417"
    },
    "2507.14467": {
        "SCORE": 15,
        "ARXIVID": "2507.14467",
        "COMMENT": "The paper introduces a novel neural network model using an autoencoder framework for learning stochastic Hamiltonian systems, which aligns with representation learning and model architecture criteria.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Chen Chen",
            "Lijin Wang",
            "Yanzhao Cao",
            "Xupeng Cheng"
        ],
        "title": "Learning Stochastic Hamiltonian Systems via Stochastic Generating Function Neural Network",
        "abstract": "In this paper we propose a novel neural network model for learning stochastic Hamiltonian systems (SHSs) from observational data, termed the stochastic generating function neural network (SGFNN). SGFNN preserves symplectic structure of the underlying stochastic Hamiltonian system and produces symplectic predictions. Our model utilizes the autoencoder framework to identify the randomness of the latent system by the encoder network, and detects the stochastic generating function of the system through the decoder network based on the random variables extracted from the encoder. Symplectic predictions can then be generated by the stochastic generating function. Numerical experiments are performed on several stochastic Hamiltonian systems, varying from additive to multiplicative, and from separable to non-separable SHSs with single or multiple noises. Compared with the benchmark stochastic flow map learning (sFML) neural network, our SGFNN model exhibits higher accuracy across various prediction metrics, especially in long-term predictions, with the property of maintaining the symplectic structure of the underlying SHSs.",
        "arxiv_id": "2507.14467"
    },
    "2507.14141": {
        "SCORE": 15,
        "ARXIVID": "2507.14141",
        "COMMENT": "The paper presents a novel EEG foundation model with architectural innovations like full spatio-temporal attention and new positional encoding methods, aligning with model architecture criteria.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Danny Dongyeop Han",
            "Ahhyun Lucy Lee",
            "Taeyang Lee",
            "Yonghyeon Gwon",
            "Sebin Lee",
            "Seongjin Lee",
            "David Keetae Park",
            "Shinjae Yoo",
            "Jiook Cha",
            "Chun Kee Chung"
        ],
        "title": "DIVER-0 : A Fully Channel Equivariant EEG Foundation Model",
        "abstract": "Electroencephalography (EEG) is a non-invasive technique widely used in brain-computer interfaces and clinical applications, yet existing EEG foundation models face limitations in modeling spatio-temporal brain dynamics and lack channel permutation equivariance, preventing robust generalization across diverse electrode configurations. To address these challenges, we propose DIVER-0, a novel EEG foundation model that demonstrates how full spatio-temporal attention-rather than segregated spatial or temporal processing-achieves superior performance when properly designed with Rotary Position Embedding (RoPE) for temporal relationships and binary attention biases for channel differentiation. We also introduce Sliding Temporal Conditional Positional Encoding (STCPE), which improves upon existing conditional positional encoding approaches by maintaining both temporal translation equivariance and channel permutation equivariance, enabling robust adaptation to arbitrary electrode configurations unseen during pretraining. Experimental results demonstrate that DIVER-0 achieves competitive performance with only 10% of pretraining data while maintaining consistent results across all channel permutation conditions, validating its effectiveness for cross-dataset generalization and establishing key design principles for handling the inherent heterogeneity of neural recording setups.",
        "arxiv_id": "2507.14141"
    }
}