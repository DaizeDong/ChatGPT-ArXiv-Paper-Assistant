> This is a remedial run for missed papers from 1/13/2025 to 1/13/2025.
> 
> Results generated on 3/20/2025.

# Personalized Daily Arxiv Papers 1/14/2025

| *[gpt-4o]*   | Prompt   | Completion   | Total   |
|--------------|----------|--------------|---------|
| **Token**    | 24931    | 3489         | 28420   |
| **Cost**     | $0.06    | $0.03        | $0.1    |

Total arXiv papers: 169

Total scanned papers: 169

Total relevant papers: 14

**Table of contents with paper titles:**

1. [FlexQuant: Elastic Quantization Framework for Locally Hosted LLM on Edge Devices](#user-content-link1)
**Authors:** Yuji Chai, Mujin Kwen, David Brooks, Gu-Yeon Wei

2. [Synthesis and Analysis of Data as Probability Measures with Entropy-Regularized Optimal Transport](#user-content-link2)
**Authors:** Brendan Mallery, James M. Murphy, Shuchin Aeron

3. [Parallel Key-Value Cache Fusion for Position Invariant RAG](#user-content-link3)
**Authors:** Philhoon Oh, Jinwoo Shin, James Thorne

4. [Derivation of effective gradient flow equations and dynamical truncation of training data in Deep Learning](#user-content-link4)
**Authors:** Thomas Chen

5. [Neural Probabilistic Circuits: Enabling Compositional and Interpretable Predictions through Logical Reasoning](#user-content-link5)
**Authors:** Weixin Chen, Simon Yu, Huajie Shao, Lui Sha, Han Zhao

6. [Breaking Memory Limits: Gradient Wavelet Transform Enhances LLMs Training](#user-content-link6)
**Authors:** Ziqing Wen, Ping Luo, Jiahuan Wang, Xiaoge Deng, Jinping Zou, Kun Yuan, Tao Sun, Dongsheng Li

7. [AlphaNet: Scaling Up Local Frame-based Atomistic Interatomic Potential](#user-content-link7)
**Authors:** Bangchen Yin, Jiaao Wang, Weitao Du, Pengbo Wang, Penghua Ying, Haojun Jia, Zisheng Zhang, Yuanqi Du, Carla P. Gomes, Graeme Henkelman, Chenru Duan, Hai Xiao

8. [Universal Training of Neural Networks to Achieve Bayes Optimal Classification Accuracy](#user-content-link8)
**Authors:** Mohammadreza Tavasoli Naeini, Ali Bereyhi, Morteza Noshad, Ben Liang, Alfred O. Hero III

9. [Dataset Distillation as Pushforward Optimal Quantization](#user-content-link9)
**Authors:** Hong Ye Tan, Emma Slade

10. [ACCon: Angle-Compensated Contrastive Regularizer for Deep Regression](#user-content-link10)
**Authors:** Botao Zhao, Xiaoyang Qu, Zuheng Kang, Junqing Peng, Jing Xiao, Jianzong Wang

11. [QuantuneV2: Compiler-Based Local Metric-Driven Mixed Precision Quantization for Practical Embedded AI Applications](#user-content-link11)
**Authors:** Jeongseok Kim, Jemin Lee, Yongin Kwon, Daeyoung Kim

12. [PRKAN: Parameter-Reduced Kolmogorov-Arnold Networks](#user-content-link12)
**Authors:** Hoang-Thang Ta, Duy-Quy Thai, Anh Tran, Grigori Sidorov, Alexander Gelbukh

13. [Variable Bregman Majorization-Minimization Algorithm and its Application to Dirichlet Maximum Likelihood Estimation](#user-content-link13)
**Authors:** Ségolène Martin, Jean-Christophe Pesquet, Gabriele Steidl, Ismail Ben Ayed

14. [LLM360 K2: Building a 65B 360-Open-Source Large Language Model from Scratch](#user-content-link14)
**Authors:** Zhengzhong Liu, Bowen Tan, Hongyi Wang, Willie Neiswanger, Tianhua Tao, Haonan Li, Fajri Koto, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller, Liqun Ma, Liping Tang, Nikhil Ranjan, Yonghao Zhuang, Guowei He, Renxi Wang, Mingkai Deng, Robin Algayres, Yuanzhi Li, Zhiqiang Shen, Preslav Nakov, Eric Xing

---

## 1. [FlexQuant: Elastic Quantization Framework for Locally Hosted LLM on Edge Devices](https://arxiv.org/abs/2501.07139) <a id="link1"></a>

**ArXiv ID:** 2501.07139

**Authors:** Yuji Chai, Mujin Kwen, David Brooks, Gu-Yeon Wei

**Abstract:** Deploying LLMs on edge devices presents serious technical challenges. Memory elasticity is crucial for edge devices with unified memory, where memory is shared and fluctuates dynamically. Existing solutions suffer from either poor transition granularity or high storage costs. We propose FlexQuant, a novel elasticity framework that generates an ensemble of quantized models, providing an elastic hosting solution with 15x granularity improvement and 10x storage reduction compared to SoTA methods. FlexQuant works with most quantization methods and creates a family of trade-off options under various storage limits through our pruning method. It brings great performance and flexibility to the edge deployment of LLMs.

**Comment:** FlexQuant proposes an elastic quantization framework for LLMs on edge devices, which aligns with foundational research in model compression and efficiency.

**Relevance:** 9
**Novelty:** 8

---

## 2. [Synthesis and Analysis of Data as Probability Measures with Entropy-Regularized Optimal Transport](https://arxiv.org/abs/2501.07446) <a id="link2"></a>

**ArXiv ID:** 2501.07446

**Authors:** Brendan Mallery, James M. Murphy, Shuchin Aeron

**Abstract:** We consider synthesis and analysis of probability measures using the entropy-regularized Wasserstein-2 cost and its unbiased version, the Sinkhorn divergence. The synthesis problem consists of computing the barycenter, with respect to these costs, of $m$ reference measures given a set of coefficients belonging to the $m$-dimensional simplex. The analysis problem consists of finding the coefficients for the closest barycenter in the Wasserstein-2 distance to a given measure $\mu$. Under the weakest assumptions on the measures thus far in the literature, we compute the derivative of the entropy-regularized Wasserstein-2 cost. We leverage this to establish a characterization of regularized barycenters as solutions to a fixed-point equation for the average of the entropic maps from the barycenter to the reference measures. This characterization yields a finite-dimensional, convex, quadratic program for solving the analysis problem when $\mu$ is a barycenter. It is shown that these coordinates, as well as the value of the barycenter functional, can be estimated from samples with dimension-independent rates of convergence, a hallmark of entropy-regularized optimal transport, and we verify these rates experimentally. We also establish that barycentric coordinates are stable with respect to perturbations in the Wasserstein-2 metric, suggesting a robustness of these coefficients to corruptions. We employ the barycentric coefficients as features for classification of corrupted point cloud data, and show that compared to neural network baselines, our approach is more efficient in small training data regimes.

**Comment:** The paper explores entropy-regularized optimal transport and its application to barycenter computation, which aligns with foundational research in representation learning and efficiency. The focus on theoretical insights and robustness is highly relevant.

**Relevance:** 9
**Novelty:** 8

---

## 3. [Parallel Key-Value Cache Fusion for Position Invariant RAG](https://arxiv.org/abs/2501.07523) <a id="link3"></a>

**ArXiv ID:** 2501.07523

**Authors:** Philhoon Oh, Jinwoo Shin, James Thorne

**Abstract:** Recent advancements in Large Language Models (LLMs) underscore the necessity of Retrieval Augmented Generation (RAG) to leverage external information. However, LLMs are sensitive to the position of relevant information within contexts and tend to generate incorrect responses when such information is placed in the middle, known as `Lost in the Middle' phenomenon. In this paper, we introduce a framework that generates consistent outputs for decoder-only models, irrespective of the input context order. Experimental results for three open domain question answering tasks demonstrate position invariance, where the model is not sensitive to input context order, and superior robustness to irrelevent passages compared to prevailing approaches for RAG pipelines.

**Comment:** The paper introduces a framework for position-invariant RAG using key-value cache fusion, which aligns with model compression and efficiency breakthroughs, particularly in LLMs.

**Relevance:** 9
**Novelty:** 8

---

## 4. [Derivation of effective gradient flow equations and dynamical truncation of training data in Deep Learning](https://arxiv.org/abs/2501.07400) <a id="link4"></a>

**ArXiv ID:** 2501.07400

**Authors:** Thomas Chen

**Abstract:** We derive explicit equations governing the cumulative biases and weights in Deep Learning with ReLU activation function, based on gradient descent for the Euclidean cost in the input layer, and under the assumption that the weights are, in a precise sense, adapted to the coordinate system distinguished by the activations. We show that gradient descent corresponds to a dynamical process in the input layer, whereby clusters of data are progressively reduced in complexity ("truncated") at an exponential rate that increases with the number of data points that have already been truncated. We provide a detailed discussion of several types of solutions to the gradient flow equations. A main motivation for this work is to shed light on the interpretability question in supervised learning.

**Comment:** This paper derives gradient flow equations and explores training dynamics in deep learning, which directly aligns with the representation learning criterion. The theoretical insights into gradient descent and data truncation are highly relevant.

**Relevance:** 9
**Novelty:** 8

---

## 5. [Neural Probabilistic Circuits: Enabling Compositional and Interpretable Predictions through Logical Reasoning](https://arxiv.org/abs/2501.07021) <a id="link5"></a>

**ArXiv ID:** 2501.07021

**Authors:** Weixin Chen, Simon Yu, Huajie Shao, Lui Sha, Han Zhao

**Abstract:** End-to-end deep neural networks have achieved remarkable success across various domains but are often criticized for their lack of interpretability. While post hoc explanation methods attempt to address this issue, they often fail to accurately represent these black-box models, resulting in misleading or incomplete explanations. To overcome these challenges, we propose an inherently transparent model architecture called Neural Probabilistic Circuits (NPCs), which enable compositional and interpretable predictions through logical reasoning. In particular, an NPC consists of two modules: an attribute recognition model, which predicts probabilities for various attributes, and a task predictor built on a probabilistic circuit, which enables logical reasoning over recognized attributes to make class predictions. To train NPCs, we introduce a three-stage training algorithm comprising attribute recognition, circuit construction, and joint optimization. Moreover, we theoretically demonstrate that an NPC's error is upper-bounded by a linear combination of the errors from its modules. To further demonstrate the interpretability of NPC, we provide both the most probable explanations and the counterfactual explanations. Empirical results on four benchmark datasets show that NPCs strike a balance between interpretability and performance, achieving results competitive even with those of end-to-end black-box models while providing enhanced interpretability.

**Comment:** The paper introduces Neural Probabilistic Circuits, which align with the model architecture criterion by proposing a novel interpretable architecture combining probabilistic circuits and neural networks. The focus on logical reasoning adds theoretical depth.

**Relevance:** 9
**Novelty:** 8

---

## 6. [Breaking Memory Limits: Gradient Wavelet Transform Enhances LLMs Training](https://arxiv.org/abs/2501.07237) <a id="link6"></a>

**ArXiv ID:** 2501.07237

**Authors:** Ziqing Wen, Ping Luo, Jiahuan Wang, Xiaoge Deng, Jinping Zou, Kun Yuan, Tao Sun, Dongsheng Li

**Abstract:** Large language models (LLMs) have shown impressive performance across a range of natural language processing tasks. However, their vast number of parameters introduces significant memory challenges during training, particularly when using memory-intensive optimizers like Adam. Existing memory-efficient algorithms often rely on techniques such as singular value decomposition projection or weight freezing. While these approaches help alleviate memory constraints, they generally produce suboptimal results compared to full-rank updates. In this paper, we investigate the memory-efficient method beyond low-rank training, proposing a novel solution called Gradient Wavelet Transform (GWT), which applies wavelet transforms to gradients in order to significantly reduce the memory requirements for maintaining optimizer states. We demonstrate that GWT can be seamlessly integrated with memory-intensive optimizers, enabling efficient training without sacrificing performance. Through extensive experiments on both pre-training and fine-tuning tasks, we show that GWT achieves state-of-the-art performance compared with advanced memory-efficient optimizers and full-rank approaches in terms of both memory usage and training performance.

**Comment:** The paper introduces Gradient Wavelet Transform (GWT) to reduce memory requirements during LLM training, which is relevant to model compression and efficiency breakthroughs.

**Relevance:** 9
**Novelty:** 8

---

## 7. [AlphaNet: Scaling Up Local Frame-based Atomistic Interatomic Potential](https://arxiv.org/abs/2501.07155) <a id="link7"></a>

**ArXiv ID:** 2501.07155

**Authors:** Bangchen Yin, Jiaao Wang, Weitao Du, Pengbo Wang, Penghua Ying, Haojun Jia, Zisheng Zhang, Yuanqi Du, Carla P. Gomes, Graeme Henkelman, Chenru Duan, Hai Xiao

**Abstract:** Molecular dynamics simulations demand unprecedented accuracy and scalability to tackle grand challenges in energy materials, catalytic processes, and biomolecular design. To bridge this gap, we present AlphaNet, a local frame-based equivariant model that simultaneously advances computational efficiency and predictive precision for atomistic systems. By constructing equivariant local frames with learnable geometric transitions, AlphaNet encodes atomic environments with enhanced representational capacity, achieving state of the art accuracy in energy and force predictions. Extensive benchmarks spanning defected graphene, formate decomposition, inorganic bulks, and large-scale datasets (OC2M and Matbench Discovery) demonstrate its superior performance over existing neural network interatomic potentials while ensuring scalability across diverse system sizes. The synergy of accuracy, efficiency, and transferability positions AlphaNet as a transformative tool for simulating multiscale phenomena, from catalyst dynamics to energy storage interfaces, with direct implications for accelerating the discovery of functional materials and complex molecular systems.

**Comment:** AlphaNet proposes a novel equivariant model for atomistic simulations, which aligns with foundational research in AI for Science by introducing architectural innovations for molecular modeling.

**Relevance:** 8
**Novelty:** 8

---

## 8. [Universal Training of Neural Networks to Achieve Bayes Optimal Classification Accuracy](https://arxiv.org/abs/2501.07754) <a id="link8"></a>

**ArXiv ID:** 2501.07754

**Authors:** Mohammadreza Tavasoli Naeini, Ali Bereyhi, Morteza Noshad, Ben Liang, Alfred O. Hero III

**Abstract:** This work invokes the notion of $f$-divergence to introduce a novel upper bound on the Bayes error rate of a general classification task. We show that the proposed bound can be computed by sampling from the output of a parameterized model. Using this practical interpretation, we introduce the Bayes optimal learning threshold (BOLT) loss whose minimization enforces a classification model to achieve the Bayes error rate. We validate the proposed loss for image and text classification tasks, considering MNIST, Fashion-MNIST, CIFAR-10, and IMDb datasets. Numerical experiments demonstrate that models trained with BOLT achieve performance on par with or exceeding that of cross-entropy, particularly on challenging datasets. This highlights the potential of BOLT in improving generalization.

**Comment:** The introduction of the BOLT loss for achieving Bayes optimal classification accuracy is a novel contribution to representation learning, with potential implications for training dynamics and generalization.

**Relevance:** 8
**Novelty:** 8

---

## 9. [Dataset Distillation as Pushforward Optimal Quantization](https://arxiv.org/abs/2501.07681) <a id="link9"></a>

**ArXiv ID:** 2501.07681

**Authors:** Hong Ye Tan, Emma Slade

**Abstract:** Dataset distillation aims to find a synthetic training set such that training on the synthetic data achieves similar performance to training on real data, with orders of magnitude less computational requirements. Existing methods can be broadly categorized as either bi-level optimization problems that have neural network training heuristics as the lower level problem, or disentangled methods that bypass the bi-level optimization by matching distributions of data. The latter method has the major advantages of speed and scalability in terms of size of both training and distilled datasets. We demonstrate that when equipped with an encoder-decoder structure, the empirically successful disentangled methods can be reformulated as an optimal quantization problem, where a finite set of points is found to approximate the underlying probability measure by minimizing the expected projection distance. In particular, we link existing disentangled dataset distillation methods to the classical optimal quantization and Wasserstein barycenter problems, demonstrating consistency of distilled datasets for diffusion-based generative priors. We propose a simple extension of the state-of-the-art data distillation method D4M, achieving better performance on the ImageNet-1K dataset with trivial additional computation, and state-of-the-art performance in higher image-per-class settings.

**Comment:** The paper links dataset distillation to optimal quantization and introduces a novel extension to an existing method, which aligns with representation learning and foundational generative paradigms.

**Relevance:** 8
**Novelty:** 8

---

## 10. [ACCon: Angle-Compensated Contrastive Regularizer for Deep Regression](https://arxiv.org/abs/2501.07045) <a id="link10"></a>

**ArXiv ID:** 2501.07045

**Authors:** Botao Zhao, Xiaoyang Qu, Zuheng Kang, Junqing Peng, Jing Xiao, Jianzong Wang

**Abstract:** In deep regression, capturing the relationship among continuous labels in feature space is a fundamental challenge that has attracted increasing interest. Addressing this issue can prevent models from converging to suboptimal solutions across various regression tasks, leading to improved performance, especially for imbalanced regression and under limited sample sizes. However, existing approaches often rely on order-aware representation learning or distance-based weighting. In this paper, we hypothesize a linear negative correlation between label distances and representation similarities in regression tasks. To implement this, we propose an angle-compensated contrastive regularizer for deep regression, which adjusts the cosine distance between anchor and negative samples within the contrastive learning framework. Our method offers a plug-and-play compatible solution that extends most existing contrastive learning methods for regression tasks. Extensive experiments and theoretical analysis demonstrate that our proposed angle-compensated contrastive regularizer not only achieves competitive regression performance but also excels in data efficiency and effectiveness on imbalanced datasets.

**Comment:** The paper introduces an angle-compensated contrastive regularizer for deep regression, which aligns with representation learning by addressing label relationships in feature space. The method is novel and could have broader implications for contrastive learning in regression tasks.

**Relevance:** 8
**Novelty:** 7

---

## 11. [QuantuneV2: Compiler-Based Local Metric-Driven Mixed Precision Quantization for Practical Embedded AI Applications](https://arxiv.org/abs/2501.07161) <a id="link11"></a>

**ArXiv ID:** 2501.07161

**Authors:** Jeongseok Kim, Jemin Lee, Yongin Kwon, Daeyoung Kim

**Abstract:** Mixed-precision quantization methods have been proposed to reduce model size while minimizing accuracy degradation. However, existing studies require retraining and do not consider the computational overhead and intermediate representations (IR) generated during the compilation process, limiting their application at the compiler level. This computational overhead refers to the runtime latency caused by frequent quantization and dequantization operations during inference. Performing these operations at the individual operator level causes significant runtime delays. To address these issues, we propose QuantuneV2, a compiler-based mixed-precision quantization method designed for practical embedded AI applications. QuantuneV2 performs inference only twice, once before quantization and once after quantization, and operates with a computational complexity of O(n) that increases linearly with the number of model parameters. We also made the sensitivity analysis more stable by using local metrics like weights, activation values, the Signal to Quantization Noise Ratio, and the Mean Squared Error. We also cut down on computational overhead by choosing the best IR and using operator fusion. Experimental results show that QuantuneV2 achieved up to a 10.28 percent improvement in accuracy and a 12.52 percent increase in speed compared to existing methods across five models: ResNet18v1, ResNet50v1, SqueezeNetv1, VGGNet, and MobileNetv2. This demonstrates that QuantuneV2 enhances model performance while maintaining computational efficiency, making it suitable for deployment in embedded AI environments.

**Comment:** The paper proposes a compiler-based mixed-precision quantization method, which aligns with the model compression criterion. The use of local metrics and operator fusion adds methodological insights, making it relevant to foundational efficiency research.

**Relevance:** 8
**Novelty:** 7

---

## 12. [PRKAN: Parameter-Reduced Kolmogorov-Arnold Networks](https://arxiv.org/abs/2501.07032) <a id="link12"></a>

**ArXiv ID:** 2501.07032

**Authors:** Hoang-Thang Ta, Duy-Quy Thai, Anh Tran, Grigori Sidorov, Alexander Gelbukh

**Abstract:** Kolmogorov-Arnold Networks (KANs) represent an innovation in neural network architectures, offering a compelling alternative to Multi-Layer Perceptrons (MLPs) in models such as Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Transformers. By advancing network design, KANs drive groundbreaking research and enable transformative applications across various scientific domains involving neural networks. However, existing KANs often require significantly more parameters in their network layers than MLPs. To address this limitation, this paper introduces PRKANs (Parameter-Reduced Kolmogorov-Arnold Networks), which employ several methods to reduce the parameter count in KAN layers, making them comparable to MLP layers. Experimental results on the MNIST and Fashion-MNIST datasets demonstrate that PRKANs outperform several existing KANs, and their variant with attention mechanisms rivals the performance of MLPs, albeit with slightly longer training times. Furthermore, the study highlights the advantages of Gaussian Radial Basis Functions (GRBFs) and layer normalization in KAN designs. The repository for this work is available at: https://github.com/hoangthangta/All-KAN.

**Comment:** The paper introduces PRKANs, a parameter-reduced version of Kolmogorov-Arnold Networks, which aligns with the model architecture criterion. The focus on reducing parameter count and exploring Gaussian Radial Basis Functions adds novelty.

**Relevance:** 8
**Novelty:** 7

---

## 13. [Variable Bregman Majorization-Minimization Algorithm and its Application to Dirichlet Maximum Likelihood Estimation](https://arxiv.org/abs/2501.07306) <a id="link13"></a>

**ArXiv ID:** 2501.07306

**Authors:** Ségolène Martin, Jean-Christophe Pesquet, Gabriele Steidl, Ismail Ben Ayed

**Abstract:** We propose a novel Bregman descent algorithm for minimizing a convex function that is expressed as the sum of a differentiable part (defined over an open set) and a possibly nonsmooth term. The approach, referred to as the Variable Bregman Majorization-Minimization (VBMM) algorithm, extends the Bregman Proximal Gradient method by allowing the Bregman function used in the divergence to adaptively vary at each iteration, provided it satisfies a majorizing condition on the objective function. This adaptive framework enables the algorithm to approximate the objective more precisely at each iteration, thereby allowing for accelerated convergence compared to the traditional Bregman Proximal Gradient descent. We establish the convergence of the VBMM algorithm to a minimizer under mild assumptions on the family of metrics used. Furthermore, we introduce a novel application of both the Bregman Proximal Gradient method and the VBMM algorithm to the estimation of the multidimensional parameters of a Dirichlet distribution through the maximization of its log-likelihood. Numerical experiments confirm that the VBMM algorithm outperforms existing approaches in terms of convergence speed.

**Comment:** The VBMM algorithm introduces a novel adaptive Bregman descent method, which is a foundational contribution to optimization techniques and has potential applications in representation learning.

**Relevance:** 7
**Novelty:** 8

---

## 14. [LLM360 K2: Building a 65B 360-Open-Source Large Language Model from Scratch](https://arxiv.org/abs/2501.07124) <a id="link14"></a>

**ArXiv ID:** 2501.07124

**Authors:** Zhengzhong Liu, Bowen Tan, Hongyi Wang, Willie Neiswanger, Tianhua Tao, Haonan Li, Fajri Koto, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller, Liqun Ma, Liping Tang, Nikhil Ranjan, Yonghao Zhuang, Guowei He, Renxi Wang, Mingkai Deng, Robin Algayres, Yuanzhi Li, Zhiqiang Shen, Preslav Nakov, Eric Xing

**Abstract:** We detail the training of the LLM360 K2-65B model, scaling up our 360-degree OPEN SOURCE approach to the largest and most powerful models under project LLM360. While open-source LLMs continue to advance, the answer to "How are the largest LLMs trained?" remains unclear within the community. The implementation details for such high-capacity models are often protected due to business considerations associated with their high cost. This lack of transparency prevents LLM researchers from leveraging valuable insights from prior experience, e.g., "What are the best practices for addressing loss spikes?" The LLM360 K2 project addresses this gap by providing full transparency and access to resources accumulated during the training of LLMs at the largest scale. This report highlights key elements of the K2 project, including our first model, K2 DIAMOND, a 65 billion-parameter LLM that surpasses LLaMA-65B and rivals LLaMA2-70B, while requiring fewer FLOPs and tokens. We detail the implementation steps and present a longitudinal analysis of K2 DIAMOND's capabilities throughout its training process. We also outline ongoing projects such as TXT360, setting the stage for future models in the series. By offering previously unavailable resources, the K2 project also resonates with the 360-degree OPEN SOURCE principles of transparency, reproducibility, and accessibility, which we believe are vital in the era of resource-intensive AI research.

**Comment:** The paper details the training of a 65B open-source LLM and provides transparency into large-scale model training. While it is significant for the community, it focuses on implementation details rather than foundational breakthroughs.

**Relevance:** 7
**Novelty:** 6

---

# Paper Selection Prompt

You are a helpful paper reading assistant whose job is to read daily posts from ArXiv and identify a few papers that your friend will enjoy reading.
Your job is to carefully read the paper titles and abstracts below and find the ones that match the criteria below.

## Instructions

Write the response in JSONL format with {ARXIVID, COMMENT, RELEVANCE, NOVELTY} on each line, one for each paper.

- ARXIVID: should be the ArXiv ID.
- COMMENT: should identify whether there is a criteria that match the paper very closely. These matches should not be based on general terms like "language modeling" or "advancements" and should specifically refer to a criterion. No need to mention the non-matching criteria.
- RELEVANCE: should be a score from 1-10.
- NOVELTY: should be a score from 1-10.

## Scoring Criteria

> The "Relevance" score measures how closely the paper aligns with the core topics of the prompt.
> The "Novelty" score assesses the originality and impact of the paper.
> They are two **ORTHONORMAL** axes and **SHOULD NOT** be confused with each other.

### Relevance Scoring

- Relevance 9-10 (Completely Relevant)
  - Focus: Fully aligned with core topics with no deviation, score the highest if contains relevant keywords in it.
  - Examples: Papers focused on foundational methods or theoretical research, whose titles contain topic keywords like "MoE".

- Relevance 7-8 (Relevant)
  - Focus: Retain a solid link to the main research area, though may touch on peripheral elements.
  - Examples: Papers research on the fundamental part of MoE through a less critical aspect like its behavior in GNN.

- Relevance 5-6 (Borderline)
  - Focus: Maintains a link to the core topic but also extends into at least one other domain/area beyond the primary focus.
  - Examples: Work referencing MoE centered on reinforcement learning.

- Relevance 3-4 (Irrelevant)
  - Focus: Largely outside our interests with no association to our topics.
  - Examples: Application-focused papers like using MoE to solve a problem in the real world.

- Relevance 1-2 (Ignore)
  - Focus: Purely unrelated to our topics. Completely a different domain.
  - **Exception**: If the paper hints at a cutting-edge, radically new direction that could eventually transform the primary domain, consider a score of 9–10 despite initial appearances. (Usually a very rare concept that belongs to the fundamental research)

### Novelty Scoring

- Novelty 9-10 (Breakthrough)
  - Definition: Groundbreaking methods/theory introducing new directions or solving major challenges.
  - Examples: Entirely new paradigm for foundational models; a novel theory transforming representation learning.

- Novelty 7-8 (Improvements)
  - Definition: Substantial insights/enhancements, though not a full paradigm shift.
  - Examples: Modifications on existing methods yielding significantly better results.

- Novelty 5-6 (Borderline)
  - Definition: Incremental contributions with possible long-term benefits, not immediately transformative.
  - Examples: Moderately novel extension to an existing architecture; refining current methods without fundamentally altering them.

- Novelty 3-4 (Tangential)
  - Definition: Minor or domain-specific improvements with limited broader impact.
  - Examples: Slight modifications to known methods with strange motivation; purely engineering jobs like a new benchmark/dataset.

- Novelty 1-2 (Low)
  - Definition: Minimal originality, applying standard approaches without real innovation.
  - Examples: Using an off-the-shelf model without adding new insights; purely application-driven studies like finetuning a pretrained model using existing methods.

## Papers

[PAPER LIST HERE]

## Relevant Topics

Use the following relevance criteria to focus on foundational research. Keep **relevant** papers and filter out **irrelevant** ones. Avoid purely **application-driven** work.

1. Representation Learning
   - Relevant: Insights into how deep networks encode information, feature/dictionary learning, sparse/contrastive methods, training dynamics in neural networks.
   - Irrelevant: Standard applications of known techniques lacking new theoretical or methodological contributions.

2. Model Architecture
   - Relevant: Mixture-of-Experts (MoE), Transformers, Conditional/Dynamic Networks, Autoencoders, analysis on existing architectures (like encoder-decoder), or other architectural innovations.
   - Irrelevant: Merely using existing architectures for a certain task without insights into the structure themselves.

3. Model Compression
   - Relevant: Sparsity, pruning, quantization, low-rank approaches, KV cache, or other algorithmic/theoretical efficiency breakthroughs.
   - Irrelevant: Straightforward applications of existing compression methods to new tasks.

4. Large Language Models (LLMs)
   - Relevant: Major breakthroughs in pretraining or architecture, theoretical insights into LLM behavior/interpretability.
   - Irrelevant: Domain-specific usage (e.g., translation, jail-breaking), finetuning or inference tricks (e.g., instruction tuning, chain-of-thoughts, data mixing), or empirical dataset/benchmark studies and text-level analysis (e.g. hallucination, reasoning, safety).

5. AI for Science
   - Relevant: Foundational research in molecular/protein modeling, new generative paradigms, or significant architecture-level innovations.
   - Irrelevant: Conventional, domain-specific applications without new theoretical perspectives.

6. Emerging Trends
   - Relevant: Cutting-edge theoretical work challenging established assumptions or introducing broad new paradigms.
   - Irrelevant: Incremental improvements or trend-following without novel insights.

**Keywords:**

- Relevant: Mixture of Experts (MoE), Representation Learning, Compression/Efficiency, Sparse/Sparsity, Pruning, Quantization, Low-rank, Foundation Model, etc.
- Irrelevant: Reinforcement Learning, Transfer Learning, Federated Learning, Online Learning, Diffusion Models, etc.
- Application: Image Segmentation, Medical Imaging, 3D Vision, Video Understanding, Information Retrieval, Summarization, Recommendation Systems, Machine Translation, Speech Recognition, Signal Processing, Spatial/Temporal Modeling, Time Series, Knowledge Graph, etc.