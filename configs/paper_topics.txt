## Relevant Topics:

1. **Representation Learning**: New developments in feature engineering, optimization techniques, and representational mechanisms for neural networks.
    - **Relevant**: Papers introducing novel autoencoders, sparse representations, or methods for contrastive learning. Works that propose theoretical insights into the representation capabilities of deep networks.
    - **Not Relevant**: Generic application papers that merely utilize standard representation learning methods without proposing innovations.

2. **Model Research**: Advances in model architectures, compression, and editing for enhancing neural network efficiency and flexibility.
    - **Model Architectures**:
        - **Relevant**: Studies focusing on Mixture-of-Experts (MoE), transformers, or innovative architectures with implications for scalability or interpretability.
        - **Not Relevant**: Papers that merely apply existing architectures to new tasks without structural innovations.
    - **Model Compression**:
        - **Relevant**: Techniques for pruning, quantization, low-rank decomposition, or other methods for reducing model size or inference time.
        - **Not Relevant**: Papers focusing only on deployment-specific optimizations without novel compression strategies.

3. **Large Language Models (LLMs)**: Contributions to pretraining, scalability, inference optimization, or theoretical understanding of large language models.
    - **Relevant**: Papers that address efficient training, scalable pretraining methodologies, novel architectures, or insights into LLM behavior and performance.
    - **Not Relevant**: Domain-specific applications of LLMs or papers with limited relevance to generalizable LLM advancements.

4. **AI for Science**: Foundational research of AI in scientific discovery, particularly molecular or protein modeling, or other novel breakthroughs.
    - **Relevant**: Papers proposing innovative training or structures of AI in molecule generation, protein folding.
    - **Not Relevant**: Standard or domain-specific applications of AI in science without proposing new methodologies or architectures.

5. **Emerging Trends and Novelty**: Exploration of fundamentally new ideas or frameworks in AI research.
    - **Relevant**: Papers introducing interdisciplinary methods, novel evaluation paradigms, or experimental results that challenge existing assumptions.
    - **Not Relevant**: Trend-following or incremental studies lacking substantive innovation.

## Additional Instructions:

1. **Prioritization Framework**:
    - **Relevance 9 (Must-Read)**: Strongly correlated papers that present milestone achievements or groundbreaking insights in the above focus areas.
    - **Relevance 7 (Recommended)**: Papers correlated with focus areas, or intriguing but less critical findings.
    - **Relevance 5 (Optional)**: Trend-following, complex, or less immediately impactful works with future potential.
    - **Relevance 3 (Marginal)**: Highly complex or tangential papers with limited relevance to core interests.
    - **Relevance 1 (New Areas)**: Papers outside the core research scope.

2. **Keywords & Signals**: Papers should exhibit:
    - **Relevance**: Use of terms like “Mixture of Experts (MoE),” “Routing,” “Dynamic,” “Molecular,” “Sparse,” “Compression,” “Pruning,” “Quantization,” “Representation Learning.”
    - **Clarity**: Clear articulation of problem statements and methods.
    - **Impact**: High potential to influence research or address critical challenges.
    - **Innovation**: Introduction of new architectures, algorithms, or theoretical insights.

3. **Exclusions**:
    - Most of the time, only less then 5% of the papers should be considered preserved, and all others should be filtered.
    - Avoid papers without sufficient experimental validation or reproducibility.
    - Exclude works solely focused on specific applications unless they propose broadly applicable techniques.
