{
    "2505.13447": {
        "COMMENT": "Author match",
        "SCORE": 20.0,
        "authors": [
            "Zhengyang Geng",
            "Mingyang Deng",
            "Xingjian Bai",
            "J. Zico Kolter",
            "Kaiming He"
        ],
        "title": "Mean Flows for One-step Generative Modeling",
        "abstract": "We propose a principled and effective framework for one-step generative modeling. We introduce the notion of average velocity to characterize flow fields, in contrast to instantaneous velocity modeled by Flow Matching methods. A well-defined identity between average and instantaneous velocities is derived and used to guide neural network training. Our method, termed the MeanFlow model, is self-contained and requires no pre-training, distillation, or curriculum learning. MeanFlow demonstrates strong empirical performance: it achieves an FID of 3.43 with a single function evaluation (1-NFE) on ImageNet 256x256 trained from scratch, significantly outperforming previous state-of-the-art one-step diffusion/flow models. Our study substantially narrows the gap between one-step diffusion/flow models and their multi-step predecessors, and we hope it will motivate future research to revisit the foundations of these powerful models.",
        "arxiv_id": "2505.13447"
    },
    "2505.11824": {
        "COMMENT": "Author match",
        "SCORE": 20.0,
        "authors": [
            "Minsu Kim",
            "Jean-Pierre Falet",
            "Oliver E. Richardson",
            "Xiaoyin Chen",
            "Moksh Jain",
            "Sungjin Ahn",
            "Sungsoo Ahn",
            "Yoshua Bengio"
        ],
        "title": "Search-Based Correction of Reasoning Chains for Language Models",
        "abstract": "Chain-of-Thought (CoT) reasoning has advanced the capabilities and transparency of language models (LMs); however, reasoning chains can contain inaccurate statements that reduce performance and trustworthiness. To address this, we introduce a new self-correction framework that augments each reasoning step in a CoT with a latent variable indicating its veracity, enabling modeling of all possible truth assignments rather than assuming correctness throughout. To efficiently explore this expanded space, we introduce Search Corrector, a discrete search algorithm over boolean-valued veracity assignments. It efficiently performs otherwise intractable inference in the posterior distribution over veracity assignments by leveraging the LM's joint likelihood over veracity and the final answer as a proxy reward. This efficient inference-time correction method facilitates supervised fine-tuning of an Amortized Corrector by providing pseudo-labels for veracity. The Amortized Corrector generalizes self-correction, enabling accurate zero-shot veracity inference in novel contexts. Empirical results demonstrate that Search Corrector reliably identifies errors in logical (ProntoQA) and mathematical reasoning (GSM8K) benchmarks. The Amortized Corrector achieves comparable zero-shot accuracy and improves final answer accuracy by up to 25%.",
        "arxiv_id": "2505.11824"
    }
}