{
    "2509.19161": {
        "SCORE": 19,
        "ARXIVID": "2509.19161",
        "COMMENT": "Model Architecture/Theory: introduces new circuit complexity classes capturing physical constraints and derives scaling limits for attention-based Transformers.",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "Benjamin Prada",
            "Ankur Mali"
        ],
        "title": "Circuit Complexity From Physical Constraints: Scaling Limitations of Attention",
        "abstract": "We argue that the standard circuit complexity measures derived from $NC, AC, TC$ provide limited practical information and are now insufficient to further differentiate model expressivity. To address these new limitations, we define a novel notion of local uniformity and a family of circuit complexity classes $RC(\\cdot)$ that capture the fundamental constraints of scaling physical circuits. Through the lens of $RC(\\cdot)$, we show that attention mechanisms with $\\omega(n^{3/2})$ runtime cannot scale to accommodate the entropy of increasingly complex datasets. Our results simultaneously provide a methodology for defining meaningful bounds on transformer expressivity and naturally expose the restricted viability of attention.",
        "arxiv_id": "2509.19161"
    },
    "2509.17765": {
        "SCORE": 18,
        "ARXIVID": "2509.17765",
        "COMMENT": "Model Architecture: MoE (Thinker\u2013Talker) unifying text/image/audio/video; Efficiency: low-latency streaming replacing diffusion with causal ConvNet and codebook prediction.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Jin Xu",
            "Zhifang Guo",
            "Hangrui Hu",
            "Yunfei Chu",
            "Xiong Wang",
            "Jinzheng He",
            "Yuxuan Wang",
            "Xian Shi",
            "Ting He",
            "Xinfa Zhu",
            "Yuanjun Lv",
            "Yongqi Wang",
            "Dake Guo",
            "He Wang",
            "Linhan Ma",
            "Pei Zhang",
            "Xinyu Zhang",
            "Hongkun Hao",
            "Zishan Guo",
            "Baosong Yang",
            "Bin Zhang",
            "Ziyang Ma",
            "Xipin Wei",
            "Shuai Bai",
            "Keqin Chen",
            "Xuejing Liu",
            "Peng Wang",
            "Mingkun Yang",
            "Dayiheng Liu",
            "Xingzhang Ren",
            "Bo Zheng",
            "Rui Men",
            "Fan Zhou",
            "Bowen Yu",
            "Jianxin Yang",
            "Le Yu",
            "Jingren Zhou",
            "Junyang Lin"
        ],
        "title": "Qwen3-Omni Technical Report",
        "abstract": "We present Qwen3-Omni, a single multimodal model that, for the first time, maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages, and speech generation in 10 languages. To reduce first-packet latency in streaming synthesis, Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms. To further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license.",
        "arxiv_id": "2509.17765"
    },
    "2509.17238": {
        "SCORE": 18,
        "ARXIVID": "2509.17238",
        "COMMENT": "MoE inference-time method: training-free hyper-parallel token-level scaling (RoE) with efficient batching and specialized KV cache for better accuracy\u2013compute tradeoffs.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Soheil Zibakhsh",
            "Mohammad Samragh",
            "Kumari Nishu",
            "Lauren Hannah",
            "Arnav Kundu",
            "Minsik Cho"
        ],
        "title": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with RoE",
        "abstract": "The generation quality of large language models (LLMs) is often improved by utilizing inference-time sequence-level scaling methods (e.g., Chain-of-Thought). We introduce hyper-parallel scaling, a complementary framework that improves prediction quality at the token level. Hyper-parallel scaling computes and aggregates multiple output proposals for a single token from the model. We implement this concept in Mixture-of-Experts (MoE) models, which we refer to as Roster of Experts (RoE). RoE is a training-free inference algorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects controlled stochasticity into the expert routing mechanism, enabling it to sample multiple diverse experts for each token and aggregate their outputs for a more accurate final prediction.To overcome the computational cost, we introduce an efficient batching strategy and a specialized KV-caching mechanism that minimizes compute and memory overhead. For example, RoE enables a 7B MoE model to match the performance of a 10.5B MoE model while using 30% less compute for inference. These gains are achieved without any fine-tuning of model parameters.",
        "arxiv_id": "2509.17238"
    },
    "2509.18993": {
        "SCORE": 18,
        "ARXIVID": "2509.18993",
        "COMMENT": "Cross-layer low-rank residual architecture with activation recomputation for memory/compute efficiency (Compression/Efficiency; Low-rank; Systems-level memory optimization).",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Boao Kong",
            "Junzhu Liang",
            "Yuxi Liu",
            "Renjia Deng",
            "Kun Yuan"
        ],
        "title": "CR-Net: Scaling Parameter-Efficient Training with Cross-Layer Low-Rank Structure",
        "abstract": "Low-rank architectures have become increasingly important for efficient large language model (LLM) pre-training, providing substantial reductions in both parameter complexity and memory/computational demands. Despite these advantages, current low-rank methods face three critical shortcomings: (1) compromised model performance, (2) considerable computational overhead, and (3) limited activation memory savings. To address these limitations, we propose Cross-layer Low-Rank residual Network (CR-Net), an innovative parameter-efficient framework inspired by our discovery that inter-layer activation residuals possess low-rank properties. CR-Net implements this insight through a dual-path architecture that efficiently reconstructs layer activations by combining previous-layer outputs with their low-rank differences, thereby maintaining high-rank information with minimal parameters. We further develop a specialized activation recomputation strategy tailored for CR-Net that dramatically reduces memory requirements. Extensive pre-training experiments across model scales from 60M to 7B parameters demonstrate that CR-Net consistently outperforms state-of-the-art low-rank frameworks while requiring fewer computational resources and less memory.",
        "arxiv_id": "2509.18993"
    },
    "2509.16989": {
        "SCORE": 18,
        "ARXIVID": "2509.16989",
        "COMMENT": "Model Compression and Efficiency: ternary trit-plane PTQ enabling multiplication-free inference with a progressive approximation guaranteeing global weight consistency.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "He Xiao",
            "Runming Yang",
            "Qingyao Yang",
            "Wendong Xu",
            "Zheng Li",
            "Yupeng Su",
            "Zhengwu Liu",
            "Hongxia Yang",
            "Ngai Wong"
        ],
        "title": "PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models",
        "abstract": "Post-training quantization (PTQ) of large language models (LLMs) to extremely low bit-widths remains challenging due to the fundamental trade-off between computational efficiency and model expressiveness. While existing ultra-low-bit PTQ methods rely on binary approximations or complex compensation mechanisms, they suffer from either limited representational capacity or computational overhead that undermines their efficiency gains. We introduce PTQ to Trit-Planes (PTQTP), the first ternary-weight PTQ framework that decomposes weight matrices into structured ternary {-1, 0, 1} trit-planes using 2x1.58-bit representation. PTQTP achieves multiplication-free inference, identical to 1-bit quantization, while maintaining superior expressiveness through its novel structured decomposition. Our approach provides: (1) a theoretically grounded progressive approximation algorithm ensuring global weight consistency; (2) model-agnostic deployment across diverse modern LLMs without architectural modifications; and (3) uniform ternary operations that eliminate the need for mixed-precision or compensation schemes. Comprehensive experiments across LLaMA3.x and Qwen3 model families (0.6B-70B parameters) demonstrate that PTQTP significantly outperforms existing low-bit PTQ methods, achieving 82.4% mathematical reasoning retention versus 0% for competing approaches. PTQTP approaches and sometimes surpasses 1.58-bit quantization-aware training performance while requiring only single-hour quantization compared to 10-14 GPU days for training-based methods. These results establish PTQTP as a practical solution for efficient LLM deployment in resource-constrained environments.",
        "arxiv_id": "2509.16989"
    },
    "2509.18542": {
        "SCORE": 18,
        "ARXIVID": "2509.18542",
        "COMMENT": "Model Architecture (MoE): builds a coherent MoE from disparate pretrained models via training-free functional alignment and router learning.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Qi Wang",
            "Hanyang Peng",
            "Yue Yu"
        ],
        "title": "Symphony-MoE: Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts",
        "abstract": "Mixture-of-Experts (MoE) models enable scalable performance by activating large parameter sets sparsely, minimizing computational overhead. To circumvent the prohibitive cost of training MoEs from scratch, recent work employs upcycling, reusing a single pre-trained dense model by replicating its feed-forward network (FFN) layers into experts. However, this limits expert diversity, as all experts originate from a single pre-trained dense model. This paper addresses this limitation by constructing powerful MoE models using experts sourced from multiple identically-architected but disparate pre-trained models (e.g., Llama2-Chat and Code Llama). A key challenge lies in the fact that these source models occupy disparate, dissonant regions of the parameter space, making direct upcycling prone to severe performance degradation. To overcome this, we propose Symphony-MoE, a novel two-stage framework designed to harmonize these models into a single, coherent expert mixture. First, we establish this harmony in a training-free manner: we construct a shared backbone via a layer-aware fusion strategy and, crucially, alleviate parameter misalignment among experts using activation-based functional alignment. Subsequently, a single lightweight stage of router training coordinates the entire architecture. Experiments demonstrate that our method successfully integrates experts from heterogeneous sources, achieving an MoE model that significantly surpasses baselines in multi-domain tasks and out-of-distribution generalization.",
        "arxiv_id": "2509.18542"
    },
    "2509.18169": {
        "SCORE": 18,
        "ARXIVID": "2509.18169",
        "COMMENT": "Model Architecture (MoE): token-level routing in a physically-isolated Mixture-of-Experts integrating computation modules with reasoning within a single chain of thought.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Hengbo Xiao",
            "Jingyuan Fan",
            "Xin Tong",
            "Jingzhao Zhang",
            "Chao Lu",
            "Guannan He"
        ],
        "title": "PiMoE: Token-Level Routing for Integrating High-Precision Computation and Reasoning",
        "abstract": "Complex systems typically rely on high-precision numerical computation to support decisions, but current large language models (LLMs) cannot yet incorporate such computations as an intrinsic and interpretable capability with existing architectures. Mainstream multi-agent approaches can leverage external experts, but inevitably introduce communication overhead and suffer from inefficient multimodal emergent capability and limited scalability. To this end, we propose PiMoE (Physically-isolated Mixture of Experts), a training and inference architecture for integrating computation and reasoning. Instead of the workflow paradigm of tool invocation, PiMoE endogenously integrates computational capabilities into neural networks after separately training experts, a text-to-computation module, and a router. At inference, the router directs computation and reasoning at the token level, thereby enabling iterative alternation within a single chain of thought. We evaluate PiMoE on two reasoning-computation tasks against LLM finetuning and the multi-agent system approaches. Results show that the PiMoE architecture achieves not only higher accuracy than directly finetuning LLMs but also significant improvements in response latency, token usage, and GPU energy consumption compared with mainstream multi-agent approaches. PiMoE offers an efficient, interpretable, and scalable paradigm for next-generation scientific or industrial intelligent systems.",
        "arxiv_id": "2509.18169"
    },
    "2509.18990": {
        "SCORE": 18,
        "ARXIVID": "2509.18990",
        "COMMENT": "Representation Learning: foundational theory showing SGNNs perform amortized Bayesian inference under a simulation prior, with generalization bounds and mechanistic interpretability.",
        "RELEVANCE": 9,
        "NOVELTY": 9,
        "authors": [
            "Carson Dudley",
            "Marisa Eisenberg"
        ],
        "title": "Learning From Simulators: A Theory of Simulation-Grounded Learning",
        "abstract": "Simulation-Grounded Neural Networks (SGNNs) are predictive models trained entirely on synthetic data from mechanistic simulations. They have achieved state-of-the-art performance in domains where real-world labels are limited or unobserved, but lack a formal underpinning.   We present the foundational theory of simulation-grounded learning. We show that SGNNs implement amortized Bayesian inference under a simulation prior and converge to the Bayes-optimal predictor. We derive generalization bounds under model misspecification and prove that SGNNs can learn unobservable scientific quantities that empirical methods provably cannot. We also formalize a novel form of mechanistic interpretability uniquely enabled by SGNNs: by attributing predictions to the simulated mechanisms that generated them, SGNNs yield posterior-consistent, scientifically grounded explanations.   We provide numerical experiments to validate all theoretical predictions. SGNNs recover latent parameters, remain robust under mismatch, and outperform classical tools: in a model selection task, SGNNs achieve half the error of AIC in distinguishing mechanistic dynamics. These results establish SGNNs as a principled and practical framework for scientific prediction in data-limited regimes.",
        "arxiv_id": "2509.18990"
    },
    "2509.16882": {
        "SCORE": 17,
        "ARXIVID": "2509.16882",
        "COMMENT": "Model Architecture: MoE adaptation with dynamic expert specialization and router distillation to avoid catastrophic forgetting; training schedule isolates domain-specific gradients.",
        "RELEVANCE": 10,
        "NOVELTY": 7,
        "authors": [
            "Junzhuo Li",
            "Bo Wang",
            "Xiuze Zhou",
            "Xuming Hu"
        ],
        "title": "Dynamic Expert Specialization: Towards Catastrophic Forgetting-Free Multi-Domain MoE Adaptation",
        "abstract": "Mixture-of-Experts (MoE) models offer immense capacity via sparsely gated expert subnetworks, yet adapting them to multiple domains without catastrophic forgetting remains an open challenge. Existing approaches either incur prohibitive computation, suffer cross-domain interference, or require separate runs per domain. We propose DES-MoE, a dynamic expert specialization framework for multi-domain adaptation of Mixture-of-Experts models. DES-MoE addresses catastrophic forgetting through three innovations: (1) an adaptive router balancing pre-trained knowledge retention and task-specific updates via distillation, (2) real-time expert-domain correlation mapping to isolate domain-specific gradients, and (3) a three-phase adaptive fine-tuning schedule that progressively freezes non-specialized parameters. Evaluated on six domains (math, code, law, etc.), DES-MoE matches single-domain ESFT performance while training one unified model, reduces forgetting by 89% compared to full fine-tuning as domains scale from 2 to 6, and achieves 68% faster convergence than conventional methods. Our work establishes dynamic expert isolation as a scalable paradigm for multi-task MoE adaptation.",
        "arxiv_id": "2509.16882"
    },
    "2509.17786": {
        "SCORE": 17,
        "ARXIVID": "2509.17786",
        "COMMENT": "Model Compression/Efficiency: low-rank (LoRA) model merging via a shared Core Space with proof of no information loss and complexity analysis.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Aniello Panariello",
            "Daniel Marczak",
            "Simone Magistri",
            "Angelo Porrello",
            "Bart{\\l}omiej Twardowski",
            "Andrew D. Bagdanov",
            "Simone Calderara",
            "Joost van de Weijer"
        ],
        "title": "Accurate and Efficient Low-Rank Model Merging in Core Space",
        "abstract": "In this paper, we address the challenges associated with merging low-rank adaptations of large neural networks. With the rise of parameter-efficient adaptation techniques, such as Low-Rank Adaptation (LoRA), model fine-tuning has become more accessible. While fine-tuning models with LoRA is highly efficient, existing merging methods often sacrifice this efficiency by merging fully-sized weight matrices. We propose the Core Space merging framework, which enables the merging of LoRA-adapted models within a common alignment basis, thereby preserving the efficiency of low-rank adaptation while substantially improving accuracy across tasks. We further provide a formal proof that projection into Core Space ensures no loss of information and provide a complexity analysis showing the efficiency gains. Extensive empirical results demonstrate that Core Space significantly improves existing merging techniques and achieves state-of-the-art results on both vision and language tasks while utilizing a fraction of the computational resources. Codebase is available at https://github.com/apanariello4/core-space-merging.",
        "arxiv_id": "2509.17786"
    },
    "2509.18552": {
        "SCORE": 17,
        "ARXIVID": "2509.18552",
        "COMMENT": "Representation Learning/Theory: analyzes sigmoid contrastive loss with trainable temperature/bias (SigLIP), introduces constellation characterization and reparameterization.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Kiril Bangachev",
            "Guy Bresler",
            "Iliyas Noman",
            "Yury Polyanskiy"
        ],
        "title": "Global Minimizers of Sigmoid Contrastive Loss",
        "abstract": "The meta-task of obtaining and aligning representations through contrastive pretraining is steadily gaining importance since its introduction in CLIP and ALIGN. In this paper we theoretically explain the advantages of synchronizing with trainable inverse temperature and bias under the sigmoid loss, as implemented in the recent SigLIP and SigLIP2 models of Google DeepMind. Temperature and bias can drive the loss function to zero for a rich class of configurations that we call $(\\mathsf{m}, \\mathsf{b}_{\\mathsf{rel}})$-Constellations. $(\\mathsf{m}, \\mathsf{b}_{\\mathsf{rel}})$-Constellations are a novel combinatorial object related to spherical codes and are parametrized by a margin $\\mathsf{m}$ and relative bias $\\mathsf{b}_{\\mathsf{rel}}$. We use our characterization of constellations to theoretically justify the success of SigLIP on retrieval, to explain the modality gap present in SigLIP, and to identify the necessary dimension for producing high-quality representations. Finally, we propose a reparameterization of the sigmoid loss with explicit relative bias, which improves training dynamics in experiments with synthetic data.",
        "arxiv_id": "2509.18552"
    },
    "2509.17196": {
        "SCORE": 17,
        "ARXIVID": "2509.17196",
        "COMMENT": "Representation Learning: analyzes training dynamics via sparse dictionary crosscoders to track interpretable features through pre-training.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Xuyang Ge",
            "Wentao Shu",
            "Jiaxing Wu",
            "Yunhua Zhou",
            "Zhengfu He",
            "Xipeng Qiu"
        ],
        "title": "Evolution of Concepts in Language Model Pre-Training",
        "abstract": "Language models obtain extensive capabilities through pre-training. However, the pre-training process remains a black box. In this work, we track linear interpretable feature evolution across pre-training snapshots using a sparse dictionary learning method called crosscoders. We find that most features begin to form around a specific point, while more complex patterns emerge in later training stages. Feature attribution analyses reveal causal connections between feature evolution and downstream performance. Our feature-level observations are highly consistent with previous findings on Transformer's two-stage learning process, which we term a statistical learning phase and a feature learning phase. Our work opens up the possibility to track fine-grained representation progress during language model learning dynamics.",
        "arxiv_id": "2509.17196"
    },
    "2509.18216": {
        "SCORE": 17,
        "ARXIVID": "2509.18216",
        "COMMENT": "Proposes intrinsic latent-geometry metrics (spectral curvature, thermodynamic length, belief vector field) to fingerprint models (Representation Learning theory).",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Amitava Das"
        ],
        "title": "nDNA -- the Semantic Helix of Artificial Cognition",
        "abstract": "As AI foundation models grow in capability, a deeper question emerges: What shapes their internal cognitive identity -- beyond fluency and output? Benchmarks measure behavior, but the soul of a model resides in its latent geometry. In this work, we propose Neural DNA (nDNA) as a semantic-genotypic representation that captures this latent identity through the intrinsic geometry of belief. At its core, nDNA is synthesized from three principled and indispensable dimensions of latent geometry: spectral curvature, which reveals the curvature of conceptual flow across layers; thermodynamic length, which quantifies the semantic effort required to traverse representational transitions through layers; and belief vector field, which delineates the semantic torsion fields that guide a model's belief directional orientations. Like biological DNA, it encodes ancestry, mutation, and semantic inheritance, found in finetuning and alignment scars, cultural imprints, and architectural drift. In naming it, we open a new field: Neural Genomics, where models are not just tools, but digital semantic organisms with traceable inner cognition.   Modeling statement. We read AI foundation models as semantic fluid--dynamics: meaning is transported through layers like fluid in a shaped conduit; nDNA is the physics-grade readout of that flow -- a geometry-first measure of how meaning is bent, paid for, and pushed -- yielding a stable, coordinate-free neural DNA fingerprint tied to on-input behavior; with this fingerprint we cross into biology: tracing lineages across pretraining, fine-tuning, alignment, pruning, distillation, and merges; measuring inheritance between checkpoints; detecting drift as traits shift under new data or objectives; and, ultimately, studying the evolution of artificial cognition to compare models, diagnose risks, and govern change over time.",
        "arxiv_id": "2509.18216"
    },
    "2509.18968": {
        "SCORE": 17,
        "ARXIVID": "2509.18968",
        "COMMENT": "Model Architecture and Efficiency: spiking Transformer with optical TTFS encoding and QNN-to-SNN conversion for energy-efficient inference.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Zhanglu Yan",
            "Jiayi Mao",
            "Qianhui Liu",
            "Fanfan Li",
            "Gang Pan",
            "Tao Luo",
            "Bowen Zhu",
            "Weng-Fai Wong"
        ],
        "title": "Otters: An Energy-Efficient SpikingTransformer via Optical Time-to-First-Spike Encoding",
        "abstract": "Spiking neural networks (SNNs) promise high energy efficiency, particularly with time-to-first-spike (TTFS) encoding, which maximizes sparsity by emitting at most one spike per neuron. However, such energy advantage is often unrealized because inference requires evaluating a temporal decay function and subsequent multiplication with the synaptic weights. This paper challenges this costly approach by repurposing a physical hardware `bug', namely, the natural signal decay in optoelectronic devices, as the core computation of TTFS. We fabricated a custom indium oxide optoelectronic synapse, showing how its natural physical decay directly implements the required temporal function. By treating the device's analog output as the fused product of the synaptic weight and temporal decay, optoelectronic synaptic TTFS (named Otters) eliminates these expensive digital operations. To use the Otters paradigm in complex architectures like the transformer, which are challenging to train directly due to the sparsity issue, we introduce a novel quantized neural network-to-SNN conversion algorithm. This complete hardware-software co-design enables our model to achieve state-of-the-art accuracy across seven GLUE benchmark datasets and demonstrates a 1.77$\\times$ improvement in energy efficiency over previous leading SNNs, based on a comprehensive analysis of compute, data movement, and memory access costs using energy measurements from a commercial 22nm process. Our work thus establishes a new paradigm for energy-efficient SNNs, translating fundamental device physics directly into powerful computational primitives. All codes and data are open source.",
        "arxiv_id": "2509.18968"
    },
    "2509.18362": {
        "SCORE": 17,
        "ARXIVID": "2509.18362",
        "COMMENT": "Model Compression and Efficiency/HPC: accelerates LLM inference via enhanced multi-token prediction aligned with inference and dynamic vocabulary compression for speculative decoding.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Yuxuan Cai",
            "Xiaozhuan Liang",
            "Xinghua Wang",
            "Jin Ma",
            "Haijin Liang",
            "Jinwen Luo",
            "Xinyu Zuo",
            "Lisheng Duan",
            "Yuyang Yin",
            "Xi Chen"
        ],
        "title": "FastMTP: Accelerating LLM Inference with Enhanced Multi-Token Prediction",
        "abstract": "As large language models (LLMs) become increasingly powerful, the sequential nature of autoregressive generation creates a fundamental throughput bottleneck that limits the practical deployment. While Multi-Token Prediction (MTP) has demonstrated remarkable benefits for model training efficiency and performance, its inherent potential for inference acceleration remains largely unexplored. This paper introduces FastMTP, a simple yet effective method that improves multi-step draft quality by aligning MTP training with its inference pattern, significantly enhancing speculative decoding performance. Our approach fine-tunes a single MTP head with position-shared weights on self-distilled data, enabling it to capture dependencies among consecutive future tokens and maintain high acceptance rates across multiple recursive draft steps. By integrating language-aware dynamic vocabulary compression into the MTP head, we further reduce computational overhead in the drafting process. Experimental results across seven diverse benchmarks demonstrate that FastMTP achieves an average of 2.03x speedup compared to standard next token prediction with lossless output quality, outperforming vanilla MTP by 82%. FastMTP requires only lightweight training and seamlessly integrates with existing inference frameworks, offering a practical and rapidly deployable solution for accelerating LLM inference.",
        "arxiv_id": "2509.18362"
    },
    "2509.18629": {
        "SCORE": 17,
        "ARXIVID": "2509.18629",
        "COMMENT": "Model compression and efficiency (PEFT): row/column-wise diagonal scaling inducing high-rank updates with only O(n+m) trainable parameters per matrix.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Abel Gurung",
            "Joseph Campbell"
        ],
        "title": "HyperAdapt: Simple High-Rank Adaptation",
        "abstract": "Foundation models excel across diverse tasks, but adapting them to specialized applications often requires fine-tuning, an approach that is memory and compute-intensive. Parameter-efficient fine-tuning (PEFT) methods mitigate this by updating only a small subset of weights. In this paper, we introduce HyperAdapt, a parameter-efficient fine-tuning method that significantly reduces the number of trainable parameters compared to state-of-the-art methods like LoRA. Specifically, HyperAdapt adapts a pre-trained weight matrix by applying row- and column-wise scaling through diagonal matrices, thereby inducing a high-rank update while requiring only $n+m$ trainable parameters for an $n \\times m$ matrix. Theoretically, we establish an upper bound on the rank of HyperAdapt's updates, and empirically, we confirm that it consistently induces high-rank transformations across model layers. Experiments on GLUE, arithmetic reasoning, and commonsense reasoning benchmarks with models up to 14B parameters demonstrate that HyperAdapt matches or nearly matches the performance of full fine-tuning and state-of-the-art PEFT methods while using orders of magnitude fewer trainable parameters.",
        "arxiv_id": "2509.18629"
    },
    "2509.18744": {
        "SCORE": 17,
        "ARXIVID": "2509.18744",
        "COMMENT": "Model architecture: introduces periodic CNNs and proves sharp approximation properties, characterizing expressive power.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Yuqing Liu"
        ],
        "title": "Theory of periodic convolutional neural network",
        "abstract": "We introduce a novel convolutional neural network architecture, termed the \\emph{periodic CNN}, which incorporates periodic boundary conditions into the convolutional layers. Our main theoretical contribution is a rigorous approximation theorem: periodic CNNs can approximate ridge functions depending on $d-1$ linear variables in a $d$-dimensional input space, while such approximation is impossible in lower-dimensional ridge settings ($d-2$ or fewer variables). This result establishes a sharp characterization of the expressive power of periodic CNNs. Beyond the theory, our findings suggest that periodic CNNs are particularly well-suited for problems where data naturally admits a ridge-like structure of high intrinsic dimension, such as image analysis on wrapped domains, physics-informed learning, and materials science. The work thus both expands the mathematical foundation of CNN approximation theory and highlights a class of architectures with surprising and practically relevant approximation capabilities.",
        "arxiv_id": "2509.18744"
    },
    "2509.16857": {
        "SCORE": 17,
        "ARXIVID": "2509.16857",
        "COMMENT": "HPC/systems: SmartNIC-offloaded, interference-free KV cache fetching for distributed prefix caching; pipeline and memory designs for serving efficiency.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Xingyu Xiang",
            "Raj Joshi",
            "Yuhan Liu",
            "Jiayi Yao",
            "Chenxingyu Zhao",
            "Junchen Jiang",
            "Yang Zhou",
            "Eddie Kohler",
            "Minlan Yu"
        ],
        "title": "ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix Caching",
        "abstract": "Distributed prefix caching accelerates long-context LLM serving by reusing KV cache entries for common context prefixes. However, KV cache fetches can become a bottleneck when network bandwidth is limited. Compression mitigates the bandwidth issue, but can degrade overall performance when decompression interferes with model computation.   We present ShadowServe, the first SmartNIC-accelerated, interference-free prefix caching system for LLM serving. ShadowServe separates a control plane on the host and a data plane fully offloaded to the SmartNIC, which eliminates interference to both host GPU and CPU. To overcome the SmartNIC's limited compute and memory resources, we design a chunked pipeline that parallelizes data plane operations across the SmartNIC's compute resources, and a minimal-copy memory management scheme that reduces memory pressure on the SmartNIC. Compared to state-of-the-art solutions, ShadowServe achieves up to 2.2x lower loaded time-per-output-token (TPOT), and reduces time-to-first-token (TTFT) by up to 1.38x in low-bandwidth scenarios (<= 20 Gbps), translating to up to 1.35x higher throughput.",
        "arxiv_id": "2509.16857"
    },
    "2509.18172": {
        "SCORE": 17,
        "ARXIVID": "2509.18172",
        "COMMENT": "Strong match to Model Compression and Efficiency: non-uniform PTQ with hardware-friendly codes and custom CUDA enabling compute in quantized domain.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Wonjun Bang",
            "Jongseok Park",
            "Hongseung Yu",
            "Kyungmin Bin",
            "Kyunghan Lee"
        ],
        "title": "SBVR: Summation of BitVector Representation for Efficient LLM Quantization",
        "abstract": "With the advent of large language models (LLMs), numerous Post-Training Quantization (PTQ) strategies have been proposed to alleviate deployment barriers created by their enormous parameter counts. Quantization achieves compression by limiting the number of representable points in the data. Therefore, the key to achieving efficient quantization is selecting the optimal combination of representation points, or codes, for the given data. Existing PTQ solutions adopt two major approaches to this problem: Round-To-Nearest (RTN)-based methods and codebook-based methods. RTN-based methods map LLM weights onto uniformly distributed integer grids, failing to account for the Gaussian-like weight distribution of LLM weights. Codebook-based methods mitigate this issue by constructing distribution-aware codebooks; however, they suffer from random and strided memory access patterns, resulting in degraded inference speed that is exacerbated by the limited size of GPU L1 cache. To overcome these limitations, we propose a novel LLM quantization method, SBVR (Summation of BitVector Representation), that enables Gaussian-like code representation in a hardware-friendly manner for fast inference. SBVR maps weight values to non-uniform representation points whose distribution follows the actual distribution of LLM weights, enabling more accurate compression. Additionally, we design a custom CUDA kernel that allows matrix-vector multiplication directly in the SBVR format without decompression, thereby enabling high-performance execution of SBVR-compressed models. Our evaluations of SBVR on various models demonstrate state-of-the-art perplexity and accuracy benchmark performance while delivering a 2.21x- 3.04x end-to-end token-generation speedup over naive FP16 models in the 4-bit quantization regime.",
        "arxiv_id": "2509.18172"
    },
    "2509.16825": {
        "SCORE": 17,
        "ARXIVID": "2509.16825",
        "COMMENT": "Model Architecture: introduces a new neural operator (KANO) combining spectral and spatial bases with theoretical expressivity gains over FNO.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Jin Lee",
            "Ziming Liu",
            "Xinling Yu",
            "Yixuan Wang",
            "Haewon Jeong",
            "Murphy Yuezhen Niu",
            "Zheng Zhang"
        ],
        "title": "KANO: Kolmogorov-Arnold Neural Operator",
        "abstract": "We introduce Kolmogorov--Arnold Neural Operator (KANO), a dual-domain neural operator jointly parameterized by both spectral and spatial bases with intrinsic symbolic interpretability. We theoretically demonstrate that KANO overcomes the pure-spectral bottleneck of Fourier Neural Operator (FNO): KANO remains expressive over generic position-dependent dynamics for any physical input, whereas FNO stays practical only for spectrally sparse operators and strictly imposes a fast-decaying input Fourier tail. We verify our claims empirically on position-dependent differential operators, for which KANO robustly generalizes but FNO fails to. In the quantum Hamiltonian learning benchmark, KANO reconstructs ground-truth Hamiltonians in closed-form symbolic representations accurate to the fourth decimal place in coefficients and attains $\\approx 6\\times10^{-6}$ state infidelity from projective measurement data, substantially outperforming that of the FNO trained with ideal full wave function data, $\\approx 1.5\\times10^{-2}$, by orders of magnitude.",
        "arxiv_id": "2509.16825"
    },
    "2509.17866": {
        "SCORE": 17,
        "ARXIVID": "2509.17866",
        "COMMENT": "Representation Learning/Training Dynamics: SVD-based analysis reveals uniform singular value scaling and coordinated singular-vector rotations after post-training.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Xinyu He",
            "Xianghui Cao"
        ],
        "title": "Understanding Post-Training Structural Changes in Large Language Models",
        "abstract": "Post-training fundamentally alters the behavior of large language models (LLMs), yet its impact on the internal parameter space remains poorly understood. In this work, we conduct a systematic singular value decomposition (SVD) analysis of principal linear layers in pretrained LLMs, focusing on two widely adopted post-training methods: instruction tuning and long-chain-of-thought (Long-CoT) distillation. Our analysis reveals two consistent and unexpected structural changes:(1) a near-uniform geometric scaling of singular values across layers, which theoretically modulates attention scores; and (2) highly consistent orthogonal transformations are applied to the left and right singular vectors of each matrix. Disrupting this orthogonal consistency leads to catastrophic performance degradation. Based on these findings, we propose a simple yet effective framework that interprets post-training as a reparameterization of fixed subspaces in the pretrained parameter space. Further experiments reveal that singular value scaling behaves as a secondary effect, analogous to a temperature adjustment, whereas the core functional transformation lies in the coordinated rotation of singular vectors. These results challenge the prevailing view of the parameter space in large models as a black box, uncovering the first clear regularities in how parameters evolve during training, and providing a new perspective for deeper investigation into model parameter changes.",
        "arxiv_id": "2509.17866"
    },
    "2509.18107": {
        "SCORE": 16,
        "ARXIVID": "2509.18107",
        "COMMENT": "Model Architecture (MoE): introduces an adaptive weighted mixture of multi-scale expert Transformers with a gating network for time series forecasting.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Huanyao Zhang",
            "Jiaye Lin",
            "Wentao Zhang",
            "Haitao Yuan",
            "Guoliang Li"
        ],
        "title": "AdaMixT: Adaptive Weighted Mixture of Multi-Scale Expert Transformers for Time Series Forecasting",
        "abstract": "Multivariate time series forecasting involves predicting future values based on historical observations. However, existing approaches primarily rely on predefined single-scale patches or lack effective mechanisms for multi-scale feature fusion. These limitations hinder them from fully capturing the complex patterns inherent in time series, leading to constrained performance and insufficient generalizability. To address these challenges, we propose a novel architecture named Adaptive Weighted Mixture of Multi-Scale Expert Transformers (AdaMixT). Specifically, AdaMixT introduces various patches and leverages both General Pre-trained Models (GPM) and Domain-specific Models (DSM) for multi-scale feature extraction. To accommodate the heterogeneity of temporal features, AdaMixT incorporates a gating network that dynamically allocates weights among different experts, enabling more accurate predictions through adaptive multi-scale fusion. Comprehensive experiments on eight widely used benchmarks, including Weather, Traffic, Electricity, ILI, and four ETT datasets, consistently demonstrate the effectiveness of AdaMixT in real-world scenarios.",
        "arxiv_id": "2509.18107"
    },
    "2509.18133": {
        "SCORE": 16,
        "ARXIVID": "2509.18133",
        "COMMENT": "Mixture-of-Experts design with dual LoRA experts and an adversarial discriminator for continual instruction tuning\u2014parameter-efficient architecture (Model Architecture; Efficiency).",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Le Huang",
            "Jiazheng Kang",
            "Cheng Hou",
            "Zhe Zhao",
            "Zhenxiang Yan",
            "Chuan Shi",
            "Ting Bai"
        ],
        "title": "Self-Evolving LLMs via Continual Instruction Tuning",
        "abstract": "In real-world industrial settings, large language models (LLMs) must learn continually to keep pace with diverse and evolving tasks, requiring self-evolution to refine knowledge under dynamic data distributions. However, existing continual learning (CL) approaches, such as replay and parameter isolation, often suffer from catastrophic forgetting: training on new tasks degrades performance on earlier ones by overfitting to the new distribution and weakening generalization.We propose MoE-CL, a parameter-efficient adversarial mixture-of-experts framework for industrial-scale, self-evolving continual instruction tuning of LLMs. MoE-CL uses a dual-expert design: (1) a dedicated LoRA expert per task to preserve task-specific knowledge via parameter independence, mitigating forgetting; and (2) a shared LoRA expert to enable cross-task transfer. To prevent transferring task-irrelevant noise through the shared pathway, we integrate a task-aware discriminator within a GAN. The discriminator encourages the shared expert to pass only task-aligned information during sequential training. Through adversarial learning, the shared expert acquires generalized representations that mimic the discriminator, while dedicated experts retain task-specific details, balancing knowledge retention and cross-task generalization and thereby supporting self-evolution.Extensive experiments on the public MTL5 benchmark and an industrial Tencent3 benchmark validate the effectiveness of MoE-CL for continual instruction tuning. In real-world A/B testing for content compliance review on the Tencent Video platform, MoE-CL reduced manual review costs by 15.3%. These results demonstrate that MoE-CL is practical for large-scale industrial deployment where continual adaptation and stable transfer are critical.",
        "arxiv_id": "2509.18133"
    },
    "2509.17000": {
        "SCORE": 16,
        "ARXIVID": "2509.17000",
        "COMMENT": "Conditional/Dynamic Networks: real-time adaptive control of reasoning compute via uncertainty signals and input complexity for better accuracy-latency trade-offs.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Shuhao Jiang",
            "Songbo Wang",
            "Yang Qiao",
            "Chun Xu",
            "Chaoyang Zheng",
            "Shengyi Zhou",
            "Huanjun Wang",
            "Fangming Li",
            "Cong Zhang",
            "Jiyu Wang"
        ],
        "title": "Adaptive Overclocking: Dynamic Control of Thinking Path Length via Real-Time Reasoning Signals",
        "abstract": "Large Reasoning Models (LRMs) often suffer from computational inefficiency due to overthinking, where a fixed reasoning budget fails to match the varying complexity of tasks. To address this issue, we propose Adaptive Overclocking, a method that makes the overclocking hyperparameter $\\alpha$ dynamic and context-aware. Our method adjusts reasoning speed in real time through two complementary signals: (1) token-level model uncertainty for fine-grained step-wise control, and (2) input complexity estimation for informed initialization. We implement this approach with three strategies: Uncertainty-Aware Alpha Scheduling (UA-$\\alpha$S), Complexity-Guided Alpha Initialization (CG-$\\alpha$I), and a Hybrid Adaptive Control (HAC) that combines both. Experiments on GSM8K, MATH, and SVAMP show that HAC achieves superior accuracy-latency trade-offs, reducing unnecessary computation on simple problems while allocating more resources to challenging ones. By mitigating overthinking, Adaptive Overclocking enhances both efficiency and overall reasoning performance.",
        "arxiv_id": "2509.17000"
    },
    "2509.17885": {
        "SCORE": 16,
        "ARXIVID": "2509.17885",
        "COMMENT": "Model Compression and Efficiency / Conditional/Dynamic Networks: confidence-gated training for early-exit networks aligns training with inference to reduce compute while maintaining accuracy.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Saad Mokssit",
            "Ouassim Karrakchou",
            "Alejandro Mousist",
            "Mounir Ghogho"
        ],
        "title": "Confidence-gated training for efficient early-exit neural networks",
        "abstract": "Early-exit neural networks reduce inference cost by enabling confident predictions at intermediate layers. However, joint training often leads to gradient interference, with deeper classifiers dominating optimization. We propose Confidence-Gated Training (CGT), a paradigm that conditionally propagates gradients from deeper exits only when preceding exits fail. This encourages shallow classifiers to act as primary decision points while reserving deeper layers for harder inputs. By aligning training with the inference-time policy, CGT mitigates overthinking, improves early-exit accuracy, and preserves efficiency. Experiments on the Indian Pines and Fashion-MNIST benchmarks show that CGT lowers average inference cost while improving overall accuracy, offering a practical solution for deploying deep models in resource-constrained environments.",
        "arxiv_id": "2509.17885"
    },
    "2509.18389": {
        "SCORE": 16,
        "ARXIVID": "2509.18389",
        "COMMENT": "Representation Learning/Training Dynamics: theoretical analysis showing when Transformer pretraining yields in-context RL.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Jiuqi Wang",
            "Rohan Chandra",
            "Shangtong Zhang"
        ],
        "title": "Towards Provable Emergence of In-Context Reinforcement Learning",
        "abstract": "Typically, a modern reinforcement learning (RL) agent solves a task by updating its neural network parameters to adapt its policy to the task. Recently, it has been observed that some RL agents can solve a wide range of new out-of-distribution tasks without parameter updates after pretraining on some task distribution. When evaluated in a new task, instead of making parameter updates, the pretrained agent conditions its policy on additional input called the context, e.g., the agent's interaction history in the new task. The agent's performance increases as the information in the context increases, with the agent's parameters fixed. This phenomenon is typically called in-context RL (ICRL). The pretrained parameters of the agent network enable the remarkable ICRL phenomenon. However, many ICRL works perform the pretraining with standard RL algorithms. This raises the central question this paper aims to address: Why can the RL pretraining algorithm generate network parameters that enable ICRL? We hypothesize that the parameters capable of ICRL are minimizers of the pretraining loss. This work provides initial support for this hypothesis through a case study. In particular, we prove that when a Transformer is pretrained for policy evaluation, one of the global minimizers of the pretraining loss can enable in-context temporal difference learning.",
        "arxiv_id": "2509.18389"
    },
    "2509.18150": {
        "SCORE": 16,
        "ARXIVID": "2509.18150",
        "COMMENT": "Model Compression/Efficiency: sparse training with visual token compression and dynamic layer skipping during forward/backward passes across MLLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Kean Shi",
            "Liang Chen",
            "Haozhe Zhao",
            "Baobao Chang"
        ],
        "title": "Sparse Training Scheme for Multimodal LLM",
        "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated outstanding performance across a variety of domains. However, training MLLMs is often inefficient due to the significantly longer input sequences introduced by multimodal data and the low utilization of inter-layer computations. To address this challenge, we shift the focus to the training process itself and propose a novel training-efficient framework based on sparse representations, termed the Sparse Training Scheme (STS). This scheme consists of two key components: the Visual Token Compressor, which reduces the information load by compressing visual tokens, and the Layer Dynamic Skipper, which mitigates the computational overhead by dynamically skipping unnecessary layers in the language model during both forward and backward passes. Our approach is broadly applicable to diverse MLLM architectures and has been extensively evaluated on multiple benchmarks, demonstrating its effectiveness and efficiency.",
        "arxiv_id": "2509.18150"
    },
    "2509.19170": {
        "SCORE": 16,
        "ARXIVID": "2509.19170",
        "COMMENT": "Representation Learning/Training Dynamics: learning continuous (soft) Chain-of-Thought tokens via RL to enhance reasoning diversity and performance.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Natasha Butt",
            "Ariel Kwiatkowski",
            "Ismail Labiad",
            "Julia Kempe",
            "Yann Ollivier"
        ],
        "title": "Soft Tokens, Hard Truths",
        "abstract": "The use of continuous instead of discrete tokens during the Chain-of-Thought (CoT) phase of reasoning LLMs has garnered attention recently, based on the intuition that a continuous mixture of discrete tokens could simulate a superposition of several reasoning paths simultaneously. Theoretical results have formally proven that continuous tokens have much greater expressivity and can solve specific problems more efficiently. However, practical use of continuous tokens has been limited by strong training difficulties: previous works either just use continuous tokens at inference time on a pre-trained discrete-token model, or must distill the continuous CoT from ground-truth discrete CoTs and face computational costs that limit the CoT to very few tokens.   This is the first work introducing a scalable method to learn continuous CoTs via reinforcement learning (RL), without distilling from reference discrete CoTs. We use \"soft\" tokens: mixtures of tokens together with noise on the input embedding to provide RL exploration. Computational overhead is minimal, enabling us to learn continuous CoTs with hundreds of tokens. On math reasoning benchmarks with Llama and Qwen models up to 8B, training with continuous CoTs match discrete-token CoTs for pass@1 and surpass them for pass@32, showing greater CoT diversity. In systematic comparisons, the best-performing scenario is to train with continuous CoT tokens then use discrete tokens for inference, meaning the \"soft\" models can be deployed in a standard way. Finally, we show continuous CoT RL training better preserves the predictions of the base model on out-of-domain tasks, thus providing a softer touch to the base model.",
        "arxiv_id": "2509.19170"
    },
    "2509.19189": {
        "SCORE": 16,
        "ARXIVID": "2509.19189",
        "COMMENT": "Representation learning/training dynamics: theoretical analysis of SGD dynamics and learning-rate schedules via a functional scaling law.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Binghui Li",
            "Fengling Chen",
            "Zixun Huang",
            "Lean Wang",
            "Lei Wu"
        ],
        "title": "Unveiling the Role of Learning Rate Schedules via Functional Scaling Laws",
        "abstract": "Scaling laws have played a cornerstone role in guiding the training of large language models (LLMs). However, most existing works on scaling laws primarily focus on the final-step loss, overlooking the loss dynamics during the training process and, crucially, the impact of learning rate schedule (LRS). In this paper, we aim to bridge this gap by studying a teacher-student kernel regression setup trained via online stochastic gradient descent (SGD). Leveraging a novel intrinsic time viewpoint and stochastic differential equation (SDE) modeling of SGD, we introduce the Functional Scaling Law (FSL), which characterizes the evolution of population risk during the training process for general LRSs. Remarkably, the impact of the LRSs is captured through an explicit convolution-type functional term, making their effects fully tractable. To illustrate the utility of FSL, we analyze three widely used LRSs -- constant, exponential decay, and warmup-stable-decay (WSD) -- under both data-limited and compute-limited regimes. We provide theoretical justification for widely adopted empirical practices in LLMs pre-training such as (i) higher-capacity models are more data- and compute-efficient; (ii) learning rate decay can improve training efficiency; (iii) WSD-like schedules can outperform direct-decay schedules. Lastly, we explore the practical relevance of FSL as a surrogate model for fitting, predicting and optimizing the loss curves in LLM pre-training, with experiments conducted across model sizes ranging from 0.1B to 1B parameters. We hope our FSL framework can deepen the understanding of LLM pre-training dynamics and provide insights for improving large-scale model training.",
        "arxiv_id": "2509.19189"
    },
    "2509.18129": {
        "SCORE": 16,
        "ARXIVID": "2509.18129",
        "COMMENT": "High Performance Computing: distributed optimization algorithm with tunable comm/comp tradeoffs and near-optimal iteration complexity guarantees.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Yan Huang",
            "Jinming Xu",
            "Li Chai",
            "Jiming Chen",
            "Karl H. Johansson"
        ],
        "title": "Pareto-optimal Tradeoffs Between Communication and Computation with Flexible Gradient Tracking",
        "abstract": "This paper addresses distributed optimization problems in non-i.i.d. scenarios, focusing on the interplay between communication and computation efficiency. To this end, we propose FlexGT, a flexible snapshot gradient tracking method with tunable numbers of local updates and neighboring communications in each round. Leveraging a unified convergence analysis framework, we prove that FlexGT achieves a linear or sublinear convergence rate depending on objective-specific properties--from (strongly) convex to nonconvex--and the above-mentioned tunable parameters. FlexGT is provably robust to the heterogeneity across nodes and attains the best-known communication and computation complexity among existing results. Moreover, we introduce an accelerated gossip-based variant, termed Acc-FlexGT, and show that with prior knowledge of the graph, it achieves a Pareto-optimal trade-off between communication and computation. Particularly, Acc-FlexGT achieves the optimal iteration complexity of $\\tilde{\\mathcal{O}} \\left( L/\\epsilon +L\\sigma ^2/\\left( n\\epsilon^2 \\sqrt{1-\\sqrt{\\rho _W}} \\right) \\right) $ for the nonconvex case, matching the existing lower bound up to a logarithmic factor, and improves the existing results for the strongly convex case by a factor of $\\tilde{\\mathcal{O}} \\left( 1/\\sqrt{\\epsilon} \\right)$, where $\\epsilon$ is the targeted accuracy, $n$ the number of nodes, $L$ the Lipschitz constant, $\\rho_W$ the spectrum gap of the graph, and $\\sigma$ the stochastic gradient variance. Numerical examples are provided to demonstrate the effectiveness of the proposed methods.",
        "arxiv_id": "2509.18129"
    },
    "2509.17401": {
        "SCORE": 15,
        "ARXIVID": "2509.17401",
        "COMMENT": "Representation Learning/Interpretability: interprets ViTs using sparse autoencoders and a residual replacement model to reveal feature circuits.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Jinyeong Kim",
            "Junhyeok Kim",
            "Yumin Shim",
            "Joohyeok Kim",
            "Sunyoung Jung",
            "Seong Jae Hwang"
        ],
        "title": "Interpreting vision transformers via residual replacement model",
        "abstract": "How do vision transformers (ViTs) represent and process the world? This paper addresses this long-standing question through the first systematic analysis of 6.6K features across all layers, extracted via sparse autoencoders, and by introducing the residual replacement model, which replaces ViT computations with interpretable features in the residual stream. Our analysis reveals not only a feature evolution from low-level patterns to high-level semantics, but also how ViTs encode curves and spatial positions through specialized feature types. The residual replacement model scalably produces a faithful yet parsimonious circuit for human-scale interpretability by significantly simplifying the original computations. As a result, this framework enables intuitive understanding of ViT mechanisms. Finally, we demonstrate the utility of our framework in debiasing spurious correlations.",
        "arxiv_id": "2509.17401"
    },
    "2509.18116": {
        "SCORE": 15,
        "ARXIVID": "2509.18116",
        "COMMENT": "Efficiency: amortized latent steering collapses test-time optimization into a constant-cost vector applied at inference.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Nathan Egbuna",
            "Saatvik Gaur",
            "Sunishchal Dev",
            "Ashwinee Panda",
            "Maheep Chaudhary"
        ],
        "title": "Amortized Latent Steering: Low-Cost Alternative to Test-Time Optimization",
        "abstract": "Test-time optimization remains impractical at scale due to prohibitive inference costs\\textemdash techniques like iterative refinement and multi-step verification can require $10$--$100\\times$ more compute per query than standard decoding. Latent space test-time optimization methods like LatentSeek offer a more direct approach by steering hidden representations, but still demand expensive per-query optimization loops with multiple backward passes. We propose Amortized Latent Steering (ALS), which collapses this iterative optimization into a single offline-computed vector applied at constant cost during inference. ALS computes the mean difference between hidden states from successful versus unsuccessful generations, then uses this direction to calibrate the model's hidden representations: when decoding drifts away from the success manifold, ALS nudges activations back toward it. Across GSM8K and MATH-$500$ benchmarks, ALS achieves $2$--$5\\times$ speedup over iterative methods while matching or surpassing greedy Chain-of-Thought (CoT) and Self-Consistency baselines, yielding up to 101\\% improvement in efficiency--accuracy trade-off. These results show that much of latent optimization's benefit can be captured offline, making sophisticated reasoning techniques viable for production deployment. Code is available at~\\href{https://anonymous.4open.science/r/steering-17F2}{https://anonymous.4open.science/r/steering-17F2}",
        "arxiv_id": "2509.18116"
    },
    "2509.17276": {
        "SCORE": 15,
        "ARXIVID": "2509.17276",
        "COMMENT": "Model Architecture/Fusion: probabilistic token alignment via optimal transport enables soft vocabulary mapping for LLM fusion across architectures.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Runjia Zeng",
            "James Chenhao Liang",
            "Cheng Han",
            "Zhiwen Cao",
            "Jiahao Liu",
            "Xiaojun Quan",
            "Yingjie Victor Chen",
            "Lifu Huang",
            "Tong Geng",
            "Qifan Wang",
            "Dongfang Liu"
        ],
        "title": "Probabilistic Token Alignment for Large Language Model Fusion",
        "abstract": "Training large language models (LLMs) from scratch can yield models with unique functionalities and strengths, but it is costly and often leads to redundant capabilities. A more cost-effective alternative is to fuse existing pre-trained LLMs with different architectures into a more powerful model. However, a key challenge in existing model fusion is their dependence on manually predefined vocabulary alignment, which may not generalize well across diverse contexts, leading to performance degradation in several evaluation. To solve this, we draw inspiration from distribution learning and propose the probabilistic token alignment method as a general and soft mapping for alignment, named as PTA-LLM. Our approach innovatively reformulates token alignment into a classic mathematical problem: optimal transport, seamlessly leveraging distribution-aware learning to facilitate more coherent model fusion. Apart from its inherent generality, PTA-LLM exhibits interpretability from a distributional perspective, offering insights into the essence of the token alignment. Empirical results demonstrate that probabilistic token alignment enhances the target model's performance across multiple capabilities. Our code is avaliable at https://runjia.tech/neurips_pta-llm/.",
        "arxiv_id": "2509.17276"
    },
    "2509.18349": {
        "SCORE": 15,
        "ARXIVID": "2509.18349",
        "COMMENT": "Representation learning theory: latent predictor subspace characterization and task diversity quantification for meta-learning performance analysis.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Saptati Datta",
            "Nicolas W. Hengartner",
            "Yulia Pimonova",
            "Natalie E. Klein",
            "Nicholas Lubbers"
        ],
        "title": "Statistical Insight into Meta-Learning via Predictor Subspace Characterization and Quantification of Task Diversity",
        "abstract": "Meta-learning has emerged as a powerful paradigm for leveraging information across related tasks to improve predictive performance on new tasks. In this paper, we propose a statistical framework for analyzing meta-learning through the lens of predictor subspace characterization and quantification of task diversity. Specifically, we model the shared structure across tasks using a latent subspace and introduce a measure of diversity that captures heterogeneity across task-specific predictors. We provide both simulation-based and theoretical evidence indicating that achieving the desired prediction accuracy in meta-learning depends on the proportion of predictor variance aligned with the shared subspace, as well as on the accuracy of subspace estimation.",
        "arxiv_id": "2509.18349"
    },
    "2509.18469": {
        "SCORE": 15,
        "ARXIVID": "2509.18469",
        "COMMENT": "Representation Learning: extends PPCA to explicitly model data around a known nonlinear manifold and derives an EM algorithm (PGPCA) for manifold-aware probabilistic dimensionality reduction.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Han-Lin Hsieh",
            "Maryam M. Shanechi"
        ],
        "title": "Probabilistic Geometric Principal Component Analysis with application to neural data",
        "abstract": "Dimensionality reduction is critical across various domains of science including neuroscience. Probabilistic Principal Component Analysis (PPCA) is a prominent dimensionality reduction method that provides a probabilistic approach unlike the deterministic approach of PCA and serves as a connection between PCA and Factor Analysis (FA). Despite their power, PPCA and its extensions are mainly based on linear models and can only describe the data in a Euclidean coordinate system. However, in many neuroscience applications, data may be distributed around a nonlinear geometry (i.e., manifold) rather than lying in the Euclidean space. We develop Probabilistic Geometric Principal Component Analysis (PGPCA) for such datasets as a new dimensionality reduction algorithm that can explicitly incorporate knowledge about a given nonlinear manifold that is first fitted from these data. Further, we show how in addition to the Euclidean coordinate system, a geometric coordinate system can be derived for the manifold to capture the deviations of data from the manifold and noise. We also derive a data-driven EM algorithm for learning the PGPCA model parameters. As such, PGPCA generalizes PPCA to better describe data distributions by incorporating a nonlinear manifold geometry. In simulations and brain data analyses, we show that PGPCA can effectively model the data distribution around various given manifolds and outperforms PPCA for such data. Moreover, PGPCA provides the capability to test whether the new geometric coordinate system better describes the data than the Euclidean one. Finally, PGPCA can perform dimensionality reduction and learn the data distribution both around and on the manifold. These capabilities make PGPCA valuable for enhancing the efficacy of dimensionality reduction for analysis of high-dimensional data that exhibit noise and are distributed around a nonlinear manifold.",
        "arxiv_id": "2509.18469"
    },
    "2509.17030": {
        "SCORE": 15,
        "ARXIVID": "2509.17030",
        "COMMENT": "Representation Learning: mechanistic analysis identifying transfer neurons that move representations between language-specific and shared latent spaces in multilingual LLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Hinata Tezuka",
            "Naoya Inoue"
        ],
        "title": "The Transfer Neurons Hypothesis: An Underlying Mechanism for Language Latent Space Transitions in Multilingual LLMs",
        "abstract": "Recent studies have suggested a processing framework for multilingual inputs in decoder-based LLMs: early layers convert inputs into English-centric and language-agnostic representations; middle layers perform reasoning within an English-centric latent space; and final layers generate outputs by transforming these representations back into language-specific latent spaces. However, the internal dynamics of such transformation and the underlying mechanism remain underexplored. Towards a deeper understanding of this framework, we propose and empirically validate The Transfer Neurons Hypothesis: certain neurons in the MLP module are responsible for transferring representations between language-specific latent spaces and a shared semantic latent space. Furthermore, we show that one function of language-specific neurons, as identified in recent studies, is to facilitate movement between latent spaces. Finally, we show that transfer neurons are critical for reasoning in multilingual LLMs.",
        "arxiv_id": "2509.17030"
    },
    "2509.19250": {
        "SCORE": 15,
        "ARXIVID": "2509.19250",
        "COMMENT": "Efficiency and Representation Learning: Nystr\u00f6m and matrix completion approaches to recover costly Wasserstein distance matrices with stability analysis for MDS, enabling scalable manifold learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Muhammad Rana",
            "Abiy Tasissa",
            "HanQin Cai",
            "Yakov Gavriyelov",
            "Keaton Hamm"
        ],
        "title": "Recovering Wasserstein Distance Matrices from Few Measurements",
        "abstract": "This paper proposes two algorithms for estimating square Wasserstein distance matrices from a small number of entries. These matrices are used to compute manifold learning embeddings like multidimensional scaling (MDS) or Isomap, but contrary to Euclidean distance matrices, are extremely costly to compute. We analyze matrix completion from upper triangular samples and Nystr\\\"{o}m completion in which $\\mathcal{O}(d\\log(d))$ columns of the distance matrices are computed where $d$ is the desired embedding dimension, prove stability of MDS under Nystr\\\"{o}m completion, and show that it can outperform matrix completion for a fixed budget of sample distances. Finally, we show that classification of the OrganCMNIST dataset from the MedMNIST benchmark is stable on data embedded from the Nystr\\\"{o}m estimation of the distance matrix even when only 10\\% of the columns are computed.",
        "arxiv_id": "2509.19250"
    },
    "2509.18893": {
        "SCORE": 15,
        "ARXIVID": "2509.18893",
        "COMMENT": "Representation Learning/Theory: spectral analysis of heterophily for graph-level tasks yielding guidance for GNN architecture design.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Qinhan Hou",
            "Yilun Zheng",
            "Xichun Zhang",
            "Sitao Luan",
            "Jing Tang"
        ],
        "title": "Exploring Heterophily in Graph-level Tasks",
        "abstract": "While heterophily has been widely studied in node-level tasks, its impact on graph-level tasks remains unclear. We present the first analysis of heterophily in graph-level learning, combining theoretical insights with empirical validation. We first introduce a taxonomy of graph-level labeling schemes, and focus on motif-based tasks within local structure labeling, which is a popular labeling scheme. Using energy-based gradient flow analysis, we reveal a key insight: unlike frequency-dominated regimes in node-level tasks, motif detection requires mixed-frequency dynamics to remain flexible across multiple spectral components. Our theory shows that motif objectives are inherently misaligned with global frequency dominance, demanding distinct architectural considerations. Experiments on synthetic datasets with controlled heterophily and real-world molecular property prediction support our findings, showing that frequency-adaptive model outperform frequency-dominated models. This work establishes a new theoretical understanding of heterophily in graph-level learning and offers guidance for designing effective GNN architectures.",
        "arxiv_id": "2509.18893"
    },
    "2509.18165": {
        "SCORE": 15,
        "ARXIVID": "2509.18165",
        "COMMENT": "Representation Learning/Regularization: inverse-mapping regularizer preserves information and smooths gradients; model-agnostic and plug-and-play.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Xiuding Cai",
            "Yaoyao Zhu",
            "Linjie Fu",
            "Dong Miao",
            "Yu Yao"
        ],
        "title": "Self Identity Mapping",
        "abstract": "Regularization is essential in deep learning to enhance generalization and mitigate overfitting. However, conventional techniques often rely on heuristics, making them less reliable or effective across diverse settings. We propose Self Identity Mapping (SIM), a simple yet effective, data-intrinsic regularization framework that leverages an inverse mapping mechanism to enhance representation learning. By reconstructing the input from its transformed output, SIM reduces information loss during forward propagation and facilitates smoother gradient flow. To address computational inefficiencies, We instantiate SIM as $ \\rho\\text{SIM} $ by incorporating patch-level feature sampling and projection-based method to reconstruct latent features, effectively lowering complexity. As a model-agnostic, task-agnostic regularizer, SIM can be seamlessly integrated as a plug-and-play module, making it applicable to different network architectures and tasks.   We extensively evaluate $\\rho\\text{SIM}$ across three tasks: image classification, few-shot prompt learning, and domain generalization. Experimental results show consistent improvements over baseline methods, highlighting $\\rho\\text{SIM}$'s ability to enhance representation learning across various tasks. We also demonstrate that $\\rho\\text{SIM}$ is orthogonal to existing regularization methods, boosting their effectiveness. Moreover, our results confirm that $\\rho\\text{SIM}$ effectively preserves semantic information and enhances performance in dense-to-dense tasks, such as semantic segmentation and image translation, as well as in non-visual domains including audio classification and time series anomaly detection. The code is publicly available at https://github.com/XiudingCai/SIM-pytorch.",
        "arxiv_id": "2509.18165"
    },
    "2509.18147": {
        "SCORE": 15,
        "ARXIVID": "2509.18147",
        "COMMENT": "Representation Learning/Interpretability: assigns concepts to filters and models concept propagation across CNN layers via concept attentions and transition matrices.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Xinyu Mu",
            "Hui Dou",
            "Furao Shen",
            "Jian Zhao"
        ],
        "title": "ConceptFlow: Hierarchical and Fine-grained Concept-Based Explanation for Convolutional Neural Networks",
        "abstract": "Concept-based interpretability for Convolutional Neural Networks (CNNs) aims to align internal model representations with high-level semantic concepts, but existing approaches largely overlook the semantic roles of individual filters and the dynamic propagation of concepts across layers. To address these limitations, we propose ConceptFlow, a concept-based interpretability framework that simulates the internal \"thinking path\" of a model by tracing how concepts emerge and evolve across layers. ConceptFlow comprises two key components: (i) concept attentions, which associate each filter with relevant high-level concepts to enable localized semantic interpretation, and (ii) conceptual pathways, derived from a concept transition matrix that quantifies how concepts propagate and transform between filters. Together, these components offer a unified and structured view of internal model reasoning. Experimental results demonstrate that ConceptFlow yields semantically meaningful insights into model reasoning, validating the effectiveness of concept attentions and conceptual pathways in explaining decision behavior. By modeling hierarchical conceptual pathways, ConceptFlow provides deeper insight into the internal logic of CNNs and supports the generation of more faithful and human-aligned explanations.",
        "arxiv_id": "2509.18147"
    },
    "2509.16596": {
        "SCORE": 15,
        "ARXIVID": "2509.16596",
        "COMMENT": "Analyzes SFT effects on knowledge at token/parameter levels; identifies non-contributory parameter updates (Representation Learning; training dynamics).",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Junjie Ye",
            "Yuming Yang",
            "Yang Nan",
            "Shuo Li",
            "Qi Zhang",
            "Tao Gui",
            "Xuanjing Huang",
            "Peng Wang",
            "Zhongchao Shi",
            "Jianping Fan"
        ],
        "title": "Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token and Parameter Levels",
        "abstract": "Large language models (LLMs) acquire substantial world knowledge during pre-training, which is further shaped by post-training techniques such as supervised fine-tuning (SFT). However, the impact of SFT on a model's knowledge remains underexplored, limiting our ability to control knowledge change behavior in fine-tuned models. To address this gap, we evaluate closed-book question answering (CBQA) performance across five LLMs from the LLaMA-2 and LLaMA-3 families. Surprisingly, models fine-tuned on 1,920 samples perform up to 14% worse than those fine-tuned on only 240 samples. Furthermore, varying the level of knowledge mastery in the fine-tuning data leads to performance fluctuations of over 12%. To investigate these effects, we analyze model behavior at both the token and parameter levels. Our analysis reveals that up to 90% of parameter updates during SFT do not contribute to knowledge enhancement. Restoring these updates can improve performance on the CBQA task, depending on the characteristics of the fine-tuning data. These insights offer practical guidance for developing fine-tuning strategies that more effectively strengthen model knowledge.",
        "arxiv_id": "2509.16596"
    },
    "2509.17186": {
        "SCORE": 15,
        "ARXIVID": "2509.17186",
        "COMMENT": "Model Architecture/Efficiency: introduces a dendritic Resonate-and-Fire neuron with adaptive thresholding for sparse, efficient long-sequence modeling.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Dehao Zhang",
            "Malu Zhang",
            "Shuai Wang",
            "Jingya Wang",
            "Wenjie Wei",
            "Zeyu Ma",
            "Guoqing Wang",
            "Yang Yang",
            "HaiZhou Li"
        ],
        "title": "Dendritic Resonate-and-Fire Neuron for Effective and Efficient Long Sequence Modeling",
        "abstract": "The explosive growth in sequence length has intensified the demand for effective and efficient long sequence modeling. Benefiting from intrinsic oscillatory membrane dynamics, Resonate-and-Fire (RF) neurons can efficiently extract frequency components from input signals and encode them into spatiotemporal spike trains, making them well-suited for long sequence modeling. However, RF neurons exhibit limited effective memory capacity and a trade-off between energy efficiency and training speed on complex temporal tasks. Inspired by the dendritic structure of biological neurons, we propose a Dendritic Resonate-and-Fire (D-RF) model, which explicitly incorporates a multi-dendritic and soma architecture. Each dendritic branch encodes specific frequency bands by utilizing the intrinsic oscillatory dynamics of RF neurons, thereby collectively achieving comprehensive frequency representation. Furthermore, we introduce an adaptive threshold mechanism into the soma structure that adjusts the threshold based on historical spiking activity, reducing redundant spikes while maintaining training efficiency in long sequence tasks. Extensive experiments demonstrate that our method maintains competitive accuracy while substantially ensuring sparse spikes without compromising computational efficiency during training. These results underscore its potential as an effective and efficient solution for long sequence modeling on edge platforms.",
        "arxiv_id": "2509.17186"
    },
    "2509.18445": {
        "SCORE": 15,
        "ARXIVID": "2509.18445",
        "COMMENT": "Model Architecture: integrates GNN spatial reasoning with Neural ODE continuous-time dynamics to improve long-term stability for mesh-based systems.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Kangzheng Liu",
            "Leixin Ma"
        ],
        "title": "MeshODENet: A Graph-Informed Neural Ordinary Differential Equation Neural Network for Simulating Mesh-Based Physical Systems",
        "abstract": "The simulation of complex physical systems using a discretized mesh is a cornerstone of applied mechanics, but traditional numerical solvers are often computationally prohibitive for many-query tasks. While Graph Neural Networks (GNNs) have emerged as powerful surrogate models for mesh-based data, their standard autoregressive application for long-term prediction is often plagued by error accumulation and instability. To address this, we introduce MeshODENet, a general framework that synergizes the spatial reasoning of GNNs with the continuous-time modeling of Neural Ordinary Differential Equations. We demonstrate the framework's effectiveness and versatility on a series of challenging structural mechanics problems, including one- and two-dimensional elastic bodies undergoing large, non-linear deformations. The results demonstrate that our approach significantly outperforms baseline models in long-term predictive accuracy and stability, while achieving substantial computational speed-ups over traditional solvers. This work presents a powerful and generalizable approach for developing data-driven surrogates to accelerate the analysis and modeling of complex structural systems.",
        "arxiv_id": "2509.18445"
    },
    "2509.18208": {
        "SCORE": 15,
        "ARXIVID": "2509.18208",
        "COMMENT": "Matches Model Compression/Efficiency and Representation Learning: sparse (Spike-and-Slab) variational task-vector composition with sample-specific coefficients.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Boyuan Zhang",
            "Yingjun Du",
            "Xiantong Zhen",
            "Ling Shao"
        ],
        "title": "Variational Task Vector Composition",
        "abstract": "Task vectors capture how a model changes during fine-tuning by recording the difference between pre-trained and task-specific weights. The composition of task vectors, a key operator in task arithmetic, enables models to integrate knowledge from multiple tasks without incurring additional inference costs. In this paper, we propose variational task vector composition, where composition coefficients are taken as latent variables and estimated in a Bayesian inference framework. Unlike previous methods that operate at the task level, our framework focuses on sample-specific composition. Motivated by the observation of structural redundancy in task vectors, we introduce a Spike-and-Slab prior that promotes sparsity and preserves only the most informative components. To further address the high variance and sampling inefficiency in sparse, high-dimensional spaces, we develop a gated sampling mechanism that constructs a controllable posterior by filtering the composition coefficients based on both uncertainty and importance. This yields a more stable and interpretable variational framework by deterministically selecting reliable task components, reducing sampling variance while improving transparency and generalization. Experimental results demonstrate that our method consistently outperforms existing approaches across all datasets by selectively leveraging the most reliable and informative components in task vectors. These findings highlight the practical value of our approach, establishing a new standard for efficient and effective task vector composition.",
        "arxiv_id": "2509.18208"
    },
    "2509.18001": {
        "SCORE": 15,
        "ARXIVID": "2509.18001",
        "COMMENT": "Strong match to Representation Learning/Training Dynamics: theoretical analysis of SAM via SDE/SGN and an algorithmic variant (Reweighted SAM).",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Haocheng Luo",
            "Mehrtash Harandi",
            "Dinh Phung",
            "Trung Le"
        ],
        "title": "Unveiling m-Sharpness Through the Structure of Stochastic Gradient Noise",
        "abstract": "Sharpness-aware minimization (SAM) has emerged as a highly effective technique for improving model generalization, but its underlying principles are not fully understood. We investigated the phenomenon known as m-sharpness, where the performance of SAM improves monotonically as the micro-batch size for computing perturbations decreases. Leveraging an extended Stochastic Differential Equation (SDE) framework, combined with an analysis of the structure of stochastic gradient noise (SGN), we precisely characterize the dynamics of various SAM variants. Our findings reveal that the stochastic noise introduced during SAM perturbations inherently induces a variance-based sharpness regularization effect. Motivated by our theoretical insights, we introduce Reweighted SAM, which employs sharpness-weighted sampling to mimic the generalization benefits of m-SAM while remaining parallelizable. Comprehensive experiments validate the effectiveness of our theoretical analysis and proposed method.",
        "arxiv_id": "2509.18001"
    },
    "2509.16443": {
        "SCORE": 15,
        "ARXIVID": "2509.16443",
        "COMMENT": "Strong match to High Performance Computing: compiler/IR (Stacked Graph) for mapping LLM inference across photonic-electronic accelerators optimizing latency/energy.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Ryan Tomich",
            "Zhizhen Zhong",
            "Dirk Englund"
        ],
        "title": "LightCode: Compiling LLM Inference for Photonic-Electronic Systems",
        "abstract": "The growing demand for low-latency, energy-efficient inference in large language models (LLMs) has catalyzed interest in heterogeneous architectures. While GPUs remain dominant, they are poorly suited for integration with emerging domain-specific accelerators like the Photonic Tensor Units (PTUs), which offer low-power, high-throughput linear computation. This motivates hybrid compilation strategies that combine photonic and electronic resources. We present LightCode, a compiler framework and simulator for mapping LLM inference workloads across hybrid photonic-electronic systems. LightCode introduces the Stacked Graph, an intermediate representation that encodes multiple hardware-specific realizations of each tensor operation. Hardware assignment is formulated as a constrained subgraph selection problem optimized for latency or energy under parametric cost models. We evaluate LightCode on the prefill stage of GPT-2 and Llama-7B showing that under our workload and hardware assumptions, (i) Photonic hardware reduced energy by up to 50% in our simulated workloads at maximum sequence length; (ii) multiplexing and assignment strategy yielded latency improvements exceeding 10x; and (iii) Optimizing for latency or energy resulted in distinct hardware mappings in our simulations. LightCode offers a module, foundational framework and simulator for compiling LLMs to emerging photonic accelerators.",
        "arxiv_id": "2509.16443"
    },
    "2509.18842": {
        "SCORE": 15,
        "ARXIVID": "2509.18842",
        "COMMENT": "Model Architecture/Efficiency: dynamic network expansion (SWE) to integrate new neurons + gradient-based allocation (SVoD) to grow capacity without retraining.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Nikolas Chatzis",
            "Ioannis Kordonis",
            "Manos Theodosis",
            "Petros Maragos"
        ],
        "title": "Shared-Weights Extender and Gradient Voting for Neural Network Expansion",
        "abstract": "Expanding neural networks during training is a promising way to augment capacity without retraining larger models from scratch. However, newly added neurons often fail to adjust to a trained network and become inactive, providing no contribution to capacity growth. We propose the Shared-Weights Extender (SWE), a novel method explicitly designed to prevent inactivity of new neurons by coupling them with existing ones for smooth integration. In parallel, we introduce the Steepest Voting Distributor (SVoD), a gradient-based method for allocating neurons across layers during deep network expansion. Our extensive benchmarking on four datasets shows that our method can effectively suppress neuron inactivity and achieve better performance compared to other expanding methods and baselines.",
        "arxiv_id": "2509.18842"
    },
    "2509.19284": {
        "SCORE": 15,
        "ARXIVID": "2509.19284",
        "COMMENT": "Representation Learning/Training Dynamics: structural analysis of chain-of-thought with a new metric (Failed-Step Fraction) guiding structure-aware test-time scaling.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Yunzhen Feng",
            "Julia Kempe",
            "Cheng Zhang",
            "Parag Jain",
            "Anthony Hartshorn"
        ],
        "title": "What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT",
        "abstract": "Large reasoning models (LRMs) spend substantial test-time compute on long chain-of-thought (CoT) traces, but what *characterizes* an effective CoT remains unclear. While prior work reports gains from lengthening CoTs and increasing review (revisiting earlier steps) via appended *wait* tokens, recent studies suggest that shorter thinking can outperform longer traces. We therefore conduct a systematic evaluation across ten LRMs on math and scientific reasoning. Contrary to the \"longer-is-better\" narrative, we find that both naive CoT lengthening and increased review are associated with *lower* accuracy.   As CoT unfolds step by step, token-level metrics can conflate verbosity with process quality. We introduce a graph view of CoT to extract structure and identify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction of steps in abandoned branches-that consistently outpredicts length and review ratio for correctness across models. To probe causality, we design two interventions. First, we rank candidate CoTs by each metric at test time, where FSF yields the largest pass@1 gains; second, we edit CoTs to remove failed branches, which significantly improves accuracy, indicating that failed branches bias subsequent reasoning. Taken together, these results characterize effective CoTs as those that *fail less* and support *structure-aware* test-time scaling over indiscriminately generating long CoT.",
        "arxiv_id": "2509.19284"
    },
    "2509.16293": {
        "SCORE": 15,
        "ARXIVID": "2509.16293",
        "COMMENT": "High Performance Computing: systems-level innovations for robust large-scale LLM training (failure detection, demarcation, and recovery) enabling continuous distributed training at massive scale.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Borui Wan",
            "Gaohong Liu",
            "Zuquan Song",
            "Jun Wang",
            "Yun Zhang",
            "Guangming Sheng",
            "Shuguang Wang",
            "Houmin Wei",
            "Chenyuan Wang",
            "Weiqiang Lou",
            "Xi Yang",
            "Mofan Zhang",
            "Kaihua Jiang",
            "Cheng Ren",
            "Xiaoyun Zhi",
            "Menghan Yu",
            "Zhe Nan",
            "Zhuolin Zheng",
            "Baoquan Zhong",
            "Qinlong Wang",
            "Huan Yu",
            "Jinxin Chi",
            "Wang Zhang",
            "Yuhan Li",
            "Zixian Du",
            "Sida Zhao",
            "Yongqiang Zhang",
            "Jingzhe Tang",
            "Zherui Liu",
            "Chuan Wu",
            "Yanghua Peng",
            "Haibin Lin",
            "Wencong Xiao",
            "Xin Liu",
            "Liang Xiang"
        ],
        "title": "Robust LLM Training Infrastructure at ByteDance",
        "abstract": "The training scale of large language models (LLMs) has reached tens of thousands of GPUs and is still continuously expanding, enabling faster learning of larger models. Accompanying the expansion of the resource scale is the prevalence of failures (CUDA error, NaN values, job hang, etc.), which poses significant challenges to training stability. Any large-scale LLM training infrastructure should strive for minimal training interruption, efficient fault diagnosis, and effective failure tolerance to enable highly efficient continuous training. This paper presents ByteRobust, a large-scale GPU infrastructure management system tailored for robust and stable training of LLMs. It exploits the uniqueness of LLM training process and gives top priorities to detecting and recovering failures in a routine manner. Leveraging parallelisms and characteristics of LLM training, ByteRobust enables high-capacity fault tolerance, prompt fault demarcation, and localization with an effective data-driven approach, comprehensively ensuring continuous and efficient training of LLM tasks. ByteRobust is deployed on a production GPU platform with over 200,000 GPUs and achieves 97% ETTR for a three-month training job on 9,600 GPUs.",
        "arxiv_id": "2509.16293"
    },
    "2509.18154": {
        "SCORE": 15,
        "ARXIVID": "2509.18154",
        "COMMENT": "Model Architecture and Efficiency: introduces a unified 3D-Resampler for compact image/video encoding with substantial memory and inference-time reductions in an 8B MLLM.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Tianyu Yu",
            "Zefan Wang",
            "Chongyi Wang",
            "Fuwei Huang",
            "Wenshuo Ma",
            "Zhihui He",
            "Tianchi Cai",
            "Weize Chen",
            "Yuxiang Huang",
            "Yuanqian Zhao",
            "Bokai Xu",
            "Junbo Cui",
            "Yingjing Xu",
            "Liqing Ruan",
            "Luoyuan Zhang",
            "Hanyu Liu",
            "Jingkun Tang",
            "Hongyuan Liu",
            "Qining Guo",
            "Wenhao Hu",
            "Bingxiang He",
            "Jie Zhou",
            "Jie Cai",
            "Ji Qi",
            "Zonghao Guo",
            "Chi Chen",
            "Guoyang Zeng",
            "Yuxuan Li",
            "Ganqu Cui",
            "Ning Ding",
            "Xu Han",
            "Yuan Yao",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "title": "MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe",
        "abstract": "Multimodal Large Language Models (MLLMs) are undergoing rapid progress and represent the frontier of AI development. However, their training and inference efficiency have emerged as a core bottleneck in making MLLMs more accessible and scalable. To address the challenges, we present MiniCPM-V 4.5, an 8B parameter model designed for high efficiency and strong performance. We introduce three core improvements in model architecture, data strategy and training method: a unified 3D-Resampler model architecture for highly compact encoding over images and videos, a unified learning paradigm for document knowledge and text recognition without heavy data engineering, and a hybrid reinforcement learning strategy for proficiency in both short and long reasoning modes. Comprehensive experimental results in OpenCompass evaluation show that MiniCPM-V 4.5 surpasses widely used proprietary models such as GPT-4o-latest, and significantly larger open-source models such as Qwen2.5-VL 72B. Notably, the strong performance is achieved with remarkable efficiency. For example, on the widely adopted VideoMME benchmark, MiniCPM-V 4.5 achieves state-of-the-art performance among models under 30B size, using just 46.7\\% GPU memory cost and 8.7\\% inference time of Qwen2.5-VL 7B.",
        "arxiv_id": "2509.18154"
    },
    "2509.18085": {
        "SCORE": 15,
        "ARXIVID": "2509.18085",
        "COMMENT": "Efficiency: lossless speculative decoding for diffusion LLMs using directed draft graphs and offline calibration to multiply decoding speedups.",
        "RELEVANCE": 7,
        "NOVELTY": 8,
        "authors": [
            "Sudhanshu Agrawal",
            "Risheek Garrepalli",
            "Raghavv Goel",
            "Mingu Lee",
            "Christopher Lott",
            "Fatih Porikli"
        ],
        "title": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative Decoding",
        "abstract": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to autoregressive LLMs (AR-LLMs) with the potential to operate at significantly higher token generation rates. However, currently available open-source dLLMs often generate at much lower rates, typically decoding only a single token at every denoising timestep in order to maximize output quality. We present Spiffy, a speculative decoding algorithm that accelerates dLLM inference by $\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output distribution. This work addresses the unique challenges involved in applying ideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes draft states by leveraging the dLLM's distribution itself in an auto-speculative manner. This approach is efficient and effective, and eliminates the overheads of training and running an independent draft model. To structure the candidate draft states, we propose a novel directed draft graph which is uniquely designed to take advantage of the bidirectional, block-wise nature of dLLM generation and can be verified in parallel by the dLLM. To further optimize the structure of these draft graphs, we introduce an efficient, offline calibration algorithm that procedurally determines high-quality graph configurations. These optimized draft graphs, enabling increased acceptance rates, lead to a significant boost in the overall speedup achieved by the system. Crucially, Spiffy is also complementary to other recent innovations in improving dLLM generation speeds such as KV-caching and multi-token unmasking. We demonstrate that when combined with such parallel decoding algorithms, Spiffy is able to effectively multiply the benefits of these methods leading to total speedups of up to $\\mathbf{7.9\\times}$.",
        "arxiv_id": "2509.18085"
    },
    "2509.18826": {
        "SCORE": 14,
        "ARXIVID": "2509.18826",
        "COMMENT": "Representation Learning: revised graph-based clustering with low-rank, nonnegative, doubly stochastic structure and theoretical optimization guarantees.",
        "RELEVANCE": 7,
        "NOVELTY": 7,
        "authors": [
            "Wenlong Lyu",
            "Yuheng Jia",
            "Hui Liu",
            "Junhui Hou"
        ],
        "title": "Graph-based Clustering Revisited: A Relaxation of Kernel $k$-Means Perspective",
        "abstract": "The well-known graph-based clustering methods, including spectral clustering, symmetric non-negative matrix factorization, and doubly stochastic normalization, can be viewed as relaxations of the kernel $k$-means approach. However, we posit that these methods excessively relax their inherent low-rank, nonnegative, doubly stochastic, and orthonormal constraints to ensure numerical feasibility, potentially limiting their clustering efficacy. In this paper, guided by our theoretical analyses, we propose \\textbf{Lo}w-\\textbf{R}ank \\textbf{D}oubly stochastic clustering (\\textbf{LoRD}), a model that only relaxes the orthonormal constraint to derive a probabilistic clustering results. Furthermore, we theoretically establish the equivalence between orthogonality and block diagonality under the doubly stochastic constraint. By integrating \\textbf{B}lock diagonal regularization into LoRD, expressed as the maximization of the Frobenius norm, we propose \\textbf{B-LoRD}, which further enhances the clustering performance. To ensure numerical solvability, we transform the non-convex doubly stochastic constraint into a linear convex constraint through the introduction of a class probability parameter. We further theoretically demonstrate the gradient Lipschitz continuity of our LoRD and B-LoRD enables the proposal of a globally convergent projected gradient descent algorithm for their optimization. Extensive experiments validate the effectiveness of our approaches. The code is publicly available at https://github.com/lwl-learning/LoRD.",
        "arxiv_id": "2509.18826"
    },
    "2509.19018": {
        "SCORE": 14,
        "ARXIVID": "2509.19018",
        "COMMENT": "Model Architecture: unified multimodal framework with bidirectional latent alignment; Representation Learning: shared cross-modal latent space via semantic-guided diffusion alignment.",
        "RELEVANCE": 7,
        "NOVELTY": 7,
        "authors": [
            "Teng Xiao",
            "Zuchao Li",
            "Lefei Zhang"
        ],
        "title": "OmniBridge: Unified Multimodal Understanding, Generation, and Retrieval via Latent Space Alignment",
        "abstract": "Recent advances in multimodal large language models (LLMs) have led to significant progress in understanding, generation, and retrieval tasks. However, current solutions often treat these tasks in isolation or require training LLMs from scratch, resulting in high computational costs and limited generalization across modalities. In this work, we present OmniBridge, a unified and modular multimodal framework that supports vision-language understanding, generation, and retrieval within a unified architecture. OmniBridge adopts a language-centric design that reuses pretrained LLMs and introduces a lightweight bidirectional latent alignment module. To address the challenge of task interference, we propose a two-stage decoupled training strategy: supervised fine-tuning and latent space alignment for aligning LLM behavior with multimodal reasoning, and semantic-guided diffusion training to align cross-modal latent spaces via learnable query embeddings. Extensive experiments across a wide range of benchmarks demonstrate that OmniBridge achieves competitive or state-of-the-art performance in all three tasks. Moreover, our results highlight the effectiveness of latent space alignment for unifying multimodal modeling under a shared representation space. Code and models are released at https://github.com/xiao-xt/OmniBridge.",
        "arxiv_id": "2509.19018"
    },
    "2509.18121": {
        "SCORE": 14,
        "ARXIVID": "2509.18121",
        "COMMENT": "High Performance/Efficiency: hardware-aware training on memristive devices with a symmetry-point shifting algorithm to improve energy\u2013convergence trade-offs.",
        "RELEVANCE": 7,
        "NOVELTY": 7,
        "authors": [
            "Nikhil Garg",
            "Paul Uriarte Vicandi",
            "Yanming Zhang",
            "Alexandre Baigol",
            "Donato Francesco Falcone",
            "Saketh Ram Mamidala",
            "Bert Jan Offrein",
            "Laura B\\'egon-Lours"
        ],
        "title": "Energy-convergence trade off for the training of neural networks on bio-inspired hardware",
        "abstract": "The increasing deployment of wearable sensors and implantable devices is shifting AI processing demands to the extreme edge, necessitating ultra-low power for continuous operation. Inspired by the brain, emerging memristive devices promise to accelerate neural network training by eliminating costly data transfers between compute and memory. Though, balancing performance and energy efficiency remains a challenge. We investigate ferroelectric synaptic devices based on HfO2/ZrO2 superlattices and feed their experimentally measured weight updates into hardware-aware neural network simulations. Across pulse widths from 20 ns to 0.2 ms, shorter pulses lower per-update energy but require more training epochs while still reducing total energy without sacrificing accuracy. Classification accuracy using plain stochastic gradient descent (SGD) is diminished compared to mixed-precision SGD. We analyze the causes and propose a ``symmetry point shifting'' technique, addressing asymmetric updates and restoring accuracy. These results highlight a trade-off among accuracy, convergence speed, and energy use, showing that short-pulse programming with tailored training significantly enhances on-chip learning efficiency.",
        "arxiv_id": "2509.18121"
    },
    "2509.16598": {
        "SCORE": 14,
        "ARXIVID": "2509.16598",
        "COMMENT": "Compression/Efficiency + Decoding: leverages layer pruning to build a contrastive auxiliary model for factuality-improving decoding with minimal overhead.",
        "RELEVANCE": 7,
        "NOVELTY": 7,
        "authors": [
            "Byeongho Yu",
            "Changhun Lee",
            "Jungyu Jin",
            "Eunhyeok Park"
        ],
        "title": "PruneCD: Contrasting Pruned Self Model to Improve Decoding Factuality",
        "abstract": "To mitigate the hallucination problem in large language models, DoLa exploits early exit logits from the same model as a contrastive prior. However, we found that these early exit logits tend to be flat, low in magnitude, and fail to reflect meaningful contrasts. To address this, we propose PruneCD, a novel contrastive decoding method that constructs the amateur model via layer pruning rather than early exit. This design leads to more informative and well-aligned logits, enabling more effective contrastive decoding. Through qualitative and quantitative analyses, we demonstrate that PruneCD consistently improves factuality with minimal inference overhead, offering a robust and practical approach to mitigating hallucinations in LLMs.",
        "arxiv_id": "2509.16598"
    },
    "2509.18653": {
        "SCORE": 14,
        "ARXIVID": "2509.18653",
        "COMMENT": "Representation learning: subspace/dictionary learning via tensor block-term decomposition with identifiability results.",
        "RELEVANCE": 7,
        "NOVELTY": 7,
        "authors": [
            "Paris A. Karakasis",
            "Nicholas D. Sidiropoulos"
        ],
        "title": "Subspace Clustering of Subspaces: Unifying Canonical Correlation Analysis and Subspace Clustering",
        "abstract": "We introduce a novel framework for clustering a collection of tall matrices based on their column spaces, a problem we term Subspace Clustering of Subspaces (SCoS). Unlike traditional subspace clustering methods that assume vectorized data, our formulation directly models each data sample as a matrix and clusters them according to their underlying subspaces. We establish conceptual links to Subspace Clustering and Generalized Canonical Correlation Analysis (GCCA), and clarify key differences that arise in this more general setting. Our approach is based on a Block Term Decomposition (BTD) of a third-order tensor constructed from the input matrices, enabling joint estimation of cluster memberships and partially shared subspaces. We provide the first identifiability results for this formulation and propose scalable optimization algorithms tailored to large datasets. Experiments on real-world hyperspectral imaging datasets demonstrate that our method achieves superior clustering accuracy and robustness, especially under high noise and interference, compared to existing subspace clustering techniques. These results highlight the potential of the proposed framework in challenging high-dimensional applications where structure exists beyond individual data vectors.",
        "arxiv_id": "2509.18653"
    },
    "2509.17153": {
        "SCORE": 14,
        "ARXIVID": "2509.17153",
        "COMMENT": "Matches Model Compression/Efficiency: compresses neural weight uncertainty via low-dimensional inducing weights with normalizing-flow priors and spectral regularization.",
        "RELEVANCE": 7,
        "NOVELTY": 7,
        "authors": [
            "Moule Lin",
            "Andrea Patane",
            "Weipeng Jing",
            "Shuhao Guan",
            "Goetz Botterweck"
        ],
        "title": "Flow-Induced Diagonal Gaussian Processes",
        "abstract": "We present Flow-Induced Diagonal Gaussian Processes (FiD-GP), a compression framework that incorporates a compact inducing weight matrix to project a neural network's weight uncertainty into a lower-dimensional subspace. Critically, FiD-GP relies on normalising-flow priors and spectral regularisations to augment its expressiveness and align the inducing subspace with feature-gradient geometry through a numerically stable projection mechanism objective. Furthermore, we demonstrate how the prediction framework in FiD-GP can help to design a single-pass projection for Out-of-Distribution (OoD) detection. Our analysis shows that FiD-GP improves uncertainty estimation ability on various tasks compared with SVGP-based baselines, satisfies tight spectral residual bounds with theoretically guaranteed OoD detection, and significantly compresses the neural network's storage requirements at the cost of increased inference computation dependent on the number of inducing weights employed. Specifically, in a comprehensive empirical study spanning regression, image classification, semantic segmentation, and out-of-distribution detection benchmarks, it cuts Bayesian training cost by several orders of magnitude, compresses parameters by roughly 51%, reduces model size by about 75%, and matches state-of-the-art accuracy and uncertainty estimation.",
        "arxiv_id": "2509.17153"
    },
    "2509.18151": {
        "SCORE": 14,
        "ARXIVID": "2509.18151",
        "COMMENT": "Model Architecture/Representation Learning: hypernetwork-based architecture representation for NAS predictors with global encoding and multi-task loss.",
        "RELEVANCE": 7,
        "NOVELTY": 7,
        "authors": [
            "Jindi Lv",
            "Yuhao Zhou",
            "Yuxin Tian",
            "Qing Ye",
            "Wentao Feng",
            "Jiancheng Lv"
        ],
        "title": "HyperNAS: Enhancing Architecture Representation for NAS Predictor via Hypernetwork",
        "abstract": "Time-intensive performance evaluations significantly impede progress in Neural Architecture Search (NAS). To address this, neural predictors leverage surrogate models trained on proxy datasets, allowing for direct performance predictions for new architectures. However, these predictors often exhibit poor generalization due to their limited ability to capture intricate relationships among various architectures. In this paper, we propose HyperNAS, a novel neural predictor paradigm for enhancing architecture representation learning. HyperNAS consists of two primary components: a global encoding scheme and a shared hypernetwork. The global encoding scheme is devised to capture the comprehensive macro-structure information, while the shared hypernetwork serves as an auxiliary task to enhance the investigation of inter-architecture patterns. To ensure training stability, we further develop a dynamic adaptive multi-task loss to facilitate personalized exploration on the Pareto front. Extensive experiments across five representative search spaces, including ViTs, demonstrate the advantages of HyperNAS, particularly in few-shot scenarios. For instance, HyperNAS strikes new state-of-the-art results, with 97.60\\% top-1 accuracy on CIFAR-10 and 82.4\\% top-1 accuracy on ImageNet, using at least 5.0$\\times$ fewer samples.",
        "arxiv_id": "2509.18151"
    }
}