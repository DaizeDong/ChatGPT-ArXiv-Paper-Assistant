{
    "2509.09001": {
        "SCORE": 19,
        "ARXIVID": "2509.09001",
        "COMMENT": "Compression/Efficiency and Model Architecture \u2014 introduces sub-quadratic Approximate Nearest Neighbor Attention with theoretical guarantees (MPC-equivalence) and connections to low-rank transformers.",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "Jingwen Liu",
            "Hantao Yu",
            "Clayton Sanford",
            "Alexandr Andoni",
            "Daniel Hsu"
        ],
        "title": "Fast attention mechanisms: a tale of parallelism",
        "abstract": "Transformers have the representational capacity to simulate Massively Parallel Computation (MPC) algorithms, but they suffer from quadratic time complexity, which severely limits their scalability. We introduce an efficient attention mechanism called Approximate Nearest Neighbor Attention (ANNA) with sub-quadratic time complexity. We prove that ANNA-transformers (1) retain the expressive power previously established for standard attention in terms of matching the capabilities of MPC algorithms, and (2) can solve key reasoning tasks such as Match2 and $k$-hop with near-optimal depth. Using the MPC framework, we further prove that constant-depth ANNA-transformers can simulate constant-depth low-rank transformers, thereby providing a unified way to reason about a broad class of efficient attention approximations.",
        "arxiv_id": "2509.09001"
    },
    "2509.09660": {
        "SCORE": 18,
        "ARXIVID": "2509.09660",
        "COMMENT": "Model Architecture (MoE): identifies behavior-linked experts via activation patterns and steers behavior by selective expert (de)activation at inference without retraining.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Mohsen Fayyaz",
            "Ali Modarressi",
            "Hanieh Deilamsalehy",
            "Franck Dernoncourt",
            "Ryan Rossi",
            "Trung Bui",
            "Hinrich Sch\\\"utze",
            "Nanyun Peng"
        ],
        "title": "Steering MoE LLMs via Expert (De)Activation",
        "abstract": "Mixture-of-Experts (MoE) in Large Language Models (LLMs) routes each token through a subset of specialized Feed-Forward Networks (FFN), known as experts. We present SteerMoE, a framework for steering MoE models by detecting and controlling behavior-linked experts. Our detection method identifies experts with distinct activation patterns across paired inputs exhibiting contrasting behaviors. By selectively (de)activating such experts during inference, we control behaviors like faithfulness and safety without retraining or modifying weights. Across 11 benchmarks and 6 LLMs, our steering raises safety by up to +20% and faithfulness by +27%. In adversarial attack mode, it drops safety by -41% alone, and -100% when combined with existing jailbreak methods, bypassing all safety guardrails and exposing a new dimension of alignment faking hidden within experts.",
        "arxiv_id": "2509.09660"
    },
    "2509.09679": {
        "SCORE": 18,
        "ARXIVID": "2509.09679",
        "COMMENT": "Model Compression and Efficiency: ultra-low-bit LLM quantization via learnable orthogonal butterfly transforms (O(n log n) complexity) enabling layer-adaptive rotations for outlier suppression.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Bingxin Xu",
            "Zhen Dong",
            "Oussama Elachqar",
            "Yuzhang Shang"
        ],
        "title": "ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable Orthogonal Butterfly Transforms",
        "abstract": "Large language models require massive memory footprints, severely limiting deployment on consumer hardware. Quantization reduces memory through lower numerical precision, but extreme 2-bit quantization suffers from catastrophic performance loss due to outliers in activations. Rotation-based methods such as QuIP and QuaRot apply orthogonal transforms to eliminate outliers before quantization, using computational invariance: $\\mathbf{y} = \\mathbf{Wx} = (\\mathbf{WQ}^T)(\\mathbf{Qx})$ for orthogonal $\\mathbf{Q}$. However, these methods use fixed transforms--Hadamard matrices achieving optimal worst-case coherence $\\mu = 1/\\sqrt{n}$--that cannot adapt to specific weight distributions. We identify that different transformer layers exhibit distinct outlier patterns, motivating layer-adaptive rotations rather than one-size-fits-all approaches. We propose ButterflyQuant, which replaces Hadamard rotations with learnable butterfly transforms parameterized by continuous Givens rotation angles. Unlike Hadamard's discrete $\\{+1, -1\\}$ entries that are non-differentiable and prohibit gradient-based learning, butterfly transforms' continuous parameterization enables smooth optimization while guaranteeing orthogonality by construction. This orthogonal constraint ensures theoretical guarantees in outlier suppression while achieving $O(n \\log n)$ computational complexity with only $\\frac{n \\log n}{2}$ learnable parameters. We further introduce a uniformity regularization on post-transformation activations to promote smoother distributions amenable to quantization. Learning requires only 128 calibration samples and converges in minutes on a single GPU--a negligible one-time cost. On LLaMA-2-7B with 2-bit quantization, ButterflyQuant achieves 15.4 perplexity versus 22.1 for QuaRot.",
        "arxiv_id": "2509.09679"
    },
    "2509.09371": {
        "SCORE": 17,
        "ARXIVID": "2509.09371",
        "COMMENT": "Representation Learning: introduces representation-aware Wasserstein DRO with theoretical reformulations (seminorm regularization equivalence) and an optimization method on the solution surface.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Zitao Wang",
            "Nian Si",
            "Molei Liu"
        ],
        "title": "Representation-Aware Distributionally Robust Optimization: A Knowledge Transfer Framework",
        "abstract": "We propose REpresentation-Aware Distributionally Robust Estimation (READ), a novel framework for Wasserstein distributionally robust learning that accounts for predictive representations when guarding against distributional shifts. Unlike classical approaches that treat all feature perturbations equally, READ embeds a multidimensional alignment parameter into the transport cost, allowing the model to differentially discourage perturbations along directions associated with informative representations. This yields robustness to feature variation while preserving invariant structure. Our first contribution is a theoretical foundation: we show that seminorm regularizations for linear regression and binary classification arise as Wasserstein distributionally robust objectives, thereby providing tractable reformulations of READ and unifying a broad class of regularized estimators under the DRO lens. Second, we adopt a principled procedure for selecting the Wasserstein radius using the techniques of robust Wasserstein profile inference. This further enables the construction of valid, representation-aware confidence regions for model parameters with distinct geometric features. Finally, we analyze the geometry of READ estimators as the alignment parameters vary and propose an optimization algorithm to estimate the projection of the global optimum onto this solution surface. This procedure selects among equally robust estimators while optimally constructing a representation structure. We conclude by demonstrating the effectiveness of our framework through extensive simulations and a real-world study, providing a powerful robust estimation grounded in learning representation.",
        "arxiv_id": "2509.09371"
    },
    "2509.09088": {
        "SCORE": 17,
        "ARXIVID": "2509.09088",
        "COMMENT": "Foundational analysis of deep linear networks via Riemannian geometry and an entropy formula\u2014insights into representation/training dynamics.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Govind Menon",
            "Tianmin Yu"
        ],
        "title": "An entropy formula for the Deep Linear Network",
        "abstract": "We study the Riemannian geometry of the Deep Linear Network (DLN) as a foundation for a thermodynamic description of the learning process. The main tools are the use of group actions to analyze overparametrization and the use of Riemannian submersion from the space of parameters to the space of observables. The foliation of the balanced manifold in the parameter space by group orbits is used to define and compute a Boltzmann entropy. We also show that the Riemannian geometry on the space of observables defined in [2] is obtained by Riemannian submersion of the balanced manifold. The main technical step is an explicit construction of an orthonormal basis for the tangent space of the balanced manifold using the theory of Jacobi matrices.",
        "arxiv_id": "2509.09088"
    },
    "2509.09424": {
        "SCORE": 17,
        "ARXIVID": "2509.09424",
        "COMMENT": "HE\u2013LLM co-design (BitNet integration, sigmoid attention under HE, bootstrapping fused with RMSNorm) for efficient secure inference\u2014systems/efficiency innovation.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Zhiyu He",
            "Maojiang Wang",
            "Xinwen Gao",
            "Yuchuan Luo",
            "Lin Liu",
            "Shaojing Fu"
        ],
        "title": "ENSI: Efficient Non-Interactive Secure Inference for Large Language Models",
        "abstract": "Secure inference enables privacy-preserving machine learning by leveraging cryptographic protocols that support computations on sensitive user data without exposing it. However, integrating cryptographic protocols with large language models (LLMs) presents significant challenges, as the inherent complexity of these protocols, together with LLMs' massive parameter scale and sophisticated architectures, severely limits practical usability. In this work, we propose ENSI, a novel non-interactive secure inference framework for LLMs, based on the principle of co-designing the cryptographic protocols and LLM architecture. ENSI employs an optimized encoding strategy that seamlessly integrates CKKS scheme with a lightweight LLM variant, BitNet, significantly reducing the computational complexity of encrypted matrix multiplications. In response to the prohibitive computational demands of softmax under homomorphic encryption (HE), we pioneer the integration of the sigmoid attention mechanism with HE as a seamless, retraining-free alternative. Furthermore, by embedding the Bootstrapping operation within the RMSNorm process, we efficiently refresh ciphertexts while markedly decreasing the frequency of costly bootstrapping invocations. Experimental evaluations demonstrate that ENSI achieves approximately an 8x acceleration in matrix multiplications and a 2.6x speedup in softmax inference on CPU compared to state-of-the-art method, with the proportion of bootstrapping is reduced to just 1%.",
        "arxiv_id": "2509.09424"
    },
    "2509.09090": {
        "SCORE": 17,
        "ARXIVID": "2509.09090",
        "COMMENT": "Model Compression and Efficiency: co-designed quantization-aware token pruning with improved quantizer; training-free, structured inference acceleration for VLA models.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Hengyu Fang",
            "Yijiang Liu",
            "Yuan Du",
            "Li Du",
            "Huanrui Yang"
        ],
        "title": "SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models",
        "abstract": "Vision-Language-Action (VLA) models exhibit unprecedented capabilities for embodied intelligence. However, their extensive computational and memory costs hinder their practical deployment. Existing VLA compression and acceleration approaches conduct quantization or token pruning in an ad-hoc manner but fail to enable both for a holistic efficiency improvement due to an observed incompatibility. This work introduces SQAP-VLA, the first structured, training-free VLA inference acceleration framework that simultaneously enables state-of-the-art quantization and token pruning. We overcome the incompatibility by co-designing the quantization and token pruning pipeline, where we propose new quantization-aware token pruning criteria that work on an aggressively quantized model while improving the quantizer design to enhance pruning effectiveness. When applied to standard VLA models, SQAP-VLA yields significant gains in computational efficiency and inference speed while successfully preserving core model performance, achieving a $\\times$1.93 speedup and up to a 4.5\\% average success rate enhancement compared to the original model.",
        "arxiv_id": "2509.09090"
    },
    "2509.09362": {
        "SCORE": 17,
        "ARXIVID": "2509.09362",
        "COMMENT": "Representation Learning/Theory: simultaneous Sobolev approximation on manifolds with bounded-weight ReLU^k networks and matching lower bounds, leveraging sparse architectural structure.",
        "RELEVANCE": 8,
        "NOVELTY": 9,
        "authors": [
            "Hanfei Zhou",
            "Lei Shi"
        ],
        "title": "Expressive Power of Deep Networks on Manifolds: Simultaneous Approximation",
        "abstract": "A key challenge in scientific machine learning is solving partial differential equations (PDEs) on complex domains, where the curved geometry complicates the approximation of functions and their derivatives required by differential operators. This paper establishes the first simultaneous approximation theory for deep neural networks on manifolds. We prove that a constant-depth $\\mathrm{ReLU}^{k-1}$ network with bounded weights--a property that plays a crucial role in controlling generalization error--can approximate any function in the Sobolev space $\\mathcal{W}_p^{k}(\\mathcal{M}^d)$ to an error of $\\varepsilon$ in the $\\mathcal{W}_p^{s}(\\mathcal{M}^d)$ norm, for $k\\geq 3$ and $s<k$, using $\\mathcal{O}(\\varepsilon^{-d/(k-s)})$ nonzero parameters, a rate that overcomes the curse of dimensionality by depending only on the intrinsic dimension $d$. These results readily extend to functions in H\\\"older-Zygmund spaces. We complement this result with a matching lower bound, proving our construction is nearly optimal by showing the required number of parameters matches up to a logarithmic factor. Our proof of the lower bound introduces novel estimates for the Vapnik-Chervonenkis dimension and pseudo-dimension of the network's high-order derivative classes. These complexity bounds provide a theoretical cornerstone for learning PDEs on manifolds involving derivatives. Our analysis reveals that the network architecture leverages a sparse structure to efficiently exploit the manifold's low-dimensional geometry.",
        "arxiv_id": "2509.09362"
    },
    "2509.09052": {
        "SCORE": 16,
        "ARXIVID": "2509.09052",
        "COMMENT": "Model Architecture (MoE): ViT-based gating for conditional per-grid, per-lead-time expert weighting to combine multiple forecasters; also emphasizes computational efficiency of training the gating network.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Dibyajyoti Chakraborty",
            "Romit Maulik",
            "Peter Harrington",
            "Dallas Foster",
            "Mohammad Amin Nabian",
            "Sanjay Choudhry"
        ],
        "title": "MoWE : A Mixture of Weather Experts",
        "abstract": "Data-driven weather models have recently achieved state-of-the-art performance, yet progress has plateaued in recent years. This paper introduces a Mixture of Experts (MoWE) approach as a novel paradigm to overcome these limitations, not by creating a new forecaster, but by optimally combining the outputs of existing models. The MoWE model is trained with significantly lower computational resources than the individual experts. Our model employs a Vision Transformer-based gating network that dynamically learns to weight the contributions of multiple \"expert\" models at each grid point, conditioned on forecast lead time. This approach creates a synthesized deterministic forecast that is more accurate than any individual component in terms of Root Mean Squared Error (RMSE). Our results demonstrate the effectiveness of this method, achieving up to a 10% lower RMSE than the best-performing AI weather model on a 2-day forecast horizon, significantly outperforming individual experts as well as a simple average across experts. This work presents a computationally efficient and scalable strategy to push the state of the art in data-driven weather prediction by making the most out of leading high-quality forecast models.",
        "arxiv_id": "2509.09052"
    },
    "2509.09119": {
        "SCORE": 16,
        "ARXIVID": "2509.09119",
        "COMMENT": "PEFT with low-rank adaptation (LoRA) using Hessian-based sensitivity for dynamic rank allocation\u2014compression/efficiency criterion.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Hao Zhang",
            "Bo Huang",
            "Zhenjia Li",
            "Xi Xiao",
            "Hui Yi Leong",
            "Zumeng Zhang",
            "Xinwei Long",
            "Tianyang Wang",
            "Hao Xu"
        ],
        "title": "Sensitivity-LoRA: Low-Load Sensitivity-Based Fine-Tuning for Large Language Models",
        "abstract": "Large Language Models (LLMs) have transformed both everyday life and scientific research. However, adapting LLMs from general-purpose models to specialized tasks remains challenging, particularly in resource-constrained environments. Low-Rank Adaptation (LoRA), a prominent method within Parameter-Efficient Fine-Tuning (PEFT), has emerged as a promising approach to LLMs by approximating model weight updates using low-rank decomposition. However, LoRA is limited by its uniform rank ( r ) allocation to each incremental matrix, and existing rank allocation techniques aimed at addressing this issue remain computationally inefficient, complex, and unstable, hindering practical applications. To address these limitations, we propose Sensitivity-LoRA, an efficient fine-tuning method that dynamically allocates ranks to weight matrices based on both their global and local sensitivities. It leverages the second-order derivatives (Hessian Matrix) of the loss function to effectively capture weight sensitivity, enabling optimal rank allocation with minimal computational overhead. Our experimental results have demonstrated robust effectiveness, efficiency and stability of Sensitivity-LoRA across diverse tasks and benchmarks.",
        "arxiv_id": "2509.09119"
    },
    "2509.09337": {
        "SCORE": 16,
        "ARXIVID": "2509.09337",
        "COMMENT": "Model Architecture \u2014 Mixture-of-Experts for graphs (Mixture of Subgraph Experts) with dynamic routing over subgraph semantics and formal expressivity beyond SWL.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Junda Ye",
            "Zhongbao Zhang",
            "Li Sun",
            "Siqiang Luo"
        ],
        "title": "MoSE: Unveiling Structural Patterns in Graphs via Mixture of Subgraph Experts",
        "abstract": "While graph neural networks (GNNs) have achieved great success in learning from graph-structured data, their reliance on local, pairwise message passing restricts their ability to capture complex, high-order subgraph patterns. leading to insufficient structural expressiveness. Recent efforts have attempted to enhance structural expressiveness by integrating random walk kernels into GNNs. However, these methods are inherently designed for graph-level tasks, which limits their applicability to other downstream tasks such as node classification. Moreover, their fixed kernel configurations hinder the model's flexibility in capturing diverse subgraph structures. To address these limitations, this paper proposes a novel Mixture of Subgraph Experts (MoSE) framework for flexible and expressive subgraph-based representation learning across diverse graph tasks. Specifically, MoSE extracts informative subgraphs via anonymous walks and dynamically routes them to specialized experts based on structural semantics, enabling the model to capture diverse subgraph patterns with improved flexibility and interpretability. We further provide a theoretical analysis of MoSE's expressivity within the Subgraph Weisfeiler-Lehman (SWL) Test, proving that it is more powerful than SWL. Extensive experiments, together with visualizations of learned subgraph experts, demonstrate that MoSE not only outperforms competitive baselines but also provides interpretable insights into structural patterns learned by the model.",
        "arxiv_id": "2509.09337"
    },
    "2509.09611": {
        "SCORE": 16,
        "ARXIVID": "2509.09611",
        "COMMENT": "Strong match to Model Architecture and Representation Learning: reduced-basis\u2013driven adaptive network construction yielding compact operator learners with discretization invariance.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Haolan Zheng",
            "Yanlai Chen",
            "Jiequn Han",
            "Yue Yu"
        ],
        "title": "ReBaNO: Reduced Basis Neural Operator Mitigating Generalization Gaps and Achieving Discretization Invariance",
        "abstract": "We propose a novel data-lean operator learning algorithm, the Reduced Basis Neural Operator (ReBaNO), to solve a group of PDEs with multiple distinct inputs. Inspired by the Reduced Basis Method and the recently introduced Generative Pre-Trained Physics-Informed Neural Networks, ReBaNO relies on a mathematically rigorous greedy algorithm to build its network structure offline adaptively from the ground up. Knowledge distillation via task-specific activation function allows ReBaNO to have a compact architecture requiring minimal computational cost online while embedding physics. In comparison to state-of-the-art operator learning algorithms such as PCA-Net, DeepONet, FNO, and CNO, numerical results demonstrate that ReBaNO significantly outperforms them in terms of eliminating/shrinking the generalization gap for both in- and out-of-distribution tests and being the only operator learning algorithm achieving strict discretization invariance.",
        "arxiv_id": "2509.09611"
    },
    "2509.09429": {
        "SCORE": 15,
        "ARXIVID": "2509.09429",
        "COMMENT": "Representation Learning \u2014 proposes explicit semantic concentration for dense SSL via a noise-tolerant AP-based ranking loss and an object-aware prototype filtering mechanism for patch features.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Peisong Wen",
            "Qianqian Xu",
            "Siran Dai",
            "Runmin Cong",
            "Qingming Huang"
        ],
        "title": "Semantic Concentration for Self-Supervised Dense Representations Learning",
        "abstract": "Recent advances in image-level self-supervised learning (SSL) have made significant progress, yet learning dense representations for patches remains challenging. Mainstream methods encounter an over-dispersion phenomenon that patches from the same instance/category scatter, harming downstream performance on dense tasks. This work reveals that image-level SSL avoids over-dispersion by involving implicit semantic concentration. Specifically, the non-strict spatial alignment ensures intra-instance consistency, while shared patterns, i.e., similar parts of within-class instances in the input space, ensure inter-image consistency. Unfortunately, these approaches are infeasible for dense SSL due to their spatial sensitivity and complicated scene-centric data. These observations motivate us to explore explicit semantic concentration for dense SSL. First, to break the strict spatial alignment, we propose to distill the patch correspondences. Facing noisy and imbalanced pseudo labels, we propose a noise-tolerant ranking loss. The core idea is extending the Average Precision (AP) loss to continuous targets, such that its decision-agnostic and adaptive focusing properties prevent the student model from being misled. Second, to discriminate the shared patterns from complicated scenes, we propose the object-aware filter to map the output space to an object-based space. Specifically, patches are represented by learnable prototypes of objects via cross-attention. Last but not least, empirical studies across various tasks soundly support the effectiveness of our method. Code is available in https://github.com/KID-7391/CoTAP.",
        "arxiv_id": "2509.09429"
    },
    "2509.09485": {
        "SCORE": 15,
        "ARXIVID": "2509.09485",
        "COMMENT": "Matches Model Compression and Efficiency via random projection\u2013accelerated SGD and dynamic DP (algorithmic optimizer improving training efficiency with provable rates).",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Zhanhong Jiang",
            "Md Zahid Hasan",
            "Nastaran Saadati",
            "Aditya Balu",
            "Chao Liu",
            "Soumik Sarkar"
        ],
        "title": "Balancing Utility and Privacy: Dynamically Private SGD with Random Projection",
        "abstract": "Stochastic optimization is a pivotal enabler in modern machine learning, producing effective models for various tasks. However, several existing works have shown that model parameters and gradient information are susceptible to privacy leakage. Although Differentially Private SGD (DPSGD) addresses privacy concerns, its static noise mechanism impacts the error bounds for model performance. Additionally, with the exponential increase in model parameters, efficient learning of these models using stochastic optimizers has become more challenging. To address these concerns, we introduce the Dynamically Differentially Private Projected SGD (D2P2-SGD) optimizer. In D2P2-SGD, we combine two important ideas: (i) dynamic differential privacy (DDP) with automatic gradient clipping and (ii) random projection with SGD, allowing dynamic adjustment of the tradeoff between utility and privacy of the model. It exhibits provably sub-linear convergence rates across different objective functions, matching the best available rate. The theoretical analysis further suggests that DDP leads to better utility at the cost of privacy, while random projection enables more efficient model learning. Extensive experiments across diverse datasets show that D2P2-SGD remarkably enhances accuracy while maintaining privacy. Our code is available here.",
        "arxiv_id": "2509.09485"
    },
    "2509.08942": {
        "SCORE": 15,
        "ARXIVID": "2509.08942",
        "COMMENT": "Representation Learning/Robust Training: group-level Wasserstein DRO to optimize worst-group performance under distributional uncertainty with a descent\u2013ascent algorithm and convergence results.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Xenia Konti",
            "Yi Shen",
            "Zifan Wang",
            "Karl Henrik Johansson",
            "Michael J. Pencina",
            "Nicoleta J. Economou-Zavlanos",
            "Michael M. Zavlanos"
        ],
        "title": "Group Distributionally Robust Machine Learning under Group Level Distributional Uncertainty",
        "abstract": "The performance of machine learning (ML) models critically depends on the quality and representativeness of the training data. In applications with multiple heterogeneous data generating sources, standard ML methods often learn spurious correlations that perform well on average but degrade performance for atypical or underrepresented groups. Prior work addresses this issue by optimizing the worst-group performance. However, these approaches typically assume that the underlying data distributions for each group can be accurately estimated using the training data, a condition that is frequently violated in noisy, non-stationary, and evolving environments. In this work, we propose a novel framework that relies on Wasserstein-based distributionally robust optimization (DRO) to account for the distributional uncertainty within each group, while simultaneously preserving the objective of improving the worst-group performance. We develop a gradient descent-ascent algorithm to solve the proposed DRO problem and provide convergence results. Finally, we validate the effectiveness of our method on real-world data.",
        "arxiv_id": "2509.08942"
    },
    "2509.08954": {
        "SCORE": 14,
        "ARXIVID": "2509.08954",
        "COMMENT": "Training dynamics/optimization theory \u2014 derives sharp step-size thresholds for convexity of GD optimization curves and links discrete GD with continuous-time gradient flow.",
        "RELEVANCE": 7,
        "NOVELTY": 7,
        "authors": [
            "Le Duc Hieu"
        ],
        "title": "Convexity of Optimization Curves: Local Sharp Thresholds, Robustness Impossibility, and New Counterexamples",
        "abstract": "We study when the \\emph{optimization curve} of first-order methods -- the sequence \\${f(x\\_n)}*{n\\ge0}\\$ produced by constant-stepsize iterations -- is convex, equivalently when the forward differences \\$f(x\\_n)-f(x*{n+1})\\$ are nonincreasing. For gradient descent (GD) on convex \\$L\\$-smooth functions, the curve is convex for all stepsizes \\$\\eta \\le 1.75/L\\$, and this threshold is tight. Moreover, gradient norms are nonincreasing for all \\$\\eta \\le 2/L\\$, and in continuous time (gradient flow) the curve is always convex. These results complement and refine the classical smooth convex optimization toolbox, connecting discrete and continuous dynamics as well as worst-case analyses.",
        "arxiv_id": "2509.08954"
    },
    "2509.08972": {
        "SCORE": 14,
        "ARXIVID": "2509.08972",
        "COMMENT": "Training dynamics/loss design \u2014 introduces Truncated Cross Entropy to mitigate recursive training collapse by down-weighting overconfident predictions with theoretical and cross-modal support.",
        "RELEVANCE": 7,
        "NOVELTY": 7,
        "authors": [
            "Soheil Zibakhsh Shabgahi",
            "Pedram Aghazadeh",
            "Azalia Mirhosseini",
            "Farinaz Koushanfar"
        ],
        "title": "ForTIFAI: Fending Off Recursive Training Induced Failure for AI Models",
        "abstract": "The increasing reliance on generative AI models has accelerated the generation rate of synthetic data, with some projections suggesting that most available new data for training could be machine-generated by 2030. This shift to a mainly synthetic content presents a critical challenge: repeated training in synthetic data leads to a phenomenon known as model collapse, where model performance degrades over generations of training, eventually rendering the models ineffective. Although prior studies have explored the causes and detection of model collapse, existing mitigation strategies remain limited.   In this paper, we identify model overconfidence in their self-generated data as a key driver of collapse. Building on this observation, we propose a confidence-aware loss function that downweights high-confidence predictions during training. We introduce a novel loss function we call Truncated Cross Entropy (TCE). We demonstrate that TCE significantly delays model collapse in recursive training.   We provide a model-agnostic framework that links the loss function design to model collapse mitigation and validate our approach both theoretically and empirically, showing that it can extend the model's fidelity interval before collapse by more than 2.3x. Finally, we show that our method generalizes across modalities. These findings suggest that the design of loss functions provides a simple yet powerful tool for preserving the quality of generative models in the era of increasing synthetic data.",
        "arxiv_id": "2509.08972"
    },
    "2509.09677": {
        "SCORE": 14,
        "ARXIVID": "2509.09677",
        "COMMENT": "Representation Learning/Training Dynamics: analyzes long-horizon execution, identifies self-conditioning error effects, and proposes a measurement framework for execution capability.",
        "RELEVANCE": 7,
        "NOVELTY": 7,
        "authors": [
            "Akshit Sinha",
            "Arvindh Arun",
            "Shashwat Goel",
            "Steffen Staab",
            "Jonas Geiping"
        ],
        "title": "The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs",
        "abstract": "Does continued scaling of large language models (LLMs) yield diminishing returns? Real-world value often stems from the length of task an agent can complete. We start this work by observing the simple but counterintuitive fact that marginal gains in single-step accuracy can compound into exponential improvements in the length of a task a model can successfully complete. Then, we argue that failures of LLMs when simple tasks are made longer arise from mistakes in execution, rather than an inability to reason. We propose isolating execution capability, by explicitly providing the knowledge and plan needed to solve a long-horizon task. We find that larger models can correctly execute significantly more turns even when small models have 100\\% single-turn accuracy. We observe that the per-step accuracy of models degrades as the number of steps increases. This is not just due to long-context limitations -- curiously, we observe a self-conditioning effect -- models become more likely to make mistakes when the context contains their errors from prior turns. Self-conditioning does not reduce by just scaling the model size. In contrast, recent thinking models do not self-condition, and can also execute much longer tasks in a single turn. We conclude by benchmarking frontier thinking models on the length of task they can execute in a single turn. Overall, by focusing on the ability to execute, we hope to reconcile debates on how LLMs can solve complex reasoning problems yet fail at simple tasks when made longer, and highlight the massive benefits of scaling model size and sequential test-time compute for long-horizon tasks.",
        "arxiv_id": "2509.09677"
    }
}