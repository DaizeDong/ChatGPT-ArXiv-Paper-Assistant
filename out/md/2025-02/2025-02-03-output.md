# Personalized Daily Arxiv Papers 02/03/2025

|           | Prompt   | Completion   | Total   |
|-----------|----------|--------------|---------|
| **Token** | 21638    | 1823         | 23461   |
| **Cost**  | $0.54    | $0.18        | $0.72   |

Total scanned papers: 298

Total relevant papers: 9

**Table of contents with paper titles:**

1. [Statistical Physics of Deep Neural Networks: Generalization Capability, Beyond the Infinite Width, and Feature Learning](#user-content-link1)
**Authors:** Sebastiano Ariosto

2. [Structure Development in List-Sorting Transformers](#user-content-link2)
**Authors:** Einar Urdshals, Jasmina Urdshals

3. [Scalable-Softmax Is Superior for Attention](#user-content-link3)
**Authors:** Ken M. Nakanishi

4. [Position: Curvature Matrices Should Be Democratized via Linear Operators](#user-content-link4)
**Authors:** Felix Dangel, Runa Eschenhagen, Weronika Ormaniec, Andres Fernandez, Lukas Tatzel, Agustinus Kristiadi

5. [Adaptivity and Convergence of Probability Flow ODEs in Diffusion Generative Models](#user-content-link5)
**Authors:** Jiaqi Tang, Yuling Yan

6. [On the inductive bias of infinite-depth ResNets and the bottleneck rank](#user-content-link6)
**Authors:** Enric Boix-Adsera

7. [GPO-VAE: Modeling Explainable Gene Perturbation Responses utilizing GRN-Aligned Parameter Optimization](#user-content-link7)
**Authors:** Seungheun Baek, Soyon Park, Yan Ting Chok, Mogan Gim, Jaewoo Kang

8. [Estimating the Probability of Sampling a Trained Neural Network at Random](#user-content-link8)
**Authors:** Adam Scherlis, Nora Belrose

9. [Relating Misfit to Gain in Weak-to-Strong Generalization Beyond the Squared Loss](#user-content-link9)
**Authors:** Abhijeet Mulgund, Chirag Pabbaraju

---

## 1. [Statistical Physics of Deep Neural Networks: Generalization Capability, Beyond the Infinite Width, and Feature Learning](https://arxiv.org/abs/2501.19281) <a id="link1"></a>

**ArXiv ID:** 2501.19281

**Authors:** Sebastiano Ariosto

**Abstract:** Deep Neural Networks (DNNs) excel at many tasks, often rivaling or surpassing human performance. Yet their internal processes remain elusive, frequently described as "black boxes." While performance can be refined experimentally, achieving a fundamental grasp of their inner workings is still a challenge.   Statistical Mechanics has long tackled computational problems, and this thesis applies physics-based insights to understand DNNs via three complementary approaches.   First, by averaging over data, we derive an asymptotic bound on generalization that depends solely on the size of the last layer, rather than on the total number of parameters -- revealing how deep architectures process information differently across layers.   Second, adopting a data-dependent viewpoint, we explore a finite-width thermodynamic limit beyond the infinite-width regime. This leads to: (i) a closed-form expression for the generalization error in a finite-width one-hidden-layer network (regression task); (ii) an approximate partition function for deeper architectures; and (iii) a link between deep networks in this thermodynamic limit and Student's t-processes.   Finally, from a task-explicit perspective, we present a preliminary analysis of how DNNs interact with a controlled dataset, investigating whether they truly internalize its structure -- collapsing to the teacher -- or merely memorize it. By understanding when a network must learn data structure rather than just memorize, it sheds light on fostering meaningful internal representations.   In essence, this thesis leverages the synergy between Statistical Physics and Machine Learning to illuminate the inner behavior of DNNs.

**Comment:** The paper explores deep learning through a statistical physics lens, offering theoretical insights into generalization, finite-width behaviors, and feature learning. It aligns strongly with foundational topics such as representation learning and training dynamics, with potentially transformative insights into neural architectures.

**Relevance:** 9
**Novelty:** 9

---

## 2. [Structure Development in List-Sorting Transformers](https://arxiv.org/abs/2501.18666) <a id="link2"></a>

**ArXiv ID:** 2501.18666

**Authors:** Einar Urdshals, Jasmina Urdshals

**Abstract:** We study how a one-layer attention-only transformer develops relevant structures while learning to sort lists of numbers. At the end of training, the model organizes its attention heads in two main modes that we refer to as vocabulary-splitting and copy-suppression. Both represent simpler modes than having multiple heads handle overlapping ranges of numbers. Interestingly, vocabulary-splitting is present regardless of whether we use weight decay, a common regularization technique thought to drive simplification, supporting the thesis that neural networks naturally prefer simpler solutions. We relate copy-suppression to a mechanism in GPT-2 and investigate its functional role in our model. Guided by insights from a developmental analysis of the model, we identify features in the training data that drive the model's final acquired solution. This provides a concrete example of how the training data shape the internal organization of transformers, paving the way for future studies that could help us better understand how LLMs develop their internal structures.

**Comment:** The paper provides insights into how deep networks encode information, focusing on the development of internal structures in transformers during training. It aligns with the Representation Learning criteria and provides theoretical insights relevant to LLMs as it studies mechanisms resembling those seen in GPT-2.

**Relevance:** 9
**Novelty:** 8

---

## 3. [Scalable-Softmax Is Superior for Attention](https://arxiv.org/abs/2501.19399) <a id="link3"></a>

**ArXiv ID:** 2501.19399

**Authors:** Ken M. Nakanishi

**Abstract:** The maximum element of the vector output by the Softmax function approaches zero as the input vector size increases. Transformer-based language models rely on Softmax to compute attention scores, causing the attention distribution to flatten as the context size grows. This reduces the model's ability to prioritize key information effectively and potentially limits its length generalization. To address this problem, we propose Scalable-Softmax (SSMax), which replaces Softmax in scenarios where the input vector size varies. SSMax can be seamlessly integrated into existing Transformer-based architectures. Experimental results in language modeling show that models using SSMax not only achieve faster loss reduction during pretraining but also significantly improve performance in long contexts and key information retrieval. Furthermore, an analysis of attention scores reveals that SSMax enables the model to focus attention on key information even in long contexts. Additionally, although models that use SSMax from the beginning of pretraining achieve better length generalization, those that have already started pretraining can still gain some of this ability by replacing Softmax in the attention layers with SSMax, either during or after pretraining.

**Comment:** This paper introduces Scalable-Softmax (SSMax), a modification to the attention mechanism in Transformers, addressing issues with attention flattening in long contexts. It is highly relevant under Model Architecture as it proposes a significant improvement to attention mechanisms in foundational Transformer models.

**Relevance:** 9
**Novelty:** 8

---

## 4. [Position: Curvature Matrices Should Be Democratized via Linear Operators](https://arxiv.org/abs/2501.19183) <a id="link4"></a>

**ArXiv ID:** 2501.19183

**Authors:** Felix Dangel, Runa Eschenhagen, Weronika Ormaniec, Andres Fernandez, Lukas Tatzel, Agustinus Kristiadi

**Abstract:** Structured large matrices are prevalent in machine learning. A particularly important class is curvature matrices like the Hessian, which are central to understanding the loss landscape of neural nets (NNs), and enable second-order optimization, uncertainty quantification, model pruning, data attribution, and more. However, curvature computations can be challenging due to the complexity of automatic differentiation, and the variety and structural assumptions of curvature proxies, like sparsity and Kronecker factorization. In this position paper, we argue that linear operators -- an interface for performing matrix-vector products -- provide a general, scalable, and user-friendly abstraction to handle curvature matrices. To support this position, we developed $\textit{curvlinops}$, a library that provides curvature matrices through a unified linear operator interface. We demonstrate with $\textit{curvlinops}$ how this interface can hide complexity, simplify applications, be extensible and interoperable with other libraries, and scale to large NNs.

**Comment:** The paper argues for democratizing curvature matrix computations via linear operators, which is pertinent to understanding loss landscapes and second-order optimization, aligning with foundational research in deep network behavior and sparsity-related methods.

**Relevance:** 9
**Novelty:** 8

---

## 5. [Adaptivity and Convergence of Probability Flow ODEs in Diffusion Generative Models](https://arxiv.org/abs/2501.18863) <a id="link5"></a>

**ArXiv ID:** 2501.18863

**Authors:** Jiaqi Tang, Yuling Yan

**Abstract:** Score-based generative models, which transform noise into data by learning to reverse a diffusion process, have become a cornerstone of modern generative AI. This paper contributes to establishing theoretical guarantees for the probability flow ODE, a widely used diffusion-based sampler known for its practical efficiency. While a number of prior works address its general convergence theory, it remains unclear whether the probability flow ODE sampler can adapt to the low-dimensional structures commonly present in natural image data. We demonstrate that, with accurate score function estimation, the probability flow ODE sampler achieves a convergence rate of $O(k/T)$ in total variation distance (ignoring logarithmic factors), where $k$ is the intrinsic dimension of the target distribution and $T$ is the number of iterations. This dimension-free convergence rate improves upon existing results that scale with the typically much larger ambient dimension, highlighting the ability of the probability flow ODE sampler to exploit intrinsic low-dimensional structures in the target distribution for faster sampling.

**Comment:** The paper introduces a theoretical analysis of the probability flow ODE sampler in diffusion generative models, focusing on its ability to exploit intrinsic low-dimensional structures. This aligns well with cutting-edge theoretical work and insights into representation learning.

**Relevance:** 8
**Novelty:** 8

---

## 6. [On the inductive bias of infinite-depth ResNets and the bottleneck rank](https://arxiv.org/abs/2501.19149) <a id="link6"></a>

**ArXiv ID:** 2501.19149

**Authors:** Enric Boix-Adsera

**Abstract:** We compute the minimum-norm weights of a deep linear ResNet, and find that the inductive bias of this architecture lies between minimizing nuclear norm and rank. This implies that, with appropriate hyperparameters, deep nonlinear ResNets have an inductive bias towards minimizing bottleneck rank.

**Comment:** The study examines the inductive bias of deep ResNets, specifically focusing on bottleneck ranks and the interplay of nuclear norm and rank minimization. This foundational research aligns with the Model Architecture criterion, offering insights into training dynamics.

**Relevance:** 8
**Novelty:** 7

---

## 7. [GPO-VAE: Modeling Explainable Gene Perturbation Responses utilizing GRN-Aligned Parameter Optimization](https://arxiv.org/abs/2501.18973) <a id="link7"></a>

**ArXiv ID:** 2501.18973

**Authors:** Seungheun Baek, Soyon Park, Yan Ting Chok, Mogan Gim, Jaewoo Kang

**Abstract:** Motivation: Predicting cellular responses to genetic perturbations is essential for understanding biological systems and developing targeted therapeutic strategies. While variational autoencoders (VAEs) have shown promise in modeling perturbation responses, their limited explainability poses a significant challenge, as the learned features often lack clear biological meaning. Nevertheless, model explainability is one of the most important aspects in the realm of biological AI. One of the most effective ways to achieve explainability is incorporating the concept of gene regulatory networks (GRNs) in designing deep learning models such as VAEs. GRNs elicit the underlying causal relationships between genes and are capable of explaining the transcriptional responses caused by genetic perturbation treatments. Results: We propose GPO-VAE, an explainable VAE enhanced by GRN-aligned Parameter Optimization that explicitly models gene regulatory networks in the latent space. Our key approach is to optimize the learnable parameters related to latent perturbation effects towards GRN-aligned explainability. Experimental results on perturbation prediction show our model achieves state-of-the-art performance in predicting transcriptional responses across multiple benchmark datasets. Furthermore, additional results on evaluating the GRN inference task reveal our model's ability to generate meaningful GRNs compared to other methods. According to qualitative analysis, GPO-VAE posseses the ability to construct biologically explainable GRNs that align with experimentally validated regulatory pathways. GPO-VAE is available at https://github.com/dmis-lab/GPO-VAE

**Comment:** The paper introduces GPO-VAE, a VAE variant incorporating explainability through GRN-aligned parameter optimization, addressing foundational challenges in representation learning by enhancing biological interpretability. Although domain-specific (biology), the emphasis on latent space structure and optimization aligns with representation learning criteria.

**Relevance:** 7
**Novelty:** 8

---

## 8. [Estimating the Probability of Sampling a Trained Neural Network at Random](https://arxiv.org/abs/2501.18812) <a id="link8"></a>

**ArXiv ID:** 2501.18812

**Authors:** Adam Scherlis, Nora Belrose

**Abstract:** We present an algorithm for estimating the probability mass, under a Gaussian or uniform prior, of a region in neural network parameter space corresponding to a particular behavior, such as achieving test loss below some threshold. When the prior is uniform, this problem is equivalent to measuring the volume of a region. We show empirically and theoretically that existing algorithms for estimating volumes in parameter space underestimate the true volume by millions of orders of magnitude. We find that this error can be dramatically reduced, but not entirely eliminated, with an importance sampling method using gradient information that is already provided by popular optimizers. The negative logarithm of this probability can be interpreted as a measure of a network's information content, in accordance with minimum description length (MDL) principles and rate-distortion theory. As expected, this quantity increases during language model training. We also find that badly-generalizing behavioral regions are smaller, and therefore less likely to be sampled at random, demonstrating an inductive bias towards well-generalizing functions.

**Comment:** The paper explores estimating the probability of sampling neural network behaviors in parameter space and ties this to minimum description length principles and inductive biases. This connects to insights about training dynamics and generalization in representation learning.

**Relevance:** 8
**Novelty:** 7

---

## 9. [Relating Misfit to Gain in Weak-to-Strong Generalization Beyond the Squared Loss](https://arxiv.org/abs/2501.19105) <a id="link9"></a>

**ArXiv ID:** 2501.19105

**Authors:** Abhijeet Mulgund, Chirag Pabbaraju

**Abstract:** The paradigm of weak-to-strong generalization constitutes the training of a strong AI model on data labeled by a weak AI model, with the goal that the strong model nevertheless outperforms its weak supervisor on the target task of interest. For the setting of real-valued regression with the squared loss, recent work quantitatively characterizes the gain in performance of the strong model over the weak model in terms of the misfit between the strong and weak model. We generalize such a characterization to learning tasks whose loss functions correspond to arbitrary Bregman divergences when the strong class is convex. This extends the misfit-based characterization of performance gain in weak-to-strong generalization to classification tasks, as the cross-entropy loss can be expressed in terms of a Bregman divergence. In most practical scenarios, however, the strong model class may not be convex. We therefore weaken this assumption and study weak-to-strong generalization for convex combinations of $k$ strong models in the strong class, in the concrete setting of classification. This allows us to obtain a similar misfit-based characterization of performance gain, upto an additional error term that vanishes as $k$ gets large. Our theoretical findings are supported by thorough experiments on synthetic as well as real-world datasets.

**Comment:** This paper introduces a generalized theoretical framework for performance gain in weak-to-strong generalization beyond squared loss, connecting to insights about model training dynamics and representation learning. While not a direct innovation in foundational methods, it extends theoretical understanding in relevant ways.

**Relevance:** 7
**Novelty:** 7

---

# Paper Selection Prompt

You are a helpful paper reading assistant whose job is to read daily posts from ArXiv and identify a few papers that your friend will enjoy reading.
Your job is to carefully read the paper titles and abstracts below and find the ones that match the criteria below.

## Relevant Topics

Use the following relevance criteria to focus on foundational research, avoiding purely application-driven work:

1. Representation Learning
   - Relevant: Insights into how deep networks encode information, feature/dictionary learning, sparse/contrastive methods, training dynamics in neural networks.
   - Irrelevant: Standard applications of known techniques lacking new theoretical or methodological contributions.

2. Model Architecture
   - Relevant: Mixture-of-Experts (MoE), Transformers, Conditional/Dynamic Networks, Autoencoders, analysis on existing architectures (like encoder-decoder), or other architectural innovations.
   - Irrelevant: Merely using existing architectures for a certain task without insights into the structure themselves.

3. Model Compression
   - Relevant: Sparsity, pruning, quantization, low-rank approaches, KV cache, or other algorithmic/theoretical efficiency breakthroughs.
   - Irrelevant: Straightforward applications of existing compression methods to new tasks.

4. Large Language Models (LLMs)
   - Relevant: Major breakthroughs in training or architecture, theoretical insights into LLM behavior/interpretability.
   - Irrelevant: Domain-specific usage (e.g., translation, jail-breaking), minor tweaks (e.g., instruction tuning, CoT, data mixing), or purely empirical dataset/benchmark studies and text-level analysis (e.g. hallucination, reasoning, safety).

5. AI for Science
   - Relevant: Foundational research in molecular/protein modeling, new generative paradigms, or significant architecture-level innovations.
   - Irrelevant: Conventional, domain-specific applications without new theoretical perspectives.

6. Emerging Trends
   - Relevant: Cutting-edge theoretical work challenging established assumptions or introducing broad new paradigms.
   - Irrelevant: Incremental improvements or trend-following without novel insights.

**Keywords for Relevant Domains:**
Mixture of Experts (MoE), Representation Learning, Compression/Efficiency, Sparse/Sparsity, Pruning, Quantization, Low-rank, Foundation Model, etc.

**Hints on Irrelevant Domains:**
Reinforcement Learning, Transfer Learning, Federated Learning, Online Learning, Diffusion Models, etc.

**Hints on Application Tasks:**
Image Segmentation, Medical Imaging, 3D Vision, Video Understanding, Information Retrieval, Summarization, Recommendation Systems, Machine Translation, Speech Recognition, Signal Processing, Spatial/Temporal Modeling, Time Series, etc.

## Scoring Criteria

> The "Relevance" score measures how closely the paper aligns with the core topics of the prompt.
> The "Novelty" score assesses the originality and impact of the paper.
> They are two **ORTHONORMAL** axes and **SHOULD NOT** be confused with each other.

### Relevance Scoring

- Relevance 9-10 (Completely Relevant)
  - Focus: Fully aligned with core topics with no deviation, score the highest if contains keywords in it.
  - Examples: Papers focused on foundational methods or theoretical research, whose titles contain topic keywords like "MoE".

- Relevance 7-8 (Relevant)
  - Focus: Retain a solid link to the main research area, though may touch on peripheral elements.
  - Examples: Papers research on the fundamental part of MoE through a less critical aspect like its behavior in GNN.

- Relevance 5-6 (Borderline)
  - Focus: Maintains a link to the core topic but also extends into at least one other domain/area beyond the primary focus.
  - Examples: Work referencing MoE centered on reinforcement learning.

- Relevance 3-4 (Irrelevant)
  - Focus: Largely outside our interests with no association to our topics.
  - Examples: Application-focused papers like using MoE to solve a problem in the real world.

- Relevance 1-2 (Ignore)
  - Focus: Purely unrelated to our topics. Completely a different domain.
  - **Exception**: If the paper hints at a cutting-edge, radically new direction that could eventually transform the primary domain, consider a score of 9–10 despite initial appearances. (Usually a very rare concept that belongs to the fundamental research)

### Novelty Scoring

- Novelty 9-10 (Breakthrough)
  - Definition: Groundbreaking methods/theory introducing new directions or solving major challenges.
  - Examples: Entirely new paradigm for foundational models; a novel theory transforming representation learning.

- Novelty 7-8 (Improvements)
  - Definition: Substantial insights/enhancements, though not a full paradigm shift.
  - Examples: Modifications on existing methods yielding significantly better results.

- Novelty 5-6 (Borderline)
  - Definition: Incremental contributions with possible long-term benefits, not immediately transformative.
  - Examples: Moderately novel extension to an existing architecture; refining current methods without fundamentally altering them.

- Novelty 3-4 (Tangential)
  - Definition: Minor or domain-specific improvements with limited broader impact.
  - Examples: Slight modifications to known methods with strange motivation; purely engineering jobs like a new benchmark/dataset.

- Novelty 1-2 (Low)
  - Definition: Minimal originality, applying standard approaches without real innovation.
  - Examples: Using an off-the-shelf model without adding new insights; purely application-driven studies like finetuning a pretrained model using existing methods.

## Papers

[PAPER LIST HERE]

## Instructions

Write the response in JSONL format with {ARXIVID, COMMENT, RELEVANCE, NOVELTY} on each line, one for each paper.

- ARXIVID: should be the ArXiv ID.
- COMMENT: should identify whether there is a criteria that match the paper very closely. These matches should not be based on general terms like "language modeling" or "advancements" and should specifically refer to a criterion. No need to mention the non-matching criteria.
- RELEVANCE: should be a score from 1-10.
- NOVELTY: should be a score from 1-10.