{
    "2506.12891": {
        "SCORE": 18,
        "ARXIVID": "2506.12891",
        "COMMENT": "The paper discusses a new design paradigm for AI based on evolutionary developmental biology, which is relevant to emerging trends in AI.",
        "RELEVANCE": 9,
        "NOVELTY": 9,
        "authors": [
            "Zeki Doruk Erden",
            "Boi Faltings"
        ],
        "title": "Evolutionary Developmental Biology Can Serve as the Conceptual Foundation for a New Design Paradigm in Artificial Intelligence",
        "abstract": "Artificial intelligence (AI), propelled by advancements in machine learning, has made significant strides in solving complex tasks. However, the current neural network-based paradigm, while effective, is heavily constrained by inherent limitations, primarily a lack of structural organization and a progression of learning that displays undesirable properties. As AI research progresses without a unifying framework, it either tries to patch weaknesses heuristically or draws loosely from biological mechanisms without strong theoretical foundations. Meanwhile, the recent paradigm shift in evolutionary understanding -- driven primarily by evolutionary developmental biology (EDB) -- has been largely overlooked in AI literature, despite a striking analogy between the Modern Synthesis and contemporary machine learning, evident in their shared assumptions, approaches, and limitations upon careful analysis. Consequently, the principles of adaptation from EDB that reshaped our understanding of the evolutionary process can also form the foundation of a unifying conceptual framework for the next design philosophy in AI, going beyond mere inspiration and grounded firmly in biology's first principles. This article provides a detailed overview of the analogy between the Modern Synthesis and modern machine learning, and outlines the core principles of a new AI design paradigm based on insights from EDB. To exemplify our analysis, we also present two learning system designs grounded in specific developmental principles -- regulatory connections, somatic variation and selection, and weak linkage -- that resolve multiple major limitations of contemporary machine learning in an organic manner, while also providing deeper insights into the role of these mechanisms in biological evolution.",
        "arxiv_id": "2506.12891"
    },
    "2506.13131": {
        "SCORE": 18,
        "ARXIVID": "2506.13131",
        "COMMENT": "The paper introduces AlphaEvolve, an evolutionary coding agent for scientific and algorithmic discovery, which is relevant to AI for Science and emerging trends.",
        "RELEVANCE": 9,
        "NOVELTY": 9,
        "authors": [
            "Alexander Novikov",
            "Ng\\^an V\\~u",
            "Marvin Eisenberger",
            "Emilien Dupont",
            "Po-Sen Huang",
            "Adam Zsolt Wagner",
            "Sergey Shirobokov",
            "Borislav Kozlovskii",
            "Francisco J. R. Ruiz",
            "Abbas Mehrabian",
            "M. Pawan Kumar",
            "Abigail See",
            "Swarat Chaudhuri",
            "George Holland",
            "Alex Davies",
            "Sebastian Nowozin",
            "Pushmeet Kohli",
            "Matej Balog"
        ],
        "title": "AlphaEvolve: A coding agent for scientific and algorithmic discovery",
        "abstract": "In this white paper, we present AlphaEvolve, an evolutionary coding agent that substantially enhances capabilities of state-of-the-art LLMs on highly challenging tasks such as tackling open scientific problems or optimizing critical pieces of computational infrastructure. AlphaEvolve orchestrates an autonomous pipeline of LLMs, whose task is to improve an algorithm by making direct changes to the code. Using an evolutionary approach, continuously receiving feedback from one or more evaluators, AlphaEvolve iteratively improves the algorithm, potentially leading to new scientific and practical discoveries. We demonstrate the broad applicability of this approach by applying it to a number of important computational problems. When applied to optimizing critical components of large-scale computational stacks at Google, AlphaEvolve developed a more efficient scheduling algorithm for data centers, found a functionally equivalent simplification in the circuit design of hardware accelerators, and accelerated the training of the LLM underpinning AlphaEvolve itself. Furthermore, AlphaEvolve discovered novel, provably correct algorithms that surpass state-of-the-art solutions on a spectrum of problems in mathematics and computer science, significantly expanding the scope of prior automated discovery methods (Romera-Paredes et al., 2023). Notably, AlphaEvolve developed a search algorithm that found a procedure to multiply two $4 \\times 4$ complex-valued matrices using $48$ scalar multiplications; offering the first improvement, after 56 years, over Strassen's algorithm in this setting. We believe AlphaEvolve and coding agents like it can have a significant impact in improving solutions of problems across many areas of science and computation.",
        "arxiv_id": "2506.13131"
    },
    "2506.13139": {
        "SCORE": 18,
        "ARXIVID": "2506.13139",
        "COMMENT": "The paper extends Random Matrix Theory to address challenges in deep learning, aligning with the emerging trends criterion.",
        "RELEVANCE": 9,
        "NOVELTY": 9,
        "authors": [
            "Zhenyu Liao",
            "Michael W. Mahoney"
        ],
        "title": "Random Matrix Theory for Deep Learning: Beyond Eigenvalues of Linear Models",
        "abstract": "Modern Machine Learning (ML) and Deep Neural Networks (DNNs) often operate on high-dimensional data and rely on overparameterized models, where classical low-dimensional intuitions break down. In particular, the proportional regime where the data dimension, sample size, and number of model parameters are all large and comparable, gives rise to novel and sometimes counterintuitive behaviors. This paper extends traditional Random Matrix Theory (RMT) beyond eigenvalue-based analysis of linear models to address the challenges posed by nonlinear ML models such as DNNs in this regime. We introduce the concept of High-dimensional Equivalent, which unifies and generalizes both Deterministic Equivalent and Linear Equivalent, to systematically address three technical challenges: high dimensionality, nonlinearity, and the need to analyze generic eigenspectral functionals. Leveraging this framework, we provide precise characterizations of the training and generalization performance of linear models, nonlinear shallow networks, and deep networks. Our results capture rich phenomena, including scaling laws, double descent, and nonlinear learning dynamics, offering a unified perspective on the theoretical understanding of deep learning in high dimensions.",
        "arxiv_id": "2506.13139"
    },
    "2506.12876": {
        "SCORE": 17,
        "ARXIVID": "2506.12876",
        "COMMENT": "The paper introduces a novel linear-space probabilistic framework for achieving (N:M)-sparsity in LLMs, which is relevant to model compression through sparsity and pruning.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Yan Sun",
            "Qixin Zhang",
            "Zhiyuan Yu",
            "Xikun Zhang",
            "Li Shen",
            "Dacheng Tao"
        ],
        "title": "MaskPro: Linear-Space Probabilistic Learning for Strict (N:M)-Sparsity on Large Language Models",
        "abstract": "The rapid scaling of large language models (LLMs) has made inference efficiency a primary bottleneck in the practical deployment. To address this, semi-structured sparsity offers a promising solution by strategically retaining $N$ elements out of every $M$ weights, thereby enabling hardware-friendly acceleration and reduced memory. However, existing (N:M)-compatible approaches typically fall into two categories: rule-based layerwise greedy search, which suffers from considerable errors, and gradient-driven combinatorial learning, which incurs prohibitive training costs. To tackle these challenges, we propose a novel linear-space probabilistic framework named MaskPro, which aims to learn a prior categorical distribution for every $M$ consecutive weights and subsequently leverages this distribution to generate the (N:M)-sparsity throughout an $N$-way sampling without replacement. Furthermore, to mitigate the training instability induced by the high variance of policy gradients in the super large combinatorial space, we propose a novel update method by introducing a moving average tracker of loss residuals instead of vanilla loss. Finally, we conduct comprehensive theoretical analysis and extensive experiments to validate the superior performance of MaskPro, as well as its excellent scalability in memory efficiency and exceptional robustness to data samples. Our code is available at https://github.com/woodenchild95/Maskpro.git.",
        "arxiv_id": "2506.12876"
    },
    "2506.12597": {
        "SCORE": 17,
        "ARXIVID": "2506.12597",
        "COMMENT": "The paper presents a Sparse Interpolated Mixture-of-Experts (SIMoE) method, which is relevant to model architecture innovations, specifically MoE.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Shengzhuang Chen",
            "Ying Wei",
            "Jonathan Richard Schwarz"
        ],
        "title": "Automatic Expert Discovery in LLM Upcycling via Sparse Interpolated Mixture-of-Experts",
        "abstract": "We present Sparse Interpolated Mixture-of-Experts (SIMoE) instruction-tuning, an end-to-end algorithm designed to fine-tune a dense pre-trained Large Language Model (LLM) into a MoE-style model that possesses capabilities in multiple specialized domains. During instruction-tuning, SIMoE automatically identifies multiple specialized experts under a specified sparsity constraint, with each expert representing a structurally sparse subset of the seed LLM's parameters that correspond to domain-specific knowledge within the data. SIMoE simultaneously learns an input-dependent expert merging strategy via a router network, leveraging rich cross-expert knowledge for superior downstream generalization that surpasses existing baselines. Empirically, SIMoE consistently achieves state-of-the-art performance on common instruction-tuning benchmarks while maintaining an optimal performance-compute trade-off compared to all baselines.",
        "arxiv_id": "2506.12597"
    },
    "2506.13688": {
        "SCORE": 17,
        "ARXIVID": "2506.13688",
        "COMMENT": "The paper investigates the training dynamics of Transformers, focusing on the abrupt learning phenomenon and representation collapse, which aligns with representation learning and insights into how deep networks encode information.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Pulkit Gopalani",
            "Wei Hu"
        ],
        "title": "What Happens During the Loss Plateau? Understanding Abrupt Learning in Transformers",
        "abstract": "Training Transformers on algorithmic tasks frequently demonstrates an intriguing abrupt learning phenomenon: an extended performance plateau followed by a sudden, sharp improvement. This work investigates the underlying mechanisms for such dynamics, primarily in shallow Transformers. We reveal that during the plateau, the model often develops an interpretable partial solution while simultaneously exhibiting a strong repetition bias in their outputs. This output degeneracy is accompanied by internal representation collapse, where hidden states across different tokens become nearly parallel. We further identify the slow learning of optimal attention maps as a key bottleneck. Hidden progress in attention configuration during the plateau precedes the eventual rapid convergence, and directly intervening on attention significantly alters plateau duration and the severity of repetition bias and representational collapse. We validate that these identified phenomena-repetition bias and representation collapse-are not artifacts of toy setups but also manifest in the early pre-training stage of large language models like Pythia and OLMo.",
        "arxiv_id": "2506.13688"
    },
    "2506.13727": {
        "SCORE": 17,
        "ARXIVID": "2506.13727",
        "COMMENT": "The paper presents attribution-guided pruning for LLMs, focusing on model compression and interpretability, which aligns with model compression and efficiency breakthroughs.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Sayed Mohammad Vakilzadeh Hatefi",
            "Maximilian Dreyer",
            "Reduan Achtibat",
            "Patrick Kahardipraja",
            "Thomas Wiegand",
            "Wojciech Samek",
            "Sebastian Lapuschkin"
        ],
        "title": "Attribution-guided Pruning for Compression, Circuit Discovery, and Targeted Correction in LLMs",
        "abstract": "Large Language Models (LLMs) are central to many contemporary AI applications, yet their extensive parameter counts pose significant challenges for deployment in memory- and compute-constrained environments. Recent works in eXplainable AI (XAI), particularly on attribution methods, suggest that interpretability can also enable model compression by identifying and removing components irrelevant to inference. In this paper, we leverage Layer-wise Relevance Propagation (LRP) to perform attribution-guided pruning of LLMs. While LRP has shown promise in structured pruning for vision models, we extend it to unstructured pruning in LLMs and demonstrate that it can substantially reduce model size with minimal performance loss. Our method is especially effective in extracting task-relevant subgraphs -- so-called ``circuits'' -- which can represent core functions (e.g., indirect object identification). Building on this, we introduce a technique for model correction, by selectively removing circuits responsible for spurious behaviors (e.g., toxic outputs). All in all, we gather these techniques as a uniform holistic framework and showcase its effectiveness and limitations through extensive experiments for compression, circuit discovery and model correction on Llama and OPT models, highlighting its potential for improving both model efficiency and safety. Our code is publicly available at https://github.com/erfanhatefi/SparC3.",
        "arxiv_id": "2506.13727"
    },
    "2506.13541": {
        "SCORE": 17,
        "ARXIVID": "2506.13541",
        "COMMENT": "The paper introduces a novel mixture-of-expert (MoE) approach for dynamic token-wise KV optimization in transformers, which aligns with the model architecture and model compression criteria.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Guanghui Song",
            "Dongping Liao",
            "Yiren Zhao",
            "Kejiang Ye",
            "Cheng-zhong Xu",
            "Xitong Gao"
        ],
        "title": "Mixture of Weight-shared Heterogeneous Group Attention Experts for Dynamic Token-wise KV Optimization",
        "abstract": "Transformer models face scalability challenges in causal language modeling (CLM) due to inefficient memory allocation for growing key-value (KV) caches, which strains compute and storage resources. Existing methods like Grouped Query Attention (GQA) and token-level KV optimization improve efficiency but rely on rigid resource allocation, often discarding \"low-priority\" tokens or statically grouping them, failing to address the dynamic spectrum of token importance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that dynamically optimizes token-wise computation and memory allocation. Unlike prior approaches, mixSGA retains all tokens while adaptively routing them to specialized experts with varying KV group sizes, balancing granularity and efficiency. Our key novelties include: (1) a token-wise expert-choice routing mechanism guided by learned importance scores, enabling proportional resource allocation without token discard; (2) weight-sharing across grouped attention projections to minimize parameter overhead; and (3) an auxiliary loss to ensure one-hot routing decisions for training-inference consistency in CLMs. Extensive evaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show mixSGA's superiority over static baselines. On instruction-following and continued pretraining tasks, mixSGA achieves higher ROUGE-L and lower perplexity under the same KV budgets.",
        "arxiv_id": "2506.13541"
    },
    "2506.13181": {
        "SCORE": 17,
        "ARXIVID": "2506.13181",
        "COMMENT": "The paper proposes a novel framework for unlearning in LLMs using embedding alignment, which is relevant to foundational research in LLM behavior and interpretability.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Philipp Spohn",
            "Leander Girrbach",
            "Jessica Bader",
            "Zeynep Akata"
        ],
        "title": "Align-then-Unlearn: Embedding Alignment for LLM Unlearning",
        "abstract": "As large language models (LLMs) are trained on massive datasets, they have raised significant privacy and ethical concerns due to their potential to inadvertently retain sensitive information. Unlearning seeks to selectively remove specific data from trained models, such as personal information or copyrighted content. Current approaches targeting specific output sequences at the token level often fail to achieve complete forgetting and remain susceptible to prompt rephrasing. We propose Align-then-Unlearn, a novel framework that performs unlearning in the semantic embedding space rather than directly on output tokens. Align-then-Unlearn first augments the LLM with an embedding prediction module trained to anticipate future context representations. Unlearning is then achieved by fine-tuning the model to minimize the similarity between these predicted embeddings and a target embedding that represents the concept to be removed. Initial results show that Align-then-Unlearn effectively removes targeted knowledge with minimal degradation in overall model utility. These findings suggest that embedding-based unlearning offers a promising and robust approach to removing conceptual knowledge. Our code is available at https://github.com/ExplainableML/align-then-unlearn.",
        "arxiv_id": "2506.13181"
    },
    "2506.13410": {
        "SCORE": 17,
        "ARXIVID": "2506.13410",
        "COMMENT": "The paper proposes a parameter-efficient neural architecture by optimizing neuron positions, which is relevant to model compression and efficiency.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Laura Erb",
            "Tommaso Boccato",
            "Alexandru Vasilache",
            "Juergen Becker",
            "Nicola Toschi"
        ],
        "title": "Training Neural Networks by Optimizing Neuron Positions",
        "abstract": "The high computational complexity and increasing parameter counts of deep neural networks pose significant challenges for deployment in resource-constrained environments, such as edge devices or real-time systems. To address this, we propose a parameter-efficient neural architecture where neurons are embedded in Euclidean space. During training, their positions are optimized and synaptic weights are determined as the inverse of the spatial distance between connected neurons. These distance-dependent wiring rules replace traditional learnable weight matrices and significantly reduce the number of parameters while introducing a biologically inspired inductive bias: connection strength decreases with spatial distance, reflecting the brain's embedding in three-dimensional space where connections tend to minimize wiring length. We validate this approach for both multi-layer perceptrons and spiking neural networks. Through a series of experiments, we demonstrate that these spatially embedded neural networks achieve a performance competitive with conventional architectures on the MNIST dataset. Additionally, the models maintain performance even at pruning rates exceeding 80% sparsity, outperforming traditional networks with the same number of parameters under similar conditions. Finally, the spatial embedding framework offers an intuitive visualization of the network structure.",
        "arxiv_id": "2506.13410"
    },
    "2506.13274": {
        "SCORE": 17,
        "ARXIVID": "2506.13274",
        "COMMENT": "The paper proposes AdaLRS, an adaptive learning rate search algorithm for foundation model pretraining, which is relevant to foundational research in model training dynamics.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Hongyuan Dong",
            "Dingkang Yang",
            "Xiao Liang",
            "Chao Feng",
            "Jiao Ran"
        ],
        "title": "AdaLRS: Loss-Guided Adaptive Learning Rate Search for Efficient Foundation Model Pretraining",
        "abstract": "Learning rate is widely regarded as crucial for effective foundation model pretraining. Recent research explores and demonstrates the transferability of learning rate configurations across varying model and dataset sizes, etc. Nevertheless, these approaches are constrained to specific training scenarios and typically necessitate extensive hyperparameter tuning on proxy models. In this work, we propose \\textbf{AdaLRS}, a plug-in-and-play adaptive learning rate search algorithm that conducts online optimal learning rate search via optimizing loss descent velocities. We provide experiment results to show that the optimization of training loss and loss descent velocity in foundation model pretraining are both convex and share the same optimal learning rate. Relying solely on training loss dynamics, AdaLRS involves few extra computations to guide the search process, and its convergence is guaranteed via theoretical analysis. Experiments on both LLM and VLM pretraining show that AdaLRS adjusts suboptimal learning rates to the neighborhood of optimum with marked efficiency and effectiveness, with model performance improved accordingly. We also show the robust generalizability of AdaLRS across varying training scenarios, such as different model sizes, training paradigms, and base learning rate scheduler choices.",
        "arxiv_id": "2506.13274"
    },
    "2506.12790": {
        "SCORE": 17,
        "ARXIVID": "2506.12790",
        "COMMENT": "The paper introduces Global Fourier Modulation for neural representations, which is relevant to representation learning and architectural innovations.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Minju Jo",
            "Woojin Cho",
            "Uvini Balasuriya Mudiyanselage",
            "Seungjun Lee",
            "Noseong Park",
            "Kookjin Lee"
        ],
        "title": "PDEfuncta: Spectrally-Aware Neural Representation for PDE Solution Modeling",
        "abstract": "Scientific machine learning often involves representing complex solution fields that exhibit high-frequency features such as sharp transitions, fine-scale oscillations, and localized structures. While implicit neural representations (INRs) have shown promise for continuous function modeling, capturing such high-frequency behavior remains a challenge-especially when modeling multiple solution fields with a shared network. Prior work addressing spectral bias in INRs has primarily focused on single-instance settings, limiting scalability and generalization. In this work, we propose Global Fourier Modulation (GFM), a novel modulation technique that injects high-frequency information at each layer of the INR through Fourier-based reparameterization. This enables compact and accurate representation of multiple solution fields using low-dimensional latent vectors. Building upon GFM, we introduce PDEfuncta, a meta-learning framework designed to learn multi-modal solution fields and support generalization to new tasks. Through empirical studies on diverse scientific problems, we demonstrate that our method not only improves representational quality but also shows potential for forward and inverse inference tasks without the need for retraining.",
        "arxiv_id": "2506.12790"
    },
    "2506.12041": {
        "SCORE": 17,
        "ARXIVID": "2506.12041",
        "COMMENT": "The paper proposes a novel meta-learning framework for network pruning, which is relevant to model compression and introduces a new approach using metanetworks.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Yewei Liu",
            "Xiyuan Wang",
            "Muhan Zhang"
        ],
        "title": "Meta Pruning via Graph Metanetworks : A Meta Learning Framework for Network Pruning",
        "abstract": "Network pruning, aimed at reducing network size while preserving accuracy, has attracted significant research interest. Numerous pruning techniques have been proposed over time. They are becoming increasingly effective, but more complex and harder to interpret as well. Given the inherent complexity of neural networks, we argue that manually designing pruning criteria has reached a bottleneck. To address this, we propose a novel approach in which we \"use a neural network to prune neural networks\". More specifically, we introduce the newly developed idea of metanetwork from meta-learning into pruning. A metanetwork is a network that takes another network as input and produces a modified network as output. In this paper, we first establish a bijective mapping between neural networks and graphs, and then employ a graph neural network as our metanetwork. We train a metanetwork that learns the pruning strategy automatically which can transform a network that is hard to prune into another network that is much easier to prune. Once the metanetwork is trained, our pruning needs nothing more than a feedforward through the metanetwork and the standard finetuning to prune at state-of-the-art. Our method achieved outstanding results on many popular and representative pruning tasks (including ResNet56 on CIFAR10, VGG19 on CIFAR100, ResNet50 on ImageNet). Our code is available at https://github.com/Yewei-Liu/MetaPruning",
        "arxiv_id": "2506.12041"
    },
    "2506.12027": {
        "SCORE": 17,
        "ARXIVID": "2506.12027",
        "COMMENT": "The paper proves that constant bit-size transformers are Turing complete, providing theoretical insights into transformer models.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Qian Li",
            "Yuyi Wang"
        ],
        "title": "Constant Bit-size Transformers Are Turing Complete",
        "abstract": "We prove that any Turing machine running on inputs of arbitrary length can be simulated by a constant bit-size transformer, as long as the context window is sufficiently long. This improves previous works, which require scaling up either the model's precision or the number of parameters on longer inputs. Furthermore, we prove that the complexity class SPACE$[s(n)]$ exactly characterizes the expressive power of a constant bit-size transformer with a context window of length $s(n)$. Our approach relies on simulating Post machines, a Turing-complete computational model. Post machines can be modeled as automata equipped with a queue, exhibiting computational behaviors naturally aligned with those of transformers. The behavioral similarity between transformers and Post machines may offer new insights into the mechanisms underlying the reasoning abilities of transformers.",
        "arxiv_id": "2506.12027"
    },
    "2506.13331": {
        "SCORE": 17,
        "ARXIVID": "2506.13331",
        "COMMENT": "The paper introduces the Mixture of Cognitive Reasoners architecture, which is relevant to model architecture and offers insights into modular reasoning with brain-like specialization.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Badr AlKhamissi",
            "C. Nicol\\`o De Sabbata",
            "Zeming Chen",
            "Martin Schrimpf",
            "Antoine Bosselut"
        ],
        "title": "Mixture of Cognitive Reasoners: Modular Reasoning with Brain-Like Specialization",
        "abstract": "Human intelligence emerges from the interaction of specialized brain networks, each dedicated to distinct cognitive functions such as language processing, logical reasoning, social understanding, and memory retrieval. Inspired by this biological observation, we introduce the Mixture of Cognitive Reasoners (MiCRo) architecture and training paradigm: a modular transformer-based language model with a training curriculum that encourages the emergence of functional specialization among different modules. Inspired by studies in neuroscience, we partition the layers of a pretrained transformer model into four expert modules, each corresponding to a well-studied cognitive brain network. Our Brain-Like model has three key benefits over the state of the art: First, the specialized experts are highly interpretable and functionally critical, where removing a module significantly impairs performance on domain-relevant benchmarks. Second, our model outperforms comparable baselines that lack specialization on seven reasoning benchmarks. And third, the model's behavior can be steered at inference time by selectively emphasizing certain expert modules (e.g., favoring social over logical reasoning), enabling fine-grained control over the style of its response. Our findings suggest that biologically inspired inductive biases involved in human cognition lead to significant modeling gains in interpretability, performance, and controllability.",
        "arxiv_id": "2506.13331"
    },
    "2506.13717": {
        "SCORE": 17,
        "ARXIVID": "2506.13717",
        "COMMENT": "The paper introduces a novel framework for contrastive self-supervised learning, aligning with the representation learning criterion.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Guanming Zhang",
            "David J. Heeger",
            "Stefano Martiniani"
        ],
        "title": "Contrastive Self-Supervised Learning As Neural Manifold Packing",
        "abstract": "Contrastive self-supervised learning based on point-wise comparisons has been widely studied for vision tasks. In the visual cortex of the brain, neuronal responses to distinct stimulus classes are organized into geometric structures known as neural manifolds. Accurate classification of stimuli can be achieved by effectively separating these manifolds, akin to solving a packing problem. We introduce Contrastive Learning As Manifold Packing (CLAMP), a self-supervised framework that recasts representation learning as a manifold packing problem. CLAMP introduces a loss function inspired by the potential energy of short-range repulsive particle systems, such as those encountered in the physics of simple liquids and jammed packings. In this framework, each class consists of sub-manifolds embedding multiple augmented views of a single image. The sizes and positions of the sub-manifolds are dynamically optimized by following the gradient of a packing loss. This approach yields interpretable dynamics in the embedding space that parallel jamming physics, and introduces geometrically meaningful hyperparameters within the loss function. Under the standard linear evaluation protocol, which freezes the backbone and trains only a linear classifier, CLAMP achieves competitive performance with state-of-the-art self-supervised models. Furthermore, our analysis reveals that neural manifolds corresponding to different categories emerge naturally and are effectively separated in the learned representation space, highlighting the potential of CLAMP to bridge insights from physics, neural science, and machine learning.",
        "arxiv_id": "2506.13717"
    },
    "2506.13585": {
        "SCORE": 17,
        "ARXIVID": "2506.13585",
        "COMMENT": "The paper introduces a hybrid Mixture-of-Experts architecture with a novel attention mechanism, which is relevant to model architecture and efficiency.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "MiniMax",
            ":",
            "Aili Chen",
            "Aonian Li",
            "Bangwei Gong",
            "Binyang Jiang",
            "Bo Fei",
            "Bo Yang",
            "Boji Shan",
            "Changqing Yu",
            "Chao Wang",
            "Cheng Zhu",
            "Chengjun Xiao",
            "Chengyu Du",
            "Chi Zhang",
            "Chu Qiao",
            "Chunhao Zhang",
            "Chunhui Du",
            "Congchao Guo",
            "Da Chen",
            "Deming Ding",
            "Dianjun Sun",
            "Dong Li",
            "Enwei Jiao",
            "Haigang Zhou",
            "Haimo Zhang",
            "Han Ding",
            "Haohai Sun",
            "Haoyu Feng",
            "Huaiguang Cai",
            "Haichao Zhu",
            "Jian Sun",
            "Jiaqi Zhuang",
            "Jiaren Cai",
            "Jiayuan Song",
            "Jin Zhu",
            "Jingyang Li",
            "Jinhao Tian",
            "Jinli Liu",
            "Junhao Xu",
            "Junjie Yan",
            "Junteng Liu",
            "Junxian He",
            "Kaiyi Feng",
            "Ke Yang",
            "Kecheng Xiao",
            "Le Han",
            "Leyang Wang",
            "Lianfei Yu",
            "Liheng Feng",
            "Lin Li",
            "Lin Zheng",
            "Linge Du",
            "Lingyu Yang",
            "Lunbin Zeng",
            "Minghui Yu",
            "Mingliang Tao",
            "Mingyuan Chi",
            "Mozhi Zhang",
            "Mujie Lin",
            "Nan Hu",
            "Nongyu Di",
            "Peng Gao",
            "Pengfei Li",
            "Pengyu Zhao",
            "Qibing Ren",
            "Qidi Xu",
            "Qile Li",
            "Qin Wang",
            "Rong Tian",
            "Ruitao Leng",
            "Shaoxiang Chen",
            "Shaoyu Chen",
            "Shengmin Shi",
            "Shitong Weng",
            "Shuchang Guan",
            "Shuqi Yu",
            "Sichen Li",
            "Songquan Zhu",
            "Tengfei Li",
            "Tianchi Cai",
            "Tianrun Liang",
            "Weiyu Cheng",
            "Weize Kong",
            "Wenkai Li",
            "Xiancai Chen",
            "Xiangjun Song",
            "Xiao Luo",
            "Xiao Su",
            "Xiaobo Li",
            "Xiaodong Han",
            "Xinzhu Hou",
            "Xuan Lu",
            "Xun Zou",
            "Xuyang Shen",
            "Yan Gong",
            "Yan Ma",
            "Yang Wang",
            "Yiqi Shi",
            "Yiran Zhong",
            "Yonghong Duan",
            "Yongxiang Fu",
            "Yongyi Hu",
            "Yu Gao",
            "Yuanxiang Fan",
            "Yufeng Yang",
            "Yuhao Li",
            "Yulin Hu",
            "Yunan Huang",
            "Yunji Li",
            "Yunzhi Xu",
            "Yuxin Mao",
            "Yuxuan Shi",
            "Yuze Wenren",
            "Zehan Li",
            "Zelin Li",
            "Zhanxu Tian",
            "Zhengmao Zhu",
            "Zhenhua Fan",
            "Zhenzhen Wu",
            "Zhichao Xu",
            "Zhihang Yu",
            "Zhiheng Lyu",
            "Zhuo Jiang",
            "Zibo Gao",
            "Zijia Wu",
            "Zijian Song",
            "Zijun Sun"
        ],
        "title": "MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention",
        "abstract": "We introduce MiniMax-M1, the world's first open-weight, large-scale hybrid-attention reasoning model. MiniMax-M1 is powered by a hybrid Mixture-of-Experts (MoE) architecture combined with a lightning attention mechanism. The model is developed based on our previous MiniMax-Text-01 model, which contains a total of 456 billion parameters with 45.9 billion parameters activated per token. The M1 model natively supports a context length of 1 million tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning attention mechanism in MiniMax-M1 enables efficient scaling of test-time compute. These properties make M1 particularly suitable for complex tasks that require processing long inputs and thinking extensively. MiniMax-M1 is trained using large-scale reinforcement learning (RL) on diverse problems including sandbox-based, real-world software engineering environments. In addition to M1's inherent efficiency advantage for RL training, we propose CISPO, a novel RL algorithm to further enhance RL efficiency. CISPO clips importance sampling weights rather than token updates, outperforming other competitive RL variants. Combining hybrid-attention and CISPO enables MiniMax-M1's full RL training on 512 H800 GPUs to complete in only three weeks, with a rental cost of just $534,700. We release two versions of MiniMax-M1 models with 40K and 80K thinking budgets respectively, where the 40K model represents an intermediate phase of the 80K training. Experiments on standard benchmarks show that our models are comparable or superior to strong open-weight models such as the original DeepSeek-R1 and Qwen3-235B, with particular strengths in complex software engineering, tool utilization, and long-context tasks. We publicly release MiniMax-M1 at https://github.com/MiniMax-AI/MiniMax-M1.",
        "arxiv_id": "2506.13585"
    },
    "2506.12903": {
        "SCORE": 16,
        "ARXIVID": "2506.12903",
        "COMMENT": "The paper analyzes the implicit regularization of variational learning through the Edge of Stability framework, contributing to understanding training dynamics in neural networks.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Avrajit Ghosh",
            "Bai Cong",
            "Rio Yokota",
            "Saiprasad Ravishankar",
            "Rongrong Wang",
            "Molei Tao",
            "Mohammad Emtiyaz Khan",
            "Thomas M\\\"ollenhoff"
        ],
        "title": "Variational Learning Finds Flatter Solutions at the Edge of Stability",
        "abstract": "Variational Learning (VL) has recently gained popularity for training deep neural networks and is competitive to standard learning methods. Part of its empirical success can be explained by theories such as PAC-Bayes bounds, minimum description length and marginal likelihood, but there are few tools to unravel the implicit regularization in play. Here, we analyze the implicit regularization of VL through the Edge of Stability (EoS) framework. EoS has previously been used to show that gradient descent can find flat solutions and we extend this result to VL to show that it can find even flatter solutions. This is obtained by controlling the posterior covariance and the number of Monte Carlo samples from the posterior. These results are derived in a similar fashion as the standard EoS literature for deep learning, by first deriving a result for a quadratic problem and then extending it to deep neural networks. We empirically validate these findings on a wide variety of large networks, such as ResNet and ViT, to find that the theoretical results closely match the empirical ones. Ours is the first work to analyze the EoS dynamics in VL.",
        "arxiv_id": "2506.12903"
    },
    "2506.12119": {
        "SCORE": 16,
        "ARXIVID": "2506.12119",
        "COMMENT": "The paper investigates whether Mixture-of-Experts models can surpass dense LLMs under equal resources, relevant to model architecture and offering insights into MoE performance.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Houyi Li",
            "Ka Man Lo",
            "Ziqi Wang",
            "Zili Wang",
            "Wenzhen Zheng",
            "Shuigeng Zhou",
            "Xiangyu Zhang",
            "Daxin Jiang"
        ],
        "title": "Can Mixture-of-Experts Surpass Dense LLMs Under Strictly Equal Resources?",
        "abstract": "Mixture-of-Experts (MoE) language models dramatically expand model capacity and achieve remarkable performance without increasing per-token compute. However, can MoEs surpass dense architectures under strictly equal resource constraints - that is, when the total parameter count, training compute, and data budget are identical? This question remains under-explored despite its significant practical value and potential. In this paper, we propose a novel perspective and methodological framework to study this question thoroughly. First, we comprehensively investigate the architecture of MoEs and achieve an optimal model design that maximizes the performance. Based on this, we subsequently find that an MoE model with activation rate in an optimal region is able to outperform its dense counterpart under the same total parameter, training compute and data resource. More importantly, this optimal region remains consistent across different model sizes. Although additional amount of data turns out to be a trade-off for the enhanced performance, we show that this can be resolved via reusing data. We validate our findings through extensive experiments, training nearly 200 language models at 2B scale and over 50 at 7B scale, cumulatively processing 50 trillion tokens. All models will be released publicly.",
        "arxiv_id": "2506.12119"
    },
    "2506.12965": {
        "SCORE": 16,
        "ARXIVID": "2506.12965",
        "COMMENT": "The paper introduces distributional training data attribution, providing new insights into how randomness affects model outputs, which is relevant to representation learning.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Bruno Mlodozeniec",
            "Isaac Reid",
            "Sam Power",
            "David Krueger",
            "Murat Erdogdu",
            "Richard E. Turner",
            "Roger Grosse"
        ],
        "title": "Distributional Training Data Attribution",
        "abstract": "Randomness is an unavoidable part of training deep learning models, yet something that traditional training data attribution algorithms fail to rigorously account for. They ignore the fact that, due to stochasticity in the initialisation and batching, training on the same dataset can yield different models. In this paper, we address this shortcoming through introducing distributional training data attribution (d-TDA), the goal of which is to predict how the distribution of model outputs (over training runs) depends upon the dataset. We demonstrate the practical significance of d-TDA in experiments, e.g. by identifying training examples that drastically change the distribution of some target measurement without necessarily changing the mean. Intriguingly, we also find that influence functions (IFs), a popular but poorly-understood data attribution tool, emerge naturally from our distributional framework as the limit to unrolled differentiation; without requiring restrictive convexity assumptions. This provides a new mathematical motivation for their efficacy in deep learning, and helps to characterise their limitations.",
        "arxiv_id": "2506.12965"
    },
    "2506.12355": {
        "SCORE": 16,
        "ARXIVID": "2506.12355",
        "COMMENT": "The paper introduces QiMeng-Attention, a self-optimizing paradigm for generating high-performance attention operators, which is relevant to model architecture and efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Qirui Zhou",
            "Shaohui Peng",
            "Weiqiang Xiong",
            "Haixin Chen",
            "Yuanbo Wen",
            "Haochen Li",
            "Ling Li",
            "Qi Guo",
            "Yongwei Zhao",
            "Ke Gao",
            "Ruizhi Chen",
            "Yanjun Wu",
            "Chen Zhao",
            "Yunji Chen"
        ],
        "title": "QiMeng-Attention: SOTA Attention Operator is generated by SOTA Attention Algorithm",
        "abstract": "The attention operator remains a critical performance bottleneck in large language models (LLMs), particularly for long-context scenarios. While FlashAttention is the most widely used and effective GPU-aware acceleration algorithm, it must require time-consuming and hardware-specific manual implementation, limiting adaptability across GPU architectures. Existing LLMs have shown a lot of promise in code generation tasks, but struggle to generate high-performance attention code. The key challenge is it cannot comprehend the complex data flow and computation process of the attention operator and utilize low-level primitive to exploit GPU performance.   To address the above challenge, we propose an LLM-friendly Thinking Language (LLM-TL) to help LLMs decouple the generation of high-level optimization logic and low-level implementation on GPU, and enhance LLMs' understanding of attention operator. Along with a 2-stage reasoning workflow, TL-Code generation and translation, the LLMs can automatically generate FlashAttention implementation on diverse GPUs, establishing a self-optimizing paradigm for generating high-performance attention operators in attention-centric algorithms. Verified on A100, RTX8000, and T4 GPUs, the performance of our methods significantly outshines that of vanilla LLMs, achieving a speed-up of up to 35.16x. Besides, our method not only surpasses human-optimized libraries (cuDNN and official library) in most scenarios but also extends support to unsupported hardware and data types, reducing development time from months to minutes compared with human experts.",
        "arxiv_id": "2506.12355"
    },
    "2506.13174": {
        "SCORE": 16,
        "ARXIVID": "2506.13174",
        "COMMENT": "The paper introduces a novel graph-level pretraining framework for molecular representation learning, focusing on global structural features, which is relevant to AI for Science and representation learning.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Shaoheng Yan",
            "Zian Li",
            "Muhan Zhang"
        ],
        "title": "GeoRecon: Graph-Level Representation Learning for 3D Molecules via Reconstruction-Based Pretraining",
        "abstract": "The pretraining-and-finetuning paradigm has driven significant advances across domains, such as natural language processing and computer vision, with representative pretraining paradigms such as masked language modeling and next-token prediction. However, in molecular representation learning, the task design remains largely limited to node-level denoising, which is effective at modeling local atomic environments, yet maybe insufficient for capturing the global molecular structure required by graph-level property prediction tasks, such as energy estimation and molecular regression. In this work, we present GeoRecon, a novel graph-level pretraining framework that shifts the focus from individual atoms to the molecule as an integrated whole. GeoRecon introduces a graph-level reconstruction task: during pretraining, the model is trained to generate an informative graph representation capable of accurately guiding reconstruction of the molecular geometry. This encourages the model to learn coherent, global structural features rather than isolated atomic details. Without relying on additional supervision or external data, GeoRecon outperforms node-centric baselines on multiple molecular benchmarks (e.g., QM9, MD17), demonstrating the benefit of incorporating graph-level reconstruction for learning more holistic and geometry-aware molecular embeddings.",
        "arxiv_id": "2506.13174"
    },
    "2506.13633": {
        "SCORE": 16,
        "ARXIVID": "2506.13633",
        "COMMENT": "The paper studies the global convergence of adjoint-optimized neural PDEs, contributing to the theoretical understanding of neural network PDE models, which is relevant to AI for Science.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Konstantin Riedl",
            "Justin Sirignano",
            "Konstantinos Spiliopoulos"
        ],
        "title": "Global Convergence of Adjoint-Optimized Neural PDEs",
        "abstract": "Many engineering and scientific fields have recently become interested in modeling terms in partial differential equations (PDEs) with neural networks. The resulting neural-network PDE model, being a function of the neural network parameters, can be calibrated to available data by optimizing over the PDE using gradient descent, where the gradient is evaluated in a computationally efficient manner by solving an adjoint PDE. These neural-network PDE models have emerged as an important research area in scientific machine learning. In this paper, we study the convergence of the adjoint gradient descent optimization method for training neural-network PDE models in the limit where both the number of hidden units and the training time tend to infinity. Specifically, for a general class of nonlinear parabolic PDEs with a neural network embedded in the source term, we prove convergence of the trained neural-network PDE solution to the target data (i.e., a global minimizer). The global convergence proof poses a unique mathematical challenge that is not encountered in finite-dimensional neural network convergence analyses due to (1) the neural network training dynamics involving a non-local neural network kernel operator in the infinite-width hidden layer limit where the kernel lacks a spectral gap for its eigenvalues and (2) the nonlinearity of the limit PDE system, which leads to a non-convex optimization problem, even in the infinite-width hidden layer limit (unlike in typical neual network training cases where the optimization problem becomes convex in the large neuron limit). The theoretical results are illustrated and empirically validated by numerical studies.",
        "arxiv_id": "2506.13633"
    },
    "2506.12284": {
        "SCORE": 16,
        "ARXIVID": "2506.12284",
        "COMMENT": "The paper provides insights into the training dynamics of deep networks, specifically grokking, which is relevant to representation learning.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Thomas Walker",
            "Ahmed Imtiaz Humayun",
            "Randall Balestriero",
            "Richard Baraniuk"
        ],
        "title": "GrokAlign: Geometric Characterisation and Acceleration of Grokking",
        "abstract": "A key challenge for the machine learning community is to understand and accelerate the training dynamics of deep networks that lead to delayed generalisation and emergent robustness to input perturbations, also known as grokking. Prior work has associated phenomena like delayed generalisation with the transition of a deep network from a linear to a feature learning regime, and emergent robustness with changes to the network's functional geometry, in particular the arrangement of the so-called linear regions in deep networks employing continuous piecewise affine nonlinearities. Here, we explain how grokking is realised in the Jacobian of a deep network and demonstrate that aligning a network's Jacobians with the training data (in the sense of cosine similarity) ensures grokking under a low-rank Jacobian assumption. Our results provide a strong theoretical motivation for the use of Jacobian regularisation in optimizing deep networks -- a method we introduce as GrokAlign -- which we show empirically to induce grokking much sooner than more conventional regularizers like weight decay. Moreover, we introduce centroid alignment as a tractable and interpretable simplification of Jacobian alignment that effectively identifies and tracks the stages of deep network training dynamics. Accompanying \\href{https://thomaswalker1.github.io/blog/grokalign.html}{webpage} and \\href{https://github.com/ThomasWalker1/grokalign}{code}.",
        "arxiv_id": "2506.12284"
    },
    "2506.13277": {
        "SCORE": 16,
        "ARXIVID": "2506.13277",
        "COMMENT": "The paper introduces SeqPE, a novel position encoding framework for transformers, which is relevant to model architecture and efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Huyang Li",
            "Yahui Liu",
            "Hongyu Sun",
            "Deng Cai",
            "Leyang Cui",
            "Wei Bi",
            "Peilin Zhao",
            "Taro Watanabe"
        ],
        "title": "SeqPE: Transformer with Sequential Position Encoding",
        "abstract": "Since self-attention layers in Transformers are permutation invariant by design, positional encodings must be explicitly incorporated to enable spatial understanding. However, fixed-size lookup tables used in traditional learnable position embeddings (PEs) limit extrapolation capabilities beyond pre-trained sequence lengths. Expert-designed methods such as ALiBi and RoPE, mitigate this limitation but demand extensive modifications for adapting to new modalities, underscoring fundamental challenges in adaptability and scalability. In this work, we present SeqPE, a unified and fully learnable position encoding framework that represents each $n$-dimensional position index as a symbolic sequence and employs a lightweight sequential position encoder to learn their embeddings in an end-to-end manner. To regularize SeqPE's embedding space, we introduce two complementary objectives: a contrastive objective that aligns embedding distances with a predefined position-distance function, and a knowledge distillation loss that anchors out-of-distribution position embeddings to in-distribution teacher representations, further enhancing extrapolation performance. Experiments across language modeling, long-context question answering, and 2D image classification demonstrate that SeqPE not only surpasses strong baselines in perplexity, exact match (EM), and accuracy--particularly under context length extrapolation--but also enables seamless generalization to multi-dimensional inputs without requiring manual architectural redesign. We release our code, data, and checkpoints at https://github.com/ghrua/seqpe.",
        "arxiv_id": "2506.13277"
    },
    "2506.13187": {
        "SCORE": 15,
        "ARXIVID": "2506.13187",
        "COMMENT": "The paper introduces a novel low-rank adaptation method for LLMs, which is relevant to model compression and efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Yibo Yang",
            "Sihao Liu",
            "Chuan Rao",
            "Bang An",
            "Tiancheng Shen",
            "Philip H. S. Torr",
            "Ming-Hsuan Yang",
            "Bernard Ghanem"
        ],
        "title": "Dynamic Context-oriented Decomposition for Task-aware Low-rank Adaptation with Less Forgetting and Faster Convergence",
        "abstract": "Conventional low-rank adaptation methods build adapters without considering data context, leading to sub-optimal fine-tuning performance and severe forgetting of inherent world knowledge. In this paper, we propose context-oriented decomposition adaptation (CorDA), a novel method that initializes adapters in a task-aware manner. Concretely, we develop context-oriented singular value decomposition, where we collect covariance matrices of input activations for each linear layer using sampled data from the target task, and apply SVD to the product of weight matrix and its corresponding covariance matrix. By doing so, the task-specific capability is compacted into the principal components. Thanks to the task awareness, our method enables two optional adaptation modes, knowledge-preserved mode (KPM) and instruction-previewed mode (IPM), providing flexibility to choose between freezing the principal components to preserve their associated knowledge or adapting them to better learn a new task. We further develop CorDA++ by deriving a metric that reflects the compactness of task-specific principal components, and then introducing dynamic covariance selection and dynamic rank allocation strategies based on the same metric. The two strategies provide each layer with the most representative covariance matrix and a proper rank allocation. Experimental results show that CorDA++ outperforms CorDA by a significant margin. CorDA++ in KPM not only achieves better fine-tuning performance than LoRA, but also mitigates the forgetting of pre-trained knowledge in both large language models and vision language models. For IPM, our method exhibits faster convergence, \\emph{e.g.,} 4.5x speedup over QLoRA, and improves adaptation performance in various scenarios, outperforming strong baseline methods. Our method has been integrated into the PEFT library developed by Hugging Face.",
        "arxiv_id": "2506.13187"
    },
    "2506.12383": {
        "SCORE": 15,
        "ARXIVID": "2506.12383",
        "COMMENT": "The paper introduces a novel sparse and structured parameterization for probabilistic circuits using Monarch matrices, which is relevant to model compression and efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Honghua Zhang",
            "Meihua Dang",
            "Benjie Wang",
            "Stefano Ermon",
            "Nanyun Peng",
            "Guy Van den Broeck"
        ],
        "title": "Scaling Probabilistic Circuits via Monarch Matrices",
        "abstract": "Probabilistic Circuits (PCs) are tractable representations of probability distributions allowing for exact and efficient computation of likelihoods and marginals. Recent advancements have improved the scalability of PCs either by leveraging their sparse properties or through the use of tensorized operations for better hardware utilization. However, no existing method fully exploits both aspects simultaneously. In this paper, we propose a novel sparse and structured parameterization for the sum blocks in PCs. By replacing dense matrices with sparse Monarch matrices, we significantly reduce the memory and computation costs, enabling unprecedented scaling of PCs. From a theory perspective, our construction arises naturally from circuit multiplication; from a practical perspective, compared to previous efforts on scaling up tractable probabilistic models, our approach not only achieves state-of-the-art generative modeling performance on challenging benchmarks like Text8, LM1B and ImageNet, but also demonstrates superior scaling behavior, achieving the same performance with substantially less compute as measured by the number of floating-point operations (FLOPs) during training.",
        "arxiv_id": "2506.12383"
    },
    "2506.12038": {
        "SCORE": 15,
        "ARXIVID": "2506.12038",
        "COMMENT": "The paper presents a novel approach to low-bit clustering for LLMs using knowledge distillation, which is relevant to model compression and efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Fangxin Liu",
            "Ning Yang",
            "Junping Zhao",
            "Tao Yang",
            "Haibing Guan",
            "Li Jiang"
        ],
        "title": "LCD: Advancing Extreme Low-Bit Clustering for Large Language Models via Knowledge Distillation",
        "abstract": "Large language models (LLMs) have achieved significant progress in natural language processing but face challenges in deployment due to high memory and computational requirements. Weight quantization is a common approach to address these issues, yet achieving effective low-bit compression remains challenging. This paper presents LCD, which unifies the learning of clustering-based quantization within a knowledge distillation framework. Using carefully designed optimization techniques, LCD preserves LLM performance even at ultra-low bit widths of 2-3 bits. Additionally, LCD compresses activations through smoothing and accelerates inference with a LUT-based design. Experimental results show that LCD outperforms existing methods and delivers up to a 6.2x speedup in inference. Notably, LCD is shown to be more cost-effective, making it a practical solution for real-world applications.",
        "arxiv_id": "2506.12038"
    },
    "2506.12352": {
        "SCORE": 15,
        "ARXIVID": "2506.12352",
        "COMMENT": "The paper proposes Network Automatic Relevance Determination (NARD) for modeling sparse relationships, which is relevant to representation learning and model compression.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Hongwei Zhang",
            "Ziqi Ye",
            "Xinyuan Wang",
            "Xin Guo",
            "Zenglin Xu",
            "Yuan Cheng",
            "Zixin Hu",
            "Yuan Qi"
        ],
        "title": "Efficient Network Automatic Relevance Determination",
        "abstract": "We propose Network Automatic Relevance Determination (NARD), an extension of ARD for linearly probabilistic models, to simultaneously model sparse relationships between inputs $X \\in \\mathbb R^{d \\times N}$ and outputs $Y \\in \\mathbb R^{m \\times N}$, while capturing the correlation structure among the $Y$. NARD employs a matrix normal prior which contains a sparsity-inducing parameter to identify and discard irrelevant features, thereby promoting sparsity in the model. Algorithmically, it iteratively updates both the precision matrix and the relationship between $Y$ and the refined inputs. To mitigate the computational inefficiencies of the $\\mathcal O(m^3 + d^3)$ cost per iteration, we introduce Sequential NARD, which evaluates features sequentially, and a Surrogate Function Method, leveraging an efficient approximation of the marginal likelihood and simplifying the calculation of determinant and inverse of an intermediate matrix. Combining the Sequential update with the Surrogate Function method further reduces computational costs. The computational complexity per iteration for these three methods is reduced to $\\mathcal O(m^3+p^3)$, $\\mathcal O(m^3 + d^2)$, $\\mathcal O(m^3+p^2)$, respectively, where $p \\ll d$ is the final number of features in the model. Our methods demonstrate significant improvements in computational efficiency with comparable performance on both synthetic and real-world datasets.",
        "arxiv_id": "2506.12352"
    },
    "2506.12024": {
        "SCORE": 15,
        "ARXIVID": "2506.12024",
        "COMMENT": "The paper proposes FlexQuant, a dynamic precision-switching framework for LLM quantization, which is relevant to model compression and efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Fangxin Liu",
            "Zongwu Wang",
            "JinHong Xia",
            "Junping Zhao",
            "Jian Liu",
            "Haibing Guan",
            "Li Jiang"
        ],
        "title": "FlexQuant: A Flexible and Efficient Dynamic Precision Switching Framework for LLM Quantization",
        "abstract": "The rapid advancement of large language models (LLMs) has exacerbated the memory bottleneck due to the widening gap between model parameter scaling and hardware capabilities. While post-training quantization (PTQ) techniques effectively reduce memory overhead, existing methods predominantly rely on static quantization strategies, which struggle to adapt to dynamic workloads. To address this, we propose FlexQuant, a dynamic precision-switching framework that optimizes the trade-off between inference speed and accuracy. Leveraging model perplexity entropy and Kullback-Leibler (KL) divergence, FlexQuant enables fine-grained, layer-wise mixed-precision quantization and dynamically adjusts bit-widths during each token generation. Our work provides a comprehensive analysis of quantization strategies, introduces a precision requirement model for optimal switching, and implements efficient fine-grained precision management. Experimental results demonstrate that FlexQuant achieves a 1.3x end-to-end speedup across diverse language tasks with negligible accuracy loss introduced. This framework offers a flexible and adaptive solution for efficient LLM deployment.",
        "arxiv_id": "2506.12024"
    },
    "2506.12034": {
        "SCORE": 15,
        "ARXIVID": "2506.12034",
        "COMMENT": "The study explores human-like forgetting curves in neural networks, contributing to representation learning by examining information retention and catastrophic forgetting.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Dylan Kline"
        ],
        "title": "Human-like Forgetting Curves in Deep Neural Networks",
        "abstract": "This study bridges cognitive science and neural network design by examining whether artificial models exhibit human-like forgetting curves. Drawing upon Ebbinghaus' seminal work on memory decay and principles of spaced repetition, we propose a quantitative framework to measure information retention in neural networks. Our approach computes the recall probability by evaluating the similarity between a network's current hidden state and previously stored prototype representations. This retention metric facilitates the scheduling of review sessions, thereby mitigating catastrophic forgetting during deployment and enhancing training efficiency by prompting targeted reviews. Our experiments with Multi-Layer Perceptrons reveal human-like forgetting curves, with knowledge becoming increasingly robust through scheduled reviews. This alignment between neural network forgetting curves and established human memory models identifies neural networks as an architecture that naturally emulates human memory decay and can inform state-of-the-art continual learning algorithms.",
        "arxiv_id": "2506.12034"
    },
    "2506.12655": {
        "SCORE": 15,
        "ARXIVID": "2506.12655",
        "COMMENT": "The paper proposes a novel statistical inference framework for streaming PCA, focusing on uncertainty quantification and efficient estimation, which is relevant to representation learning and efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Syamantak Kumar",
            "Shourya Pandey",
            "Purnamrita Sarkar"
        ],
        "title": "Beyond Sin-Squared Error: Linear-Time Entrywise Uncertainty Quantification for Streaming PCA",
        "abstract": "We propose a novel statistical inference framework for streaming principal component analysis (PCA) using Oja's algorithm, enabling the construction of confidence intervals for individual entries of the estimated eigenvector. Most existing works on streaming PCA focus on providing sharp sin-squared error guarantees. Recently, there has been some interest in uncertainty quantification for the sin-squared error. However, uncertainty quantification or sharp error guarantees for entries of the estimated eigenvector in the streaming setting remains largely unexplored. We derive a sharp Bernstein-type concentration bound for elements of the estimated vector matching the optimal error rate up to logarithmic factors. We also establish a Central Limit Theorem for a suitably centered and scaled subset of the entries. To efficiently estimate the coordinate-wise variance, we introduce a provably consistent subsampling algorithm that leverages the median-of-means approach, empirically achieving similar accuracy to multiplier bootstrap methods while being significantly more computationally efficient. Numerical experiments demonstrate its effectiveness in providing reliable uncertainty estimates with a fraction of the computational cost of existing methods.",
        "arxiv_id": "2506.12655"
    },
    "2506.12388": {
        "SCORE": 15,
        "ARXIVID": "2506.12388",
        "COMMENT": "The paper proposes a dynamic mixture-of-experts model for multilingual LLMs, addressing the curse of multilinguality, which is relevant to model architecture.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Chong Li",
            "Yingzhuo Deng",
            "Jiajun Zhang",
            "Chengqing Zong"
        ],
        "title": "Group then Scale: Dynamic Mixture-of-Experts Multilingual Language Model",
        "abstract": "The curse of multilinguality phenomenon is a fundamental problem of multilingual Large Language Models (LLMs), where the competition between massive languages results in inferior performance. It mainly comes from limited capacity and negative transfer between dissimilar languages. To address this issue, we propose a method to dynamically group and scale up the parameters of multilingual LLM while boosting positive transfer among similar languages. Specifically, the model is first tuned on monolingual corpus to determine the parameter deviation in each layer and quantify the similarity between languages. Layers with more deviations are extended to mixture-of-experts layers to reduce competition between languages, where one expert module serves one group of similar languages. Experimental results on 18 to 128 languages show that our method reduces the negative transfer between languages and significantly boosts multilingual performance with fewer parameters. Such language group specialization on experts benefits the new language adaptation and reduces the inference on the previous multilingual knowledge learned.",
        "arxiv_id": "2506.12388"
    },
    "2506.13192": {
        "SCORE": 15,
        "ARXIVID": "2506.13192",
        "COMMENT": "The paper proposes a novel framework combining MoE and CoT reasoning for LLMs, which is relevant to model architecture and LLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Xintong Tang",
            "Meiru Zhang",
            "Shang Xiao",
            "Junzhao Jin",
            "Zihan Zhao",
            "Liwei Li",
            "Yang Zheng",
            "Bangyi Wu"
        ],
        "title": "Breaking Thought Patterns: A Multi-Dimensional Reasoning Framework for LLMs",
        "abstract": "Large language models (LLMs) are often constrained by rigid reasoning processes, limiting their ability to generate creative and diverse responses. To address this, a novel framework called LADDER is proposed, combining Chain-of-Thought (CoT) reasoning, Mixture of Experts (MoE) models, and multi-dimensional up/down-sampling strategies which breaks the limitations of traditional LLMs. First, CoT reasoning guides the model through multi-step logical reasoning, expanding the semantic space and breaking the rigidity of thought. Next, MoE distributes the reasoning tasks across multiple expert modules, each focusing on specific sub-tasks. Finally, dimensionality reduction maps the reasoning outputs back to a lower-dimensional semantic space, yielding more precise and creative responses. Extensive experiments across multiple tasks demonstrate that LADDER significantly improves task completion, creativity, and fluency, generating innovative and coherent responses that outperform traditional models. Ablation studies reveal the critical roles of CoT and MoE in enhancing reasoning abilities and creative output. This work contributes to the development of more flexible and creative LLMs, capable of addressing complex and novel tasks.",
        "arxiv_id": "2506.13192"
    },
    "2506.12384": {
        "SCORE": 15,
        "ARXIVID": "2506.12384",
        "COMMENT": "The paper proposes a model merging framework for knowledge editing in LLMs, which is relevant to foundational research in LLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Zichuan Fu",
            "Xian Wu",
            "Guojing Li",
            "Yingying Zhang",
            "Yefeng Zheng",
            "Tianshi Ming",
            "Yejing Wang",
            "Wanyu Wang",
            "Xiangyu Zhao"
        ],
        "title": "Model Merging for Knowledge Editing",
        "abstract": "Large Language Models (LLMs) require continuous updates to maintain accurate and current knowledge as the world evolves. While existing knowledge editing approaches offer various solutions for knowledge updating, they often struggle with sequential editing scenarios and harm the general capabilities of the model, thereby significantly hampering their practical applicability. This paper proposes a two-stage framework combining robust supervised fine-tuning (R-SFT) with model merging for knowledge editing. Our method first fine-tunes the LLM to internalize new knowledge fully, then merges the fine-tuned model with the original foundation model to preserve newly acquired knowledge and general capabilities. Experimental results demonstrate that our approach significantly outperforms existing methods in sequential editing while better preserving the original performance of the model, all without requiring any architectural changes. Code is available at: https://github.com/Applied-Machine-Learning-Lab/MM4KE.",
        "arxiv_id": "2506.12384"
    },
    "2506.12551": {
        "SCORE": 15,
        "ARXIVID": "2506.12551",
        "COMMENT": "The paper presents MEraser, a method for removing backdoor-based fingerprints from LLMs, which is relevant to foundational research in LLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Jingxuan Zhang",
            "Zhenhua Xu",
            "Rui Hu",
            "Wenpeng Xing",
            "Xuhong Zhang",
            "Meng Han"
        ],
        "title": "MEraser: An Effective Fingerprint Erasure Approach for Large Language Models",
        "abstract": "Large Language Models (LLMs) have become increasingly prevalent across various sectors, raising critical concerns about model ownership and intellectual property protection. Although backdoor-based fingerprinting has emerged as a promising solution for model authentication, effective attacks for removing these fingerprints remain largely unexplored. Therefore, we present Mismatched Eraser (MEraser), a novel method for effectively removing backdoor-based fingerprints from LLMs while maintaining model performance. Our approach leverages a two-phase fine-tuning strategy utilizing carefully constructed mismatched and clean datasets. Through extensive evaluation across multiple LLM architectures and fingerprinting methods, we demonstrate that MEraser achieves complete fingerprinting removal while maintaining model performance with minimal training data of fewer than 1,000 samples. Furthermore, we introduce a transferable erasure mechanism that enables effective fingerprinting removal across different models without repeated training. In conclusion, our approach provides a practical solution for fingerprinting removal in LLMs, reveals critical vulnerabilities in current fingerprinting techniques, and establishes comprehensive evaluation benchmarks for developing more resilient model protection methods in the future.",
        "arxiv_id": "2506.12551"
    },
    "2506.13406": {
        "SCORE": 15,
        "ARXIVID": "2506.13406",
        "COMMENT": "The paper proposes CALM, a method for model merging in multi-task learning, which is relevant to model architecture and efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Kunda Yan",
            "Min Zhang",
            "Sen Cui",
            "Zikun Qu",
            "Bo Jiang",
            "Feng Liu",
            "Changshui Zhang"
        ],
        "title": "CALM: Consensus-Aware Localized Merging for Multi-Task Learning",
        "abstract": "Model merging aims to integrate the strengths of multiple fine-tuned models into a unified model while preserving task-specific capabilities. Existing methods, represented by task arithmetic, are typically classified into global- and local-aware methods. However, global-aware methods inevitably cause parameter interference, while local-aware methods struggle to maintain the effectiveness of task-specific details in the merged model. To address these limitations, we propose a Consensus-Aware Localized Merging (CALM) method which incorporates localized information aligned with global task consensus, ensuring its effectiveness post-merging. CALM consists of three key components: (1) class-balanced entropy minimization sampling, providing a more flexible and reliable way to leverage unsupervised data; (2) an efficient-aware framework, selecting a small set of tasks for sequential merging with high scalability; (3) a consensus-aware mask optimization, aligning localized binary masks with global task consensus and merging them conflict-free. Experiments demonstrate the superiority and robustness of our CALM, significantly outperforming existing methods and achieving performance close to traditional MTL.",
        "arxiv_id": "2506.13406"
    },
    "2506.12704": {
        "SCORE": 15,
        "ARXIVID": "2506.12704",
        "COMMENT": "The paper proposes a flexible realignment framework for language models, which is relevant to foundational research in LLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Wenhong Zhu",
            "Ruobing Xie",
            "Weinan Zhang",
            "Rui Wang"
        ],
        "title": "Flexible Realignment of Language Models",
        "abstract": "Realignment becomes necessary when a language model (LM) fails to meet expected performance. We propose a flexible realignment framework that supports quantitative control of alignment degree during training and inference. This framework incorporates Training-time Realignment (TrRa), which efficiently realigns the reference model by leveraging the controllable fusion of logits from both the reference and already aligned models. For example, TrRa reduces token usage by 54.63% on DeepSeek-R1-Distill-Qwen-1.5B without any performance degradation, outperforming DeepScaleR-1.5B's 33.86%. To complement TrRa during inference, we introduce a layer adapter that enables smooth Inference-time Realignment (InRa). This adapter is initialized to perform an identity transformation at the bottom layer and is inserted preceding the original layers. During inference, input embeddings are simultaneously processed by the adapter and the original layer, followed by the remaining layers, and then controllably interpolated at the logit level. We upgraded DeepSeek-R1-Distill-Qwen-7B from a slow-thinking model to one that supports both fast and slow thinking, allowing flexible alignment control even during inference. By encouraging deeper reasoning, it even surpassed its original performance.",
        "arxiv_id": "2506.12704"
    },
    "2506.13464": {
        "SCORE": 15,
        "ARXIVID": "2506.13464",
        "COMMENT": "The paper introduces a cognitive framework to evaluate LLMs' learning abilities, which aligns with the interest in theoretical insights into LLM behavior.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Zhengyu Hu",
            "Jianxun Lian",
            "Zheyuan Xiao",
            "Seraphina Zhang",
            "Tianfu Wang",
            "Nicholas Jing Yuan",
            "Xing Xie",
            "Hui Xiong"
        ],
        "title": "Unveiling the Learning Mind of Language Models: A Cognitive Framework and Empirical Study",
        "abstract": "Large language models (LLMs) have shown impressive capabilities across tasks such as mathematics, coding, and reasoning, yet their learning ability, which is crucial for adapting to dynamic environments and acquiring new knowledge, remains underexplored. In this work, we address this gap by introducing a framework inspired by cognitive psychology and education. Specifically, we decompose general learning ability into three distinct, complementary dimensions: Learning from Instructor (acquiring knowledge via explicit guidance), Learning from Concept (internalizing abstract structures and generalizing to new contexts), and Learning from Experience (adapting through accumulated exploration and feedback). We conduct a comprehensive empirical study across the three learning dimensions and identify several insightful findings, such as (i) interaction improves learning; (ii) conceptual understanding is scale-emergent and benefits larger models; and (iii) LLMs are effective few-shot learners but not many-shot learners. Based on our framework and empirical findings, we introduce a benchmark that provides a unified and realistic evaluation of LLMs' general learning abilities across three learning cognition dimensions. It enables diagnostic insights and supports evaluation and development of more adaptive and human-like models.",
        "arxiv_id": "2506.13464"
    },
    "2506.13514": {
        "SCORE": 15,
        "ARXIVID": "2506.13514",
        "COMMENT": "The paper focuses on embedding compression using Tensor-Train Decomposition, which is relevant to model compression techniques.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Mingxue Xu",
            "Yao Lei Xu",
            "Danilo P. Mandic"
        ],
        "title": "TensorSLM: Energy-efficient Embedding Compression of Sub-billion Parameter Language Models on Low-end Devices",
        "abstract": "Small Language Models (SLMs, or on-device LMs) have significantly fewer parameters than Large Language Models (LLMs). They are typically deployed on low-end devices, like mobile phones and single-board computers. Unlike LLMs, which rely on increasing model size for better generalisation, SLMs designed for edge applications are expected to have adaptivity to the deployment environments and energy efficiency given the device battery life constraints, which are not addressed in datacenter-deployed LLMs. This paper addresses these two requirements by proposing a training-free token embedding compression approach using Tensor-Train Decomposition (TTD). Each pre-trained token embedding vector is converted into a lower-dimensional Matrix Product State (MPS). We comprehensively evaluate the extracted low-rank structures across compression ratio, language task performance, latency, and energy consumption on a typical low-end device, i.e. Raspberry Pi. Taking the sub-billion parameter versions of GPT-2/Cerebres-GPT and OPT models as examples, our approach achieves a comparable language task performance to the original model with around $2.0\\times$ embedding layer compression, while the energy consumption of a single query drops by half.",
        "arxiv_id": "2506.13514"
    },
    "2506.13059": {
        "SCORE": 15,
        "ARXIVID": "2506.13059",
        "COMMENT": "The paper introduces Multipole Attention for efficient long-context reasoning, which is relevant to model architecture and efficiency improvements.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Coleman Hooper",
            "Sebastian Zhao",
            "Luca Manolache",
            "Sehoon Kim",
            "Michael W. Mahoney",
            "Yakun Sophia Shao",
            "Kurt Keutzer",
            "Amir Gholami"
        ],
        "title": "Multipole Attention for Efficient Long Context Reasoning",
        "abstract": "Large Reasoning Models (LRMs) have shown promising accuracy improvements on complex problem-solving tasks. While these models have attained high accuracy by leveraging additional computation at test time, they need to generate long chain-of-thought reasoning in order to think before answering, which requires generating thousands of tokens. While sparse attention methods can help reduce the KV cache pressure induced by this long autoregressive reasoning, these methods can introduce errors which disrupt the reasoning process. Additionally, prior methods often pre-process the input to make it easier to identify the important prompt tokens when computing attention during generation, and this pre-processing is challenging to perform online for newly generated reasoning tokens. Our work addresses these challenges by introducing Multipole Attention, which accelerates autoregressive reasoning by only computing exact attention for the most important tokens, while maintaining approximate representations for the remaining tokens. Our method first performs clustering to group together semantically similar key vectors, and then uses the cluster centroids both to identify important key vectors and to approximate the remaining key vectors in order to retain high accuracy. We design a fast cluster update process to quickly re-cluster the input and previously generated tokens, thereby allowing for accelerating attention to the previous output tokens. We evaluate our method using emerging LRMs such as Qwen-8B, demonstrating that our approach can maintain accuracy on complex reasoning tasks even with aggressive attention sparsity settings. We also provide kernel implementations to demonstrate the practical efficiency gains from our method, achieving up to 4.5$\\times$ speedup for attention in long-context reasoning applications. Our code is available at https://github.com/SqueezeAILab/MultipoleAttention.",
        "arxiv_id": "2506.13059"
    },
    "2506.12152": {
        "SCORE": 15,
        "ARXIVID": "2506.12152",
        "COMMENT": "The paper discusses agentic interpretability for LLMs, which aligns with the interest in theoretical insights into LLM behavior.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Been Kim",
            "John Hewitt",
            "Neel Nanda",
            "Noah Fiedel",
            "Oyvind Tafjord"
        ],
        "title": "Because we have LLMs, we Can and Should Pursue Agentic Interpretability",
        "abstract": "The era of Large Language Models (LLMs) presents a new opportunity for interpretability--agentic interpretability: a multi-turn conversation with an LLM wherein the LLM proactively assists human understanding by developing and leveraging a mental model of the user, which in turn enables humans to develop better mental models of the LLM. Such conversation is a new capability that traditional `inspective' interpretability methods (opening the black-box) do not use. Having a language model that aims to teach and explain--beyond just knowing how to talk--is similar to a teacher whose goal is to teach well, understanding that their success will be measured by the student's comprehension. While agentic interpretability may trade off completeness for interactivity, making it less suitable for high-stakes safety situations with potentially deceptive models, it leverages a cooperative model to discover potentially superhuman concepts that can improve humans' mental model of machines. Agentic interpretability introduces challenges, particularly in evaluation, due to what we call `human-entangled-in-the-loop' nature (humans responses are integral part of the algorithm), making the design and evaluation difficult. We discuss possible solutions and proxy goals. As LLMs approach human parity in many tasks, agentic interpretability's promise is to help humans learn the potentially superhuman concepts of the LLMs, rather than see us fall increasingly far from understanding them.",
        "arxiv_id": "2506.12152"
    },
    "2506.12394": {
        "SCORE": 15,
        "ARXIVID": "2506.12394",
        "COMMENT": "The paper proposes a low-rank regulated gradient projection algorithm for robust parameter-efficient fine-tuning, relevant to model compression and efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Haotian Zhang",
            "Liu Liu",
            "Baosheng Yu",
            "Jiayan Qiu",
            "Yanwei Ren",
            "Xianglong Liu"
        ],
        "title": "LARGO: Low-Rank Regulated Gradient Projection for Robust Parameter Efficient Fine-Tuning",
        "abstract": "The advent of parameter-efficient fine-tuning methods has significantly reduced the computational burden of adapting large-scale pretrained models to diverse downstream tasks. However, existing approaches often struggle to achieve robust performance under domain shifts while maintaining computational efficiency. To address this challenge, we propose Low-rAnk Regulated Gradient Projection (LARGO) algorithm that integrates dynamic constraints into low-rank adaptation methods. Specifically, LARGO incorporates parallel trainable gradient projections to dynamically regulate layer-wise updates, retaining the Out-Of-Distribution robustness of pretrained model while preserving inter-layer independence. Additionally, it ensures computational efficiency by mitigating the influence of gradient dependencies across layers during weight updates. Besides, through leveraging singular value decomposition of pretrained weights for structured initialization, we incorporate an SVD-based initialization strategy that minimizing deviation from pretrained knowledge. Through extensive experiments on diverse benchmarks, LARGO achieves state-of-the-art performance across in-domain and out-of-distribution scenarios, demonstrating improved robustness under domain shifts with significantly lower computational overhead compared to existing PEFT methods. The source code will be released soon.",
        "arxiv_id": "2506.12394"
    },
    "2506.13714": {
        "SCORE": 15,
        "ARXIVID": "2506.13714",
        "COMMENT": "The paper provides theoretical insights into learning invariance in deep linear networks, which aligns with the representation learning criterion.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Hao Duan",
            "Guido Mont\\'ufar"
        ],
        "title": "Understanding Learning Invariance in Deep Linear Networks",
        "abstract": "Equivariant and invariant machine learning models exploit symmetries and structural patterns in data to improve sample efficiency. While empirical studies suggest that data-driven methods such as regularization and data augmentation can perform comparably to explicitly invariant models, theoretical insights remain scarce. In this paper, we provide a theoretical comparison of three approaches for achieving invariance: data augmentation, regularization, and hard-wiring. We focus on mean squared error regression with deep linear networks, which parametrize rank-bounded linear maps and can be hard-wired to be invariant to specific group actions. We show that the critical points of the optimization problems for hard-wiring and data augmentation are identical, consisting solely of saddles and the global optimum. By contrast, regularization introduces additional critical points, though they remain saddles except for the global optimum. Moreover, we demonstrate that the regularization path is continuous and converges to the hard-wired solution.",
        "arxiv_id": "2506.13714"
    },
    "2506.12044": {
        "SCORE": 15,
        "ARXIVID": "2506.12044",
        "COMMENT": "The paper analyzes low-bit quantization errors in LLMs, which aligns with the model compression criterion.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Ting-Yun Chang",
            "Muru Zhang",
            "Jesse Thomason",
            "Robin Jia"
        ],
        "title": "Why Do Some Inputs Break Low-Bit LLM Quantization?",
        "abstract": "Low-bit weight-only quantization significantly reduces the memory footprint of large language models (LLMs), but disproportionately affects certain examples. We analyze diverse 3-4 bit methods on LLMs ranging from 7B-70B in size and find that the quantization errors of 50 pairs of methods are strongly correlated (avg. 0.82) on FineWeb examples. Moreover, the residual stream magnitudes of full-precision models are indicative of future quantization errors. We further establish a hypothesis that relates the residual stream magnitudes to error amplification and accumulation over layers. Using LLM localization techniques, early exiting, and activation patching, we show that examples with large errors rely on precise residual activations in the late layers, and that the outputs of MLP gates play a crucial role in maintaining the perplexity. Our work reveals why certain examples result in large quantization errors and which model components are most critical for performance preservation.",
        "arxiv_id": "2506.12044"
    },
    "2506.13253": {
        "SCORE": 15,
        "ARXIVID": "2506.13253",
        "COMMENT": "The paper explores how compositional curricula affect in-context learning in transformers, which is relevant to representation learning and model architecture.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Jin Hwa Lee",
            "Andrew K. Lampinen",
            "Aaditya K. Singh",
            "Andrew M. Saxe"
        ],
        "title": "Distinct Computations Emerge From Compositional Curricula in In-Context Learning",
        "abstract": "In-context learning (ICL) research often considers learning a function in-context through a uniform sample of input-output pairs. Here, we investigate how presenting a compositional subtask curriculum in context may alter the computations a transformer learns. We design a compositional algorithmic task based on the modular exponential-a double exponential task composed of two single exponential subtasks and train transformer models to learn the task in-context. We compare (a) models trained using an in-context curriculum consisting of single exponential subtasks and, (b) models trained directly on the double exponential task without such a curriculum. We show that models trained with a subtask curriculum can perform zero-shot inference on unseen compositional tasks and are more robust given the same context length. We study how the task and subtasks are represented across the two training regimes. We find that the models employ diverse strategies modulated by the specific curriculum design.",
        "arxiv_id": "2506.13253"
    },
    "2506.12379": {
        "SCORE": 15,
        "ARXIVID": "2506.12379",
        "COMMENT": "The paper presents a method for merging LLMs for multi-task learning, which involves model-wise and layer-wise pruning, relevant to model compression.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Zichuan Fu",
            "Xian Wu",
            "Yejing Wang",
            "Wanyu Wang",
            "Shanshan Ye",
            "Hongzhi Yin",
            "Yi Chang",
            "Yefeng Zheng",
            "Xiangyu Zhao"
        ],
        "title": "Training-free LLM Merging for Multi-task Learning",
        "abstract": "Large Language Models (LLMs) have demonstrated exceptional capabilities across diverse natural language processing (NLP) tasks. The release of open-source LLMs like LLaMA and Qwen has triggered the development of numerous fine-tuned models tailored for various tasks and languages. In this paper, we explore an important question: is it possible to combine these specialized models to create a unified model with multi-task capabilities. We introduces Hierarchical Iterative Merging (Hi-Merging), a training-free method for unifying different specialized LLMs into a single model. Specifically, Hi-Merging employs model-wise and layer-wise pruning and scaling, guided by contribution analysis, to mitigate parameter conflicts. Extensive experiments on multiple-choice and question-answering tasks in both Chinese and English validate Hi-Merging's ability for multi-task learning. The results demonstrate that Hi-Merging consistently outperforms existing merging techniques and surpasses the performance of models fine-tuned on combined datasets in most scenarios. Code is available at: https://github.com/Applied-Machine-Learning-Lab/Hi-Merging.",
        "arxiv_id": "2506.12379"
    }
}