{
    "2508.14140": {
        "SCORE": 17,
        "ARXIVID": "2508.14140",
        "COMMENT": "The paper presents a novel architecture inspired by biological neural circuits, focusing on sparsity and efficiency, relevant to model compression and architecture.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Orestis Konstantaropoulos",
            "Stelios Manolis Smirnakis",
            "Maria Papadopouli"
        ],
        "title": "Neuro-inspired Ensemble-to-Ensemble Communication Primitives for Sparse and Efficient ANNs",
        "abstract": "The structure of biological neural circuits-modular, hierarchical, and sparsely interconnected-reflects an efficient trade-off between wiring cost, functional specialization, and robustness. These principles offer valuable insights for artificial neural network (ANN) design, especially as networks grow in depth and scale. Sparsity, in particular, has been widely explored for reducing memory and computation, improving speed, and enhancing generalization. Motivated by systems neuroscience findings, we explore how patterns of functional connectivity in the mouse visual cortex-specifically, ensemble-to-ensemble communication, can inform ANN design. We introduce G2GNet, a novel architecture that imposes sparse, modular connectivity across feedforward layers. Despite having significantly fewer parameters than fully connected models, G2GNet achieves superior accuracy on standard vision benchmarks. To our knowledge, this is the first architecture to incorporate biologically observed functional connectivity patterns as a structural bias in ANN design. We complement this static bias with a dynamic sparse training (DST) mechanism that prunes and regrows edges during training. We also propose a Hebbian-inspired rewiring rule based on activation correlations, drawing on principles of biological plasticity. G2GNet achieves up to 75% sparsity while improving accuracy by up to 4.3% on benchmarks, including Fashion-MNIST, CIFAR-10, and CIFAR-100, outperforming dense baselines with far fewer computations.",
        "arxiv_id": "2508.14140"
    },
    "2508.14302": {
        "SCORE": 17,
        "ARXIVID": "2508.14302",
        "COMMENT": "The paper introduces a novel method for dynamic pruning in LLMs, focusing on model compression through sparsification without training, which aligns with the model compression criteria.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Amirmohsen Sattarifard",
            "Sepehr Lavasani",
            "Ehsan Imani",
            "Kunlin Zhang",
            "Hanlin Xu",
            "Fengyu Sun",
            "Negar Hassanpour",
            "Chao Gao"
        ],
        "title": "GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation",
        "abstract": "Deploying Large Language Models (LLMs) on edge hardware demands aggressive, prompt-aware dynamic pruning to reduce computation without degrading quality. Static or predictor-based schemes either lock in a single sparsity pattern or incur extra runtime overhead, and recent zero-shot methods that rely on statistics from a single prompt fail on short prompt and/or long generation scenarios. We introduce A/I-GLASS: Activation- and Impact-based Global-Local neural importance Aggregation for feed-forward network SparSification, two training-free methods that dynamically select FFN units using a rank-aggregation of prompt local and model-intrinsic global neuron statistics. Empirical results across multiple LLMs and benchmarks demonstrate that GLASS significantly outperforms prior training-free methods, particularly in challenging long-form generation scenarios, without relying on auxiliary predictors or adding any inference overhead.",
        "arxiv_id": "2508.14302"
    },
    "2508.14807": {
        "SCORE": 17,
        "ARXIVID": "2508.14807",
        "COMMENT": "The paper introduces a novel framework for generative models by modifying the source distribution, which aligns with emerging trends in foundational research by challenging established assumptions in generative modeling.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Zifan Wang",
            "Alice Harting",
            "Matthieu Barreau",
            "Michael M. Zavlanos",
            "Karl H. Johansson"
        ],
        "title": "Source-Guided Flow Matching",
        "abstract": "Guidance of generative models is typically achieved by modifying the probability flow vector field through the addition of a guidance field. In this paper, we instead propose the Source-Guided Flow Matching (SGFM) framework, which modifies the source distribution directly while keeping the pre-trained vector field intact. This reduces the guidance problem to a well-defined problem of sampling from the source distribution. We theoretically show that SGFM recovers the desired target distribution exactly. Furthermore, we provide bounds on the Wasserstein error for the generated distribution when using an approximate sampler of the source distribution and an approximate vector field. The key benefit of our approach is that it allows the user to flexibly choose the sampling method depending on their specific problem. To illustrate this, we systematically compare different sampling methods and discuss conditions for asymptotically exact guidance. Moreover, our framework integrates well with optimal flow matching models since the straight transport map generated by the vector field is preserved. Experimental results on synthetic 2D benchmarks, image datasets, and physics-informed generative tasks demonstrate the effectiveness and flexibility of the proposed framework.",
        "arxiv_id": "2508.14807"
    },
    "2508.14896": {
        "SCORE": 16,
        "ARXIVID": "2508.14896",
        "COMMENT": "The paper conducts a systematic study on quantizing diffusion-based language models, relevant to model compression and efficiency.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Haokun Lin",
            "Haobo Xu",
            "Yichen Wu",
            "Ziyu Guo",
            "Renrui Zhang",
            "Zhichao Lu",
            "Ying Wei",
            "Qingfu Zhang",
            "Zhenan Sun"
        ],
        "title": "Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs",
        "abstract": "Recent advances in diffusion large language models (dLLMs) have introduced a promising alternative to autoregressive (AR) LLMs for natural language generation tasks, leveraging full attention and denoising-based decoding strategies. However, the deployment of these models on edge devices remains challenging due to their massive parameter scale and high resource demands. While post-training quantization (PTQ) has emerged as a widely adopted technique for compressing AR LLMs, its applicability to dLLMs remains largely unexplored. In this work, we present the first systematic study on quantizing diffusion-based language models. We begin by identifying the presence of activation outliers, characterized by abnormally large activation values that dominate the dynamic range. These outliers pose a key challenge to low-bit quantization, as they make it difficult to preserve precision for the majority of values. More importantly, we implement state-of-the-art PTQ methods and conduct a comprehensive evaluation across multiple task types and model variants. Our analysis is structured along four key dimensions: bit-width, quantization method, task category, and model type. Through this multi-perspective evaluation, we offer practical insights into the quantization behavior of dLLMs under different configurations. We hope our findings provide a foundation for future research in efficient dLLM deployment. All codes and experimental setups will be released to support the community.",
        "arxiv_id": "2508.14896"
    },
    "2508.14285": {
        "SCORE": 16,
        "ARXIVID": "2508.14285",
        "COMMENT": "The paper discusses low-rank adaptation of large language models, which is relevant to model compression and efficiency improvements.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Liyi Zhang",
            "Jake Snell",
            "Thomas L. Griffiths"
        ],
        "title": "Amortized Bayesian Meta-Learning for Low-Rank Adaptation of Large Language Models",
        "abstract": "Fine-tuning large language models (LLMs) with low-rank adaptaion (LoRA) is a cost-effective way to incorporate information from a specific dataset. However, it is often unclear how well the fine-tuned LLM will generalize, i.e., how well it will perform on unseen datasets. Methods have been proposed to improve generalization by optimizing with in-context prompts, or by using meta-learning to fine-tune LLMs. However, these methods are expensive in memory and computation, requiring either long-context prompts or saving copies of parameters and using second-order gradient updates. To address these challenges, we propose Amortized Bayesian Meta-Learning for LoRA (ABMLL). This method builds on amortized Bayesian meta-learning for smaller models, adapting this approach to LLMs while maintaining its computational efficiency. We reframe task-specific and global parameters in the context of LoRA and use a set of new hyperparameters to balance reconstruction accuracy and the fidelity of task-specific parameters to the global ones. ABMLL provides effective generalization and scales to large models such as Llama3-8B. Furthermore, as a result of using a Bayesian framework, ABMLL provides improved uncertainty quantification. We test ABMLL on Unified-QA and CrossFit datasets and find that it outperforms existing methods on these benchmarks in terms of both accuracy and expected calibration error.",
        "arxiv_id": "2508.14285"
    },
    "2508.14275": {
        "SCORE": 16,
        "ARXIVID": "2508.14275",
        "COMMENT": "The paper explores disentangling concept semantics using sparse autoencoders, which aligns with representation learning and sparse methods.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Cliff O'Reilly",
            "Ernesto Jimenez-Ruiz",
            "Tillman Weyde"
        ],
        "title": "Disentangling concept semantics via multilingual averaging in Sparse Autoencoders",
        "abstract": "Connecting LLMs with formal knowledge representation and reasoning is a promising approach to address their shortcomings. Embeddings and sparse autoencoders are widely used to represent textual content, but the semantics are entangled with syntactic and language-specific information. We propose a method that isolates concept semantics in Large Langue Models by averaging concept activations derived via Sparse Autoencoders. We create English text representations from OWL ontology classes, translate the English into French and Chinese and then pass these texts as prompts to the Gemma 2B LLM. Using the open source Gemma Scope suite of Sparse Autoencoders, we obtain concept activations for each class and language version. We average the different language activations to derive a conceptual average. We then correlate the conceptual averages with a ground truth mapping between ontology classes. Our results give a strong indication that the conceptual average aligns to the true relationship between classes when compared with a single language by itself. The result hints at a new technique which enables mechanistic interpretation of internal network states with higher accuracy.",
        "arxiv_id": "2508.14275"
    },
    "2508.14112": {
        "SCORE": 16,
        "ARXIVID": "2508.14112",
        "COMMENT": "The paper presents a foundation model for heliophysics with a novel spatiotemporal transformer architecture, relevant to model architecture innovations.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Sujit Roy",
            "Johannes Schmude",
            "Rohit Lal",
            "Vishal Gaur",
            "Marcus Freitag",
            "Julian Kuehnert",
            "Theodore van Kessel",
            "Dinesha V. Hegde",
            "Andr\\'es Mu\\~noz-Jaramillo",
            "Johannes Jakubik",
            "Etienne Vos",
            "Kshitiz Mandal",
            "Ata Akbari Asanjan",
            "Joao Lucas de Sousa Almeida",
            "Amy Lin",
            "Talwinder Singh",
            "Kang Yang",
            "Chetraj Pandey",
            "Jinsu Hong",
            "Berkay Aydin",
            "Thorsten Kurth",
            "Ryan McGranaghan",
            "Spiridon Kasapis",
            "Vishal Upendran",
            "Shah Bahauddin",
            "Daniel da Silva",
            "Nikolai V. Pogorelov",
            "Campbell Watson",
            "Manil Maskey",
            "Madhulika Guhathakurta",
            "Juan Bernabe-Moreno",
            "Rahul Ramachandran"
        ],
        "title": "Surya: Foundation Model for Heliophysics",
        "abstract": "Heliophysics is central to understanding and forecasting space weather events and solar activity. Despite decades of high-resolution observations from the Solar Dynamics Observatory (SDO), most models remain task-specific and constrained by scarce labeled data, limiting their capacity to generalize across solar phenomena. We introduce Surya, a 366M parameter foundation model for heliophysics designed to learn general-purpose solar representations from multi-instrument SDO observations, including eight Atmospheric Imaging Assembly (AIA) channels and five Helioseismic and Magnetic Imager (HMI) products. Surya employs a spatiotemporal transformer architecture with spectral gating and long--short range attention, pretrained on high-resolution solar image forecasting tasks and further optimized through autoregressive rollout tuning. Zero-shot evaluations demonstrate its ability to forecast solar dynamics and flare events, while downstream fine-tuning with parameter-efficient Low-Rank Adaptation (LoRA) shows strong performance on solar wind forecasting, active region segmentation, solar flare forecasting, and EUV spectra. Surya is the first foundation model in heliophysics that uses time advancement as a pretext task on full-resolution SDO data. Its novel architecture and performance suggest that the model is able to learn the underlying physics behind solar evolution.",
        "arxiv_id": "2508.14112"
    },
    "2508.14859": {
        "SCORE": 16,
        "ARXIVID": "2508.14859",
        "COMMENT": "The paper proposes a framework integrating Graph Structure Learning with Temporal Graph Information Bottleneck, relevant to representation learning and model architecture.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Jiafeng Xiong",
            "Rizos Sakellariou"
        ],
        "title": "Graph Structure Learning with Temporal Graph Information Bottleneck for Inductive Representation Learning",
        "abstract": "Temporal graph learning is crucial for dynamic networks where nodes and edges evolve over time and new nodes continuously join the system. Inductive representation learning in such settings faces two major challenges: effectively representing unseen nodes and mitigating noisy or redundant graph information. We propose GTGIB, a versatile framework that integrates Graph Structure Learning (GSL) with Temporal Graph Information Bottleneck (TGIB). We design a novel two-step GSL-based structural enhancer to enrich and optimize node neighborhoods and demonstrate its effectiveness and efficiency through theoretical proofs and experiments. The TGIB refines the optimized graph by extending the information bottleneck principle to temporal graphs, regularizing both edges and features based on our derived tractable TGIB objective function via variational approximation, enabling stable and efficient optimization. GTGIB-based models are evaluated to predict links on four real-world datasets; they outperform existing methods in all datasets under the inductive setting, with significant and consistent improvement in the transductive setting.",
        "arxiv_id": "2508.14859"
    },
    "2508.14138": {
        "SCORE": 16,
        "ARXIVID": "2508.14138",
        "COMMENT": "The paper introduces a framework for spiking transformers with adaptive computation time, relevant to model architecture innovations.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Donghwa Kang",
            "Doohyun Kim",
            "Sang-Ki Ko",
            "Jinkyu Lee",
            "Brent ByungHoon Kang",
            "Hyeongboo Baek"
        ],
        "title": "STAS: Spatio-Temporal Adaptive Computation Time for Spiking Transformers",
        "abstract": "Spiking neural networks (SNNs) offer energy efficiency over artificial neural networks (ANNs) but suffer from high latency and computational overhead due to their multi-timestep operational nature. While various dynamic computation methods have been developed to mitigate this by targeting spatial, temporal, or architecture-specific redundancies, they remain fragmented. While the principles of adaptive computation time (ACT) offer a robust foundation for a unified approach, its application to SNN-based vision Transformers (ViTs) is hindered by two core issues: the violation of its temporal similarity prerequisite and a static architecture fundamentally unsuited for its principles. To address these challenges, we propose STAS (Spatio-Temporal Adaptive computation time for Spiking transformers), a framework that co-designs the static architecture and dynamic computation policy. STAS introduces an integrated spike patch splitting (I-SPS) module to establish temporal stability by creating a unified input representation, thereby solving the architectural problem of temporal dissimilarity. This stability, in turn, allows our adaptive spiking self-attention (A-SSA) module to perform two-dimensional token pruning across both spatial and temporal axes. Implemented on spiking Transformer architectures and validated on CIFAR-10, CIFAR-100, and ImageNet, STAS reduces energy consumption by up to 45.9%, 43.8%, and 30.1%, respectively, while simultaneously improving accuracy over SOTA models.",
        "arxiv_id": "2508.14138"
    },
    "2508.14648": {
        "SCORE": 16,
        "ARXIVID": "2508.14648",
        "COMMENT": "The paper introduces a new formulation for approximating data influence, which is relevant to representation learning and training dynamics in neural networks.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Haoru Tan",
            "Sitong Wu",
            "Xiuzhe Wu",
            "Wang Wang",
            "Bo Zhao",
            "Zeke Xie",
            "Gui-Song Xia",
            "Xiaojuan Qi"
        ],
        "title": "Understanding Data Influence with Differential Approximation",
        "abstract": "Data plays a pivotal role in the groundbreaking advancements in artificial intelligence. The quantitative analysis of data significantly contributes to model training, enhancing both the efficiency and quality of data utilization. However, existing data analysis tools often lag in accuracy. For instance, many of these tools even assume that the loss function of neural networks is convex. These limitations make it challenging to implement current methods effectively. In this paper, we introduce a new formulation to approximate a sample's influence by accumulating the differences in influence between consecutive learning steps, which we term Diff-In. Specifically, we formulate the sample-wise influence as the cumulative sum of its changes/differences across successive training iterations. By employing second-order approximations, we approximate these difference terms with high accuracy while eliminating the need for model convexity required by existing methods. Despite being a second-order method, Diff-In maintains computational complexity comparable to that of first-order methods and remains scalable. This efficiency is achieved by computing the product of the Hessian and gradient, which can be efficiently approximated using finite differences of first-order gradients. We assess the approximation accuracy of Diff-In both theoretically and empirically. Our theoretical analysis demonstrates that Diff-In achieves significantly lower approximation error compared to existing influence estimators. Extensive experiments further confirm its superior performance across multiple benchmark datasets in three data-centric tasks: data cleaning, data deletion, and coreset selection. Notably, our experiments on data pruning for large-scale vision-language pre-training show that Diff-In can scale to millions of data points and outperforms strong baselines.",
        "arxiv_id": "2508.14648"
    },
    "2508.14091": {
        "SCORE": 15,
        "ARXIVID": "2508.14091",
        "COMMENT": "The paper focuses on enhancing the explainability and expressivity of GNNs using scoring functions, which aligns with representation learning by providing insights into how GNNs encode information.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Matthew Morris",
            "David J. Tena Cucala",
            "Bernardo Cuenca Grau"
        ],
        "title": "Logical Expressivity and Explanations for Monotonic GNNs with Scoring Functions",
        "abstract": "Graph neural networks (GNNs) are often used for the task of link prediction: predicting missing binary facts in knowledge graphs (KGs). To address the lack of explainability of GNNs on KGs, recent works extract Datalog rules from GNNs with provable correspondence guarantees. The extracted rules can be used to explain the GNN's predictions; furthermore, they can help characterise the expressive power of various GNN models. However, these works address only a form of link prediction based on a restricted, low-expressivity graph encoding/decoding method. In this paper, we consider a more general and popular approach for link prediction where a scoring function is used to decode the GNN output into fact predictions. We show how GNNs and scoring functions can be adapted to be monotonic, use the monotonicity to extract sound rules for explaining predictions, and leverage existing results about the kind of rules that scoring functions can capture. We also define procedures for obtaining equivalent Datalog programs for certain classes of monotonic GNNs with scoring functions. Our experiments show that, on link prediction benchmarks, monotonic GNNs and scoring functions perform well in practice and yield many sound rules.",
        "arxiv_id": "2508.14091"
    },
    "2508.14255": {
        "SCORE": 15,
        "ARXIVID": "2508.14255",
        "COMMENT": "The paper introduces Graph Concept Bottleneck Models, which enhance interpretability and performance by leveraging concept relationships, relevant to representation learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Haotian Xu",
            "Tsui-Wei Weng",
            "Lam M. Nguyen",
            "Tengfei Ma"
        ],
        "title": "Graph Concept Bottleneck Models",
        "abstract": "Concept Bottleneck Models (CBMs) provide explicit interpretations for deep neural networks through concepts and allow intervention with concepts to adjust final predictions. Existing CBMs assume concepts are conditionally independent given labels and isolated from each other, ignoring the hidden relationships among concepts. However, the set of concepts in CBMs often has an intrinsic structure where concepts are generally correlated: changing one concept will inherently impact its related concepts. To mitigate this limitation, we propose GraphCBMs: a new variant of CBM that facilitates concept relationships by constructing latent concept graphs, which can be combined with CBMs to enhance model performance while retaining their interpretability. Our experiment results on real-world image classification tasks demonstrate Graph CBMs offer the following benefits: (1) superior in image classification tasks while providing more concept structure information for interpretability; (2) able to utilize latent concept graphs for more effective interventions; and (3) robust in performance across different training and architecture settings.",
        "arxiv_id": "2508.14255"
    },
    "2508.14085": {
        "SCORE": 15,
        "ARXIVID": "2508.14085",
        "COMMENT": "The paper presents a sparse regression framework for discovering interpretable equations, which aligns with representation learning and sparse methods.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Hanseul Kang",
            "Shervin Karimkashi",
            "Ville Vuorinen"
        ],
        "title": "Parameter-Aware Ensemble SINDy for Interpretable Symbolic SGS Closure",
        "abstract": "We present a scalable, parameter-aware sparse regression framework for discovering interpretable partial differential equations and subgrid-scale closures from multi-parameter simulation data. Building on SINDy (Sparse Identification of Nonlinear Dynamics), our approach addresses key limitations through four innovations: symbolic parameterisation enabling physical parameters to vary within unified regression; Dimensional Similarity Filter enforcing unit-consistency whilst reducing candidate libraries; memory-efficient Gram-matrix accumulation enabling batch processing; and ensemble consensus with coefficient stability analysis for robust model identification.   Validation on canonical one-dimensional benchmarks demonstrates reliable recovery of governing equations across parameter ranges. Applied to filtered Burgers datasets, the framework discovers an SGS closure $\\tau_{\\mathrm{SGS}} = 0.1603\\cdot\\Delta^2\\left(\\frac{\\partial \\bar{u}}{\\partial x}\\right)^2$, corresponding to a Smagorinsky constant of approximately 0.4004. This represents autonomous discovery of Smagorinsky-type closure structure from data without prior theoretical assumptions.   The discovered model achieves $R^2 = 0.886$ across filter scales and demonstrates improved prediction accuracy compared to classical closures. The framework's ability to identify physically meaningful SGS forms and calibrate coefficients offers a complementary approach to existing turbulence modelling methods, contributing to the growing field of data-driven closure discovery.",
        "arxiv_id": "2508.14085"
    },
    "2508.14689": {
        "SCORE": 15,
        "ARXIVID": "2508.14689",
        "COMMENT": "The paper proposes a novel foundation model for machine signal modeling, which is relevant to model architecture and representation learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Yucong Zhang",
            "Juan Liu",
            "Ming Li"
        ],
        "title": "ECHO: Frequency-aware Hierarchical Encoding for Variable-length Signal",
        "abstract": "Pre-trained foundation models have demonstrated remarkable success in vision and language, yet their potential for general machine signal modeling-covering acoustic, vibration, and other industrial sensor data-remains under-explored. Existing approach using sub-band-based encoders has achieved competitive results but are limited by fixed input lengths, and the absence of explicit frequency positional encoding. In this work, we propose a novel foundation model that integrates an advanced band-split architecture with relative frequency positional embeddings, enabling precise spectral localization across arbitrary sampling configurations. The model supports inputs of arbitrary length without padding or segmentation, producing a concise embedding that retains both temporal and spectral fidelity. We evaluate our method on SIREN (https://github.com/yucongzh/SIREN), a newly introduced large-scale benchmark for machine signal encoding that unifies multiple datasets, including all DCASE task 2 challenges (2020-2025) and widely-used industrial signal corpora. Experimental results demonstrate consistent state-of-the-art performance in anomaly detection and fault identification, confirming the effectiveness and generalization capability of the proposed model. We open-sourced ECHO on https://github.com/yucongzh/ECHO.",
        "arxiv_id": "2508.14689"
    },
    "2508.14111": {
        "SCORE": 15,
        "ARXIVID": "2508.14111",
        "COMMENT": "The paper provides a survey on autonomous scientific discovery, which aligns with the AI for Science criteria by discussing foundational capabilities and core processes.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Jiaqi Wei",
            "Yuejin Yang",
            "Xiang Zhang",
            "Yuhan Chen",
            "Xiang Zhuang",
            "Zhangyang Gao",
            "Dongzhan Zhou",
            "Guangshuai Wang",
            "Zhiqiang Gao",
            "Juntai Cao",
            "Zijie Qiu",
            "Xuming He",
            "Qiang Zhang",
            "Chenyu You",
            "Shuangjia Zheng",
            "Ning Ding",
            "Wanli Ouyang",
            "Nanqing Dong",
            "Yu Cheng",
            "Siqi Sun",
            "Lei Bai",
            "Bowen Zhou"
        ],
        "title": "From AI for Science to Agentic Science: A Survey on Autonomous Scientific Discovery",
        "abstract": "Artificial intelligence (AI) is reshaping scientific discovery, evolving from specialized computational tools into autonomous research partners. We position Agentic Science as a pivotal stage within the broader AI for Science paradigm, where AI systems progress from partial assistance to full scientific agency. Enabled by large language models (LLMs), multimodal systems, and integrated research platforms, agentic AI shows capabilities in hypothesis generation, experimental design, execution, analysis, and iterative refinement -- behaviors once regarded as uniquely human. This survey provides a domain-oriented review of autonomous scientific discovery across life sciences, chemistry, materials science, and physics. We unify three previously fragmented perspectives -- process-oriented, autonomy-oriented, and mechanism-oriented -- through a comprehensive framework that connects foundational capabilities, core processes, and domain-specific realizations. Building on this framework, we (i) trace the evolution of AI for Science, (ii) identify five core capabilities underpinning scientific agency, (iii) model discovery as a dynamic four-stage workflow, (iv) review applications across the above domains, and (v) synthesize key challenges and future opportunities. This work establishes a domain-oriented synthesis of autonomous scientific discovery and positions Agentic Science as a structured paradigm for advancing AI-driven research.",
        "arxiv_id": "2508.14111"
    },
    "2508.14330": {
        "SCORE": 15,
        "ARXIVID": "2508.14330",
        "COMMENT": "The paper proposes a novel method for graph condensation using tensor decomposition, which aligns with the model compression criteria by reducing graph size while preserving performance.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "N\\'icolas Roque dos Santos",
            "Dawon Ahn",
            "Diego Minatel",
            "Alneu de Andrade Lopes",
            "Evangelos E. Papalexakis"
        ],
        "title": "Multi-view Graph Condensation via Tensor Decomposition",
        "abstract": "Graph Neural Networks (GNNs) have demonstrated remarkable results in various real-world applications, including drug discovery, object detection, social media analysis, recommender systems, and text classification. In contrast to their vast potential, training them on large-scale graphs presents significant computational challenges due to the resources required for their storage and processing. Graph Condensation has emerged as a promising solution to reduce these demands by learning a synthetic compact graph that preserves the essential information of the original one while maintaining the GNN's predictive performance. Despite their efficacy, current graph condensation approaches frequently rely on a computationally intensive bi-level optimization. Moreover, they fail to maintain a mapping between synthetic and original nodes, limiting the interpretability of the model's decisions. In this sense, a wide range of decomposition techniques have been applied to learn linear or multi-linear functions from graph data, offering a more transparent and less resource-intensive alternative. However, their applicability to graph condensation remains unexplored. This paper addresses this gap and proposes a novel method called Multi-view Graph Condensation via Tensor Decomposition (GCTD) to investigate to what extent such techniques can synthesize an informative smaller graph and achieve comparable downstream task performance. Extensive experiments on six real-world datasets demonstrate that GCTD effectively reduces graph size while preserving GNN performance, achieving up to a 4.0\\ improvement in accuracy on three out of six datasets and competitive performance on large graphs compared to existing approaches. Our code is available at https://anonymous.4open.science/r/gctd-345A.",
        "arxiv_id": "2508.14330"
    },
    "2508.14352": {
        "SCORE": 15,
        "ARXIVID": "2508.14352",
        "COMMENT": "The paper proposes a new model for graph diffusion generative models focusing on scalability and size generalization, which aligns with foundational research in model architecture and efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Junwei Su",
            "Shan Wu"
        ],
        "title": "SBGD: Improving Graph Diffusion Generative Model via Stochastic Block Diffusion",
        "abstract": "Graph diffusion generative models (GDGMs) have emerged as powerful tools for generating high-quality graphs. However, their broader adoption faces challenges in \\emph{scalability and size generalization}. GDGMs struggle to scale to large graphs due to their high memory requirements, as they typically operate in the full graph space, requiring the entire graph to be stored in memory during training and inference. This constraint limits their feasibility for large-scale real-world graphs. GDGMs also exhibit poor size generalization, with limited ability to generate graphs of sizes different from those in the training data, restricting their adaptability across diverse applications. To address these challenges, we propose the stochastic block graph diffusion (SBGD) model, which refines graph representations into a block graph space. This space incorporates structural priors based on real-world graph patterns, significantly reducing memory complexity and enabling scalability to large graphs. The block representation also improves size generalization by capturing fundamental graph structures. Empirical results show that SBGD achieves significant memory improvements (up to 6$\\times$) while maintaining comparable or even superior graph generation performance relative to state-of-the-art methods. Furthermore, experiments demonstrate that SBGD better generalizes to unseen graph sizes. The significance of SBGD extends beyond being a scalable and effective GDGM; it also exemplifies the principle of modularization in generative modeling, offering a new avenue for exploring generative models by decomposing complex tasks into more manageable components.",
        "arxiv_id": "2508.14352"
    }
}