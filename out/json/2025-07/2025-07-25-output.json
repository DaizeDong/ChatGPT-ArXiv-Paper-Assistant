{
    "2507.18074": {
        "SCORE": 19,
        "ARXIVID": "2507.18074",
        "COMMENT": "The paper introduces ASI-Arch, an autonomous system for neural architecture discovery, which is a significant innovation in model architecture.",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "Yixiu Liu",
            "Yang Nan",
            "Weixian Xu",
            "Xiangkun Hu",
            "Lyumanshan Ye",
            "Zhen Qin",
            "Pengfei Liu"
        ],
        "title": "AlphaGo Moment for Model Architecture Discovery",
        "abstract": "While AI systems demonstrate exponentially improving capabilities, the pace of AI research itself remains linearly bounded by human cognitive capacity, creating an increasingly severe development bottleneck. We present ASI-Arch, the first demonstration of Artificial Superintelligence for AI research (ASI4AI) in the critical domain of neural architecture discovery--a fully autonomous system that shatters this fundamental constraint by enabling AI to conduct its own architectural innovation. Moving beyond traditional Neural Architecture Search (NAS), which is fundamentally limited to exploring human-defined spaces, we introduce a paradigm shift from automated optimization to automated innovation. ASI-Arch can conduct end-to-end scientific research in the domain of architecture discovery, autonomously hypothesizing novel architectural concepts, implementing them as executable code, training and empirically validating their performance through rigorous experimentation and past experience. ASI-Arch conducted 1,773 autonomous experiments over 20,000 GPU hours, culminating in the discovery of 106 innovative, state-of-the-art (SOTA) linear attention architectures. Like AlphaGo's Move 37 that revealed unexpected strategic insights invisible to human players, our AI-discovered architectures demonstrate emergent design principles that systematically surpass human-designed baselines and illuminate previously unknown pathways for architectural innovation. Crucially, we establish the first empirical scaling law for scientific discovery itself--demonstrating that architectural breakthroughs can be scaled computationally, transforming research progress from a human-limited to a computation-scalable process. We provide comprehensive analysis of the emergent design patterns and autonomous research capabilities that enabled these breakthroughs, establishing a blueprint for self-accelerating AI systems.",
        "arxiv_id": "2507.18074"
    },
    "2507.18549": {
        "SCORE": 18,
        "ARXIVID": "2507.18549",
        "COMMENT": "The paper presents a universal force-metric-bias law that unifies various learning algorithms, offering a theoretical insight into learning dynamics, which is relevant to representation learning.",
        "RELEVANCE": 9,
        "NOVELTY": 9,
        "authors": [
            "Steven A. Frank"
        ],
        "title": "The Price equation reveals a universal force-metric-bias law of algorithmic learning and natural selection",
        "abstract": "Diverse learning algorithms, optimization methods, and natural selection share a common mathematical structure, despite their apparent differences. Here I show that a simple notational partitioning of change by the Price equation reveals a universal force-metric-bias (FMB) law: $\\Delta\\mathbf{\\theta} = \\mathbf{M}\\,\\mathbf{f} + \\mathbf{b} + \\mathbf{\\xi}$. The force $\\mathbf{f}$ drives improvement in parameters, $\\Delta\\mathbf{\\theta}$, through the covariance between the parameters and performance. The metric $\\mathbf{M}$ rescales movement by inverse curvature. The bias $\\mathbf{b}$ adds momentum or changes in the frame of reference. The noise $\\mathbf{\\xi}$ enables exploration. This framework unifies natural selection, Bayesian updating, Newton's method, stochastic gradient descent, stochastic Langevin dynamics, Adam optimization, and most other algorithms as special cases of the same underlying process. The Price equation also reveals why Fisher information, Kullback-Leibler divergence, and d'Alembert's principle arise naturally in learning dynamics. By exposing this common structure, the FMB law provides a principled foundation for understanding, comparing, and designing learning algorithms across disciplines.",
        "arxiv_id": "2507.18549"
    },
    "2507.18391": {
        "SCORE": 17,
        "ARXIVID": "2507.18391",
        "COMMENT": "The paper presents a theoretical characterization of LLM reasoning using the information bottleneck principle, offering insights into LLM behavior.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Shiye Lei",
            "Zhihao Cheng",
            "Kai Jia",
            "Dacheng Tao"
        ],
        "title": "Revisiting LLM Reasoning via Information Bottleneck",
        "abstract": "Large language models (LLMs) have recently demonstrated remarkable progress in reasoning capabilities through reinforcement learning with verifiable rewards (RLVR). By leveraging simple rule-based rewards, RL effectively incentivizes LLMs to produce extended chain-of-thought (CoT) reasoning trajectories, progressively guiding them toward correct answers. However, existing approaches remain largely heuristic and intuition-driven, limiting the development of principled methodologies. In this paper, we present a theoretical characterization of LLM reasoning grounded in information bottleneck (IB) principle, introducing IB-aware reasoning optimization (IBRO), a framework that encourages reasoning trajectories to be both informative about the final correct answer and generalizable across diverse prompts. We derive a practical token-level surrogate objective and propose an efficient approximation, resulting in the lightweight IB regularization method. This technique integrates seamlessly into existing RL-based post-training frameworks without additional computational overhead, requiring only a one-line code modification. Empirically, we validate IB regularization across multiple mathematical reasoning benchmarks and RL algorithms, demonstrating consistent improvements in LLM reasoning performance.",
        "arxiv_id": "2507.18391"
    },
    "2507.18553": {
        "SCORE": 17,
        "ARXIVID": "2507.18553",
        "COMMENT": "The paper provides a theoretical foundation for GPTQ quantization by relating it to Babai's nearest plane algorithm, relevant to model compression.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Jiale Chen",
            "Torsten Hoefler",
            "Dan Alistarh"
        ],
        "title": "The Geometry of LLM Quantization: GPTQ as Babai's Nearest Plane Algorithm",
        "abstract": "Quantizing the weights of large language models (LLMs) from 16-bit to lower bitwidth is the de facto approach to deploy massive transformers onto more affordable accelerators. GPTQ emerged as one of the standard methods for one-shot post-training quantization at LLM scale. Yet, its inner workings are described as a sequence of ad-hoc algebraic updates that obscure any geometric meaning or worst-case guarantees. In this work, we show that, when executed back-to-front (from the last to first dimension) for a linear layer, GPTQ is mathematically identical to Babai's nearest plane algorithm for the classical closest vector problem (CVP) on a lattice defined by the Hessian matrix of the layer's inputs. This equivalence is based on a sophisticated mathematical argument, and has two analytical consequences: (i) the GPTQ error propagation step gains an intuitive geometric interpretation; (ii) GPTQ inherits the error upper bound of Babai's algorithm under the no-clipping condition. Taken together, these results place GPTQ on firm theoretical footing and open the door to importing decades of progress in lattice algorithms towards the design of future quantization algorithms for billion-parameter models.",
        "arxiv_id": "2507.18553"
    },
    "2507.18073": {
        "SCORE": 17,
        "ARXIVID": "2507.18073",
        "COMMENT": "The paper presents a novel quantization method for LLMs, focusing on model compression through a staged mixed-precision approach, which aligns with the model compression criteria.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Qingcheng Zhu",
            "Yangyang Ren",
            "Linlin Yang",
            "Mingbao Lin",
            "Yanjing Li",
            "Sheng Xu",
            "Zichao Feng",
            "Haodong Zhu",
            "Yuguang Yang",
            "Juan Zhang",
            "Runqi Wang",
            "Baochang Zhang"
        ],
        "title": "Squeeze10-LLM: Squeezing LLMs' Weights by 10 Times via a Staged Mixed-Precision Quantization Method",
        "abstract": "Deploying large language models (LLMs) is challenging due to their massive parameters and high computational costs. Ultra low-bit quantization can significantly reduce storage and accelerate inference, but extreme compression (i.e., mean bit-width <= 2) often leads to severe performance degradation. To address this, we propose Squeeze10-LLM, effectively \"squeezing\" 16-bit LLMs' weights by 10 times. Specifically, Squeeze10-LLM is a staged mixed-precision post-training quantization (PTQ) framework and achieves an average of 1.6 bits per weight by quantizing 80% of the weights to 1 bit and 20% to 4 bits. We introduce Squeeze10LLM with two key innovations: Post-Binarization Activation Robustness (PBAR) and Full Information Activation Supervision (FIAS). PBAR is a refined weight significance metric that accounts for the impact of quantization on activations, improving accuracy in low-bit settings. FIAS is a strategy that preserves full activation information during quantization to mitigate cumulative error propagation across layers. Experiments on LLaMA and LLaMA2 show that Squeeze10-LLM achieves state-of-the-art performance for sub-2bit weight-only quantization, improving average accuracy from 43% to 56% on six zero-shot classification tasks--a significant boost over existing PTQ methods. Our code will be released upon publication.",
        "arxiv_id": "2507.18073"
    },
    "2507.18555": {
        "SCORE": 16,
        "ARXIVID": "2507.18555",
        "COMMENT": "The paper discusses neural tangent kernels and Fisher information matrices for ReLU networks, which aligns with representation learning by providing insights into how networks encode information.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Jun'ichi Takeuchia",
            "Yoshinari Takeishia",
            "Noboru Muratab",
            "Kazushi Mimurac",
            "Ka Long Keith Hod",
            "Hiroshi Nagaoka"
        ],
        "title": "Neural Tangent Kernels and Fisher Information Matrices for Simple ReLU Networks with Random Hidden Weights",
        "abstract": "Fisher information matrices and neural tangent kernels (NTK) for 2-layer ReLU networks with random hidden weight are argued. We discuss the relation between both notions as a linear transformation and show that spectral decomposition of NTK with concrete forms of eigenfunctions with major eigenvalues. We also obtain an approximation formula of the functions presented by the 2-layer neural networks.",
        "arxiv_id": "2507.18555"
    },
    "2507.17785": {
        "SCORE": 16,
        "ARXIVID": "2507.17785",
        "COMMENT": "The paper investigates self-similarity in deep neural networks, providing insights into feature representation and training dynamics.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Jingyi Ding",
            "Chengwen Qi",
            "Hongfei Wang",
            "Jianshe Wu",
            "Licheng Jiao",
            "Yuwei Guo",
            "Jian Gao"
        ],
        "title": "Self-similarity Analysis in Deep Neural Networks",
        "abstract": "Current research has found that some deep neural networks exhibit strong hierarchical self-similarity in feature representation or parameter distribution. However, aside from preliminary studies on how the power-law distribution of weights across different training stages affects model performance,there has been no quantitative analysis on how the self-similarity of hidden space geometry influences model weight optimization, nor is there a clear understanding of the dynamic behavior of internal neurons. Therefore, this paper proposes a complex network modeling method based on the output features of hidden-layer neurons to investigate the self-similarity of feature networks constructed at different hidden layers, and analyzes how adjusting the degree of self-similarity in feature networks can enhance the classification performance of deep neural networks. Validated on three types of networks MLP architectures, convolutional networks, and attention architectures this study reveals that the degree of self-similarity exhibited by feature networks varies across different model architectures. Furthermore, embedding constraints on the self-similarity of feature networks during the training process can improve the performance of self-similar deep neural networks (MLP architectures and attention architectures) by up to 6 percentage points.",
        "arxiv_id": "2507.17785"
    },
    "2507.18607": {
        "SCORE": 16,
        "ARXIVID": "2507.18607",
        "COMMENT": "The paper explores the topological structures of LLM embedding spaces, which aligns with representation learning and provides insights into LLM behavior.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Xinyuan Yan",
            "Rita Sevastjanova",
            "Sinie van der Ben",
            "Mennatallah El-Assady",
            "Bei Wang"
        ],
        "title": "Explainable Mapper: Charting LLM Embedding Spaces Using Perturbation-Based Explanation and Verification Agents",
        "abstract": "Large language models (LLMs) produce high-dimensional embeddings that capture rich semantic and syntactic relationships between words, sentences, and concepts. Investigating the topological structures of LLM embedding spaces via mapper graphs enables us to understand their underlying structures. Specifically, a mapper graph summarizes the topological structure of the embedding space, where each node represents a topological neighborhood (containing a cluster of embeddings), and an edge connects two nodes if their corresponding neighborhoods overlap. However, manually exploring these embedding spaces to uncover encoded linguistic properties requires considerable human effort. To address this challenge, we introduce a framework for semi-automatic annotation of these embedding properties. To organize the exploration process, we first define a taxonomy of explorable elements within a mapper graph such as nodes, edges, paths, components, and trajectories. The annotation of these elements is executed through two types of customizable LLM-based agents that employ perturbation techniques for scalable and automated analysis. These agents help to explore and explain the characteristics of mapper elements and verify the robustness of the generated explanations. We instantiate the framework within a visual analytics workspace and demonstrate its effectiveness through case studies. In particular, we replicate findings from prior research on BERT's embedding properties across various layers of its architecture and provide further observations into the linguistic properties of topological neighborhoods.",
        "arxiv_id": "2507.18607"
    },
    "2507.17951": {
        "SCORE": 16,
        "ARXIVID": "2507.17951",
        "COMMENT": "The paper investigates LLM belief updates in relation to Bayes' theorem, providing theoretical insights into LLM behavior.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Sohaib Imran",
            "Ihor Kendiukhov",
            "Matthew Broerman",
            "Aditya Thomas",
            "Riccardo Campanella",
            "Rob Lamb",
            "Peter M. Atkinson"
        ],
        "title": "Are LLM Belief Updates Consistent with Bayes' Theorem?",
        "abstract": "Do larger and more capable language models learn to update their \"beliefs\" about propositions more consistently with Bayes' theorem when presented with evidence in-context? To test this, we formulate a Bayesian Coherence Coefficient (BCC) metric and generate a dataset with which to measure the BCC. We measure BCC for multiple pre-trained-only language models across five model families, comparing against the number of model parameters, the amount of training data, and model scores on common benchmarks. Our results provide evidence for our hypothesis that larger and more capable pre-trained language models assign credences that are more coherent with Bayes' theorem. These results have important implications for our understanding and governance of LLMs.",
        "arxiv_id": "2507.17951"
    },
    "2507.18520": {
        "SCORE": 16,
        "ARXIVID": "2507.18520",
        "COMMENT": "The paper addresses the correction of Euclidean distances under heteroskedastic noise, providing theoretical insights into noise estimation and correction.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Keyi Li",
            "Yuval Kluger",
            "Boris Landa"
        ],
        "title": "Euclidean Distance Deflation Under High-Dimensional Heteroskedastic Noise",
        "abstract": "Pairwise Euclidean distance calculation is a fundamental step in many machine learning and data analysis algorithms. In real-world applications, however, these distances are frequently distorted by heteroskedastic noise$\\unicode{x2014}$a prevalent form of inhomogeneous corruption characterized by variable noise magnitudes across data observations. Such noise inflates the computed distances in a nontrivial way, leading to misrepresentations of the underlying data geometry. In this work, we address the tasks of estimating the noise magnitudes per observation and correcting the pairwise Euclidean distances under heteroskedastic noise. Perhaps surprisingly, we show that in general high-dimensional settings and without assuming prior knowledge on the clean data structure or noise distribution, both tasks can be performed reliably, even when the noise levels vary considerably. Specifically, we develop a principled, hyperparameter-free approach that jointly estimates the noise magnitudes and corrects the distances. We provide theoretical guarantees for our approach, establishing probabilistic bounds on the estimation errors of both noise magnitudes and distances. These bounds, measured in the normalized $\\ell_1$ norm, converge to zero at polynomial rates as both feature dimension and dataset size increase. Experiments on synthetic datasets demonstrate that our method accurately estimates distances in challenging regimes, significantly improving the robustness of subsequent distance-based computations. Notably, when applied to single-cell RNA sequencing data, our method yields noise magnitude estimates consistent with an established prototypical model, enabling accurate nearest neighbor identification that is fundamental to many downstream analyses.",
        "arxiv_id": "2507.18520"
    },
    "2507.18183": {
        "SCORE": 16,
        "ARXIVID": "2507.18183",
        "COMMENT": "The paper introduces a novel framework for learning with noisy labels, focusing on training dynamics and memory architecture, which is relevant to representation learning.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Jianchao Wang",
            "Qingfeng Li",
            "Pengcheng Zheng",
            "Xiaorong Pu",
            "Yazhou Ren"
        ],
        "title": "ChronoSelect: Robust Learning with Noisy Labels via Dynamics Temporal Memory",
        "abstract": "Training deep neural networks on real-world datasets is often hampered by the presence of noisy labels, which can be memorized by over-parameterized models, leading to significant degradation in generalization performance. While existing methods for learning with noisy labels (LNL) have made considerable progress, they fundamentally suffer from static snapshot evaluations and fail to leverage the rich temporal dynamics of learning evolution. In this paper, we propose ChronoSelect (chrono denoting its temporal nature), a novel framework featuring an innovative four-stage memory architecture that compresses prediction history into compact temporal distributions. Our unique sliding update mechanism with controlled decay maintains only four dynamic memory units per sample, progressively emphasizing recent patterns while retaining essential historical knowledge. This enables precise three-way sample partitioning into clean, boundary, and noisy subsets through temporal trajectory analysis and dual-branch consistency. Theoretical guarantees prove the mechanism's convergence and stability under noisy conditions. Extensive experiments demonstrate ChronoSelect's state-of-the-art performance across synthetic and real-world benchmarks.",
        "arxiv_id": "2507.18183"
    },
    "2507.18533": {
        "SCORE": 15,
        "ARXIVID": "2507.18533",
        "COMMENT": "The paper introduces a data-free knowledge distillation framework using PCA-constrained generators, which aligns with model compression and efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Magnus Bengtsson",
            "Kenneth \\\"Ostberg"
        ],
        "title": "C2G-KD: PCA-Constrained Generator for Data-Free Knowledge Distillation",
        "abstract": "We introduce C2G-KD, a data-free knowledge distillation framework where a class-conditional generator is trained to produce synthetic samples guided by a frozen teacher model and geometric constraints derived from PCA. The generator never observes real training data but instead learns to activate the teacher's output through a combination of semantic and structural losses. By constraining generated samples to lie within class-specific PCA subspaces estimated from as few as two real examples per class, we preserve topological consistency and diversity. Experiments on MNIST show that even minimal class structure is sufficient to bootstrap useful synthetic training pipelines.",
        "arxiv_id": "2507.18533"
    },
    "2507.18145": {
        "SCORE": 15,
        "ARXIVID": "2507.18145",
        "COMMENT": "The paper provides insights into the expressive power of GNNs with mean aggregation, which aligns with representation learning by analyzing how these networks encode information.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Moritz Sch\\\"onherr",
            "Carsten Lutz"
        ],
        "title": "Logical Characterizations of GNNs with Mean Aggregation",
        "abstract": "We study the expressive power of graph neural networks (GNNs) with mean as the aggregation function. In the non-uniform setting, we show that such GNNs have exactly the same expressive power as ratio modal logic, which has modal operators expressing that at least a certain ratio of the successors of a vertex satisfies a specified property. The non-uniform expressive power of mean GNNs is thus higher than that of GNNs with max aggregation, but lower than for sum aggregation--the latter are characterized by modal logic and graded modal logic, respectively. In the uniform setting, we show that the expressive power relative to MSO is exactly that of alternation-free modal logic, under the natural assumptions that combination functions are continuous and classification functions are thresholds. This implies that, relative to MSO and in the uniform setting, mean GNNs are strictly less expressive than sum GNNs and max GNNs. When any of the assumptions is dropped, the expressive power increases.",
        "arxiv_id": "2507.18145"
    },
    "2507.18171": {
        "SCORE": 15,
        "ARXIVID": "2507.18171",
        "COMMENT": "The paper investigates 'sticky tokens' in text embedding models, which relates to representation learning by analyzing how these models encode information.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Kexin Chen",
            "Dongxia Wang",
            "Yi Liu",
            "Haonan Zhang",
            "Wenhai Wang"
        ],
        "title": "Sticking to the Mean: Detecting Sticky Tokens in Text Embedding Models",
        "abstract": "Despite the widespread use of Transformer-based text embedding models in NLP tasks, surprising 'sticky tokens' can undermine the reliability of embeddings. These tokens, when repeatedly inserted into sentences, pull sentence similarity toward a certain value, disrupting the normal distribution of embedding distances and degrading downstream performance. In this paper, we systematically investigate such anomalous tokens, formally defining them and introducing an efficient detection method, Sticky Token Detector (STD), based on sentence and token filtering. Applying STD to 40 checkpoints across 14 model families, we discover a total of 868 sticky tokens. Our analysis reveals that these tokens often originate from special or unused entries in the vocabulary, as well as fragmented subwords from multilingual corpora. Notably, their presence does not strictly correlate with model size or vocabulary size. We further evaluate how sticky tokens affect downstream tasks like clustering and retrieval, observing significant performance drops of up to 50%. Through attention-layer analysis, we show that sticky tokens disproportionately dominate the model's internal representations, raising concerns about tokenization robustness. Our findings show the need for better tokenization strategies and model design to mitigate the impact of sticky tokens in future text embedding applications.",
        "arxiv_id": "2507.18171"
    },
    "2507.17768": {
        "SCORE": 15,
        "ARXIVID": "2507.17768",
        "COMMENT": "The paper enhances quantization-aware training on edge devices, focusing on model compression through coreset selection and layer correction.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Yujia Tong",
            "Jingling Yuan",
            "Chuang Hu"
        ],
        "title": "Enhancing Quantization-Aware Training on Edge Devices via Relative Entropy Coreset Selection and Cascaded Layer Correction",
        "abstract": "With the development of mobile and edge computing, the demand for low-bit quantized models on edge devices is increasing to achieve efficient deployment. To enhance the performance, it is often necessary to retrain the quantized models using edge data. However, due to privacy concerns, certain sensitive data can only be processed on edge devices. Therefore, employing Quantization-Aware Training (QAT) on edge devices has become an effective solution. Nevertheless, traditional QAT relies on the complete dataset for training, which incurs a huge computational cost. Coreset selection techniques can mitigate this issue by training on the most representative subsets. However, existing methods struggle to eliminate quantization errors in the model when using small-scale datasets (e.g., only 10% of the data), leading to significant performance degradation. To address these issues, we propose QuaRC, a QAT framework with coresets on edge devices, which consists of two main phases: In the coreset selection phase, QuaRC introduces the ``Relative Entropy Score\" to identify the subsets that most effectively capture the model's quantization errors. During the training phase, QuaRC employs the Cascaded Layer Correction strategy to align the intermediate layer outputs of the quantized model with those of the full-precision model, thereby effectively reducing the quantization errors in the intermediate layers. Experimental results demonstrate the effectiveness of our approach. For instance, when quantizing ResNet-18 to 2-bit using a 1% data subset, QuaRC achieves a 5.72% improvement in Top-1 accuracy on the ImageNet-1K dataset compared to state-of-the-art techniques.",
        "arxiv_id": "2507.17768"
    },
    "2507.17766": {
        "SCORE": 15,
        "ARXIVID": "2507.17766",
        "COMMENT": "The paper introduces a new architecture for distributed training of LLMs, focusing on scalability and efficiency, which aligns with model architecture and compression topics.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Felix Quinque",
            "Alan Aboudib",
            "Szymon Fonau",
            "Rodrigo Lopez Portillo Alcocer",
            "Brian McCrindle",
            "Steffen Cruz"
        ],
        "title": "Incentivised Orchestrated Training Architecture (IOTA): A Technical Primer for Release",
        "abstract": "In August 2024, Bittensor's Subnet 9 (SN9) demonstrated that a distributed network of incentivized, permissionless actors could each pretrain large language models (LLMs) ranging from 700 million to 14 billion parameters, while surpassing established baselines. While that work validated blockchain-based decentralized pretraining as viable, it contained core issues: (i) every miner had to fit an entire model locally, and (ii) \"winner-takes-all\" rewards encouraged model hoarding.   Here we introduce IOTA (Incentivized Orchestrated Training Architecture), an architecture that addresses these limitations by transforming SN9's previously isolated competitors into a single cooperating unit that can scale arbitrarily while still rewarding each contributor fairly.   Key preliminary results: (1) Data- and Pipeline-parallel SWARM architecture - An orchestrator distributes model layers across heterogeneous miners and streams activations between them, enabling model sizes to scale with the number of participants rather than being constrained by the VRAM of a single machine; (2) Granular, continuous incentives - Validators measure each miner's contribution and allocate token emissions proportionally; (3) Activation compression - We used model-bottlenecks to cut communication bandwidths of activations by up to 128x, vastly improving training speed; (4) Butterfly All-Reduce - Miners average disjoint parameter slices in O(1) bandwidth, offering linear scalability, redundancy and built-in collusion detection; (5) CLASP (Contribution Loss Assessment via Sampling of Pathways) - A fair attribution scheme assigns credit to miners proportional to their marginal utility and detects exploits, even when contributions are interdependent across the pipeline.",
        "arxiv_id": "2507.17766"
    },
    "2507.18043": {
        "SCORE": 15,
        "ARXIVID": "2507.18043",
        "COMMENT": "The paper introduces a novel inference-time steering method for LLMs and VLMs, which involves modifying internal activations and aligns with representation learning and model architecture criteria.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Duy Nguyen",
            "Archiki Prasad",
            "Elias Stengel-Eskin",
            "Mohit Bansal"
        ],
        "title": "GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs and VLMs",
        "abstract": "Inference-time steering methods offer a lightweight alternative to fine-tuning large language models (LLMs) and vision-language models (VLMs) by modifying internal activations at test time without updating model weights. However, most existing approaches rely on fixed, global intervention vectors, overlook the causal influence of individual input tokens, and fail to leverage informative gradients from the model's logits, particularly in multimodal settings where visual and textual inputs contribute unevenly. To address these limitations, we introduce GrAInS, an inference-time steering approach that operates across both language-only and vision-language models and tasks. GrAInS uses contrastive, gradient-based attribution via Integrated Gradients to identify the top-k most influential tokens, both positively and negatively attributed based on their contribution to preferred versus dispreferred outputs. These tokens are then used to construct directional steering vectors that capture semantic shifts from undesirable to desirable behavior. During inference, GrAInS adjusts hidden activations at transformer layers guided by token-level attribution signals, and normalizes activations to preserve representational scale. This enables fine-grained, interpretable, and modular control over model behavior, without retraining or auxiliary supervision. Empirically, GrAInS consistently outperforms both fine-tuning and existing steering baselines: it achieves a 13.22% accuracy gain on TruthfulQA using Llama-3.1-8B, reduces hallucination rates on MMHal-Bench from 0.624 to 0.514 with LLaVA-1.6-7B, and improves alignment win rates on SPA-VL by 8.11%, all while preserving the model's fluency and general capabilities.",
        "arxiv_id": "2507.18043"
    }
}