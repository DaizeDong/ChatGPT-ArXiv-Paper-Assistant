{
    "2505.17117": {
        "COMMENT": "Author match",
        "SCORE": 20.0,
        "authors": [
            "Chen Shani",
            "Dan Jurafsky",
            "Yann LeCun",
            "Ravid Shwartz-Ziv"
        ],
        "title": "From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning",
        "abstract": "Humans organize knowledge into compact categories through semantic compression by mapping diverse instances to abstract representations while preserving meaning (e.g., robin and blue jay are both birds; most birds can fly). These concepts reflect a trade-off between expressive fidelity and representational simplicity. Large Language Models (LLMs) demonstrate remarkable linguistic abilities, yet whether their internal representations strike a human-like trade-off between compression and semantic fidelity is unclear. We introduce a novel information-theoretic framework, drawing from Rate-Distortion Theory and the Information Bottleneck principle, to quantitatively compare these strategies. Analyzing token embeddings from a diverse suite of LLMs against seminal human categorization benchmarks, we uncover key divergences. While LLMs form broad conceptual categories that align with human judgment, they struggle to capture the fine-grained semantic distinctions crucial for human understanding. More fundamentally, LLMs demonstrate a strong bias towards aggressive statistical compression, whereas human conceptual systems appear to prioritize adaptive nuance and contextual richness, even if this results in lower compressional efficiency by our measures. These findings illuminate critical differences between current AI and human cognitive architectures, guiding pathways toward LLMs with more human-aligned conceptual representations.",
        "arxiv_id": "2505.17117"
    },
    "2505.18150": {
        "SCORE": 18,
        "ARXIVID": "2505.18150",
        "COMMENT": "The paper introduces a new framework for learning representations of distributions, relevant to representation learning and generative paradigms.",
        "RELEVANCE": 9,
        "NOVELTY": 9,
        "authors": [
            "Nic Fishman",
            "Gokul Gowri",
            "Peng Yin",
            "Jonathan Gootenberg",
            "Omar Abudayyeh"
        ],
        "title": "Generative Distribution Embeddings",
        "abstract": "Many real-world problems require reasoning across multiple scales, demanding models which operate not on single data points, but on entire distributions. We introduce generative distribution embeddings (GDE), a framework that lifts autoencoders to the space of distributions. In GDEs, an encoder acts on sets of samples, and the decoder is replaced by a generator which aims to match the input distribution. This framework enables learning representations of distributions by coupling conditional generative models with encoder networks which satisfy a criterion we call distributional invariance. We show that GDEs learn predictive sufficient statistics embedded in the Wasserstein space, such that latent GDE distances approximately recover the $W_2$ distance, and latent interpolation approximately recovers optimal transport trajectories for Gaussian and Gaussian mixture distributions. We systematically benchmark GDEs against existing approaches on synthetic datasets, demonstrating consistently stronger performance. We then apply GDEs to six key problems in computational biology: learning representations of cell populations from lineage-tracing data (150K cells), predicting perturbation effects on single-cell transcriptomes (1M cells), predicting perturbation effects on cellular phenotypes (20M single-cell images), modeling tissue-specific DNA methylation patterns (253M sequences), designing synthetic yeast promoters (34M sequences), and spatiotemporal modeling of viral protein sequences (1M sequences).",
        "arxiv_id": "2505.18150"
    },
    "2505.17967": {
        "SCORE": 17,
        "ARXIVID": "2505.17967",
        "COMMENT": "The paper presents a novel low-rank optimization method for LLMs, which is relevant to model compression through low-rank approaches.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Ionut-Vlad Modoranu",
            "Mher Safaryan",
            "Erik Schultheis",
            "Dan Alistarh"
        ],
        "title": "SVD-Free Low-Rank Adaptive Gradient Optimization for Large Language Models",
        "abstract": "Low-rank optimization has emerged as a promising direction in training large language models (LLMs) to reduce the memory usage of adaptive optimizers by constraining learning to a lower-dimensional space. Prior work typically projects gradients of linear layers using approaches based on Singular Value Decomposition (SVD). However, applying SVD-based procedures individually to each layer in large models is computationally expensive and incurs additional memory costs due to storing the projection matrices. In this work, we propose a computationally efficient and conceptually simple two-step procedure to approximate SVD-based gradient projections into lower-dimensional spaces. First, we construct a complete orthogonal basis using predefined orthogonal matrices of the Discrete Cosine Transform (DCT). Second, we adaptively select basis columns based on their alignment with the gradient of each layer. Each projection matrix in our method is obtained via a single matrix multiplication followed by a lightweight sorting step to identify the most relevant basis vectors. Due to the predefined nature of the orthogonal bases, they are computed once at the start of training. During training, we store only the indices of the selected columns, avoiding the need to store full projection matrices for each layer. Our numerical experiments on both pre-training and fine-tuning tasks demonstrate the effectiveness of our dual strategy in approximating optimal low-rank projections, matching the performance of costly SVD-based methods while achieving faster runtime and reduced memory usage.",
        "arxiv_id": "2505.17967"
    },
    "2505.17331": {
        "SCORE": 17,
        "ARXIVID": "2505.17331",
        "COMMENT": "The paper introduces ECHO-LLaMA, focusing on efficient caching and computational efficiency in LLMs, aligning with model compression and efficiency breakthroughs.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Maryam Dialameh",
            "Rezaul Karim",
            "Hossein Rajabzadeh",
            "Omar Mohamed Awad",
            "Hyock Ju Kwon",
            "Boxing Chen",
            "Walid Ahmed",
            "Yang Liu"
        ],
        "title": "ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training",
        "abstract": "This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to improve both the training speed and inference throughput of LLaMA architectures while maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models into shared KV caching across certain layers, significantly reducing KV computational complexity while maintaining or improving language performance. Experimental results demonstrate that ECHO-LLaMA achieves up to 77\\% higher token-per-second throughput during training, up to 16\\% higher Model FLOPs Utilization (MFU), and up to 14\\% lower loss when trained on an equal number of tokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\\% higher test-time throughput compared to the baseline. By introducing a computationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable and cost-effective solution for pretraining and finetuning large language models, enabling faster and more resource-efficient training without compromising performance.",
        "arxiv_id": "2505.17331"
    },
    "2505.17872": {
        "SCORE": 17,
        "ARXIVID": "2505.17872",
        "COMMENT": "The paper introduces a Mixture-of-Low-Rank Adaptation model for time series forecasting, aligning with model architecture and representation learning criteria.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Licheng Pan",
            "Zhichao Chen",
            "Haoxuan Li",
            "Guangyi Liu",
            "Zhijian Xu",
            "Zhaoran Liu",
            "Hao Wang",
            "Ying Wei"
        ],
        "title": "Mixture of Low Rank Adaptation with Partial Parameter Sharing for Time Series Forecasting",
        "abstract": "Multi-task forecasting has become the standard approach for time-series forecasting (TSF). However, we show that it suffers from an Expressiveness Bottleneck, where predictions at different time steps share the same representation, leading to unavoidable errors even with optimal representations. To address this issue, we propose a two-stage framework: first, pre-train a foundation model for one-step-ahead prediction; then, adapt it using step-specific LoRA modules.This design enables the foundation model to handle any number of forecast steps while avoiding the expressiveness bottleneck. We further introduce the Mixture-of-LoRA (MoLA) model, which employs adaptively weighted LoRA experts to achieve partial parameter sharing across steps. This approach enhances both efficiency and forecasting performance by exploiting interdependencies between forecast steps. Experiments show that MoLA significantly improves model expressiveness and outperforms state-of-the-art time-series forecasting methods. Code is available at https://anonymous.4open.science/r/MoLA-BC92.",
        "arxiv_id": "2505.17872"
    },
    "2505.17761": {
        "SCORE": 17,
        "ARXIVID": "2505.17761",
        "COMMENT": "The paper introduces Structured Linear CDEs, a novel sequence model framework, aligning with model architecture innovations.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Benjamin Walker",
            "Lingyi Yang",
            "Nicola Muca Cirone",
            "Cristopher Salvi",
            "Terry Lyons"
        ],
        "title": "Structured Linear CDEs: Maximally Expressive and Parallel-in-Time Sequence Models",
        "abstract": "Structured Linear Controlled Differential Equations (SLiCEs) provide a unifying framework for sequence models with structured, input-dependent state-transition matrices that retain the maximal expressivity of dense matrices whilst being cheaper to compute. The framework encompasses existing architectures, such as input-dependent block-diagonal linear recurrent neural networks and DeltaNet's diagonal-plus-low-rank structure, as well as two novel variants based on sparsity and the Walsh--Hadamard transform. We prove that, unlike the diagonal state-transition matrices of S4 and Mamba, SLiCEs employing block-diagonal, sparse, or Walsh--Hadamard matrices match the maximal expressivity of dense matrices. Empirically, SLiCEs solve the $A_5$ state-tracking benchmark with a single layer, achieve best-in-class length generalisation on regular language tasks among parallel-in-time models, and match the state-of-the-art performance of log neural controlled differential equations on six multivariate time-series classification datasets while cutting the average time per training step by a factor of twenty.",
        "arxiv_id": "2505.17761"
    },
    "2505.17639": {
        "SCORE": 17,
        "ARXIVID": "2505.17639",
        "COMMENT": "PreMoe introduces a framework for efficient deployment of MoE models using expert pruning and retrieval, relevant to model architecture and compression.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Zehua Pei",
            "Ying Zhang",
            "Hui-Ling Zhen",
            "Xianzhi Yu",
            "Wulong Liu",
            "Sinno Jialin Pan",
            "Mingxuan Yuan",
            "Bei Yu"
        ],
        "title": "PreMoe: Lightening MoEs on Constrained Memory by Expert Pruning and Retrieval",
        "abstract": "Mixture-of-experts (MoE) architectures enable scaling large language models (LLMs) to vast parameter counts without a proportional rise in computational costs. However, the significant memory demands of large MoE models hinder their deployment across various computational environments, from cloud servers to consumer devices. This study first demonstrates pronounced task-specific specialization in expert activation patterns within MoE layers. Building on this, we introduce PreMoe, a novel framework that enables efficient deployment of massive MoE models in memory-constrained environments. PreMoe features two main components: probabilistic expert pruning (PEP) and task-adaptive expert retrieval (TAER). PEP employs a new metric, the task-conditioned expected selection score (TCESS), derived from router logits to quantify expert importance for specific tasks, thereby identifying a minimal set of critical experts. TAER leverages these task-specific expert importance profiles for efficient inference. It pre-computes and stores compact expert patterns for diverse tasks. When a user query is received, TAER rapidly identifies the most relevant stored task pattern and reconstructs the model by loading only the small subset of experts crucial for that task. This approach dramatically reduces the memory footprint across all deployment scenarios. DeepSeek-R1 671B maintains 97.2\\% accuracy on MATH500 when pruned to 8/128 configuration (50\\% expert reduction), and still achieves 72.0\\% with aggressive 8/32 pruning (87.5\\% expert reduction). Pangu-Ultra-MoE 718B achieves 97.15\\% on MATH500 and 81.3\\% on AIME24 with 8/128 pruning, while even more aggressive pruning to 4/64 (390GB memory) preserves 96.95\\% accuracy on MATH500. We make our code publicly available at https://github.com/JarvisPei/PreMoe.",
        "arxiv_id": "2505.17639"
    },
    "2505.17974": {
        "SCORE": 17,
        "ARXIVID": "2505.17974",
        "COMMENT": "The paper proposes a novel compression technique for LLMs using a generalized Fisher-weighted SVD, which is relevant to model compression.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Viktoriia Chekalina",
            "Daniil Moskovskiy",
            "Daria Cherniuk",
            "Maxim Kurkin",
            "Andrey Kuznetsov",
            "Evgeny Frolov"
        ],
        "title": "Generalized Fisher-Weighted SVD: Scalable Kronecker-Factored Fisher Approximation for Compressing Large Language Models",
        "abstract": "The Fisher information is a fundamental concept for characterizing the sensitivity of parameters in neural networks. However, leveraging the full observed Fisher information is too expensive for large models, so most methods rely on simple diagonal approximations. While efficient, this approach ignores parameter correlations, often resulting in reduced performance on downstream tasks. In this work, we mitigate these limitations and propose Generalized Fisher-Weighted SVD (GFWSVD), a post-training LLM compression technique that accounts for both diagonal and off-diagonal elements of the Fisher information matrix, providing a more accurate reflection of parameter importance. To make the method tractable, we introduce a scalable adaptation of the Kronecker-factored approximation algorithm for the observed Fisher information. We demonstrate the effectiveness of our method on LLM compression, showing improvements over existing compression baselines. For example, at a 20 compression rate on the MMLU benchmark, our method outperforms FWSVD, which is based on a diagonal approximation of the Fisher information, by 5 percent, SVD-LLM by 3 percent, and ASVD by 6 percent compression rate.",
        "arxiv_id": "2505.17974"
    },
    "2505.17257": {
        "SCORE": 17,
        "ARXIVID": "2505.17257",
        "COMMENT": "The paper introduces JanusDNA, a hybrid DNA foundation model using MoE architecture, relevant to model architecture and foundational research in AI for science.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Qihao Duan",
            "Bingding Huang",
            "Zhenqiao Song",
            "Irina Lehmann",
            "Lei Gu",
            "Roland Eils",
            "Benjamin Wild"
        ],
        "title": "JanusDNA: A Powerful Bi-directional Hybrid DNA Foundation Model",
        "abstract": "Large language models (LLMs) have revolutionized natural language processing and are increasingly applied to other sequential data types, including genetic sequences. However, adapting LLMs to genomics presents significant challenges. Capturing complex genomic interactions requires modeling long-range dependencies within DNA sequences, where interactions often span over 10,000 base pairs, even within a single gene, posing substantial computational burdens under conventional model architectures and training paradigms. Moreover, standard LLM training approaches are suboptimal for DNA: autoregressive training, while efficient, supports only unidirectional understanding. However, DNA is inherently bidirectional, e.g., bidirectional promoters regulate transcription in both directions and account for nearly 11% of human gene expression. Masked language models (MLMs) allow bidirectional understanding but are inefficient, as only masked tokens contribute to the loss per step. To address these limitations, we introduce JanusDNA, the first bidirectional DNA foundation model built upon a novel pretraining paradigm that combines the optimization efficiency of autoregressive modeling with the bidirectional comprehension of masked modeling. JanusDNA adopts a hybrid Mamba, Attention and Mixture of Experts (MoE) architecture, combining long-range modeling of Attention with efficient sequential learning of Mamba. MoE layers further scale model capacity via sparse activation while keeping computational cost low. Notably, JanusDNA processes up to 1 million base pairs at single nucleotide resolution on a single 80GB GPU. Extensive experiments and ablations show JanusDNA achieves new SOTA results on three genomic representation benchmarks, outperforming models with 250x more activated parameters. Code: https://github.com/Qihao-Duan/JanusDNA",
        "arxiv_id": "2505.17257"
    },
    "2505.17958": {
        "SCORE": 17,
        "ARXIVID": "2505.17958",
        "COMMENT": "The paper provides a theoretical analysis of overparameterized quadratic networks, focusing on capacity control through low-rank structures, which is relevant to representation learning and model architecture.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Vittorio Erba",
            "Emanuele Troiani",
            "Lenka Zdeborov\\'a",
            "Florent Krzakala"
        ],
        "title": "The Nuclear Route: Sharp Asymptotics of ERM in Overparameterized Quadratic Networks",
        "abstract": "We study the high-dimensional asymptotics of empirical risk minimization (ERM) in over-parametrized two-layer neural networks with quadratic activations trained on synthetic data. We derive sharp asymptotics for both training and test errors by mapping the $\\ell_2$-regularized learning problem to a convex matrix sensing task with nuclear norm penalization. This reveals that capacity control in such networks emerges from a low-rank structure in the learned feature maps. Our results characterize the global minima of the loss and yield precise generalization thresholds, showing how the width of the target function governs learnability. This analysis bridges and extends ideas from spin-glass methods, matrix factorization, and convex optimization and emphasizes the deep link between low-rank matrix sensing and learning in quadratic neural networks.",
        "arxiv_id": "2505.17958"
    },
    "2505.17083": {
        "SCORE": 17,
        "ARXIVID": "2505.17083",
        "COMMENT": "The paper proposes a scale-invariant attention mechanism, which is relevant to model architecture innovations, particularly in attention mechanisms.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Ben Anson",
            "Xi Wang",
            "Laurence Aitchison"
        ],
        "title": "Scale-invariant Attention",
        "abstract": "One persistent challenge in LLM research is the development of attention mechanisms that are able to generalise from training on shorter contexts to inference on longer contexts. We propose two conditions that we expect all effective long context attention mechanisms to have: scale-invariant total attention, and scale-invariant attention sparsity. Under a Gaussian assumption, we show that a simple position-dependent transformation of the attention logits is sufficient for these conditions to hold. Experimentally we find that the resulting scale-invariant attention scheme gives considerable benefits in terms of validation loss when zero-shot generalising from training on short contexts to validation on longer contexts, and is effective at long-context retrieval.",
        "arxiv_id": "2505.17083"
    },
    "2505.17282": {
        "SCORE": 17,
        "ARXIVID": "2505.17282",
        "COMMENT": "The paper provides theoretical insights into token embeddings and attention mechanisms, relevant to representation learning and model architecture.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Diyuan Wu",
            "Aleksandr Shevchenko",
            "Samet Oymak",
            "Marco Mondelli"
        ],
        "title": "Attention with Trained Embeddings Provably Selects Important Tokens",
        "abstract": "Token embeddings play a crucial role in language modeling but, despite this practical relevance, their theoretical understanding remains limited. Our paper addresses the gap by characterizing the structure of embeddings obtained via gradient descent. Specifically, we consider a one-layer softmax attention model with a linear head for binary classification, i.e., $\\texttt{Softmax}( p^\\top E_X^\\top ) E_X v = \\frac{ \\sum_{i=1}^T \\exp(p^\\top E_{x_i}) E_{x_i}^\\top v}{\\sum_{j=1}^T \\exp(p^\\top E_{x_{j}}) }$, where $E_X = [ E_{x_1} , \\dots, E_{x_T} ]^\\top$ contains the embeddings of the input sequence, $p$ is the embedding of the $\\mathrm{\\langle cls \\rangle}$ token and $v$ the output vector. First, we show that, already after a single step of gradient training with the logistic loss, the embeddings $E_X$ capture the importance of tokens in the dataset by aligning with the output vector $v$ proportionally to the frequency with which the corresponding tokens appear in the dataset. Then, after training $p$ via gradient flow until convergence, the softmax selects the important tokens in the sentence (i.e., those that are predictive of the label), and the resulting $\\mathrm{\\langle cls \\rangle}$ embedding maximizes the margin for such a selection. Experiments on real-world datasets (IMDB, Yelp) exhibit a phenomenology close to that unveiled by our theory.",
        "arxiv_id": "2505.17282"
    },
    "2505.17968": {
        "SCORE": 17,
        "ARXIVID": "2505.17968",
        "COMMENT": "The paper provides theoretical insights into LLM behavior, specifically in reverse-engineering black-box systems, which aligns with the LLM criterion.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Jiayi Geng",
            "Howard Chen",
            "Dilip Arumugam",
            "Thomas L. Griffiths"
        ],
        "title": "Are Large Language Models Reliable AI Scientists? Assessing Reverse-Engineering of Black-Box Systems",
        "abstract": "Using AI to create autonomous researchers has the potential to accelerate scientific discovery. A prerequisite for this vision is understanding how well an AI model can identify the underlying structure of a black-box system from its behavior. In this paper, we explore how well a large language model (LLM) learns to identify a black-box function from passively observed versus actively collected data. We investigate the reverse-engineering capabilities of LLMs across three distinct types of black-box systems, each chosen to represent different problem domains where future autonomous AI researchers may have considerable impact: Program, Formal Language, and Math Equation. Through extensive experiments, we show that LLMs fail to extract information from observations, reaching a performance plateau that falls short of the ideal of Bayesian inference. However, we demonstrate that prompting LLMs to not only observe but also intervene -- actively querying the black-box with specific inputs to observe the resulting output -- improves performance by allowing LLMs to test edge cases and refine their beliefs. By providing the intervention data from one LLM to another, we show that this improvement is partly a result of engaging in the process of generating effective interventions, paralleling results in the literature on human learning. Further analysis reveals that engaging in intervention can help LLMs escape from two common failure modes: overcomplication, where the LLM falsely assumes prior knowledge about the black-box, and overlooking, where the LLM fails to incorporate observations. These insights provide practical guidance for helping LLMs more effectively reverse-engineer black-box systems, supporting their use in making new discoveries.",
        "arxiv_id": "2505.17968"
    },
    "2505.17856": {
        "SCORE": 17,
        "ARXIVID": "2505.17856",
        "COMMENT": "The paper presents a novel approach to compress Bayesian Neural Networks using stochastic weight-sharing quantization, which aligns with the interest in model compression and efficiency breakthroughs.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Moule Lin",
            "Shuhao Guan",
            "Weipeng Jing",
            "Goetz Botterweck",
            "Andrea Patane"
        ],
        "title": "Stochastic Weight Sharing for Bayesian Neural Networks",
        "abstract": "While offering a principled framework for uncertainty quantification in deep learning, the employment of Bayesian Neural Networks (BNNs) is still constrained by their increased computational requirements and the convergence difficulties when training very deep, state-of-the-art architectures. In this work, we reinterpret weight-sharing quantization techniques from a stochastic perspective in the context of training and inference with Bayesian Neural Networks (BNNs). Specifically, we leverage 2D adaptive Gaussian distributions, Wasserstein distance estimations, and alpha blending to encode the stochastic behaviour of a BNN in a lower dimensional, soft Gaussian representation. Through extensive empirical investigation, we demonstrate that our approach significantly reduces the computational overhead inherent in Bayesian learning by several orders of magnitude, enabling the efficient Bayesian training of large-scale models, such as ResNet-101 and Vision Transformer (VIT). On various computer vision benchmarks including CIFAR10, CIFAR100, and ImageNet1k. Our approach compresses model parameters by approximately 50x and reduces model size by 75, while achieving accuracy and uncertainty estimations comparable to the state-of-the-art.",
        "arxiv_id": "2505.17856"
    },
    "2505.17863": {
        "SCORE": 17,
        "ARXIVID": "2505.17863",
        "COMMENT": "The paper studies the emergence of sparse attention in transformers, providing theoretical insights into training dynamics, which aligns with representation learning and model architecture interests.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Nicolas Zucchet",
            "Francesco d'Angelo",
            "Andrew K. Lampinen",
            "Stephanie C. Y. Chan"
        ],
        "title": "The emergence of sparse attention: impact of data distribution and benefits of repetition",
        "abstract": "Emergence is a fascinating property of large language models and neural networks more broadly: as models scale and train for longer, they sometimes develop new abilities in sudden ways. Despite initial studies, we still lack a comprehensive understanding of how and when these abilities emerge. To address this gap, we study the emergence over training of sparse attention, a critical and frequently observed attention pattern in Transformers. By combining theoretical analysis of a toy model with empirical observations on small Transformers trained on a linear regression variant, we uncover the mechanics driving sparse attention emergence and reveal that emergence timing follows power laws based on task structure, architecture, and optimizer choice. We additionally find that repetition can greatly speed up emergence. Finally, we confirm these results on a well-studied in-context associative recall task. Our findings provide a simple, theoretically grounded framework for understanding how data distributions and model design influence the learning dynamics behind one form of emergence.",
        "arxiv_id": "2505.17863"
    },
    "2505.18023": {
        "SCORE": 17,
        "ARXIVID": "2505.18023",
        "COMMENT": "The paper provides theoretical insights into the representational power of spiking neural networks, which aligns with the representation learning criterion. It explores the complexity of input space partitioning and compares SNNs with ANNs, contributing to foundational research.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Duc Anh Nguyen",
            "Ernesto Araya",
            "Adalbert Fono",
            "Gitta Kutyniok"
        ],
        "title": "Time to Spike? Understanding the Representational Power of Spiking Neural Networks in Discrete Time",
        "abstract": "Recent years have seen significant progress in developing spiking neural networks (SNNs) as a potential solution to the energy challenges posed by conventional artificial neural networks (ANNs). However, our theoretical understanding of SNNs remains relatively limited compared to the ever-growing body of literature on ANNs. In this paper, we study a discrete-time model of SNNs based on leaky integrate-and-fire (LIF) neurons, referred to as discrete-time LIF-SNNs, a widely used framework that still lacks solid theoretical foundations. We demonstrate that discrete-time LIF-SNNs with static inputs and outputs realize piecewise constant functions defined on polyhedral regions, and more importantly, we quantify the network size required to approximate continuous functions. Moreover, we investigate the impact of latency (number of time steps) and depth (number of layers) on the complexity of the input space partitioning induced by discrete-time LIF-SNNs. Our analysis highlights the importance of latency and contrasts these networks with ANNs employing piecewise linear activation functions. Finally, we present numerical experiments to support our theoretical findings.",
        "arxiv_id": "2505.18023"
    },
    "2505.17091": {
        "SCORE": 17,
        "ARXIVID": "2505.17091",
        "COMMENT": "The paper suggests that LLMs can inherently develop abilities to understand images and audio, which is a novel insight into LLM behavior.",
        "RELEVANCE": 8,
        "NOVELTY": 9,
        "authors": [
            "Prateek Verma",
            "Mert Pilanci"
        ],
        "title": "Large Language Models Implicitly Learn to See and Hear Just By Reading",
        "abstract": "This paper presents a fascinating find: By training an auto-regressive LLM model on text tokens, the text model inherently develops internally an ability to understand images and audio, thereby developing the ability to see and hear just by reading. Popular audio and visual LLM models fine-tune text LLM models to give text output conditioned on images and audio embeddings. On the other hand, our architecture takes in patches of images, audio waveforms or tokens as input. It gives us the embeddings or category labels typical of a classification pipeline. We show the generality of text weights in aiding audio classification for datasets FSD-50K and GTZAN. Further, we show this working for image classification on CIFAR-10 and Fashion-MNIST, as well on image patches. This pushes the notion of text-LLMs learning powerful internal circuits that can be utilized by activating necessary connections for various applications rather than training models from scratch every single time.",
        "arxiv_id": "2505.17091"
    },
    "2505.17500": {
        "SCORE": 17,
        "ARXIVID": "2505.17500",
        "COMMENT": "The Discovery Engine framework for AI-driven synthesis of scientific knowledge is a novel paradigm, relevant to AI for science.",
        "RELEVANCE": 8,
        "NOVELTY": 9,
        "authors": [
            "Vladimir Baulin",
            "Austin Cook",
            "Daniel Friedman",
            "Janna Lumiruusu",
            "Andrew Pashea",
            "Shagor Rahman",
            "Benedikt Waldeck"
        ],
        "title": "The Discovery Engine: A Framework for AI-Driven Synthesis and Navigation of Scientific Knowledge Landscapes",
        "abstract": "The prevailing model for disseminating scientific knowledge relies on individual publications dispersed across numerous journals and archives. This legacy system is ill suited to the recent exponential proliferation of publications, contributing to insurmountable information overload, issues surrounding reproducibility and retractions. We introduce the Discovery Engine, a framework to address these challenges by transforming an array of disconnected literature into a unified, computationally tractable representation of a scientific domain. Central to our approach is the LLM-driven distillation of publications into structured \"knowledge artifacts,\" instances of a universal conceptual schema, complete with verifiable links to source evidence. These artifacts are then encoded into a high-dimensional Conceptual Tensor. This tensor serves as the primary, compressed representation of the synthesized field, where its labeled modes index scientific components (concepts, methods, parameters, relations) and its entries quantify their interdependencies. The Discovery Engine allows dynamic \"unrolling\" of this tensor into human-interpretable views, such as explicit knowledge graphs (the CNM graph) or semantic vector spaces, for targeted exploration. Crucially, AI agents operate directly on the graph using abstract mathematical and learned operations to navigate the knowledge landscape, identify non-obvious connections, pinpoint gaps, and assist researchers in generating novel knowledge artifacts (hypotheses, designs). By converting literature into a structured tensor and enabling agent-based interaction with this compact representation, the Discovery Engine offers a new paradigm for AI-augmented scientific inquiry and accelerated discovery.",
        "arxiv_id": "2505.17500"
    },
    "2505.17553": {
        "SCORE": 16,
        "ARXIVID": "2505.17553",
        "COMMENT": "CoMoE focuses on enhancing Mixture-of-Experts (MoE) through contrastive representation, which is relevant to model architecture and representation learning.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Jinyuan Feng",
            "Chaopeng Wei",
            "Tenghai Qiu",
            "Tianyi Hu",
            "Zhiqiang Pu"
        ],
        "title": "CoMoE: Contrastive Representation for Mixture-of-Experts in Parameter-Efficient Fine-tuning",
        "abstract": "In parameter-efficient fine-tuning, mixture-of-experts (MoE), which involves specializing functionalities into different experts and sparsely activating them appropriately, has been widely adopted as a promising approach to trade-off between model capacity and computation overhead. However, current MoE variants fall short on heterogeneous datasets, ignoring the fact that experts may learn similar knowledge, resulting in the underutilization of MoE's capacity. In this paper, we propose Contrastive Representation for MoE (CoMoE), a novel method to promote modularization and specialization in MoE, where the experts are trained along with a contrastive objective by sampling from activated and inactivated experts in top-k routing. We demonstrate that such a contrastive objective recovers the mutual-information gap between inputs and the two types of experts. Experiments on several benchmarks and in multi-task settings demonstrate that CoMoE can consistently enhance MoE's capacity and promote modularization among the experts.",
        "arxiv_id": "2505.17553"
    },
    "2505.17101": {
        "SCORE": 16,
        "ARXIVID": "2505.17101",
        "COMMENT": "The paper investigates deep representations in LLMs and vision transformers, aligning with the representation learning criterion.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Santiago Acevedo",
            "Andrea Mascaretti",
            "Riccardo Rende",
            "Mat\\'eo Mahaut",
            "Marco Baroni",
            "Alessandro Laio"
        ],
        "title": "An approach to identify the most semantically informative deep representations of text and images",
        "abstract": "Deep neural networks are known to develop similar representations for semantically related data, even when they belong to different domains, such as an image and its description, or the same text in different languages. We present a method for quantitatively investigating this phenomenon by measuring the relative information content of the representations of semantically related data and probing how it is encoded into multiple tokens of large language models (LLMs) and vision transformers. Looking first at how LLMs process pairs of translated sentences, we identify inner ``semantic'' layers containing the most language-transferable information. We find moreover that, on these layers, a larger LLM (DeepSeek-V3) extracts significantly more general information than a smaller one (Llama3.1-8B). Semantic information is spread across many tokens and it is characterized by long-distance correlations between tokens and by a causal left-to-right (i.e., past-future) asymmetry. We also identify layers encoding semantic information within visual transformers. We show that caption representations in the semantic layers of LLMs predict visual representations of the corresponding images. We observe significant and model-dependent information asymmetries between image and text representations.",
        "arxiv_id": "2505.17101"
    },
    "2505.17495": {
        "SCORE": 16,
        "ARXIVID": "2505.17495",
        "COMMENT": "The paper introduces a novel method for efficient interpretability in LLMs, aligning with the LLM criterion.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Landon Butler",
            "Abhineet Agarwal",
            "Justin Singh Kang",
            "Yigit Efe Erginbas",
            "Bin Yu",
            "Kannan Ramchandran"
        ],
        "title": "ProxySPEX: Inference-Efficient Interpretability via Sparse Feature Interactions in LLMs",
        "abstract": "Large Language Models (LLMs) have achieved remarkable performance by capturing complex interactions between input features. To identify these interactions, most existing approaches require enumerating all possible combinations of features up to a given order, causing them to scale poorly with the number of inputs $n$. Recently, Kang et al. (2025) proposed SPEX, an information-theoretic approach that uses interaction sparsity to scale to $n \\approx 10^3$ features. SPEX greatly improves upon prior methods but requires tens of thousands of model inferences, which can be prohibitive for large models. In this paper, we observe that LLM feature interactions are often hierarchical -- higher-order interactions are accompanied by their lower-order subsets -- which enables more efficient discovery. To exploit this hierarchy, we propose ProxySPEX, an interaction attribution algorithm that first fits gradient boosted trees to masked LLM outputs and then extracts the important interactions. Experiments across four challenging high-dimensional datasets show that ProxySPEX more faithfully reconstructs LLM outputs by 20% over marginal attribution approaches while using $10\\times$ fewer inferences than SPEX. By accounting for interactions, ProxySPEX identifies features that influence model output over 20% more than those selected by marginal approaches. Further, we apply ProxySPEX to two interpretability tasks. Data attribution, where we identify interactions among CIFAR-10 training samples that influence test predictions, and mechanistic interpretability, where we uncover interactions between attention heads, both within and across layers, on a question-answering task. ProxySPEX identifies interactions that enable more aggressive pruning of heads than marginal approaches.",
        "arxiv_id": "2505.17495"
    },
    "2505.17155": {
        "SCORE": 16,
        "ARXIVID": "2505.17155",
        "COMMENT": "The paper proposes a framework for efficient test-time scaling in large reasoning models, focusing on compression and efficiency, which is relevant to model compression.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Weizhe Lin",
            "Xing Li",
            "Zhiyuan Yang",
            "Xiaojin Fu",
            "Hui-Ling Zhen",
            "Yaoyuan Wang",
            "Xianzhi Yu",
            "Wulong Liu",
            "Xiaosong Li",
            "Mingxuan Yuan"
        ],
        "title": "TrimR: Verifier-based Training-Free Thinking Compression for Efficient Test-Time Scaling",
        "abstract": "Large Reasoning Models (LRMs) demonstrate exceptional capability in tackling complex mathematical, logical, and coding tasks by leveraging extended Chain-of-Thought (CoT) reasoning. Test-time scaling methods, such as prolonging CoT with explicit token-level exploration, can push LRMs' accuracy boundaries, but they incur significant decoding overhead. A key inefficiency source is LRMs often generate redundant thinking CoTs, which demonstrate clear structured overthinking and underthinking patterns. Inspired by human cognitive reasoning processes and numerical optimization theories, we propose TrimR, a verifier-based, training-free, efficient framework for dynamic CoT compression to trim reasoning and enhance test-time scaling, explicitly tailored for production-level deployment. Our method employs a lightweight, pretrained, instruction-tuned verifier to detect and truncate redundant intermediate thoughts of LRMs without any LRM or verifier fine-tuning. We present both the core algorithm and asynchronous online system engineered for high-throughput industrial applications. Empirical evaluations on Ascend NPUs and vLLM show that our framework delivers substantial gains in inference efficiency under large-batch workloads. In particular, on the four MATH500, AIME24, AIME25, and GPQA benchmarks, the reasoning runtime of Pangu-R-38B, QwQ-32B, and DeepSeek-R1-Distill-Qwen-32B is improved by up to 70% with negligible impact on accuracy.",
        "arxiv_id": "2505.17155"
    },
    "2505.17491": {
        "SCORE": 16,
        "ARXIVID": "2505.17491",
        "COMMENT": "The paper presents HiLAB, a new paradigm for inverse design in nanophotonics, which aligns with AI for Science through foundational research in molecular modeling.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Reza Marzban",
            "Hamed Abiri",
            "Raphael Pestourie",
            "Ali Adibi"
        ],
        "title": "HiLAB: A Hybrid Inverse-Design Framework",
        "abstract": "HiLAB (Hybrid inverse-design with Latent-space learning, Adjoint-based partial optimizations, and Bayesian optimization) is a new paradigm for inverse design of nanophotonic structures. Combining early-terminated topological optimization (TO) with a Vision Transformer-based variational autoencoder (VAE) and a Bayesian search, HiLAB addresses multi-functional device design by generating diverse freeform configurations at reduced simulation costs. Shortened adjoint-driven TO runs, coupled with randomized physical parameters, produce robust initial structures. These structures are compressed into a compact latent space by the VAE, enabling Bayesian optimization to co-optimize geometry and physical hyperparameters. Crucially, the trained VAE can be reused for alternative objectives or constraints by adjusting only the acquisition function. Compared to conventional TO pipelines prone to local optima, HiLAB systematically explores near-global optima with considerably fewer electromagnetic simulations. Even after accounting for training overhead, the total number of full simulations decreases by over an order of magnitude, accelerating the discovery of fabrication-friendly devices. Demonstrating its efficacy, HiLAB is used to design an achromatic beam deflector for red, green, and blue wavelengths, achieving balanced diffraction efficiencies of ~25% while mitigating chromatic aberrations-a performance surpassing existing demonstrations. Overall, HiLAB provides a flexible platform for robust, multi-parameter photonic designs and rapid adaptation to next-generation nanophotonic challenges.",
        "arxiv_id": "2505.17491"
    },
    "2505.18082": {
        "SCORE": 16,
        "ARXIVID": "2505.18082",
        "COMMENT": "The paper introduces a novel iterative framework for generative backmapping of proteins, aligning with AI for Science through foundational research in molecular modeling.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Georgios Kementzidis",
            "Erin Wong",
            "John Nicholson",
            "Ruichen Xu",
            "Yuefan Deng"
        ],
        "title": "An Iterative Framework for Generative Backmapping of Coarse Grained Proteins",
        "abstract": "The techniques of data-driven backmapping from coarse-grained (CG) to fine-grained (FG) representation often struggle with accuracy, unstable training, and physical realism, especially when applied to complex systems such as proteins. In this work, we introduce a novel iterative framework by using conditional Variational Autoencoders and graph-based neural networks, specifically designed to tackle the challenges associated with such large-scale biomolecules. Our method enables stepwise refinement from CG beads to full atomistic details. We outline the theory of iterative generative backmapping and demonstrate via numerical experiments the advantages of multistep schemes by applying them to proteins of vastly different structures with very coarse representations. This multistep approach not only improves the accuracy of reconstructions but also makes the training process more computationally efficient for proteins with ultra-CG representations.",
        "arxiv_id": "2505.18082"
    },
    "2505.18046": {
        "SCORE": 16,
        "ARXIVID": "2505.18046",
        "COMMENT": "The paper provides a theoretical analysis of Restricted Boltzmann Machines (RBM) using methods like Approximate Message Passing, relevant to representation learning and emerging trends.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Yizhou Xu",
            "Florent Krzakala",
            "Lenka Zdeborov\\'a"
        ],
        "title": "Learning with Restricted Boltzmann Machines: Asymptotics of AMP and GD in High Dimensions",
        "abstract": "The Restricted Boltzmann Machine (RBM) is one of the simplest generative neural networks capable of learning input distributions. Despite its simplicity, the analysis of its performance in learning from the training data is only well understood in cases that essentially reduce to singular value decomposition of the data. Here, we consider the limit of a large dimension of the input space and a constant number of hidden units. In this limit, we simplify the standard RBM training objective into a form that is equivalent to the multi-index model with non-separable regularization. This opens a path to analyze training of the RBM using methods that are established for multi-index models, such as Approximate Message Passing (AMP) and its state evolution, and the analysis of Gradient Descent (GD) via the dynamical mean-field theory. We then give rigorous asymptotics of the training dynamics of RBM on data generated by the spiked covariance model as a prototype of a structure suitable for unsupervised learning. We show in particular that RBM reaches the optimal computational weak recovery threshold, aligning with the BBP transition, in the spiked covariance model.",
        "arxiv_id": "2505.18046"
    },
    "2505.17322": {
        "SCORE": 16,
        "ARXIVID": "2505.17322",
        "COMMENT": "The paper provides a layerwise analysis of in-context learning in LLMs, relevant to understanding LLM behavior and representation learning.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Jiachen Jiang",
            "Yuxin Dong",
            "Jinxin Zhou",
            "Zhihui Zhu"
        ],
        "title": "From Compression to Expansion: A Layerwise Analysis of In-Context Learning",
        "abstract": "In-context learning (ICL) enables large language models (LLMs) to adapt to new tasks without weight updates by learning from demonstration sequences. While ICL shows strong empirical performance, its internal representational mechanisms are not yet well understood. In this work, we conduct a statistical geometric analysis of ICL representations to investigate how task-specific information is captured across layers. Our analysis reveals an intriguing phenomenon, which we term *Layerwise Compression-Expansion*: early layers progressively produce compact and discriminative representations that encode task information from the input demonstrations, while later layers expand these representations to incorporate the query and generate the prediction. This phenomenon is observed consistently across diverse tasks and a range of contemporary LLM architectures. We demonstrate that it has important implications for ICL performance -- improving with model size and the number of demonstrations -- and for robustness in the presence of noisy examples. To further understand the effect of the compact task representation, we propose a bias-variance decomposition and provide a theoretical analysis showing how attention mechanisms contribute to reducing both variance and bias, thereby enhancing performance as the number of demonstrations increases. Our findings reveal an intriguing layerwise dynamic in ICL, highlight how structured representations emerge within LLMs, and showcase that analyzing internal representations can facilitate a deeper understanding of model behavior.",
        "arxiv_id": "2505.17322"
    },
    "2505.17939": {
        "SCORE": 16,
        "ARXIVID": "2505.17939",
        "COMMENT": "The paper introduces Semi-Simplicial Neural Networks for brain activity decoding, which is relevant to emerging trends in model architecture.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Manuel Lecha",
            "Andrea Cavallo",
            "Francesca Dominici",
            "Ran Levi",
            "Alessio Del Bue",
            "Elvin Isufi",
            "Pietro Morerio",
            "Claudio Battiloro"
        ],
        "title": "Directed Semi-Simplicial Learning with Applications to Brain Activity Decoding",
        "abstract": "Graph Neural Networks (GNNs) excel at learning from pairwise interactions but often overlook multi-way and hierarchical relationships. Topological Deep Learning (TDL) addresses this limitation by leveraging combinatorial topological spaces. However, existing TDL models are restricted to undirected settings and fail to capture the higher-order directed patterns prevalent in many complex systems, e.g., brain networks, where such interactions are both abundant and functionally significant. To fill this gap, we introduce Semi-Simplicial Neural Networks (SSNs), a principled class of TDL models that operate on semi-simplicial sets -- combinatorial structures that encode directed higher-order motifs and their directional relationships. To enhance scalability, we propose Routing-SSNs, which dynamically select the most informative relations in a learnable manner. We prove that SSNs are strictly more expressive than standard graph and TDL models. We then introduce a new principled framework for brain dynamics representation learning, grounded in the ability of SSNs to provably recover topological descriptors shown to successfully characterize brain activity. Empirically, SSNs achieve state-of-the-art performance on brain dynamics classification tasks, outperforming the second-best model by up to 27%, and message passing GNNs by up to 50% in accuracy. Our results highlight the potential of principled topological models for learning from structured brain data, establishing a unique real-world case study for TDL. We also test SSNs on standard node classification and edge regression tasks, showing competitive performance. We will make the code and data publicly available.",
        "arxiv_id": "2505.17939"
    },
    "2505.17852": {
        "SCORE": 16,
        "ARXIVID": "2505.17852",
        "COMMENT": "The paper explores zero-order optimization for training large RNNs, which is relevant to model architecture and efficiency improvements.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Francois Chaubard",
            "Mykel Kochenderfer"
        ],
        "title": "Scaling Recurrent Neural Networks to a Billion Parameters with Zero-Order Optimization",
        "abstract": "During inference, Recurrent Neural Networks (RNNs) scale constant in both FLOPs and GPU memory with increasing context length, as they compress all prior tokens into a fixed-size memory. In contrast, transformers scale linearly in FLOPs and, at best, linearly in memory during generation, since they must attend to all previous tokens explicitly. Despite this inference-time advantage, training large RNNs on long contexts remains impractical because standard optimization methods depend on Backpropagation Through Time (BPTT). BPTT requires retention of all intermediate activations during the forward pass, causing memory usage to scale linearly with both context length and model size. In this paper, we show that Zero-Order Optimization (ZOO) methods such as Random-vector Gradient Estimation (RGE) can successfully replace BPTT to train RNNs with convergence rates that match, or exceed BPTT by up to 19 fold, while using orders of magnitude less memory and cost, as the model remains in inference mode throughout training. We further demonstrate that Central-Difference RGE (CD-RGE) corresponds to optimizing a smoothed surrogate loss, inherently regularizing training and improving generalization. Our method matches or outperforms BPTT across three settings: (1) overfitting, (2) transduction, and (3) language modeling. Across all tasks, with sufficient perturbations, our models generalize as well as or better than those trained with BPTT, often in fewer steps. Despite the need for more forward passes per step, we can surpass BPTT wall-clock time per step using recent advancements such as FlashRNN and distributed inference.",
        "arxiv_id": "2505.17852"
    },
    "2505.18069": {
        "SCORE": 16,
        "ARXIVID": "2505.18069",
        "COMMENT": "The paper establishes a connection between SGD and Hebbian learning, which is relevant to emerging trends in learning dynamics.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "David Koplow",
            "Tomaso Poggio",
            "Liu Ziyin"
        ],
        "title": "Emergence of Hebbian Dynamics in Regularized Non-Local Learners",
        "abstract": "Stochastic Gradient Descent (SGD) has emerged as a remarkably effective learning algorithm, underpinning nearly all state-of-the-art machine learning models, from large language models to autonomous vehicles. Despite its practical success, SGD appears fundamentally distinct from biological learning mechanisms. It is widely believed that the biological brain can not implement gradient descent because it is nonlocal, and we have found little (if any) experimental evidence for it. In contrast, the brain is widely thought to learn via local Hebbian learning principles, which have been seen as incompatible with gradient descent. In this paper, we establish a theoretical and empirical connection between the learning signals of neural networks trained using SGD with weight decay and those trained with Hebbian learning near convergence. We show that SGD with regularization can appear to learn according to a Hebbian rule, and SGD with injected noise according to an anti-Hebbian rule. We also provide empirical evidence that Hebbian learning properties can emerge in a network with weight decay from virtually any learning rule--even random ones. These results may bridge a long-standing gap between artificial and biological learning, revealing Hebbian properties as an epiphenomenon of deeper optimization principles and cautioning against interpreting their presence in neural data as evidence against more complex hetero-synaptic mechanisms.",
        "arxiv_id": "2505.18069"
    },
    "2505.17854": {
        "SCORE": 16,
        "ARXIVID": "2505.17854",
        "COMMENT": "The paper presents a novel latent space for neural network verification, which is relevant to model architecture and efficiency improvements.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Lukas Koller",
            "Tobias Ladner",
            "Matthias Althoff"
        ],
        "title": "Out of the Shadows: Exploring a Latent Space for Neural Network Verification",
        "abstract": "Neural networks are ubiquitous. However, they are often sensitive to small input changes. Hence, to prevent unexpected behavior in safety-critical applications, their formal verification -- a notoriously hard problem -- is necessary. Many state-of-the-art verification algorithms use reachability analysis or abstract interpretation to enclose the set of possible outputs of a neural network. Often, the verification is inconclusive due to the conservatism of the enclosure. To address this problem, we design a novel latent space for formal verification that enables the transfer of output specifications to the input space for an iterative specification-driven input refinement, i.e., we iteratively reduce the set of possible inputs to only enclose the unsafe ones. The latent space is constructed from a novel view of projection-based set representations, e.g., zonotopes, which are commonly used in reachability analysis of neural networks. A projection-based set representation is a \"shadow\" of a higher-dimensional set -- a latent space -- that does not change during a set propagation through a neural network. Hence, the input set and the output enclosure are \"shadows\" of the same latent space that we can use to transfer constraints. We present an efficient verification tool for neural networks that uses our iterative refinement to significantly reduce the number of subproblems in a branch-and-bound procedure. Using zonotopes as a set representation, unlike many other state-of-the-art approaches, our approach can be realized by only using matrix operations, which enables a significant speed-up through efficient GPU acceleration. We demonstrate that our tool achieves competitive performance, which would place it among the top-ranking tools of the last neural network verification competition (VNN-COMP'24).",
        "arxiv_id": "2505.17854"
    },
    "2505.18017": {
        "SCORE": 16,
        "ARXIVID": "2505.18017",
        "COMMENT": "The paper introduces a novel sampling algorithm for generative models, aligning with the emerging trends criterion.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Matthieu Blanke",
            "Yongquan Qu",
            "Sara Shamekh",
            "Pierre Gentine"
        ],
        "title": "Strictly Constrained Generative Modeling via Split Augmented Langevin Sampling",
        "abstract": "Deep generative models hold great promise for representing complex physical systems, but their deployment is currently limited by the lack of guarantees on the physical plausibility of the generated outputs. Ensuring that known physical constraints are enforced is therefore critical when applying generative models to scientific and engineering problems. We address this limitation by developing a principled framework for sampling from a target distribution while rigorously satisfying physical constraints. Leveraging the variational formulation of Langevin dynamics, we propose Split Augmented Langevin (SAL), a novel primal-dual sampling algorithm that enforces constraints progressively through variable splitting, with convergence guarantees. While the method is developed theoretically for Langevin dynamics, we demonstrate its effective applicability to diffusion models. In particular, we use constrained diffusion models to generate physical fields satisfying energy and mass conservation laws. We apply our method to diffusion-based data assimilation on a complex physical system, where enforcing physical constraints substantially improves both forecast accuracy and the preservation of critical conserved quantities. We also demonstrate the potential of SAL for challenging feasibility problems in optimal control.",
        "arxiv_id": "2505.18017"
    },
    "2505.17838": {
        "SCORE": 16,
        "ARXIVID": "2505.17838",
        "COMMENT": "The paper provides a theoretical characterization of in-context learning in continuum transformers, which aligns with interests in large language models and theoretical insights.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Abhiti Mishra",
            "Yash Patel",
            "Ambuj Tewari"
        ],
        "title": "Continuum Transformers Perform In-Context Learning by Operator Gradient Descent",
        "abstract": "Transformers robustly exhibit the ability to perform in-context learning, whereby their predictive accuracy on a task can increase not by parameter updates but merely with the placement of training samples in their context windows. Recent works have shown that transformers achieve this by implementing gradient descent in their forward passes. Such results, however, are restricted to standard transformer architectures, which handle finite-dimensional inputs. In the space of PDE surrogate modeling, a generalization of transformers to handle infinite-dimensional function inputs, known as \"continuum transformers,\" has been proposed and similarly observed to exhibit in-context learning. Despite impressive empirical performance, such in-context learning has yet to be theoretically characterized. We herein demonstrate that continuum transformers perform in-context operator learning by performing gradient descent in an operator RKHS. We demonstrate this using novel proof strategies that leverage a generalized representer theorem for Hilbert spaces and gradient flows over the space of functionals of a Hilbert space. We additionally show the operator learned in context is the Bayes Optimal Predictor in the infinite depth limit of the transformer. We then provide empirical validations of this optimality result and demonstrate that the parameters under which such gradient descent is performed are recovered through the continuum transformer training.",
        "arxiv_id": "2505.17838"
    },
    "2505.17304": {
        "SCORE": 15,
        "ARXIVID": "2505.17304",
        "COMMENT": "The paper explores implicit regularization in gradient descent, which is relevant to representation learning and training dynamics in neural networks.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Jianhao Ma",
            "Geyu Liang",
            "Salar Fattahi"
        ],
        "title": "Implicit Regularization of Infinitesimally-perturbed Gradient Descent Toward Low-dimensional Solutions",
        "abstract": "Implicit regularization refers to the phenomenon where local search algorithms converge to low-dimensional solutions, even when such structures are neither explicitly specified nor encoded in the optimization problem. While widely observed, this phenomenon remains theoretically underexplored, particularly in modern over-parameterized problems. In this paper, we study the conditions that enable implicit regularization by investigating when gradient-based methods converge to second-order stationary points (SOSPs) within an implicit low-dimensional region of a smooth, possibly nonconvex function. We show that successful implicit regularization hinges on two key conditions: $(i)$ the ability to efficiently escape strict saddle points, while $(ii)$ maintaining proximity to the implicit region. Existing analyses enabling the convergence of gradient descent (GD) to SOSPs often rely on injecting large perturbations to escape strict saddle points. However, this comes at the cost of deviating from the implicit region. The central premise of this paper is that it is possible to achieve the best of both worlds: efficiently escaping strict saddle points using infinitesimal perturbations, while controlling deviation from the implicit region via a small deviation rate. We show that infinitesimally perturbed gradient descent (IPGD), which can be interpreted as GD with inherent ``round-off errors'', can provably satisfy both conditions. We apply our framework to the problem of over-parameterized matrix sensing, where we establish formal guarantees for the implicit regularization behavior of IPGD. We further demonstrate through extensive experiments that these insights extend to a broader class of learning problems.",
        "arxiv_id": "2505.17304"
    },
    "2505.17169": {
        "SCORE": 15,
        "ARXIVID": "2505.17169",
        "COMMENT": "The paper introduces a metric for assessing LLM perception skills, relevant to theoretical insights into LLM behavior.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Yu-Ang Cheng",
            "Leyang Hu",
            "Hai Huang",
            "Randall Balestriero"
        ],
        "title": "Next Token Perception Score: Analytical Assessment of your LLM Perception Skills",
        "abstract": "Autoregressive pretraining has become the de facto paradigm for learning general-purpose representations in large language models (LLMs). However, linear probe performance across downstream perception tasks shows substantial variability, suggesting that features optimized for next-token prediction do not consistently transfer well to downstream perception tasks. We demonstrate that representations learned via autoregression capture features that may lie outside the subspaces most informative for perception. To quantify the (mis)alignment between autoregressive pretraining and downstream perception, we introduce the Next Token Perception Score (NTPS)-a score derived under a linear setting that measures the overlap between autoregressive and perception feature subspaces. This metric can be easily computed in closed form from pretrained representations and labeled data, and is proven to both upper- and lower-bound the excess loss. Empirically, we show that NTPS correlates strongly with linear probe accuracy across 12 diverse NLP datasets and eight pretrained models ranging from 270M to 8B parameters, confirming its utility as a measure of alignment. Furthermore, we show that NTPS increases following low-rank adaptation (LoRA) fine-tuning, especially in large models, suggesting that LoRA aligning representations to perception tasks enhances subspace overlap and thus improves downstream performance. More importantly, we find that NTPS reliably predicts the additional accuracy gains attained by LoRA finetuning thereby providing a lightweight prescreening tool for LoRA adaptation. Our results offer both theoretical insights and practical tools for analytically assessing LLM perception skills.",
        "arxiv_id": "2505.17169"
    },
    "2505.17646": {
        "SCORE": 15,
        "ARXIVID": "2505.17646",
        "COMMENT": "The paper provides insights into the loss landscape of LLMs, relevant to understanding pretraining and fine-tuning dynamics.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Huanran Chen",
            "Yinpeng Dong",
            "Zeming Wei",
            "Yao Huang",
            "Yichi Zhang",
            "Hang Su",
            "Jun Zhu"
        ],
        "title": "Understanding Pre-training and Fine-tuning from Loss Landscape Perspectives",
        "abstract": "Recent studies have revealed that the loss landscape of large language models resembles a basin, within which the models perform nearly identically, and outside of which they lose all their capabilities. In this work, we conduct further studies on the loss landscape of large language models. We discover that pre-training creates a \"basic capability\" basin, and subsequent fine-tuning creates \"specific capability\" basins (e.g., math, safety, coding) within the basic capability basin. We further investigate two types of loss landscapes: the most-case landscape (i.e., the landscape along most directions) and the worst-case landscape (i.e., the landscape along the worst direction). We argue that as long as benign fine-tuning remains within the most-case basin, it will not compromise previous capabilities. Similarly, any fine-tuning (including the adversarial one) that stays within the worst-case basin would not compromise previous capabilities. Finally, we theoretically demonstrate that the size of the most-case basin can bound the size of the worst-case basin and the robustness with respect to input perturbations. We also show that, due to the over-parameterization property of current large language models, one can easily enlarge the basins by five times.",
        "arxiv_id": "2505.17646"
    },
    "2505.17420": {
        "SCORE": 15,
        "ARXIVID": "2505.17420",
        "COMMENT": "The paper proposes a dynamic layer-skipping framework for LLMs, relevant to model architecture and efficiency improvements.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Ning Yang",
            "Fangxin Liu",
            "Junjie Wang",
            "Tao Yang",
            "Kan Liu",
            "Haibing Guan",
            "Li Jiang"
        ],
        "title": "DASH: Input-Aware Dynamic Layer Skipping for Efficient LLM Inference with Markov Decision Policies",
        "abstract": "Large language models (LLMs) have achieved remarkable performance across a wide range of NLP tasks. However, their substantial inference cost poses a major barrier to real-world deployment, especially in latency-sensitive scenarios. To address this challenge, we propose \\textbf{DASH}, an adaptive layer-skipping framework that dynamically selects computation paths conditioned on input characteristics. We model the skipping process as a Markov Decision Process (MDP), enabling fine-grained token-level decisions based on intermediate representations. To mitigate potential performance degradation caused by skipping, we introduce a lightweight compensation mechanism that injects differential rewards into the decision process. Furthermore, we design an asynchronous execution strategy that overlaps layer computation with policy evaluation to minimize runtime overhead. Experiments on multiple LLM architectures and NLP benchmarks show that our method achieves significant inference acceleration while maintaining competitive task performance, outperforming existing methods.",
        "arxiv_id": "2505.17420"
    },
    "2505.17660": {
        "SCORE": 15,
        "ARXIVID": "2505.17660",
        "COMMENT": "The paper introduces a novel graph transformer architecture with dual positional encoding and attention masking, which aligns with the model architecture criterion.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Chenyang Li",
            "Jinsong Chen",
            "John E. Hopcroft",
            "Kun He"
        ],
        "title": "DAM-GT: Dual Positional Encoding-Based Attention Masking Graph Transformer for Node Classification",
        "abstract": "Neighborhood-aware tokenized graph Transformers have recently shown great potential for node classification tasks. Despite their effectiveness, our in-depth analysis of neighborhood tokens reveals two critical limitations in the existing paradigm. First, current neighborhood token generation methods fail to adequately capture attribute correlations within a neighborhood. Second, the conventional self-attention mechanism suffers from attention diversion when processing neighborhood tokens, where high-hop neighborhoods receive disproportionate focus, severely disrupting information interactions between the target node and its neighborhood tokens. To address these challenges, we propose DAM-GT, Dual positional encoding-based Attention Masking graph Transformer. DAM-GT introduces a novel dual positional encoding scheme that incorporates attribute-aware encoding via an attribute clustering strategy, effectively preserving node correlations in both topological and attribute spaces. In addition, DAM-GT formulates a new attention mechanism with a simple yet effective masking strategy to guide interactions between target nodes and their neighborhood tokens, overcoming the issue of attention diversion. Extensive experiments on various graphs with different homophily levels as well as different scales demonstrate that DAM-GT consistently outperforms state-of-the-art methods in node classification tasks.",
        "arxiv_id": "2505.17660"
    },
    "2505.17965": {
        "SCORE": 15,
        "ARXIVID": "2505.17965",
        "COMMENT": "The paper provides new theoretical bounds for SGD without variance assumption, aligning with emerging trends in theoretical work.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Daniel Cortild",
            "Lucas Ketels",
            "Juan Peypouquet",
            "Guillaume Garrigos"
        ],
        "title": "New Tight Bounds for SGD without Variance Assumption: A Computer-Aided Lyapunov Analysis",
        "abstract": "The analysis of Stochastic Gradient Descent (SGD) often relies on making some assumption on the variance of the stochastic gradients, which is usually not satisfied or difficult to verify in practice. This paper contributes to a recent line of works which attempt to provide guarantees without making any variance assumption, leveraging only the (strong) convexity and smoothness of the loss functions. In this context, we prove new theoretical bounds derived from the monotonicity of a simple Lyapunov energy, improving the current state-of-the-art and extending their validity to larger step-sizes. Our theoretical analysis is backed by a Performance Estimation Problem analysis, which allows us to claim that, empirically, the bias term in our bounds is tight within our framework.",
        "arxiv_id": "2505.17965"
    },
    "2505.17595": {
        "SCORE": 15,
        "ARXIVID": "2505.17595",
        "COMMENT": "NeUQI proposes a method for initializing quantization parameters, relevant to model compression and efficiency, particularly in the context of LLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Li Lin",
            "Xinyu Hu",
            "Xiaojun Wan"
        ],
        "title": "NeUQI: Near-Optimal Uniform Quantization Parameter Initialization",
        "abstract": "Large language models (LLMs) achieve impressive performance across domains but face significant challenges when deployed on consumer-grade GPUs or personal devices such as laptops, due to high memory consumption and inference costs. Post-training quantization (PTQ) of LLMs offers a promising solution that reduces their memory footprint and decoding latency. In practice, PTQ with uniform quantization representation is favored for its efficiency and ease of deployment since uniform quantization is widely supported by mainstream hardware and software libraries. Recent studies on $\\geq 2$-bit uniform quantization have led to noticeable improvements in post-quantization model performance; however, they primarily focus on quantization methodologies, while the initialization of quantization parameters is underexplored and still relies on the suboptimal Min-Max strategies. In this work, we propose NeUQI, a method devoted to efficiently determining near-optimal initial parameters for uniform quantization. NeUQI is orthogonal to prior quantization methodologies and can seamlessly integrate with them. The experiments with the LLaMA and Qwen families on various tasks demonstrate that our NeUQI consistently outperforms existing methods. Furthermore, when combined with a lightweight distillation strategy, NeUQI can achieve superior performance to PV-tuning, a much more resource-intensive approach.",
        "arxiv_id": "2505.17595"
    },
    "2505.17773": {
        "SCORE": 15,
        "ARXIVID": "2505.17773",
        "COMMENT": "The paper introduces C-LoRA, a novel approach for uncertainty-aware fine-tuning of LLMs using contextual low-rank adaptation, which aligns with model compression and efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Amir Hossein Rahmati",
            "Sanket Jantre",
            "Weifeng Zhang",
            "Yucheng Wang",
            "Byung-Jun Yoon",
            "Nathan M. Urban",
            "Xiaoning Qian"
        ],
        "title": "C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in Large Language Models",
        "abstract": "Low-Rank Adaptation (LoRA) offers a cost-effective solution for fine-tuning large language models (LLMs), but it often produces overconfident predictions in data-scarce few-shot settings. To address this issue, several classical statistical learning approaches have been repurposed for scalable uncertainty-aware LoRA fine-tuning. However, these approaches neglect how input characteristics affect the predictive uncertainty estimates. To address this limitation, we propose Contextual Low-Rank Adaptation (\\textbf{C-LoRA}) as a novel uncertainty-aware and parameter efficient fine-tuning approach, by developing new lightweight LoRA modules contextualized to each input data sample to dynamically adapt uncertainty estimates. Incorporating data-driven contexts into the parameter posteriors, C-LoRA mitigates overfitting, achieves well-calibrated uncertainties, and yields robust predictions. Extensive experiments demonstrate that C-LoRA consistently outperforms the state-of-the-art uncertainty-aware LoRA methods in both uncertainty quantification and model generalization. Ablation studies further confirm the critical role of our contextual modules in capturing sample-specific uncertainties. C-LoRA sets a new standard for robust, uncertainty-aware LLM fine-tuning in few-shot regimes.",
        "arxiv_id": "2505.17773"
    },
    "2505.17909": {
        "SCORE": 15,
        "ARXIVID": "2505.17909",
        "COMMENT": "The paper introduces a sparse multi-head architecture with dynamic sparsity, which is relevant to model architecture and sparsity in model compression.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Bram Grooten",
            "Farid Hasanov",
            "Chenxiang Zhang",
            "Qiao Xiao",
            "Boqian Wu",
            "Zahra Atashgahi",
            "Ghada Sokar",
            "Shiwei Liu",
            "Lu Yin",
            "Elena Mocanu",
            "Mykola Pechenizkiy",
            "Decebal Constantin Mocanu"
        ],
        "title": "NeuroTrails: Training with Dynamic Sparse Heads as the Key to Effective Ensembling",
        "abstract": "Model ensembles have long been a cornerstone for improving generalization and robustness in deep learning. However, their effectiveness often comes at the cost of substantial computational overhead. To address this issue, state-of-the-art methods aim to replicate ensemble-class performance without requiring multiple independently trained networks. Unfortunately, these algorithms often still demand considerable compute at inference. In response to these limitations, we introduce $\\textbf{NeuroTrails}$, a sparse multi-head architecture with dynamically evolving topology. This unexplored model-agnostic training paradigm improves ensemble performance while reducing the required resources. We analyze the underlying reason for its effectiveness and observe that the various neural trails induced by dynamic sparsity attain a $\\textit{Goldilocks zone}$ of prediction diversity. NeuroTrails displays efficacy with convolutional and transformer-based architectures on computer vision and language tasks. Experiments on ResNet-50/ImageNet, LLaMA-350M/C4, among many others, demonstrate increased accuracy and stronger robustness in zero-shot generalization, while requiring significantly fewer parameters.",
        "arxiv_id": "2505.17909"
    },
    "2505.17701": {
        "SCORE": 15,
        "ARXIVID": "2505.17701",
        "COMMENT": "The paper proposes a method for sparse activation in LLMs, which is relevant to model compression and efficiency improvements.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Jaewon Cheon",
            "Pilsung Kang"
        ],
        "title": "COUNTDOWN: Contextually Sparse Activation Filtering Out Unnecessary Weights in Down Projection",
        "abstract": "The growing size of large language models has created significant computational inefficiencies. To address this challenge, sparse activation methods selectively deactivates non-essential parameters during inference, reducing computational costs in FFNN layers. While existing methods focus on non-linear gating mechanisms, we hypothesize that the sparsity of the FFNN layer lies globally in the form of a linear combination over its internal down projection matrix. Based on this insight, we propose two methods: M-COUNTDOWN, leveraging indirect coefficients, and D-COUNTDOWN, utilizing direct coefficients of the linear combination. Experimental results demonstrate that D-COUNTDOWN can omit 90% of computations with performance loss as low as 5.5% ideally, while M-COUNTDOWN provides a predictor-free solution with up to 29.4% better performance preservation compared to existing methods. Our specialized kernel implementations effectively realize these theoretical gains into substantial real-world acceleration.",
        "arxiv_id": "2505.17701"
    },
    "2505.17670": {
        "SCORE": 15,
        "ARXIVID": "2505.17670",
        "COMMENT": "The paper introduces a novel continuous memory system for vision-language models, which relates to model architecture innovations.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Wenyi Wu",
            "Zixuan Song",
            "Kun Zhou",
            "Yifei Shao",
            "Zhiting Hu",
            "Biwei Huang"
        ],
        "title": "Towards General Continuous Memory for Vision-Language Models",
        "abstract": "Language models (LMs) and their extension, vision-language models (VLMs), have achieved remarkable performance across various tasks. However, they still struggle with complex reasoning tasks that require multimodal or multilingual real-world knowledge. To support such capabilities, an external memory system that can efficiently provide relevant multimodal information is essential. Existing approaches generally concatenate image and text tokens into a long sequence as memory, which, however, may drastically increase context length and even degrade performance. In contrast, we propose using continuous memory, a compact set of dense embeddings to more effectively and efficiently represent multimodal and multilingual knowledge. Our key insight is that a VLM can serve as its own continuous memory encoder. We empirically show that this design improves performance on complex multimodal reasoning tasks. Building on this, we introduce a data-efficient and parameter-efficient method to fine-tune the VLM into a memory encoder, requiring only 1.2% of the model's parameters and a small corpus of 15.6K self-synthesized samples. Our approach CoMEM utilizes VLM's original capabilities to encode arbitrary multimodal and multilingual knowledge into just 8 continuous embeddings. Since the inference-time VLM remains frozen, our memory module is plug-and-play and can be flexibly integrated as needed. Extensive experiments across eight multimodal reasoning benchmarks demonstrate the effectiveness of our approach.",
        "arxiv_id": "2505.17670"
    },
    "2505.18131": {
        "SCORE": 15,
        "ARXIVID": "2505.18131",
        "COMMENT": "The paper explores the relationship between KANs and MLPs, providing insights into training dynamics and architectural innovations.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Jonas A. Actor",
            "Graham Harper",
            "Ben Southworth",
            "Eric C. Cyr"
        ],
        "title": "Leveraging KANs for Expedient Training of Multichannel MLPs via Preconditioning and Geometric Refinement",
        "abstract": "Multilayer perceptrons (MLPs) are a workhorse machine learning architecture, used in a variety of modern deep learning frameworks. However, recently Kolmogorov-Arnold Networks (KANs) have become increasingly popular due to their success on a range of problems, particularly for scientific machine learning tasks. In this paper, we exploit the relationship between KANs and multichannel MLPs to gain structural insight into how to train MLPs faster. We demonstrate the KAN basis (1) provides geometric localized support, and (2) acts as a preconditioned descent in the ReLU basis, overall resulting in expedited training and improved accuracy. Our results show the equivalence between free-knot spline KAN architectures, and a class of MLPs that are refined geometrically along the channel dimension of each weight tensor. We exploit this structural equivalence to define a hierarchical refinement scheme that dramatically accelerates training of the multi-channel MLP architecture. We show further accuracy improvements can be had by allowing the $1$D locations of the spline knots to be trained simultaneously with the weights. These advances are demonstrated on a range of benchmark examples for regression and scientific machine learning.",
        "arxiv_id": "2505.18131"
    },
    "2505.17315": {
        "SCORE": 15,
        "ARXIVID": "2505.17315",
        "COMMENT": "The paper investigates the role of long-context capacity in reasoning, which is relevant to large language models and their architecture.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Wang Yang",
            "Zirui Liu",
            "Hongye Jin",
            "Qingyu Yin",
            "Vipin Chaudhary",
            "Xiaotian Han"
        ],
        "title": "Longer Context, Deeper Thinking: Uncovering the Role of Long-Context Ability in Reasoning",
        "abstract": "Recent language models exhibit strong reasoning capabilities, yet the influence of long-context capacity on reasoning remains underexplored. In this work, we hypothesize that current limitations in reasoning stem, in part, from insufficient long-context capacity, motivated by empirical observations such as (1) higher context window length often leads to stronger reasoning performance, and (2) failed reasoning cases resemble failed long-context cases. To test this hypothesis, we examine whether enhancing a model's long-context ability before Supervised Fine-Tuning (SFT) leads to improved reasoning performance. Specifically, we compared models with identical architectures and fine-tuning data but varying levels of long-context capacity. Our results reveal a consistent trend: models with stronger long-context capacity achieve significantly higher accuracy on reasoning benchmarks after SFT. Notably, these gains persist even on tasks with short input lengths, indicating that long-context training offers generalizable benefits for reasoning performance. These findings suggest that long-context modeling is not just essential for processing lengthy inputs, but also serves as a critical foundation for reasoning. We advocate for treating long-context capacity as a first-class objective in the design of future language models.",
        "arxiv_id": "2505.17315"
    },
    "2505.17834": {
        "SCORE": 15,
        "ARXIVID": "2505.17834",
        "COMMENT": "The paper introduces a novel hybrid architecture combining Mamba and Transformer layers, which aligns with the model architecture criterion.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Shy-el Cohen",
            "Yoni Choukroun",
            "Eliya Nachmani"
        ],
        "title": "Hybrid Mamba-Transformer Decoder for Error-Correcting Codes",
        "abstract": "We introduce a novel deep learning method for decoding error correction codes based on the Mamba architecture, enhanced with Transformer layers. Our approach proposes a hybrid decoder that leverages Mamba's efficient sequential modeling while maintaining the global context capabilities of Transformers. To further improve performance, we design a novel layer-wise masking strategy applied to each Mamba layer, allowing selective attention to relevant code features at different depths. Additionally, we introduce a progressive layer-wise loss, supervising the network at intermediate stages and promoting robust feature extraction throughout the decoding process. Comprehensive experiments across a range of linear codes demonstrate that our method significantly outperforms Transformer-only decoders and standard Mamba models.",
        "arxiv_id": "2505.17834"
    },
    "2505.17623": {
        "SCORE": 15,
        "ARXIVID": "2505.17623",
        "COMMENT": "The paper introduces a novel framework for verifiable DNN inference, which aligns with the model architecture criterion.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Ali Rahimi",
            "Babak H. Khalaj",
            "Mohammad Ali Maddah-Ali"
        ],
        "title": "\\texttt{Range-Arithmetic}: Verifiable Deep Learning Inference on an Untrusted Party",
        "abstract": "Verifiable computing (VC) has gained prominence in decentralized machine learning systems, where resource-intensive tasks like deep neural network (DNN) inference are offloaded to external participants due to blockchain limitations. This creates a need to verify the correctness of outsourced computations without re-execution. We propose \\texttt{Range-Arithmetic}, a novel framework for efficient and verifiable DNN inference that transforms non-arithmetic operations, such as rounding after fixed-point matrix multiplication and ReLU, into arithmetic steps verifiable using sum-check protocols and concatenated range proofs. Our approach avoids the complexity of Boolean encoding, high-degree polynomials, and large lookup tables while remaining compatible with finite-field-based proof systems. Experimental results show that our method not only matches the performance of existing approaches, but also reduces the computational cost of verifying the results, the computational effort required from the untrusted party performing the DNN inference, and the communication overhead between the two sides.",
        "arxiv_id": "2505.17623"
    },
    "2505.17341": {
        "SCORE": 15,
        "ARXIVID": "2505.17341",
        "COMMENT": "The paper introduces a novel framework for neural operators with adaptive time-stepping, aligning with the model architecture criterion.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Dibyajyoti Nayak",
            "Somdatta Goswami"
        ],
        "title": "TI-DeepONet: Learnable Time Integration for Stable Long-Term Extrapolation",
        "abstract": "Accurate temporal extrapolation presents a fundamental challenge for neural operators in modeling dynamical systems, where reliable predictions must extend significantly beyond the training time horizon. Conventional Deep Operator Network (DeepONet) approaches employ two inherently limited training paradigms - fixed-horizon rollouts that predict complete spatiotemporal solutions while disregarding temporal causality, and autoregressive formulations that accumulate errors through sequential predictions. We introduce TI-DeepONet, a framework that integrates neural operators with adaptive numerical time-stepping techniques to preserve the Markovian structure of dynamical systems while mitigating error propagation in extended temporal forecasting. Our approach reformulates the learning objective from direct state prediction to the approximation of instantaneous time-derivative fields, which are then integrated using established numerical schemes. This architecture supports continuous-time prediction and enables deployment of higher-precision integrators during inference than those used during training, balancing computational efficiency with predictive accuracy. We further develop TI(L)-DeepONet, which incorporates learnable coefficients for intermediate slopes in the integration process, adapting to solution-specific variations and enhancing fidelity. Evaluation across three canonical PDEs shows that TI(L)-DeepONet marginally outperforms TI-DeepONet, with both reducing relative L2 extrapolation errors: approximately 81% over autoregressive and 70% over fixed-horizon methods. Notably, both maintain prediction stability for temporal domains extending to about twice the training interval. This research establishes a physics-aware operator learning paradigm that bridges neural approximation with numerical analysis while preserving the causal structure of dynamical systems.",
        "arxiv_id": "2505.17341"
    },
    "2505.18113": {
        "SCORE": 15,
        "ARXIVID": "2505.18113",
        "COMMENT": "The paper provides a finite-sample analysis of the straight-through estimator for quantization, aligning with the model compression criterion.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Halyun Jeong",
            "Jack Xin",
            "Penghang Yin"
        ],
        "title": "Beyond Discreteness: Finite-Sample Analysis of Straight-Through Estimator for Quantization",
        "abstract": "Training quantized neural networks requires addressing the non-differentiable and discrete nature of the underlying optimization problem. To tackle this challenge, the straight-through estimator (STE) has become the most widely adopted heuristic, allowing backpropagation through discrete operations by introducing surrogate gradients. However, its theoretical properties remain largely unexplored, with few existing works simplifying the analysis by assuming an infinite amount of training data. In contrast, this work presents the first finite-sample analysis of STE in the context of neural network quantization. Our theoretical results highlight the critical role of sample size in the success of STE, a key insight absent from existing studies. Specifically, by analyzing the quantization-aware training of a two-layer neural network with binary weights and activations, we derive the sample complexity bound in terms of the data dimensionality that guarantees the convergence of STE-based optimization to the global minimum. Moreover, in the presence of label noises, we uncover an intriguing recurrence property of STE-gradient method, where the iterate repeatedly escape from and return to the optimal binary weights. Our analysis leverages tools from compressed sensing and dynamical systems theory.",
        "arxiv_id": "2505.18113"
    },
    "2505.17932": {
        "SCORE": 15,
        "ARXIVID": "2505.17932",
        "COMMENT": "The paper introduces a novel selection mechanism for sequence modeling using Linear State Space Models, which is relevant to model architecture innovations.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Umberto Casti",
            "Sandro Zampieri",
            "Fabio Pasqualetti"
        ],
        "title": "Selection Mechanisms for Sequence Modeling using Linear State Space Models",
        "abstract": "Recent advancements in language modeling tasks have been driven by architectures such as Transformers and, more recently, by Selective State Space Models (SSMs). In this paper, we introduce an alternative selection mechanism inspired by control theory methodologies. Specifically, we propose a novel residual generator for selection, drawing an analogy to fault detection strategies in Linear Time-Invariant (LTI) systems. Unlike Mamba, which utilizes Linear Time-Varying (LTV) systems, our approach combines multiple LTI systems, preserving their beneficial properties during training while achieving comparable selectivity. To evaluate the effectiveness of the proposed architecture, we test its performance on synthetic tasks. While these tasks are not inherently critical, they serve as benchmarks to test the selectivity properties of different cores architecture. This work highlights the potential of integrating theoretical insights with experimental advancements, offering a complementary perspective to deep learning innovations at the intersection of control theory and machine learning.",
        "arxiv_id": "2505.17932"
    },
    "2505.17329": {
        "SCORE": 15,
        "ARXIVID": "2505.17329",
        "COMMENT": "The paper explores the use of transformer architectures to model brain activity, focusing on the interpretability and routing of visual information, which aligns with the interest in model architecture and insights into existing architectures.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Hossein Adeli",
            "Minni Sun",
            "Nikolaus Kriegeskorte"
        ],
        "title": "Transformer brain encoders explain human high-level visual responses",
        "abstract": "A major goal of neuroscience is to understand brain computations during visual processing in naturalistic settings. A dominant approach is to use image-computable deep neural networks trained with different task objectives as a basis for linear encoding models. However, in addition to requiring tuning a large number of parameters, the linear encoding approach ignores the structure of the feature maps both in the brain and the models. Recently proposed alternatives have focused on decomposing the linear mapping to spatial and feature components but focus on finding static receptive fields for units that are applicable only in early visual areas. In this work, we employ the attention mechanism used in the transformer architecture to study how retinotopic visual features can be dynamically routed to category-selective areas in high-level visual processing. We show that this computational motif is significantly more powerful than alternative methods in predicting brain activity during natural scene viewing, across different feature basis models and modalities. We also show that this approach is inherently more interpretable, without the need to create importance maps, by interpreting the attention routing signal for different high-level categorical areas. Our approach proposes a mechanistic model of how visual information from retinotopic maps can be routed based on the relevance of the input content to different category-selective regions.",
        "arxiv_id": "2505.17329"
    },
    "2505.17859": {
        "SCORE": 15,
        "ARXIVID": "2505.17859",
        "COMMENT": "The paper proposes a new alignment loss for language models with a provable redescending property, which is relevant to large language models and theoretical insights into model behavior.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Masahiro Fujisawa",
            "Masaki Adachi",
            "Michael A. Osborne"
        ],
        "title": "Scalable Valuation of Human Feedback through Provably Robust Model Alignment",
        "abstract": "Despite the importance of aligning language models with human preferences, crowd-sourced human feedback is often noisy -- for example, preferring less desirable responses -- posing a fundamental challenge to alignment. A truly robust alignment objective should yield identical model parameters even under severe label noise, a property known as redescending. We prove that no existing alignment methods satisfy this property. To address this, we propose H\\\"older-DPO, the first principled alignment loss with a provable redescending property, enabling estimation of the clean data distribution from noisy feedback. The aligned model estimates the likelihood of clean data, providing a theoretically grounded metric for dataset valuation that identifies the location and fraction of mislabels. This metric is gradient-free, enabling scalable and automated human feedback valuation without costly manual verification or clean validation dataset. H\\\"older-DPO achieves state-of-the-art robust alignment performance while accurately detecting mislabels in controlled datasets. Finally, we apply H\\\"older-DPO to widely used alignment datasets, revealing substantial noise levels and demonstrating that removing these mislabels significantly improves alignment performance across methods.",
        "arxiv_id": "2505.17859"
    },
    "2505.17962": {
        "SCORE": 15,
        "ARXIVID": "2505.17962",
        "COMMENT": "The paper presents a Bayesian framework for training binary and spiking neural networks, which aligns with interests in model architecture and efficiency improvements.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "James A. Walker",
            "Moein Khajehnejad",
            "Adeel Razi"
        ],
        "title": "A Principled Bayesian Framework for Training Binary and Spiking Neural Networks",
        "abstract": "We propose a Bayesian framework for training binary and spiking neural networks that achieves state-of-the-art performance without normalisation layers. Unlike commonly used surrogate gradient methods -- often heuristic and sensitive to hyperparameter choices -- our approach is grounded in a probabilistic model of noisy binary networks, enabling fully end-to-end gradient-based optimisation. We introduce importance-weighted straight-through (IW-ST) estimators, a unified class generalising straight-through and relaxation-based estimators. We characterise the bias-variance trade-off in this family and derive a bias-minimising objective implemented via an auxiliary loss. Building on this, we introduce Spiking Bayesian Neural Networks (SBNNs), a variational inference framework that uses posterior noise to train Binary and Spiking Neural Networks with IW-ST. This Bayesian approach minimises gradient bias, regularises parameters, and introduces dropout-like noise. By linking low-bias conditions, vanishing gradients, and the KL term, we enable training of deep residual networks without normalisation. Experiments on CIFAR-10, DVS Gesture, and SHD show our method matches or exceeds existing approaches without normalisation or hand-tuned gradients.",
        "arxiv_id": "2505.17962"
    }
}