{
    "2510.02142": {
        "COMMENT": "Author match",
        "SCORE": 20.0,
        "authors": [
            "Lena Podina",
            "Christina Humer",
            "Alexandre Duval",
            "Victor Schmidt",
            "Ali Ramlaoui",
            "Shahana Chatterjee",
            "Yoshua Bengio",
            "Alex Hernandez-Garcia",
            "David Rolnick",
            "F\\'elix Therrien"
        ],
        "title": "Catalyst GFlowNet for electrocatalyst design: A hydrogen evolution reaction case study",
        "abstract": "Efficient and inexpensive energy storage is essential for accelerating the adoption of renewable energy and ensuring a stable supply, despite fluctuations in sources such as wind and solar. Electrocatalysts play a key role in hydrogen energy storage (HES), allowing the energy to be stored as hydrogen. However, the development of affordable and high-performance catalysts for this process remains a significant challenge. We introduce Catalyst GFlowNet, a generative model that leverages machine learning-based predictors of formation and adsorption energy to design crystal surfaces that act as efficient catalysts. We demonstrate the performance of the model through a proof-of-concept application to the hydrogen evolution reaction, a key reaction in HES, for which we successfully identified platinum as the most efficient known catalyst. In future work, we aim to extend this approach to the oxygen evolution reaction, where current optimal catalysts are expensive metal oxides, and open the search space to discover new materials. This generative modeling framework offers a promising pathway for accelerating the search for novel and efficient catalysts.",
        "arxiv_id": "2510.02142"
    },
    "2510.01643": {
        "SCORE": 19,
        "ARXIVID": "2510.01643",
        "COMMENT": "Efficient attention approximation with sub-quadratic runtime beyond bounded entries; rigorous guarantees and justification of polynomial attention.",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "Maryam Aliakbarpour",
            "Vladimir Braverman",
            "Junze Yin",
            "Haochen Zhang"
        ],
        "title": "Support Basis: Fast Attention Beyond Bounded Entries",
        "abstract": "The quadratic complexity of softmax attention remains a central bottleneck in scaling large language models (LLMs). [Alman and Song, NeurIPS 2023] proposed a sub-quadratic attention approximation algorithm, but it works only under the restrictive bounded-entry assumption. Since this assumption rarely holds in practice, its applicability to modern LLMs is limited.   In this paper, we introduce support-basis decomposition, a new framework for efficient attention approximation beyond bounded entries. We empirically demonstrate that the entries of the query and key matrices exhibit sub-Gaussian behavior. Our approach uses this property to split large and small entries, enabling exact computation on sparse components and polynomial approximation on dense components. We establish rigorous theoretical guarantees, proving a sub-quadratic runtime, and extend the method to a multi-threshold setting that eliminates all distributional assumptions. Furthermore, we provide the first theoretical justification for the empirical success of polynomial attention [Kacham, Mirrokni, and Zhong, ICML 2024], showing that softmax attention can be closely approximated by a combination of multiple polynomial attentions with sketching.",
        "arxiv_id": "2510.01643"
    },
    "2510.01650": {
        "SCORE": 19,
        "ARXIVID": "2510.01650",
        "COMMENT": "Model Compression and Efficiency \u2014 extreme sparsity/pruning for LLMs via surrogate-free ADMM; includes quantized variant and convergence guarantees.",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "Kwanhee Lee",
            "Hyeondo Jang",
            "Dongyeop Lee",
            "Dan Alistarh",
            "Namhoon Lee"
        ],
        "title": "The Unseen Frontier: Pushing the Limits of LLM Sparsity with Surrogate-Free ADMM",
        "abstract": "Neural network pruning is a promising technique to mitigate the excessive computational and memory requirements of large language models (LLMs). Despite its promise, however, progress in this area has diminished, as conventional methods are seemingly unable to surpass moderate sparsity levels (50-60%) without severely degrading model accuracy. This work breaks through the current impasse, presenting a principled and effective method called $\\texttt{Elsa}$, which achieves extreme sparsity levels of up to 90% while retaining high model fidelity. This is done by identifying several limitations in current practice, all of which can be traced back to their reliance on a surrogate objective formulation. $\\texttt{Elsa}$ tackles this issue directly and effectively via standard and well-established constrained optimization techniques based on ADMM. Our extensive experiments across a wide range of models and scales show that $\\texttt{Elsa}$ achieves substantial improvements over existing methods; e.g., it achieves 7.8$\\times$ less perplexity than the best existing method on LLaMA-2-7B at 90% sparsity. Furthermore, we present $\\texttt{Elsa}_{\\text{-L}}$, a quantized variant that scales to extremely large models (27B), and establish its theoretical convergence guarantees. These results highlight meaningful progress in advancing the frontier of LLM sparsity, while promising that significant opportunities for further advancement may remain in directions that have so far attracted limited exploration.",
        "arxiv_id": "2510.01650"
    },
    "2510.01345": {
        "SCORE": 18,
        "ARXIVID": "2510.01345",
        "COMMENT": "Theoretical unification of self-supervised representation learning via MI; explains stop-gradient and predictor networks from first principles.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Akhlaqur Rahman Sabby",
            "Yi Sui",
            "Tongzi Wu",
            "Jesse C. Cresswell",
            "Ga Wu"
        ],
        "title": "Self-Supervised Representation Learning as Mutual Information Maximization",
        "abstract": "Self-supervised representation learning (SSRL) has demonstrated remarkable empirical success, yet its underlying principles remain insufficiently understood. While recent works attempt to unify SSRL methods by examining their information-theoretic objectives or summarizing their heuristics for preventing representation collapse, architectural elements like the predictor network, stop-gradient operation, and statistical regularizer are often viewed as empirically motivated additions. In this paper, we adopt a first-principles approach and investigate whether the learning objective of an SSRL algorithm dictates its possible optimization strategies and model design choices. In particular, by starting from a variational mutual information (MI) lower bound, we derive two training paradigms, namely Self-Distillation MI (SDMI) and Joint MI (JMI), each imposing distinct structural constraints and covering a set of existing SSRL algorithms. SDMI inherently requires alternating optimization, making stop-gradient operations theoretically essential. In contrast, JMI admits joint optimization through symmetric architectures without such components. Under the proposed formulation, predictor networks in SDMI and statistical regularizers in JMI emerge as tractable surrogates for the MI objective. We show that many existing SSRL methods are specific instances or approximations of these two paradigms. This paper provides a theoretical explanation behind the choices of different architectural components of existing SSRL methods, beyond heuristic conveniences.",
        "arxiv_id": "2510.01345"
    },
    "2510.01263": {
        "SCORE": 18,
        "ARXIVID": "2510.01263",
        "COMMENT": "Matches Model Compression and Efficiency: introduces an activity-dependent pruning rule with constrained-entropy analysis to balance fan-in/fan-out (sparsity/pruning) for efficiency.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Yaron Meirovitch",
            "Fuming Yang",
            "Jeff Lichtman",
            "Nir Shavit"
        ],
        "title": "Budgeted Broadcast: An Activity-Dependent Pruning Rule for Neural Network Efficiency",
        "abstract": "Most pruning methods remove parameters ranked by impact on loss (e.g., magnitude or gradient). We propose Budgeted Broadcast (BB), which gives each unit a local traffic budget (the product of its long-term on-rate $a_i$ and fan-out $k_i$). A constrained-entropy analysis shows that maximizing coding entropy under a global traffic budget yields a selectivity-audience balance, $\\log\\frac{1-a_i}{a_i}=\\beta k_i$. BB enforces this balance with simple local actuators that prune either fan-in (to lower activity) or fan-out (to reduce broadcast). In practice, BB increases coding entropy and decorrelation and improves accuracy at matched sparsity across Transformers for ASR, ResNets for face identification, and 3D U-Nets for synapse prediction, sometimes exceeding dense baselines. On electron microscopy images, it attains state-of-the-art F1 and PR-AUC under our evaluation protocol. BB is easy to integrate and suggests a path toward learning more diverse and efficient representations.",
        "arxiv_id": "2510.01263"
    },
    "2510.01240": {
        "SCORE": 18,
        "ARXIVID": "2510.01240",
        "COMMENT": "Matches Model Compression and Efficiency: low-bit vector quantization for LLMs using Fisher-information (Riemannian) sensitivity guidance and channel-wise bit allocation.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Zukang Xu",
            "Xing Hu",
            "Qiang Wu",
            "Dawei Yang"
        ],
        "title": "RSAVQ: Riemannian Sensitivity-Aware Vector Quantization for Large Language Models",
        "abstract": "Large language models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, their exponentially increasing parameters pose significant challenges for deployment on resource-constrained devices. Vector Quantization (VQ) shows great promise for low-bit quantization (e.g., 2 to 4 bits), but existing work faces two key challenges: unconstrained direction error and suboptimal bit allocation. In this paper, we propose RSAVQ, a novel VQ framework to enhance extremely low-bit quantization for LLMs. RSAVQ introduces two geometry-driven innovations that effectively mitigate above limitations: (1) Error Direction Sensitivity Guidance (EDSG), which leverages the Fisher Information Matrix (FIM)-induced Riemannian metric to project quantization errors onto low-sensitivity directions in the parameter space. Specifically, this projection is performed along the negative natural gradient direction, which effectively suppresses error expansion. (2) Weight Channel Sensitivity Guidance (WCSG) , which constructs a channel-wise sensitivity metric via FIM curvature analysis to dynamically guide bit resource allocation. The approach facilitates a globally optimal quantization solution within prescribed bit constraints. Experiments demonstrate that RSAVQ outperforms existing methods for LLMs. For example, in 2-bit quantization of LLaMA-3 8B, RSAVQ leads baselines like VPTQ and QuIP# by 0.4 in perplexity (PPL) and 1.5 in zero-shot accuracy. This work offers a practical solution for constrained environments and a theoretical bridge between information geometry and the quantization of neural networks, advancing efficient deep learning.",
        "arxiv_id": "2510.01240"
    },
    "2510.01878": {
        "SCORE": 18,
        "ARXIVID": "2510.01878",
        "COMMENT": "High Performance Computing/Efficiency: randomized gradient subspace methods (GrassWalk/GrassJump) reduce optimizer memory for LLM pretraining by leveraging near-flat curvature.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Sahar Rajabi",
            "Nayeema Nonta",
            "Samanvay Vajpayee",
            "Sirisha Rambhatla"
        ],
        "title": "Randomized Gradient Subspaces for Efficient Large Language Model Training",
        "abstract": "Training large language models (LLMs) is often bottlenecked by extreme memory demands, with optimizer states dominating the footprint. Recent works mitigates this cost by projecting gradients into low-dimensional subspaces using sophisticated update strategies. In this paper, we analyze the dynamics of gradient space and its underlying subspaces. We find that while a small subspace captures most gradient energy, a significant portion still resides in the residual bulk; moreover, the influence of the core subspace diminishes over time and in deeper layers. We also observe that the gradient space exhibits near-flat curvature, calling for algorithms that explicitly account for this geometry. Motivated by these insights, we introduce a suite of randomized algorithms, GrassWalk and GrassJump, which exploit subspace and achieve state-of-the-art memory savings while improving performance on LLaMA-1B and LLaMA-7B pretraining.",
        "arxiv_id": "2510.01878"
    },
    "2510.01634": {
        "SCORE": 18,
        "ARXIVID": "2510.01634",
        "COMMENT": "Model Architecture: conditional routing across geometry-specific attention branches (mixture-of-geometry/MoE-like) enabling curvature-adaptive Transformers.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Ryan Y. Lin",
            "Siddhartha Ojha",
            "Nicholas Bai"
        ],
        "title": "CAT: Curvature-Adaptive Transformers for Geometry-Aware Learning",
        "abstract": "Transformers achieve strong performance across diverse domains but implicitly assume Euclidean geometry in their attention mechanisms, limiting their effectiveness on data with non-Euclidean structure. While recent extensions to hyperbolic and spherical spaces show promise for hierarchical and cyclical patterns, respectively, they require committing to a single geometry a priori, reducing flexibility when data exhibits mixed geometric properties. We introduce the Curvature-Adaptive Transformer (CAT), a novel architecture that dynamically learns per-token routing across three geometric attention branches through a lightweight, differentiable gating mechanism. Unlike fixed-geometry approaches, CAT enables adaptive geometric specialization, routing tokens to the appropriate curvature based on their local relational structure. The routing network provides interpretable curvature preferences while each branch employs geometry-specific operations optimized for its respective manifold. On knowledge graph completion benchmarks (FB15k-237, WN18RR), CAT achieves approximately 10% improvements in MRR and Hits@10 over fixed-geometry baselines with minimal overhead (5% parameter increase, comparable inference time). These results demonstrate that learned geometric adaptation outperforms any single fixed geometry for complex relational reasoning, establishing CAT as a scalable and interpretable foundation for mixture-of-geometry architectures across language, vision, and multimodal domains.",
        "arxiv_id": "2510.01634"
    },
    "2510.01290": {
        "SCORE": 18,
        "ARXIVID": "2510.01290",
        "COMMENT": "Compression/Efficiency/HPC: thought-adaptive KV-cache compression with hybrid quantization\u2013eviction and a PagedAttention-extended kernel for memory reuse.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Akshat Ramachandran",
            "Marina Neseem",
            "Charbel Sakr",
            "Rangharajan Venkatesan",
            "Brucek Khailany",
            "Tushar Krishna"
        ],
        "title": "ThinKV: Thought-Adaptive KV Cache Compression for Efficient Reasoning Models",
        "abstract": "The long-output context generation of large reasoning models enables extended chain of thought (CoT) but also drives rapid growth of the key-value (KV) cache, quickly overwhelming GPU memory. To address this challenge, we propose ThinKV, a thought-adaptive KV cache compression framework. ThinKV is based on the observation that attention sparsity reveals distinct thought types with varying importance within the CoT. It applies a hybrid quantization-eviction strategy, assigning token precision by thought importance and progressively evicting tokens from less critical thoughts as reasoning trajectories evolve. Furthermore, to implement ThinKV, we design a kernel that extends PagedAttention to enable efficient reuse of evicted tokens' memory slots, eliminating compaction overheads. Extensive experiments on DeepSeek-R1-Distill, GPT-OSS, and NVIDIA AceReason across mathematics and coding benchmarks show that ThinKV achieves near-lossless accuracy with less than 5% of the original KV cache, while improving performance with up to 5.8x higher inference throughput over state-of-the-art baselines.",
        "arxiv_id": "2510.01290"
    },
    "2510.01450": {
        "SCORE": 18,
        "ARXIVID": "2510.01450",
        "COMMENT": "Model Architecture: proposes a new attention mechanism (Local Linear Attention) as an alternative to Softmax/linear attention in Transformers; High-Performance Computing/Efficiency: introduces memory-efficient primitives and a hardware-efficient blockwise algorithm (FlashLLA) with custom kernels to reduce O(n^2 d) and O(n d^2) costs.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Yifei Zuo",
            "Yutong Yin",
            "Zhichen Zeng",
            "Ang Li",
            "Banghua Zhu",
            "Zhaoran Wang"
        ],
        "title": "Local Linear Attention: An Optimal Interpolation of Linear and Softmax Attention For Test-Time Regression",
        "abstract": "Transformer architectures have achieved remarkable success in various domains. While efficient alternatives to Softmax Attention have been widely studied, the search for more expressive mechanisms grounded in theoretical insight-even at greater computational cost-has been relatively underexplored. In this work, we bridge this gap by proposing Local Linear Attention (LLA), a novel attention mechanism derived from nonparametric statistics through the lens of test-time regression. First, we show that LLA offers theoretical advantages over Linear and Softmax Attention for associative memory via a bias-variance trade-off analysis. Next, we address its computational challenges and propose two memory-efficient primitives to tackle the $\\Theta(n^2 d)$ and $\\Theta(n d^2)$ complexity. We then introduce FlashLLA, a hardware-efficient, blockwise algorithm that enables scalable and parallel computation on modern accelerators. In addition, we implement and profile a customized inference kernel that significantly reduces memory overheads. Finally, we empirically validate the advantages and limitations of LLA on test-time regression, in-context regression, associative recall and state tracking tasks. Experiment results demonstrate that LLA effectively adapts to non-stationarity, outperforming strong baselines in test-time training and in-context learning, and exhibiting promising evidence for its scalability and applicability in large-scale models. Code is available at https://github.com/Yifei-Zuo/Flash-LLA.",
        "arxiv_id": "2510.01450"
    },
    "2510.02239": {
        "SCORE": 18,
        "ARXIVID": "2510.02239",
        "COMMENT": "Training efficiency criterion: randomized progressive layer updates with non-Euclidean optimization and convergence theory, reducing update cost.",
        "RELEVANCE": 9,
        "NOVELTY": 9,
        "authors": [
            "Kaja Gruntkowska",
            "Yassine Maziane",
            "Zheng Qu",
            "Peter Richt\\'arik"
        ],
        "title": "Drop-Muon: Update Less, Converge Faster",
        "abstract": "Conventional wisdom in deep learning optimization dictates updating all layers at every step-a principle followed by all recent state-of-the-art optimizers such as Muon. In this work, we challenge this assumption, showing that full-network updates can be fundamentally suboptimal, both in theory and in practice. We introduce a non-Euclidean Randomized Progressive Training method-Drop-Muon-a simple yet powerful framework that updates only a subset of layers per step according to a randomized schedule, combining the efficiency of progressive training with layer-specific non-Euclidean updates for top-tier performance. We provide rigorous convergence guarantees under both layer-wise smoothness and layer-wise $(L^0, L^1)$-smoothness, covering deterministic and stochastic gradient settings, marking the first such results for progressive training in the stochastic and non-smooth regime. Our cost analysis further reveals that full-network updates are not optimal unless a very specific relationship between layer smoothness constants holds. Through controlled CNN experiments, we empirically demonstrate that Drop-Muon consistently outperforms full-network Muon, achieving the same accuracy up to $1.4\\times$ faster in wall-clock time. Together, our results suggest a shift in how large-scale models can be efficiently trained, challenging the status quo and offering a highly efficient, theoretically grounded alternative to full-network updates.",
        "arxiv_id": "2510.02239"
    },
    "2510.01621": {
        "SCORE": 17,
        "ARXIVID": "2510.01621",
        "COMMENT": "Representation Learning: theoretical analysis of VAEs\u2019 training dynamics, framing posterior collapse as a phase transition with a critical boundary.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Zhen Li",
            "Fan Zhang",
            "Zheng Zhang",
            "Yu Chen"
        ],
        "title": "Posterior Collapse as a Phase Transition in Variational Autoencoders",
        "abstract": "We investigate the phenomenon of posterior collapse in variational autoencoders (VAEs) from the perspective of statistical physics, and reveal that it constitutes a phase transition governed jointly by data structure and model hyper-parameters. By analyzing the stability of the trivial solution associated with posterior collapse, we identify a critical hyper-parameter threshold. This critical boundary, separating meaningful latent inference from collapse, is characterized by a discontinuity in the KL divergence between the approximate posterior and the prior distribution. We validate this critical behavior on both synthetic and real-world datasets, confirming the existence of a phase transition. Our results demonstrate that posterior collapse is not merely an optimization failure, but rather an emerging phase transition arising from the interplay between data structure and variational constraints. This perspective offers new insights into the trainability and representational capacity of deep generative models.",
        "arxiv_id": "2510.01621"
    },
    "2510.01303": {
        "SCORE": 17,
        "ARXIVID": "2510.01303",
        "COMMENT": "Compression/Efficiency: identifies approximate low-rank structure in gradients; Representation Learning/Training Dynamics: links data/activation/regularizers to gradient rank components.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Rishi Sonthalia",
            "Michael Murray",
            "Guido Mont\\'ufar"
        ],
        "title": "Low Rank Gradients and Where to Find Them",
        "abstract": "This paper investigates low-rank structure in the gradients of the training loss for two-layer neural networks while relaxing the usual isotropy assumptions on the training data and parameters. We consider a spiked data model in which the bulk can be anisotropic and ill-conditioned, we do not require independent data and weight matrices and we also analyze both the mean-field and neural-tangent-kernel scalings. We show that the gradient with respect to the input weights is approximately low rank and is dominated by two rank-one terms: one aligned with the bulk data-residue , and another aligned with the rank one spike in the input data. We characterize how properties of the training data, the scaling regime and the activation function govern the balance between these two components. Additionally, we also demonstrate that standard regularizers, such as weight decay, input noise and Jacobian penalties, also selectively modulate these components. Experiments on synthetic and real data corroborate our theoretical predictions.",
        "arxiv_id": "2510.01303"
    },
    "2510.01796": {
        "SCORE": 17,
        "ARXIVID": "2510.01796",
        "COMMENT": "Model Architecture: rethinks MLP shape/skip placement with hourglass blocks and fixed random expansion; provides scaling insights applicable to residual networks/Transformers.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Meng-Hsi Chen",
            "Yu-Ang Lee",
            "Feng-Ting Liao",
            "Da-shan Shiu"
        ],
        "title": "Rethinking the shape convention of an MLP",
        "abstract": "Multi-layer perceptrons (MLPs) conventionally follow a narrow-wide-narrow design where skip connections operate at the input/output dimensions while processing occurs in expanded hidden spaces. We challenge this convention by proposing wide-narrow-wide (Hourglass) MLP blocks where skip connections operate at expanded dimensions while residual computation flows through narrow bottlenecks. This inversion leverages higher-dimensional spaces for incremental refinement while maintaining computational efficiency through parameter-matched designs. Implementing Hourglass MLPs requires an initial projection to lift input signals to expanded dimensions. We propose that this projection can remain fixed at random initialization throughout training, enabling efficient training and inference implementations. We evaluate both architectures on generative tasks over popular image datasets, characterizing performance-parameter Pareto frontiers through systematic architectural search. Results show that Hourglass architectures consistently achieve superior Pareto frontiers compared to conventional designs. As parameter budgets increase, optimal Hourglass configurations favor deeper networks with wider skip connections and narrower bottlenecks-a scaling pattern distinct from conventional MLPs. Our findings suggest reconsidering skip connection placement in modern architectures, with potential applications extending to Transformers and other residual networks.",
        "arxiv_id": "2510.01796"
    },
    "2510.01930": {
        "SCORE": 17,
        "ARXIVID": "2510.01930",
        "COMMENT": "Matches Representation Learning: theoretical analysis of gradient-flow dynamics in diagonal linear networks via Dynamical Mean-Field Theory.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Sota Nishiyama",
            "Masaaki Imaizumi"
        ],
        "title": "Precise Dynamics of Diagonal Linear Networks: A Unifying Analysis by Dynamical Mean-Field Theory",
        "abstract": "Diagonal linear networks (DLNs) are a tractable model that captures several nontrivial behaviors in neural network training, such as initialization-dependent solutions and incremental learning. These phenomena are typically studied in isolation, leaving the overall dynamics insufficiently understood. In this work, we present a unified analysis of various phenomena in the gradient flow dynamics of DLNs. Using Dynamical Mean-Field Theory (DMFT), we derive a low-dimensional effective process that captures the asymptotic gradient flow dynamics in high dimensions. Analyzing this effective process yields new insights into DLN dynamics, including loss convergence rates and their trade-off with generalization, and systematically reproduces many of the previously observed phenomena. These findings deepen our understanding of DLNs and demonstrate the effectiveness of the DMFT approach in analyzing high-dimensional learning dynamics of neural networks.",
        "arxiv_id": "2510.01930"
    },
    "2510.01938": {
        "SCORE": 17,
        "ARXIVID": "2510.01938",
        "COMMENT": "Model Compression and Efficiency: advances LoRA via U S V^T factorization with Stiefel manifold constraints and Riemannian optimization for low-rank adapters.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Zhizhong Li",
            "Sina Sajadmanesh",
            "Jingtao Li",
            "Lingjuan Lyu"
        ],
        "title": "StelLA: Subspace Learning in Low-rank Adaptation using Stiefel Manifold",
        "abstract": "Low-rank adaptation (LoRA) has been widely adopted as a parameter-efficient technique for fine-tuning large-scale pre-trained models. However, it still lags behind full fine-tuning in performance, partly due to its insufficient exploitation of the geometric structure underlying low-rank manifolds. In this paper, we propose a geometry-aware extension of LoRA that uses a three-factor decomposition $U\\!SV^\\top$. Analogous to the structure of singular value decomposition (SVD), it separates the adapter's input and output subspaces, $V$ and $U$, from the scaling factor $S$. Our method constrains $U$ and $V$ to lie on the Stiefel manifold, ensuring their orthonormality throughout the training. To optimize on the Stiefel manifold, we employ a flexible and modular geometric optimization design that converts any Euclidean optimizer to a Riemannian one. It enables efficient subspace learning while remaining compatible with existing fine-tuning pipelines. Empirical results across a wide range of downstream tasks, including commonsense reasoning, math and code generation, image classification, and image generation, demonstrate the superior performance of our approach against the recent state-of-the-art variants of LoRA. Code is available at https://github.com/SonyResearch/stella.",
        "arxiv_id": "2510.01938"
    },
    "2510.01510": {
        "SCORE": 17,
        "ARXIVID": "2510.01510",
        "COMMENT": "Model Architecture: introduces probabilistic node\u2013relation equivariance and random-walk sequence modeling with universality guarantees for KG link functions.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Jinwoo Kim",
            "Xingyue Huang",
            "Krzysztof Olejniczak",
            "Kyungbin Min",
            "Michael Bronstein",
            "Seunghoon Hong",
            "\\.Ismail \\.Ilkan Ceylan"
        ],
        "title": "Flock: A Knowledge Graph Foundation Model via Learning on Random Walks",
        "abstract": "We study the problem of zero-shot link prediction on knowledge graphs (KGs), which requires models to generalize over novel entities and novel relations. Knowledge graph foundation models (KGFMs) address this task by enforcing equivariance over both nodes and relations, learning from structural properties of nodes and relations, which are then transferable to novel graphs with similar structural properties. However, the conventional notion of deterministic equivariance imposes inherent limits on the expressive power of KGFMs, preventing them from distinguishing structurally similar but semantically distinct relations. To overcome this limitation, we introduce probabilistic node-relation equivariance, which preserves equivariance in distribution while incorporating a principled randomization to break symmetries during inference. Building on this principle, we present Flock, a KGFM that iteratively samples random walks, encodes them into sequences via a recording protocol, embeds them with a sequence model, and aggregates representations of nodes and relations via learned pooling. Crucially, Flock respects probabilistic node-relation equivariance and is a universal approximator for isomorphism-invariant link-level functions over KGs. Empirically, Flock perfectly solves our new diagnostic dataset Petals where current KGFMs fail, and achieves state-of-the-art performances on entity- and relation prediction tasks on 54 KGs from diverse domains.",
        "arxiv_id": "2510.01510"
    },
    "2510.02300": {
        "SCORE": 17,
        "ARXIVID": "2510.02300",
        "COMMENT": "Model Architecture/Representation Learning: implicit energy-based model learning an equilibrium gradient with optimization-driven sampling and adaptive compute\u2014foundational alternative to diffusion/flow.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Runqian Wang",
            "Yilun Du"
        ],
        "title": "Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models",
        "abstract": "We introduce Equilibrium Matching (EqM), a generative modeling framework built from an equilibrium dynamics perspective. EqM discards the non-equilibrium, time-conditional dynamics in traditional diffusion and flow-based generative models and instead learns the equilibrium gradient of an implicit energy landscape. Through this approach, we can adopt an optimization-based sampling process at inference time, where samples are obtained by gradient descent on the learned landscape with adjustable step sizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation performance of diffusion/flow models empirically, achieving an FID of 1.90 on ImageNet 256$\\times$256. EqM is also theoretically justified to learn and sample from the data manifold. Beyond generation, EqM is a flexible framework that naturally handles tasks including partially noised image denoising, OOD detection, and image composition. By replacing time-conditional velocities with a unified equilibrium landscape, EqM offers a tighter bridge between flow and energy-based models and a simple route to optimization-driven inference.",
        "arxiv_id": "2510.02300"
    },
    "2510.01336": {
        "SCORE": 17,
        "ARXIVID": "2510.01336",
        "COMMENT": "Compression/Efficiency/HPC: hierarchical speculative decoding using early-exit intermediate verification with KV-cache/hidden-state reuse for high-throughput inference.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Avinash Kumar",
            "Sujay Sanghavi",
            "Poulami Das"
        ],
        "title": "HiSpec: Hierarchical Speculative Decoding for LLMs",
        "abstract": "Speculative decoding accelerates LLM inference by using a smaller draft model to speculate tokens that a larger target model verifies. Verification is often the bottleneck (e.g. verification is $4\\times$ slower than token generation when a 3B model speculates for a 70B target model), but most prior works focus only on accelerating drafting. $\\textit{``Intermediate\"}$ verification reduces verification time by discarding inaccurate draft tokens early, but existing methods incur substantial training overheads in incorporating the intermediate verifier, increase the memory footprint to orchestrate the intermediate verification step, and compromise accuracy by relying on approximate heuristics.   We propose $\\underline{\\textit{Hi}}\\textit{erarchical }\\underline{\\textit{Spec}}\\textit{ulative Decoding (HiSpec)}$, a framework for high-throughput speculative decoding that exploits $\\textit{early-exit (EE) models}$ for low-overhead intermediate verification. EE models allow tokens to exit early by skipping layer traversal and are explicitly trained so that hidden states at selected layers can be interpreted, making them uniquely suited for intermediate verification without drastically increasing compute and memory overheads. To improve resource-efficiency even further, we design a methodology that enables HiSpec to re-use key-value caches and hidden states between the draft, intermediate verifier, and target models. To maintain accuracy, HiSpec periodically validates the draft tokens accepted by the intermediate verifier against the target model. Our evaluations using various representative benchmarks and models show that HiSpec improves throughput by 1.28$\\times$ on average and by up to 2.01$\\times$ compared to the baseline single-layer speculation without compromising accuracy.",
        "arxiv_id": "2510.01336"
    },
    "2510.02312": {
        "SCORE": 17,
        "ARXIVID": "2510.02312",
        "COMMENT": "Model Compression and Efficiency: compressed KV-cache distillation to supervise latent reasoning, leveraging cache-aware signals for efficient inference and memory savings.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Anna Kuzina",
            "Maciej Pioro",
            "Paul N. Whatmough",
            "Babak Ehteshami Bejnordi"
        ],
        "title": "KaVa: Latent Reasoning via Compressed KV-Cache Distillation",
        "abstract": "Large Language Models (LLMs) excel at multi-step reasoning problems with explicit chain-of-thought (CoT), but verbose traces incur significant computational costs and memory overhead, and often carry redundant, stylistic artifacts. Latent reasoning has emerged as an efficient alternative that internalizes the thought process, but it suffers from a critical lack of supervision, limiting its effectiveness on complex, natural-language reasoning traces. In this work, we propose KaVa, the first framework that bridges this gap by distilling knowledge directly from a compressed KV-cache of the teacher into a latent-reasoning student via self-distillation, leveraging the representational flexibility of continuous latent tokens to align stepwise KV trajectories. We show that the abstract, unstructured knowledge within compressed KV-cache, which lacks direct token correspondence, can serve as a rich supervisory signal for a latent reasoning student. Empirically, the approach consistently outperforms strong latent baselines, exhibits markedly smaller degradation from equation-only to natural-language traces, and scales to larger backbones while preserving efficiency. These results establish compressed KV-cache distillation as a scalable supervision signal for latent reasoning, combining the accuracy of CoT-trained teachers with the efficiency and deployability of latent inference.",
        "arxiv_id": "2510.02312"
    },
    "2510.02049": {
        "SCORE": 17,
        "ARXIVID": "2510.02049",
        "COMMENT": "Model Architecture: theoretical modeling of densely connected networks (DenseNet-style) via nonlinear integral equations with convergence (\u0393-convergence) results for training.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Jinshu Huang",
            "Haibin Su",
            "Xue-Cheng Tai",
            "Chunlin Wu"
        ],
        "title": "Mathematical Modeling and Convergence Analysis of Deep Neural Networks with Dense Layer Connectivities in Deep Learning",
        "abstract": "In deep learning, dense layer connectivity has become a key design principle in deep neural networks (DNNs), enabling efficient information flow and strong performance across a range of applications. In this work, we model densely connected DNNs mathematically and analyze their learning problems in the deep-layer limit. For a broad applicability, we present our analysis in a framework setting of DNNs with densely connected layers and general non-local feature transformations (with local feature transformations as special cases) within layers, which is called dense non-local (DNL) framework and includes standard DenseNets and variants as special examples. In this formulation, the densely connected networks are modeled as nonlinear integral equations, in contrast to the ordinary differential equation viewpoint commonly adopted in prior works. We study the associated training problems from an optimal control perspective and prove convergence results from the network learning problem to its continuous-time counterpart. In particular, we show the convergence of optimal values and the subsequence convergence of minimizers, using a piecewise linear extension and $\\Gamma$-convergence analysis. Our results provide a mathematical foundation for understanding densely connected DNNs and further suggest that such architectures can offer stability of training deep models.",
        "arxiv_id": "2510.02049"
    },
    "2510.01685": {
        "SCORE": 16,
        "ARXIVID": "2510.01685",
        "COMMENT": "Representation Learning: mechanistic analysis of compositionality in LLMs via logit-lens, identifying processing pathways and linking them to embedding space geometry.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Apoorv Khandelwal",
            "Ellie Pavlick"
        ],
        "title": "How Do Language Models Compose Functions?",
        "abstract": "While large language models (LLMs) appear to be increasingly capable of solving compositional tasks, it is an open question whether they do so using compositional mechanisms. In this work, we investigate how feedforward LLMs solve two-hop factual recall tasks, which can be expressed compositionally as $g(f(x))$. We first confirm that modern LLMs continue to suffer from the \"compositionality gap\": i.e. their ability to compute both $z = f(x)$ and $y = g(z)$ does not entail their ability to compute the composition $y = g(f(x))$. Then, using logit lens on their residual stream activations, we identify two processing mechanisms, one which solves tasks $\\textit{compositionally}$, computing $f(x)$ along the way to computing $g(f(x))$, and one which solves them $\\textit{directly}$, without any detectable signature of the intermediate variable $f(x)$. Finally, we find that which mechanism is employed appears to be related to the embedding space geometry, with the idiomatic mechanism being dominant in cases where there exists a linear mapping from $x$ to $g(f(x))$ in the embedding spaces. We fully release our data and code at: https://github.com/apoorvkh/composing-functions .",
        "arxiv_id": "2510.01685"
    },
    "2510.02259": {
        "SCORE": 16,
        "ARXIVID": "2510.02259",
        "COMMENT": "Model Architecture / Representation Learning: shows pure Transformers (no graph priors) learn distance-aware structure for molecular modeling, with scaling and attention analysis.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Tobias Kreiman",
            "Yutong Bai",
            "Fadi Atieh",
            "Elizabeth Weaver",
            "Eric Qu",
            "Aditi S. Krishnapriyan"
        ],
        "title": "Transformers Discover Molecular Structure Without Graph Priors",
        "abstract": "Graph Neural Networks (GNNs) are the dominant architecture for molecular machine learning, particularly for molecular property prediction and machine learning interatomic potentials (MLIPs). GNNs perform message passing on predefined graphs often induced by a fixed radius cutoff or k-nearest neighbor scheme. While this design aligns with the locality present in many molecular tasks, a hard-coded graph can limit expressivity due to the fixed receptive field and slows down inference with sparse graph operations. In this work, we investigate whether pure, unmodified Transformers trained directly on Cartesian coordinates$\\unicode{x2013}$without predefined graphs or physical priors$\\unicode{x2013}$can approximate molecular energies and forces. As a starting point for our analysis, we demonstrate how to train a Transformer to competitive energy and force mean absolute errors under a matched training compute budget, relative to a state-of-the-art equivariant GNN on the OMol25 dataset. We discover that the Transformer learns physically consistent patterns$\\unicode{x2013}$such as attention weights that decay inversely with interatomic distance$\\unicode{x2013}$and flexibly adapts them across different molecular environments due to the absence of hard-coded biases. The use of a standard Transformer also unlocks predictable improvements with respect to scaling training resources, consistent with empirical scaling laws observed in other domains. Our results demonstrate that many favorable properties of GNNs can emerge adaptively in Transformers, challenging the necessity of hard-coded graph inductive biases and pointing toward standardized, scalable architectures for molecular modeling.",
        "arxiv_id": "2510.02259"
    },
    "2510.01902": {
        "SCORE": 16,
        "ARXIVID": "2510.01902",
        "COMMENT": "Compression/Efficiency: algorithmic innovation for constrained decoding via adaptive rejection sampling that preserves the exact distribution while improving sample efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Pawe{\\l} Parys",
            "Sairam Vaidya",
            "Taylor Berg-Kirkpatrick",
            "Loris D'Antoni"
        ],
        "title": "Constrained Adaptive Rejection Sampling",
        "abstract": "Language Models (LMs) are increasingly used in applications where generated outputs must satisfy strict semantic or syntactic constraints. Existing approaches to constrained generation fall along a spectrum: greedy constrained decoding methods enforce validity during decoding but distort the LM's distribution, while rejection sampling (RS) preserves fidelity but wastes computation by discarding invalid outputs. Both extremes are problematic in domains such as program fuzzing, where both validity and diversity of samples are essential. We present Constrained Adaptive Rejection Sampling (CARS), an approach that strictly improves the sample-efficiency of RS without distributional distortion. CARS begins with unconstrained LM sampling and adaptively rules out constraint-violating continuations by recording them in a trie and subtracting their probability mass from future draws. This adaptive pruning ensures that prefixes proven invalid are never revisited, acceptance rates improve monotonically, and the resulting samples exactly follow the constrained distribution. In experiments on a variety of domains -- e.g., program fuzzing and molecular generation -- CARS consistently achieves higher efficiency -- measured in the number of LM forward passes per valid sample -- while also producing stronger sample diversity than both GCD and methods that approximate the LM's distribution.",
        "arxiv_id": "2510.01902"
    },
    "2510.01414": {
        "SCORE": 16,
        "ARXIVID": "2510.01414",
        "COMMENT": "Generalization theory in overparameterized spiked regression, classifying benign vs catastrophic overfitting\u2014training dynamics/representation theory.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Jiping Li",
            "Rishi Sonthalia"
        ],
        "title": "Risk Phase Transitions in Spiked Regression: Alignment Driven Benign and Catastrophic Overfitting",
        "abstract": "This paper analyzes the generalization error of minimum-norm interpolating solutions in linear regression using spiked covariance data models. The paper characterizes how varying spike strengths and target-spike alignments can affect risk, especially in overparameterized settings. The study presents an exact expression for the generalization error, leading to a comprehensive classification of benign, tempered, and catastrophic overfitting regimes based on spike strength, the aspect ratio $c=d/n$ (particularly as $c \\to \\infty$), and target alignment. Notably, in well-specified aligned problems, increasing spike strength can surprisingly induce catastrophic overfitting before achieving benign overfitting. The paper also reveals that target-spike alignment is not always advantageous, identifying specific, sometimes counterintuitive, conditions for its benefit or detriment. Alignment with the spike being detrimental is empirically demonstrated to persist in nonlinear models.",
        "arxiv_id": "2510.01414"
    },
    "2510.01494": {
        "SCORE": 16,
        "ARXIVID": "2510.01494",
        "COMMENT": "Representation learning insight: analyzes how latent geometry vs shared data-space affects adversarial transfer with theory and experiments.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Isha Gupta",
            "Rylan Schaeffer",
            "Joshua Kazdan",
            "Ken Liu",
            "Sanmi Koyejo"
        ],
        "title": "Understanding Adversarial Transfer: Why Representation-Space Attacks Fail Where Data-Space Attacks Succeed",
        "abstract": "The field of adversarial robustness has long established that adversarial examples can successfully transfer between image classifiers and that text jailbreaks can successfully transfer between language models (LMs). However, a pair of recent studies reported being unable to successfully transfer image jailbreaks between vision-language models (VLMs). To explain this striking difference, we propose a fundamental distinction regarding the transferability of attacks against machine learning models: attacks in the input data-space can transfer, whereas attacks in model representation space do not, at least not without geometric alignment of representations. We then provide theoretical and empirical evidence of this hypothesis in four different settings. First, we mathematically prove this distinction in a simple setting where two networks compute the same input-output map but via different representations. Second, we construct representation-space attacks against image classifiers that are as successful as well-known data-space attacks, but fail to transfer. Third, we construct representation-space attacks against LMs that successfully jailbreak the attacked models but again fail to transfer. Fourth, we construct data-space attacks against VLMs that successfully transfer to new VLMs, and we show that representation space attacks \\emph{can} transfer when VLMs' latent geometries are sufficiently aligned in post-projector space. Our work reveals that adversarial transfer is not an inherent property of all attacks but contingent on their operational domain - the shared data-space versus models' unique representation spaces - a critical insight for building more robust models.",
        "arxiv_id": "2510.01494"
    },
    "2510.01944": {
        "SCORE": 16,
        "ARXIVID": "2510.01944",
        "COMMENT": "Representation Learning/Training Dynamics: theoretical uniform-in-time convergence bounds for PCD in EBMs with an efficient continuous-time SDE formulation and stable S-ROCK integrators.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Paul Felix Valsecchi Oliva",
            "O. Deniz Akyildiz",
            "Andrew Duncan"
        ],
        "title": "Uniform-in-time convergence bounds for Persistent Contrastive Divergence Algorithms",
        "abstract": "We propose a continuous-time formulation of persistent contrastive divergence (PCD) for maximum likelihood estimation (MLE) of unnormalised densities. Our approach expresses PCD as a coupled, multiscale system of stochastic differential equations (SDEs), which perform optimisation of the parameter and sampling of the associated parametrised density, simultaneously.   From this novel formulation, we are able to derive explicit bounds for the error between the PCD iterates and the MLE solution for the model parameter. This is made possible by deriving uniform-in-time (UiT) bounds for the difference in moments between the multiscale system and the averaged regime. An efficient implementation of the continuous-time scheme is introduced, leveraging a class of explicit, stable intregators, stochastic orthogonal Runge-Kutta Chebyshev (S-ROCK), for which we provide explicit error estimates in the long-time regime. This leads to a novel method for training energy-based models (EBMs) with explicit error guarantees.",
        "arxiv_id": "2510.01944"
    },
    "2510.01349": {
        "SCORE": 16,
        "ARXIVID": "2510.01349",
        "COMMENT": "Representation Learning: proposes a metric for distributional symmetry-breaking and theory showing when equivariant methods can underperform.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Hannah Lawrence",
            "Elyssa Hofgard",
            "Vasco Portilheiro",
            "Yuxuan Chen",
            "Tess Smidt",
            "Robin Walters"
        ],
        "title": "To Augment or Not to Augment? Diagnosing Distributional Symmetry Breaking",
        "abstract": "Symmetry-aware methods for machine learning, such as data augmentation and equivariant architectures, encourage correct model behavior on all transformations (e.g. rotations or permutations) of the original dataset. These methods can improve generalization and sample efficiency, under the assumption that the transformed datapoints are highly probable, or \"important\", under the test distribution. In this work, we develop a method for critically evaluating this assumption. In particular, we propose a metric to quantify the amount of anisotropy, or symmetry-breaking, in a dataset, via a two-sample neural classifier test that distinguishes between the original dataset and its randomly augmented equivalent. We validate our metric on synthetic datasets, and then use it to uncover surprisingly high degrees of alignment in several benchmark point cloud datasets. We show theoretically that distributional symmetry-breaking can actually prevent invariant methods from performing optimally even when the underlying labels are truly invariant, as we show for invariant ridge regression in the infinite feature limit. Empirically, we find that the implication for symmetry-aware methods is dataset-dependent: equivariant methods still impart benefits on some anisotropic datasets, but not others. Overall, these findings suggest that understanding equivariance -- both when it works, and why -- may require rethinking symmetry biases in the data.",
        "arxiv_id": "2510.01349"
    },
    "2510.02308": {
        "SCORE": 15,
        "ARXIVID": "2510.02308",
        "COMMENT": "Manifold/representation learning via Laplacian eigenvector gradient orthogonalization with theoretical robustness to noise.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Dhruv Kohli",
            "Sawyer J. Robertson",
            "Gal Mishne",
            "Alexander Cloninger"
        ],
        "title": "Robust Tangent Space Estimation via Laplacian Eigenvector Gradient Orthogonalization",
        "abstract": "Estimating the tangent spaces of a data manifold is a fundamental problem in data analysis. The standard approach, Local Principal Component Analysis (LPCA), struggles in high-noise settings due to a critical trade-off in choosing the neighborhood size. Selecting an optimal size requires prior knowledge of the geometric and noise characteristics of the data that are often unavailable. In this paper, we propose a spectral method, Laplacian Eigenvector Gradient Orthogonalization (LEGO), that utilizes the global structure of the data to guide local tangent space estimation. Instead of relying solely on local neighborhoods, LEGO estimates the tangent space at each data point by orthogonalizing the gradients of low-frequency eigenvectors of the graph Laplacian. We provide two theoretical justifications of our method. First, a differential geometric analysis on a tubular neighborhood of a manifold shows that gradients of the low-frequency Laplacian eigenfunctions of the tube align closely with the manifold's tangent bundle, while an eigenfunction with high gradient in directions orthogonal to the manifold lie deeper in the spectrum. Second, a random matrix theoretic analysis also demonstrates that low-frequency eigenvectors are robust to sub-Gaussian noise. Through comprehensive experiments, we demonstrate that LEGO yields tangent space estimates that are significantly more robust to noise than those from LPCA, resulting in marked improvements in downstream tasks such as manifold learning, boundary detection, and local intrinsic dimension estimation.",
        "arxiv_id": "2510.02308"
    },
    "2510.01377": {
        "SCORE": 15,
        "ARXIVID": "2510.01377",
        "COMMENT": "Decentralized optimization with orthogonalization (Newton\u2013Schulz) and gradient tracking; systems-level advance for distributed training.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Chuan He",
            "Shuyi Ren",
            "Jingwei Mao",
            "Erik G. Larsson"
        ],
        "title": "DeMuon: A Decentralized Muon for Matrix Optimization over Graphs",
        "abstract": "In this paper, we propose DeMuon, a method for decentralized matrix optimization over a given communication topology. DeMuon incorporates matrix orthogonalization via Newton-Schulz iterations-a technique inherited from its centralized predecessor, Muon-and employs gradient tracking to mitigate heterogeneity among local functions. Under heavy-tailed noise conditions and additional mild assumptions, we establish the iteration complexity of DeMuon for reaching an approximate stochastic stationary point. This complexity result matches the best-known complexity bounds of centralized algorithms in terms of dependence on the target tolerance. To the best of our knowledge, DeMuon is the first direct extension of Muon to decentralized optimization over graphs with provable complexity guarantees. We conduct preliminary numerical experiments on decentralized transformer pretraining over graphs with varying degrees of connectivity. Our numerical results demonstrate a clear margin of improvement of DeMuon over other popular decentralized algorithms across different network topologies.",
        "arxiv_id": "2510.01377"
    },
    "2510.01658": {
        "SCORE": 15,
        "ARXIVID": "2510.01658",
        "COMMENT": "Representation learning criterion: hierarchical losses and temperature scheduling to balance uniformity\u2013tolerance in contrastive time-series embeddings.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Amin Jalali",
            "Milad Soltany",
            "Michael Greenspan",
            "Ali Etemad"
        ],
        "title": "Learning Time-Series Representations by Hierarchical Uniformity-Tolerance Latent Balancing",
        "abstract": "We propose TimeHUT, a novel method for learning time-series representations by hierarchical uniformity-tolerance balancing of contrastive representations. Our method uses two distinct losses to learn strong representations with the aim of striking an effective balance between uniformity and tolerance in the embedding space. First, TimeHUT uses a hierarchical setup to learn both instance-wise and temporal information from input time-series. Next, we integrate a temperature scheduler within the vanilla contrastive loss to balance the uniformity and tolerance characteristics of the embeddings. Additionally, a hierarchical angular margin loss enforces instance-wise and temporal contrast losses, creating geometric margins between positive and negative pairs of temporal sequences. This approach improves the coherence of positive pairs and their separation from the negatives, enhancing the capture of temporal dependencies within a time-series sample. We evaluate our approach on a wide range of tasks, namely 128 UCR and 30 UAE datasets for univariate and multivariate classification, as well as Yahoo and KPI datasets for anomaly detection. The results demonstrate that TimeHUT outperforms prior methods by considerable margins on classification, while obtaining competitive results for anomaly detection. Finally, detailed sensitivity and ablation studies are performed to evaluate different components and hyperparameters of our method.",
        "arxiv_id": "2510.01658"
    },
    "2510.02174": {
        "SCORE": 15,
        "ARXIVID": "2510.02174",
        "COMMENT": "Matches Representation Learning/training dynamics: proposes fSGLD to bias toward flat minima with theoretical guarantees (invariant measure, convergence, excess-risk).",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Stefano Bruno",
            "Youngsik Hwang",
            "Jaehyeon An",
            "Sotirios Sabanis",
            "Dong-Young Lim"
        ],
        "title": "Flatness-Aware Stochastic Gradient Langevin Dynamics",
        "abstract": "Generalization in deep learning is closely tied to the pursuit of flat minima in the loss landscape, yet classical Stochastic Gradient Langevin Dynamics (SGLD) offers no mechanism to bias its dynamics toward such low-curvature solutions. This work introduces Flatness-Aware Stochastic Gradient Langevin Dynamics (fSGLD), designed to efficiently and provably seek flat minima in high-dimensional nonconvex optimization problems. At each iteration, fSGLD uses the stochastic gradient evaluated at parameters perturbed by isotropic Gaussian noise, commonly referred to as Random Weight Perturbation (RWP), thereby optimizing a randomized-smoothing objective that implicitly captures curvature information. Leveraging these properties, we prove that the invariant measure of fSGLD stays close to a stationary measure concentrated on the global minimizers of a loss function regularized by the Hessian trace whenever the inverse temperature and the scale of random weight perturbation are properly coupled. This result provides a rigorous theoretical explanation for the benefits of random weight perturbation. In particular, we establish non-asymptotic convergence guarantees in Wasserstein distance with the best known rate and derive an excess-risk bound for the Hessian-trace regularized objective. Extensive experiments on noisy-label and large-scale vision tasks, in both training-from-scratch and fine-tuning settings, demonstrate that fSGLD achieves superior or comparable generalization and robustness to baseline algorithms while maintaining the computational cost of SGD, about half that of SAM. Hessian-spectrum analysis further confirms that fSGLD converges to significantly flatter minima.",
        "arxiv_id": "2510.02174"
    },
    "2510.01624": {
        "SCORE": 15,
        "ARXIVID": "2510.01624",
        "COMMENT": "Representation/Training Dynamics: shows SFT metrics can mispredict RL outcomes and proposes stronger proxies (generalization loss, Pass@large k) for post-training.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Feiyang Kang",
            "Michael Kuchnik",
            "Karthik Padthe",
            "Marin Vlastelica",
            "Ruoxi Jia",
            "Carole-Jean Wu",
            "Newsha Ardalani"
        ],
        "title": "Quagmires in SFT-RL Post-Training: When High SFT Scores Mislead and What to Use Instead",
        "abstract": "In post-training for reasoning Large Language Models (LLMs), the current state of practice trains LLMs in two independent stages: Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR, shortened as ``RL'' below). In this work, we challenge whether high SFT scores translate to improved performance after RL. We provide extensive counter-examples where this is not true. We find high SFT scores can be biased toward simpler or more homogeneous data and are not reliably predictive of subsequent RL gains or scaled-up post-training effectiveness. In some cases, RL training on models with improved SFT performance could lead to substantially worse outcome compared to RL on the base model without SFT. We study alternative metrics and identify generalization loss on held-out reasoning examples and Pass@large k performance to provide strong proxies for the RL outcome. We trained hundreds of models up to 12B-parameter with SFT and RLVR via GRPO and ran extensive evaluations on 7 math benchmarks with up to 256 repetitions, spending $>$1M GPU hours. Experiments include models from Llama3, Mistral-Nemo, Qwen3 and multiple state-of-the-art SFT/RL datasets. Compared to directly predicting from pre-RL performance, prediction based on generalization loss and Pass@large k achieves substantial higher precision, improving $R^2$ coefficient and Spearman's rank correlation coefficient by up to 0.5 (2x). This provides strong utility for broad use cases. For example, in most experiments, we find SFT training on unique examples for a one epoch underperforms training on half examples for two epochs, either after SFT or SFT-then-RL; With the same SFT budget, training only on short examples may lead to better SFT performance, though, it often leads to worse outcome after RL compared to training on examples with varying lengths. Evaluation tool will be open-sourced.",
        "arxiv_id": "2510.01624"
    },
    "2510.01663": {
        "SCORE": 15,
        "ARXIVID": "2510.01663",
        "COMMENT": "Model Compression/Efficiency: Shapley-value-based, shift-invariant pruning for Kolmogorov\u2013Arnold Networks enabling reliable compression.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Wangxuan Fan",
            "Ching Wang",
            "Siqi Li",
            "Nan Liu"
        ],
        "title": "Shift-Invariant Attribute Scoring for Kolmogorov-Arnold Networks via Shapley Value",
        "abstract": "For many real-world applications, understanding feature-outcome relationships is as crucial as achieving high predictive accuracy. While traditional neural networks excel at prediction, their black-box nature obscures underlying functional relationships. Kolmogorov--Arnold Networks (KANs) address this by employing learnable spline-based activation functions on edges, enabling recovery of symbolic representations while maintaining competitive performance. However, KAN's architecture presents unique challenges for network pruning. Conventional magnitude-based methods become unreliable due to sensitivity to input coordinate shifts. We propose \\textbf{ShapKAN}, a pruning framework using Shapley value attribution to assess node importance in a shift-invariant manner. Unlike magnitude-based approaches, ShapKAN quantifies each node's actual contribution, ensuring consistent importance rankings regardless of input parameterization. Extensive experiments on synthetic and real-world datasets demonstrate that ShapKAN preserves true node importance while enabling effective network compression. Our approach improves KAN's interpretability advantages, facilitating deployment in resource-constrained environments.",
        "arxiv_id": "2510.01663"
    },
    "2510.01335": {
        "SCORE": 15,
        "ARXIVID": "2510.01335",
        "COMMENT": "Representation Learning \u2014 intrinsic dimension estimation benchmark with complex manifolds; foundational evaluation of IDE methods.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Aritra Das",
            "Joseph T. Iosue",
            "Victor V. Albert"
        ],
        "title": "Quantum-inspired Benchmark for Estimating Intrinsic Dimension",
        "abstract": "Machine learning models can generalize well on real-world datasets. According to the manifold hypothesis, this is possible because datasets lie on a latent manifold with small intrinsic dimension (ID). There exist many methods for ID estimation (IDE), but their estimates vary substantially. This warrants benchmarking IDE methods on manifolds that are more complex than those in existing benchmarks. We propose a Quantum-Inspired Intrinsic-dimension Estimation (QuIIEst) benchmark consisting of infinite families of topologically non-trivial manifolds with known ID. Our benchmark stems from a quantum-optical method of embedding arbitrary homogeneous spaces while allowing for curvature modification and additive noise. The IDE methods tested were generally less accurate on QuIIEst manifolds than on existing benchmarks under identical resource allocation. We also observe minimal performance degradation with increasingly non-uniform curvature, underscoring the benchmark's inherent difficulty. As a result of independent interest, we perform IDE on the fractal Hofstadter's butterfly and identify which methods are capable of extracting the effective dimension of a space that is not a manifold.",
        "arxiv_id": "2510.01335"
    },
    "2510.02107": {
        "SCORE": 15,
        "ARXIVID": "2510.02107",
        "COMMENT": "Representation Learning/Training Dynamics \u2014 new penalized exponential loss (PENEX) with margin maximization behavior for neural network regularization.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Klaus-Rudolf Kladny",
            "Bernhard Sch\\\"olkopf",
            "Michael Muehlebach"
        ],
        "title": "PENEX: AdaBoost-Inspired Neural Network Regularization",
        "abstract": "AdaBoost sequentially fits so-called weak learners to minimize an exponential loss, which penalizes mislabeled data points more severely than other loss functions like cross-entropy. Paradoxically, AdaBoost generalizes well in practice as the number of weak learners grows. In the present work, we introduce Penalized Exponential Loss (PENEX), a new formulation of the multi-class exponential loss that is theoretically grounded and, in contrast to the existing formulation, amenable to optimization via first-order methods. We demonstrate both empirically and theoretically that PENEX implicitly maximizes margins of data points. Also, we show that gradient increments on PENEX implicitly parameterize weak learners in the boosting framework. Across computer vision and language tasks, we show that PENEX exhibits a regularizing effect often better than established methods with similar computational cost. Our results highlight PENEX's potential as an AdaBoost-inspired alternative for effective training and fine-tuning of deep neural networks.",
        "arxiv_id": "2510.02107"
    },
    "2510.02228": {
        "SCORE": 15,
        "ARXIVID": "2510.02228",
        "COMMENT": "Model Architecture and Efficiency \u2014 analysis of xLSTM scaling laws with linear-time complexity vs Transformers; insights on training/inference scaling with context length.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Maximilian Beck",
            "Kajetan Schweighofer",
            "Sebastian B\\\"ock",
            "Sebastian Lehner",
            "Sepp Hochreiter"
        ],
        "title": "xLSTM Scaling Laws: Competitive Performance with Linear Time-Complexity",
        "abstract": "Scaling laws play a central role in the success of Large Language Models (LLMs), enabling the prediction of model performance relative to compute budgets prior to training. While Transformers have been the dominant architecture, recent alternatives such as xLSTM offer linear complexity with respect to context length while remaining competitive in the billion-parameter regime. We conduct a comparative investigation on the scaling behavior of Transformers and xLSTM along the following lines, providing insights to guide future model design and deployment. First, we study the scaling behavior for xLSTM in compute-optimal and over-training regimes using both IsoFLOP and parametric fit approaches on a wide range of model sizes (80M-7B) and number of training tokens (2B-2T). Second, we examine the dependence of optimal model sizes on context length, a pivotal aspect that was largely ignored in previous work. Finally, we analyze inference-time scaling characteristics. Our findings reveal that in typical LLM training and inference scenarios, xLSTM scales favorably compared to Transformers. Importantly, xLSTM's advantage widens as training and inference contexts grow.",
        "arxiv_id": "2510.02228"
    },
    "2510.01706": {
        "SCORE": 15,
        "ARXIVID": "2510.01706",
        "COMMENT": "Representation Learning \u2014 Hierarchical Optimal Transport for global, soft alignment across layers/neurons, yielding interpretable representational correspondences.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Shaan Shah",
            "Meenakshi Khosla"
        ],
        "title": "Representational Alignment Across Model Layers and Brain Regions with Hierarchical Optimal Transport",
        "abstract": "Standard representational similarity methods align each layer of a network to its best match in another independently, producing asymmetric results, lacking a global alignment score, and struggling with networks of different depths. These limitations arise from ignoring global activation structure and restricting mappings to rigid one-to-one layer correspondences. We propose Hierarchical Optimal Transport (HOT), a unified framework that jointly infers soft, globally consistent layer-to-layer couplings and neuron-level transport plans. HOT allows source neurons to distribute mass across multiple target layers while minimizing total transport cost under marginal constraints. This yields both a single alignment score for the entire network comparison and a soft transport plan that naturally handles depth mismatches through mass distribution. We evaluate HOT on vision models, large language models, and human visual cortex recordings. Across all domains, HOT matches or surpasses standard pairwise matching in alignment quality. Moreover, it reveals smooth, fine-grained hierarchical correspondences: early layers map to early layers, deeper layers maintain relative positions, and depth mismatches are resolved by distributing representations across multiple layers. These structured patterns emerge naturally from global optimization without being imposed, yet are absent in greedy layer-wise methods. HOT thus enables richer, more interpretable comparisons between representations, particularly when networks differ in architecture or depth.",
        "arxiv_id": "2510.01706"
    },
    "2510.01454": {
        "SCORE": 15,
        "ARXIVID": "2510.01454",
        "COMMENT": "Representation Learning/Training Dynamics and Data Efficiency: proves similarity of cross-modal attention trajectories implies gradient similarity, enabling principled data selection for LVLM fine-tuning.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Nilay Naharas",
            "Dang Nguyen",
            "Nesihan Bulut",
            "Mohammadhossein Bateni",
            "Vahab Mirrokni",
            "Baharan Mirzasoleiman"
        ],
        "title": "Data Selection for Fine-tuning Vision Language Models via Cross Modal Alignment Trajectories",
        "abstract": "Data-efficient learning aims to eliminate redundancy in large training datasets by training models on smaller subsets of the most informative examples. While data selection has been extensively explored for vision models and large language models (LLMs), it remains underexplored for Large Vision-Language Models (LVLMs). Notably, none of existing methods can outperform random selection at different subset sizes. In this work, we propose the first principled method for data-efficient instruction tuning of LVLMs. We prove that examples with similar cross-modal attention matrices during instruction tuning have similar gradients. Thus, they influence model parameters in a similar manner and convey the same information to the model during training. Building on this insight, we propose XMAS, which clusters examples based on the trajectories of the top singular values of their attention matrices obtained from fine-tuning a small proxy LVLM. By sampling a balanced subset from these clusters, XMAS effectively removes redundancy in large-scale LVLM training data. Extensive experiments show that XMAS can discard 50% of the LLaVA-665k dataset and 85% of the Vision-Flan dataset while fully preserving performance of LLaVA-1.5-7B on 10 downstream benchmarks and speeding up its training by 1.2x. This is 30% more data reduction compared to the best baseline for LLaVA-665k. The project's website can be found at https://bigml-cs-ucla.github.io/XMAS-project-page/.",
        "arxiv_id": "2510.01454"
    },
    "2510.01232": {
        "SCORE": 15,
        "ARXIVID": "2510.01232",
        "COMMENT": "Representation Learning/Interpretability: gradient-based ability impact with targeted ablation to mechanistically diagnose benchmarks and decompose model competence.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Dongjun Kim",
            "Gyuho Shim",
            "Yongchan Chun",
            "Minhyuk Kim",
            "Chanjun Park",
            "Heuiseok Lim"
        ],
        "title": "Benchmark Profiling: Mechanistic Diagnosis of LLM Benchmarks",
        "abstract": "Large Language Models are commonly judged by their scores on standard benchmarks, yet such scores often overstate real capability since they mask the mix of skills a task actually demands. For example, ARC is assumed to test reasoning, while HellaSwag is designed to evaluate commonsense. However, we lack a systematic way to verify if these benchmarks actually measure these labels. We introduce Benchmark Profiling, a diagnostic framework that decomposes benchmark performance into ten cognitively grounded abilities. The method combines gradient-based importance scoring with targeted parameter ablation to compute an Ability Impact Score (AIS) that quantifies how much each ability contributes to a model's success on a given benchmark. Profiling three instruction-tuned models across ten widely used benchmarks yields four key findings: (i) most benchmarks draw on several abilities rather than one, (ii) datasets with similar labels rely on distinct ability mixtures, (iii) code-generation benchmarks reward broad, multi-skill improvement and thus show only modest gains from narrow domain-specific fine-tuning, and (iv) abilities irrelevant to the task could negatively affect performance. Benchmark Profiling therefore explains why performance gains do not always translate into user-perceived competence and offers a transparent tool for benchmark audit and model interpretability.",
        "arxiv_id": "2510.01232"
    },
    "2510.02096": {
        "SCORE": 15,
        "ARXIVID": "2510.02096",
        "COMMENT": "Representation Learning: learns weight-space representations from heterogeneous public model hubs with a new backbone for unstructured model populations.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Damian Falk",
            "Konstantin Sch\\\"urholt",
            "Konstantinos Tzevelekakis",
            "L\\'eo Meynent",
            "Damian Borth"
        ],
        "title": "Learning Model Representations Using Publicly Available Model Hubs",
        "abstract": "The weights of neural networks have emerged as a novel data modality, giving rise to the field of weight space learning. A central challenge in this area is that learning meaningful representations of weights typically requires large, carefully constructed collections of trained models, typically referred to as model zoos. These model zoos are often trained ad-hoc, requiring large computational resources, constraining the learned weight space representations in scale and flexibility. In this work, we drop this requirement by training a weight space learning backbone on arbitrary models downloaded from large, unstructured model repositories such as Hugging Face. Unlike curated model zoos, these repositories contain highly heterogeneous models: they vary in architecture and dataset, and are largely undocumented. To address the methodological challenges posed by such heterogeneity, we propose a new weight space backbone designed to handle unstructured model populations. We demonstrate that weight space representations trained on models from Hugging Face achieve strong performance, often outperforming backbones trained on laboratory-generated model zoos. Finally, we show that the diversity of the model weights in our training set allows our weight space model to generalize to unseen data modalities. By demonstrating that high-quality weight space representations can be learned in the wild, we show that curated model zoos are not indispensable, thereby overcoming a strong limitation currently faced by the weight space learning community.",
        "arxiv_id": "2510.02096"
    }
}