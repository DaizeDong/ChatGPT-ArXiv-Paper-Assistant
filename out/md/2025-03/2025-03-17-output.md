> This is a remedial run for missed papers from 01/14/2025 to 01/16/2025.
> 
> Results generated on 03/21/2025.

# Personalized Daily Arxiv Papers 1/17/2025

| *[gpt-4o]*   | Prompt   | Completion   | Total   |
|--------------|----------|--------------|---------|
| **Token**    | 72337    | 10741        | 83078   |
| **Cost**     | $0.18    | $0.11        | $0.28   |

Total arXiv papers: 604

Total scanned papers: 604

Total relevant papers: 55

**Table of contents with paper titles:**

1. [Coded Deep Learning: Framework and Algorithm](#user-content-link1)
**Authors:** En-hui Yang, Shayan Mohajer Hamidi

2. [Fokker-Planck to Callan-Symanzik: evolution of weight matrices under training](#user-content-link2)
**Authors:** Wei Bu, Uri Kol, Ziming Liu

3. [The Mathematics of Artificial Intelligence](#user-content-link3)
**Authors:** Gabriel Peyré

4. [MOGNET: A Mux-residual quantized Network leveraging Online-Generated weights](#user-content-link4)
**Authors:** Van Thien Nguyen, William Guicquero, Gilles Sicard

5. [Towards Understanding Extrapolation: a Causal Lens](#user-content-link5)
**Authors:** Lingjing Kong, Guangyi Chen, Petar Stojanov, Haoxuan Li, Eric P. Xing, Kun Zhang

6. [LeMo: Enabling LEss Token Involvement for MOre Context Fine-tuning](#user-content-link6)
**Authors:** Tuowei Wang, Xingyu Chen, Kun Li, Ting Cao, Ju Ren, Yaoxue Zhang

7. [Rethinking Post-Training Quantization: Introducing a Statistical Pre-Calibration Approach](#user-content-link7)
**Authors:** Alireza Ghaffari, Sharareh Younesian, Boxing Chen, Vahid Partovi Nia, Masoud Asgharian

8. [Optimization Strategies for Enhancing Resource Efficiency in Transformers & Large Language Models](#user-content-link8)
**Authors:** Tom Wallace, Naser Ezzati-Jivan, Beatrice Ombuki-Berman

9. [Identifying Information from Observations with Uncertainty and Novelty](#user-content-link9)
**Authors:** Derek S. Prijatelj, Timothy J. Ireland, Walter J. Scheirer

10. [SuperSAM: Crafting a SAM Supernetwork via Structured Pruning and Unstructured Parameter Prioritization](#user-content-link10)
**Authors:** Waqwoya Abebe, Sadegh Jafari, Sixing Yu, Akash Dutta, Jan Strube, Nathan R. Tallent, Luanzheng Guo, Pablo Munoz, Ali Jannesari

11. [NOMTO: Neural Operator-based symbolic Model approximaTion and discOvery](#user-content-link11)
**Authors:** Sergei Garmaev, Siddhartha Mishra, Olga Fink

12. [MAGNET: Augmenting Generative Decoders with Representation Learning and Infilling Capabilities](#user-content-link12)
**Authors:** Savya Khosla, Aditi Tiwari, Kushal Kafle, Simon Jenni, Handong Zhao, John Collomosse, Jing Shi

13. [Logarithmic Memory Networks (LMNs): Efficient Long-Range Sequence Modeling for Resource-Constrained Environments](#user-content-link13)
**Authors:** Mohamed A. Taha

14. [TriAdaptLoRA: Brain-Inspired Triangular Adaptive Low-Rank Adaptation for Parameter-Efficient Fine-Tuning](#user-content-link14)
**Authors:** Yao Liang, Yuwei Wang, Yi Zeng

15. [Is Stochastic Gradient Descent Effective? A PDE Perspective on Machine Learning processes](#user-content-link15)
**Authors:** Davide Barbieri, Matteo Bonforte, Peio Ibarrondo

16. [A Similarity Measure Between Functions with Applications to Statistical Learning and Optimization](#user-content-link16)
**Authors:** Chengpiao Huang, Kaizheng Wang

17. [MoE$^2$: Optimizing Collaborative Inference for Edge Large Language Models](#user-content-link17)
**Authors:** Lyudong Jin, Yanning Zhang, Yanhan Li, Shurong Wang, Howard H. Yang, Jian Wu, Meng Zhang

18. [Task Vectors in In-Context Learning: Emergence, Formation, and Benefit](#user-content-link18)
**Authors:** Liu Yang, Ziqian Lin, Kangwook Lee, Dimitris Papailiopoulos, Robert Nowak

19. [SWSC: Shared Weight for Similar Channel in LLM](#user-content-link19)
**Authors:** Binrui Zeng, Yongtao Tang, Xiaodong Liu, Xiaopeng Li

20. [FASP: Fast and Accurate Structured Pruning of Large Language Models](#user-content-link20)
**Authors:** Hanyu Hu, Pengxiang Zhao, Ping Li, Yi Zheng, Zhefeng Wang, Xiaoming Yuan

21. [Geometry-Preserving Encoder/Decoder in Latent Generative Models](#user-content-link21)
**Authors:** Wonjun Lee, Riley C. W. O'Neill, Dongmian Zou, Jeff Calder, Gilad Lerman

22. [PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM Serving](#user-content-link22)
**Authors:** Ahmet Caner Yüzügüler, Jiawei Zhuang, Lukas Cavigelli

23. [Transformed Low-rank Adaptation via Tensor Decomposition and Its Applications to Text-to-image Models](#user-content-link23)
**Authors:** Zerui Tao, Yuhta Takida, Naoki Murata, Qibin Zhao, Yuki Mitsufuji

24. [Mono-Forward: Backpropagation-Free Algorithm for Efficient Neural Network Training Harnessing Local Errors](#user-content-link24)
**Authors:** James Gong, Bruce Li, Waleed Abdulla

25. [Training Hybrid Neural Networks with Multimode Optical Nonlinearities Using Digital Twins](#user-content-link25)
**Authors:** Ilker Oguz, Louis J. E. Suter, Jih-Liang Hsieh, Mustafa Yildirim, Niyazi Ulas Dinc, Christophe Moser, Demetri Psaltis

26. [Diagonal Over-parameterization in Reproducing Kernel Hilbert Spaces as an Adaptive Feature Model: Generalization and Adaptivity](#user-content-link26)
**Authors:** Yicheng Li, Qian Lin

27. [Practical Continual Forgetting for Pre-trained Vision Models](#user-content-link27)
**Authors:** Hongbo Zhao, Fei Zhu, Bolin Ni, Feng Zhu, Gaofeng Meng, Zhaoxiang Zhang

28. [Decoding Interpretable Logic Rules from Neural Networks](#user-content-link28)
**Authors:** Chuqin Geng, Xiaojie Xu, Zhaoyue Wang, Ziyu Zhao, Xujie Si

29. [MatrixNet: Learning over symmetry groups using learned group representations](#user-content-link29)
**Authors:** Lucas Laird, Circe Hsu, Asilata Bapat, Robin Walters

30. [Globally Convergent Variational Inference](#user-content-link30)
**Authors:** Declan McNamara, Jackson Loper, Jeffrey Regier

31. [Self-supervised Transformation Learning for Equivariant Representations](#user-content-link31)
**Authors:** Jaemyung Yu, Jaehyun Choi, Dong-Jae Lee, HyeongGwon Hong, Junmo Kim

32. [On the Statistical Capacity of Deep Generative Models](#user-content-link32)
**Authors:** Edric Tam, David B. Dunson

33. [Lossless Compression of Vector IDs for Approximate Nearest Neighbor Search](#user-content-link33)
**Authors:** Daniel Severo, Giuseppe Ottaviano, Matthew Muckley, Karen Ullrich, Matthijs Douze

34. [Random Subspace Cubic-Regularization Methods, with Applications to Low-Rank Functions](#user-content-link34)
**Authors:** Coralia Cartis, Zhen Shao, Edward Tansley

35. [Disentangled Interleaving Variational Encoding](#user-content-link35)
**Authors:** Noelle Y. L. Wong, Eng Yeow Cheu, Zhonglin Chiam, Dipti Srinivasan

36. [Convergence Analysis of Real-time Recurrent Learning (RTRL) for a class of Recurrent Neural Networks](#user-content-link36)
**Authors:** Samuel Chun-Hei Lam, Justin Sirignano, Konstantinos Spiliopoulos

37. [Enhancing Graph Representation Learning with Localized Topological Features](#user-content-link37)
**Authors:** Zuoyu Yan, Qi Zhao, Ze Ye, Tengfei Ma, Liangcai Gao, Zhi Tang, Yusu Wang, Chao Chen

38. [PolyLUT: Ultra-low Latency Polynomial Inference with Hardware-Aware Structured Pruning](#user-content-link38)
**Authors:** Marta Andronic, Jiawen Li, George A. Constantinides

39. [Gradient Descent Converges Linearly to Flatter Minima than Gradient Flow in Shallow Linear Networks](#user-content-link39)
**Authors:** Pierfrancesco Beneventano, Blake Woodworth

40. [Towards Spectral Convergence of Locally Linear Embedding on Manifolds with Boundary](#user-content-link40)
**Authors:** Andrew Lyons

41. [Free-Knots Kolmogorov-Arnold Network: On the Analysis of Spline Knots and Advancing Stability](#user-content-link41)
**Authors:** Liangwewi Nathan Zheng, Wei Emma Zhang, Lin Yue, Miao Xu, Olaf Maennel, Weitong Chen

42. [ARMAX identification of low rank graphical models](#user-content-link42)
**Authors:** Wenqi Cao, Aming Li

43. [ASCENT-ViT: Attention-based Scale-aware Concept Learning Framework for Enhanced Alignment in Vision Transformers](#user-content-link43)
**Authors:** Sanchit Sinha, Guangzhi Xiong, Aidong Zhang

44. [Symmetry-Aware Generative Modeling through Learned Canonicalization](#user-content-link44)
**Authors:** Kusha Sareen, Daniel Levy, Arnab Kumar Mondal, Sékou-Oumar Kaba, Tara Akhound-Sadegh, Siamak Ravanbakhsh

45. [Learnings from Scaling Visual Tokenizers for Reconstruction and Generation](#user-content-link45)
**Authors:** Philippe Hansen-Estruch, David Yan, Ching-Yao Chung, Orr Zohar, Jialiang Wang, Tingbo Hou, Tao Xu, Sriram Vishwanath, Peter Vajda, Xinlei Chen

46. [Complexity Control Facilitates Reasoning-Based Compositional Generalization in Transformers](#user-content-link46)
**Authors:** Zhongwang Zhang, Pengxiao Lin, Zhiwei Wang, Yaoyu Zhang, Zhi-Qin John Xu

47. [Beyond Reward Hacking: Causal Rewards for Large Language Model Alignment](#user-content-link47)
**Authors:** Chaoqi Wang, Zhuokai Zhao, Yibo Jiang, Zhaorun Chen, Chen Zhu, Yuxin Chen, Jiayi Liu, Lizhu Zhang, Xiangjun Fan, Hao Ma, Sinong Wang

48. [Exploring Narrative Clustering in Large Language Models: A Layerwise Analysis of BERT](#user-content-link48)
**Authors:** Awritrojit Banerjee, Achim Schilling, Patrick Krauss

49. [D$^2$-DPM: Dual Denoising for Quantized Diffusion Probabilistic Models](#user-content-link49)
**Authors:** Qian Zeng, Jie Song, Han Zheng, Hao Jiang, Mingli Song

50. [Increasing Batch Size Improves Convergence of Stochastic Gradient Descent with Momentum](#user-content-link50)
**Authors:** Keisuke Kamo, Hideaki Iiduka

51. [A Closer Look at the Learnability of Out-of-Distribution (OOD) Detection](#user-content-link51)
**Authors:** Konstantin Garov, Kamalika Chaudhuri

52. [An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures](#user-content-link52)
**Authors:** Thibaut Boissin, Franck Mamalet, Thomas Fel, Agustin Martin Picard, Thomas Massena, Mathieu Serrurier

53. [Linearly Convergent Mixup Learning](#user-content-link53)
**Authors:** Gakuto Obi, Ayato Saito, Yuto Sasaki, Tsuyoshi Kato

54. [Merging Models on the Fly Without Retraining: A Sequential Approach to Scalable Continual Model Merging](#user-content-link54)
**Authors:** Anke Tang, Enneng Yang, Li Shen, Yong Luo, Han Hu, Bo Du, Dacheng Tao

55. [Weight Averaging for Out-of-Distribution Generalization and Few-Shot Domain Adaptation](#user-content-link55)
**Authors:** Shijian Xu

---

## 1. [Coded Deep Learning: Framework and Algorithm](https://arxiv.org/abs/2501.09849) <a id="link1"></a>

**ArXiv ID:** 2501.09849

**Authors:** En-hui Yang, Shayan Mohajer Hamidi

**Abstract:** The success of deep learning (DL) is often achieved with large models and high complexity during both training and post-training inferences, hindering training in resource-limited settings. To alleviate these issues, this paper introduces a new framework dubbed ``coded deep learning'' (CDL), which integrates information-theoretic coding concepts into the inner workings of DL, to significantly compress model weights and activations, reduce computational complexity at both training and post-training inference stages, and enable efficient model/data parallelism. Specifically, within CDL, (i) we first propose a novel probabilistic method for quantizing both model weights and activations, and its soft differentiable variant which offers an analytic formula for gradient calculation during training; (ii) both the forward and backward passes during training are executed over quantized weights and activations, eliminating most floating-point operations and reducing training complexity; (iii) during training, both weights and activations are entropy constrained so that they are compressible in an information-theoretic sense throughout training, thus reducing communication costs in model/data parallelism; and (iv) the trained model in CDL is by default in a quantized format with compressible quantized weights, reducing post-training inference and storage complexity. Additionally, a variant of CDL, namely relaxed CDL (R-CDL), is presented to further improve the trade-off between validation accuracy and compression though requiring full precision in training with other advantageous features of CDL intact. Extensive empirical results show that CDL and R-CDL outperform the state-of-the-art algorithms in DNN compression in the literature.

**Comment:** The paper introduces a novel framework integrating information-theoretic coding into deep learning for compression and efficiency, directly aligning with model compression and efficiency breakthroughs.

**Relevance:** 10
**Novelty:** 9

---

## 2. [Fokker-Planck to Callan-Symanzik: evolution of weight matrices under training](https://arxiv.org/abs/2501.09659) <a id="link2"></a>

**ArXiv ID:** 2501.09659

**Authors:** Wei Bu, Uri Kol, Ziming Liu

**Abstract:** The dynamical evolution of a neural network during training has been an incredibly fascinating subject of study. First principal derivation of generic evolution of variables in statistical physics systems has proved useful when used to describe training dynamics conceptually, which in practice means numerically solving equations such as Fokker-Planck equation. Simulating entire networks inevitably runs into the curse of dimensionality. In this paper, we utilize Fokker-Planck to simulate the probability density evolution of individual weight matrices in the bottleneck layers of a simple 2-bottleneck-layered auto-encoder and compare the theoretical evolutions against the empirical ones by examining the output data distributions. We also derive physically relevant partial differential equations such as Callan-Symanzik and Kardar-Parisi-Zhang equations from the dynamical equation we have.

**Comment:** The paper explores the training dynamics of neural networks using Fokker-Planck equations, which aligns with representation learning and training dynamics.

**Relevance:** 9
**Novelty:** 8

---

## 3. [The Mathematics of Artificial Intelligence](https://arxiv.org/abs/2501.10465) <a id="link3"></a>

**ArXiv ID:** 2501.10465

**Authors:** Gabriel Peyré

**Abstract:** This overview article highlights the critical role of mathematics in artificial intelligence (AI), emphasizing that mathematics provides tools to better understand and enhance AI systems. Conversely, AI raises new problems and drives the development of new mathematics at the intersection of various fields. This article focuses on the application of analytical and probabilistic tools to model neural network architectures and better understand their optimization. Statistical questions (particularly the generalization capacity of these networks) are intentionally set aside, though they are of crucial importance. We also shed light on the evolution of ideas that have enabled significant advances in AI through architectures tailored to specific tasks, each echoing distinct mathematical techniques. The goal is to encourage more mathematicians to take an interest in and contribute to this exciting field.

**Comment:** The paper discusses the role of mathematics in AI, emphasizing foundational aspects of neural network optimization and architecture modeling, which aligns with emerging trends and theoretical insights.

**Relevance:** 9
**Novelty:** 8

---

## 4. [MOGNET: A Mux-residual quantized Network leveraging Online-Generated weights](https://arxiv.org/abs/2501.09531) <a id="link4"></a>

**ArXiv ID:** 2501.09531

**Authors:** Van Thien Nguyen, William Guicquero, Gilles Sicard

**Abstract:** This paper presents a compact model architecture called MOGNET, compatible with a resource-limited hardware. MOGNET uses a streamlined Convolutional factorization block based on a combination of 2 point-wise (1x1) convolutions with a group-wise convolution in-between. To further limit the overall model size and reduce the on-chip required memory, the second point-wise convolution's parameters are on-line generated by a Cellular Automaton structure. In addition, MOGNET enables the use of low-precision weights and activations, by taking advantage of a Multiplexer mechanism with a proper Bitshift rescaling for integrating residual paths without increasing the hardware-related complexity. To efficiently train this model we also introduce a novel weight ternarization method favoring the balance between quantized levels. Experimental results show that given tiny memory budget (sub-2Mb), MOGNET can achieve higher accuracy with a clear gap up to 1% at a similar or even lower model size compared to recent state-of-the-art methods.

**Comment:** The paper introduces MOGNET, a compact architecture leveraging quantization and efficient design, aligning with model compression and architectural innovation.

**Relevance:** 9
**Novelty:** 8

---

## 5. [Towards Understanding Extrapolation: a Causal Lens](https://arxiv.org/abs/2501.09163) <a id="link5"></a>

**ArXiv ID:** 2501.09163

**Authors:** Lingjing Kong, Guangyi Chen, Petar Stojanov, Haoxuan Li, Eric P. Xing, Kun Zhang

**Abstract:** Canonical work handling distribution shifts typically necessitates an entire target distribution that lands inside the training distribution. However, practical scenarios often involve only a handful of target samples, potentially lying outside the training support, which requires the capability of extrapolation. In this work, we aim to provide a theoretical understanding of when extrapolation is possible and offer principled methods to achieve it without requiring an on-support target distribution. To this end, we formulate the extrapolation problem with a latent-variable model that embodies the minimal change principle in causal mechanisms. Under this formulation, we cast the extrapolation problem into a latent-variable identification problem. We provide realistic conditions on shift properties and the estimation objectives that lead to identification even when only one off-support target sample is available, tackling the most challenging scenarios. Our theory reveals the intricate interplay between the underlying manifold's smoothness and the shift properties. We showcase how our theoretical results inform the design of practical adaptation algorithms. Through experiments on both synthetic and real-world data, we validate our theoretical findings and their practical implications.

**Comment:** The paper provides a theoretical framework for extrapolation using causal principles, aligning with emerging trends and offering foundational insights.

**Relevance:** 9
**Novelty:** 8

---

## 6. [LeMo: Enabling LEss Token Involvement for MOre Context Fine-tuning](https://arxiv.org/abs/2501.09767) <a id="link6"></a>

**ArXiv ID:** 2501.09767

**Authors:** Tuowei Wang, Xingyu Chen, Kun Li, Ting Cao, Ju Ren, Yaoxue Zhang

**Abstract:** The escalating demand for long-context applications has intensified the necessity of extending the LLM context windows. Despite recent fine-tuning approaches successfully expanding context lengths, their high memory footprints, especially for activations, present a critical practical limitation. Current parameter-efficient fine-tuning methods prioritize reducing parameter update overhead over addressing activation memory constraints. Similarly, existing sparsity mechanisms improve computational efficiency but overlook activation memory optimization due to the phenomenon of Shadowy Activation. In this paper, we propose LeMo, the first LLM fine-tuning system that explores and exploits a new token-level sparsity mechanism inherent in long-context scenarios, termed Contextual Token Sparsity. LeMo minimizes redundant token involvement by assessing the informativeness of token embeddings while preserving model accuracy. Specifically, LeMo introduces three key techniques: (1) Token Elimination, dynamically identifying and excluding redundant tokens across varying inputs and layers. (2) Pattern Prediction, utilizing well-trained predictors to approximate token sparsity patterns with minimal overhead. (3) Kernel Optimization, employing permutation-free and segment-based strategies to boost system performance. We implement LeMo as an end-to-end fine-tuning system compatible with various LLM architectures and other optimization techniques. Comprehensive evaluations demonstrate that LeMo reduces memory consumption by up to 1.93x and achieves up to 1.36x speedups, outperforming state-of-the-art fine-tuning systems.

**Comment:** The paper introduces a token-level sparsity mechanism for LLM fine-tuning, which aligns with model compression and efficiency breakthroughs.

**Relevance:** 9
**Novelty:** 8

---

## 7. [Rethinking Post-Training Quantization: Introducing a Statistical Pre-Calibration Approach](https://arxiv.org/abs/2501.09107) <a id="link7"></a>

**ArXiv ID:** 2501.09107

**Authors:** Alireza Ghaffari, Sharareh Younesian, Boxing Chen, Vahid Partovi Nia, Masoud Asgharian

**Abstract:** As Large Language Models (LLMs) become increasingly computationally complex, developing efficient deployment strategies, such as quantization, becomes crucial. State-of-the-art Post-training Quantization (PTQ) techniques often rely on calibration processes to maintain the accuracy of these models. However, while these calibration techniques can enhance performance in certain domains, they may not be as effective in others. This paper aims to draw attention to robust statistical approaches that can mitigate such issues. We propose a weight-adaptive PTQ method that can be considered a precursor to calibration-based PTQ methods, guiding the quantization process to preserve the distribution of weights by minimizing the Kullback-Leibler divergence between the quantized weights and the originally trained weights. This minimization ensures that the quantized model retains the Shannon information content of the original model to a great extent, guaranteeing robust and efficient deployment across many tasks. As such, our proposed approach can perform on par with most common calibration-based PTQ methods, establishing a new pre-calibration step for further adjusting the quantized weights with calibration. We show that our pre-calibration results achieve the same accuracy as some existing calibration-based PTQ methods on various LLMs.

**Comment:** The paper introduces a novel weight-adaptive pre-calibration step for post-training quantization, which aligns with the model compression criterion, specifically focusing on quantization and efficiency improvements.

**Relevance:** 9
**Novelty:** 8

---

## 8. [Optimization Strategies for Enhancing Resource Efficiency in Transformers & Large Language Models](https://arxiv.org/abs/2502.00046) <a id="link8"></a>

**ArXiv ID:** 2502.00046

**Authors:** Tom Wallace, Naser Ezzati-Jivan, Beatrice Ombuki-Berman

**Abstract:** Advancements in Natural Language Processing are heavily reliant on the Transformer architecture, whose improvements come at substantial resource costs due to ever-growing model sizes. This study explores optimization techniques, including Quantization, Knowledge Distillation, and Pruning, focusing on energy and computational efficiency while retaining performance. Among standalone methods, 4-bit Quantization significantly reduces energy use with minimal accuracy loss. Hybrid approaches, like NVIDIA's Minitron approach combining KD and Structured Pruning, further demonstrate promising trade-offs between size reduction and accuracy retention. A novel optimization equation is introduced, offering a flexible framework for comparing various methods. Through the investigation of these compression methods, we provide valuable insights for developing more sustainable and efficient LLMs, shining a light on the often-ignored concern of energy efficiency.

**Comment:** The paper explores optimization strategies for transformers and LLMs, focusing on quantization, pruning, and efficiency improvements, which aligns with model compression and efficiency breakthroughs.

**Relevance:** 9
**Novelty:** 8

---

## 9. [Identifying Information from Observations with Uncertainty and Novelty](https://arxiv.org/abs/2501.09331) <a id="link9"></a>

**ArXiv ID:** 2501.09331

**Authors:** Derek S. Prijatelj, Timothy J. Ireland, Walter J. Scheirer

**Abstract:** A machine learning tasks from observations must encounter and process uncertainty and novelty, especially when it is expected to maintain performance when observing new information and to choose the best fitting hypothesis to the currently observed information. In this context, some key questions arise: what is information, how much information did the observations provide, how much information is required to identify the data-generating process, how many observations remain to get that information, and how does a predictor determine that it has observed novel information? This paper strengthens existing answers to these questions by formalizing the notion of "identifiable information" that arises from the language used to express the relationship between distinct states. Model identifiability and sample complexity are defined via computation of an indicator function over a set of hypotheses. Their properties and asymptotic statistics are described for data-generating processes ranging from deterministic processes to ergodic stationary stochastic processes. This connects the notion of identifying information in finite steps with asymptotic statistics and PAC-learning. The indicator function's computation naturally formalizes novel information and its identification from observations with respect to a hypothesis set. We also proved that computable PAC-Bayes learners' sample complexity distribution is determined by its moments in terms of the the prior probability distribution over a fixed finite hypothesis set.

**Comment:** The paper formalizes the notion of 'identifiable information' and connects it to PAC-learning, which aligns with foundational research in representation learning.

**Relevance:** 9
**Novelty:** 8

---

## 10. [SuperSAM: Crafting a SAM Supernetwork via Structured Pruning and Unstructured Parameter Prioritization](https://arxiv.org/abs/2501.08504) <a id="link10"></a>

**ArXiv ID:** 2501.08504

**Authors:** Waqwoya Abebe, Sadegh Jafari, Sixing Yu, Akash Dutta, Jan Strube, Nathan R. Tallent, Luanzheng Guo, Pablo Munoz, Ali Jannesari

**Abstract:** Neural Architecture Search (NAS) is a powerful approach of automating the design of efficient neural architectures. In contrast to traditional NAS methods, recently proposed one-shot NAS methods prove to be more efficient in performing NAS. One-shot NAS works by generating a singular weight-sharing supernetwork that acts as a search space (container) of subnetworks. Despite its achievements, designing the one-shot search space remains a major challenge. In this work we propose a search space design strategy for Vision Transformer (ViT)-based architectures. In particular, we convert the Segment Anything Model (SAM) into a weight-sharing supernetwork called SuperSAM. Our approach involves automating the search space design via layer-wise structured pruning and parameter prioritization. While the structured pruning applies probabilistic removal of certain transformer layers, parameter prioritization performs weight reordering and slicing of MLP-blocks in the remaining layers. We train supernetworks on several datasets using the sandwich rule. For deployment, we enhance subnetwork discovery by utilizing a program autotuner to identify efficient subnetworks within the search space. The resulting subnetworks are 30-70% smaller in size compared to the original pre-trained SAM ViT-B, yet outperform the pretrained model. Our work introduces a new and effective method for ViT NAS search-space design.

**Comment:** The paper introduces a pruning and parameter prioritization strategy for Vision Transformers, aligning with model compression and architectural innovations.

**Relevance:** 9
**Novelty:** 8

---

## 11. [NOMTO: Neural Operator-based symbolic Model approximaTion and discOvery](https://arxiv.org/abs/2501.08086) <a id="link11"></a>

**ArXiv ID:** 2501.08086

**Authors:** Sergei Garmaev, Siddhartha Mishra, Olga Fink

**Abstract:** While many physical and engineering processes are most effectively described by non-linear symbolic models, existing non-linear symbolic regression (SR) methods are restricted to a limited set of continuous algebraic functions, thereby limiting their applicability to discover higher order non-linear differential relations. In this work, we introduce the Neural Operator-based symbolic Model approximaTion and discOvery (NOMTO) method, a novel approach to symbolic model discovery that leverages Neural Operators to encompass a broad range of symbolic operations. We demonstrate that NOMTO can successfully identify symbolic expressions containing elementary functions with singularities, special functions, and derivatives. Additionally, our experiments demonstrate that NOMTO can accurately rediscover second-order non-linear partial differential equations. By broadening the set of symbolic operations available for discovery, NOMTO significantly advances the capabilities of existing SR methods. It provides a powerful and flexible tool for model discovery, capable of capturing complex relations in a variety of physical systems.

**Comment:** The paper introduces a symbolic model discovery method leveraging neural operators, which aligns with emerging trends in foundational AI research.

**Relevance:** 9
**Novelty:** 8

---

## 12. [MAGNET: Augmenting Generative Decoders with Representation Learning and Infilling Capabilities](https://arxiv.org/abs/2501.08648) <a id="link12"></a>

**ArXiv ID:** 2501.08648

**Authors:** Savya Khosla, Aditi Tiwari, Kushal Kafle, Simon Jenni, Handong Zhao, John Collomosse, Jing Shi

**Abstract:** While originally designed for unidirectional generative modeling, decoder-only large language models (LLMs) are increasingly being adapted for bidirectional modeling. However, unidirectional and bidirectional models are typically trained separately with distinct objectives (generation and representation learning). This separation overlooks the opportunity for developing a more versatile language model and for these objectives to complement each other. In this work, we propose MAGNET, a method for adapting decoder-only LLMs to generate robust representations and infill missing text spans. MAGNET employs three self-supervised training objectives and introduces an attention mechanism that combines bidirectional and causal attention, enabling unified training across all objectives. Our results demonstrate that LLMs adapted with MAGNET (1) surpass strong text encoders on token-level and sentence-level representation learning tasks, (2) generate contextually appropriate text infills by leveraging past and future contexts, (3) perform open-ended text generation without excessive repetition of words or phrases, and (4) preserve the knowledge and reasoning capability gained by the LLM during pretraining.

**Comment:** The paper introduces MAGNET, which adapts decoder-only LLMs for representation learning and infilling, aligning with representation learning and architectural insights.

**Relevance:** 9
**Novelty:** 8

---

## 13. [Logarithmic Memory Networks (LMNs): Efficient Long-Range Sequence Modeling for Resource-Constrained Environments](https://arxiv.org/abs/2501.07905) <a id="link13"></a>

**ArXiv ID:** 2501.07905

**Authors:** Mohamed A. Taha

**Abstract:** Long-range sequence modeling is a crucial aspect of natural language processing and time series analysis. However, traditional models like Recurrent Neural Networks (RNNs) and Transformers suffer from computational and memory inefficiencies, especially when dealing with long sequences. This paper introduces Logarithmic Memory Networks (LMNs), a novel architecture that leverages a hierarchical logarithmic tree structure to efficiently store and retrieve past information. LMNs dynamically summarize historical context, significantly reducing the memory footprint and computational complexity of attention mechanisms from O(n2) to O(log(n)). The model employs a single-vector, targeted attention mechanism to access stored information, and the memory block construction worker (summarizer) layer operates in two modes: a parallel execution mode during training for efficient processing of hierarchical tree structures and a sequential execution mode during inference, which acts as a memory management system. It also implicitly encodes positional information, eliminating the need for explicit positional encodings. These features make LMNs a robust and scalable solution for processing long-range sequences in resource-constrained environments, offering practical improvements in efficiency and scalability. The code is publicly available under the MIT License on GitHub: https://github.com/AhmedBoin/LogarithmicMemory.

**Comment:** The paper introduces Logarithmic Memory Networks (LMNs), which propose a novel architecture for efficient long-range sequence modeling, aligning with model architecture innovations.

**Relevance:** 9
**Novelty:** 8

---

## 14. [TriAdaptLoRA: Brain-Inspired Triangular Adaptive Low-Rank Adaptation for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2501.08008) <a id="link14"></a>

**ArXiv ID:** 2501.08008

**Authors:** Yao Liang, Yuwei Wang, Yi Zeng

**Abstract:** The fine-tuning of Large Language Models (LLMs) is pivotal for achieving optimal performance across diverse downstream tasks. However, while full fine-tuning delivers superior results, it entails significant computational and resource costs. Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA, address these challenges by reducing the number of trainable parameters, but they often struggle with rank adjustment efficiency and task-specific adaptability. We propose Triangular Adaptive Low-Rank Adaptation (TriAdaptLoRA), a novel PEFT framework inspired by neuroscience principles, which dynamically optimizes the allocation of trainable parameters. TriAdaptLoRA introduces three key innovations: 1) a triangular split of transformation matrices into lower and upper triangular components to maximize parameter utilization, 2) a parameter importance metric based on normalized Frobenius norms for efficient adaptation, and 3) an adaptive rank-growth strategy governed by dynamic thresholds, allowing flexible parameter allocation across training steps. Experiments conducted on a variety of natural language understanding and generation tasks demonstrate that TriAdaptLoRA consistently outperforms existing PEFT methods. It achieves superior performance, enhanced stability, and reduced computational overhead, particularly under linear threshold-driven rank growth. These results highlight its efficacy as a scalable and resource-efficient solution for fine-tuning LLMs.

**Comment:** The paper proposes TriAdaptLoRA, a novel PEFT framework for fine-tuning LLMs, aligning with model compression and efficiency innovations.

**Relevance:** 9
**Novelty:** 8

---

## 15. [Is Stochastic Gradient Descent Effective? A PDE Perspective on Machine Learning processes](https://arxiv.org/abs/2501.08425) <a id="link15"></a>

**ArXiv ID:** 2501.08425

**Authors:** Davide Barbieri, Matteo Bonforte, Peio Ibarrondo

**Abstract:** In this paper we analyze the behaviour of the stochastic gradient descent (SGD), a widely used method in supervised learning for optimizing neural network weights via a minimization of non-convex loss functions. Since the pioneering work of E, Li and Tai (2017), the underlying structure of such processes can be understood via parabolic PDEs of Fokker-Planck type, which are at the core of our analysis. Even if Fokker-Planck equations have a long history and a extensive literature, almost nothing is known when the potential is non-convex or when the diffusion matrix is degenerate, and this is the main difficulty that we face in our analysis. We identify two different regimes: in the initial phase of SGD, the loss function drives the weights to concentrate around the nearest local minimum. We refer to this phase as the drift regime and we provide quantitative estimates on this concentration phenomenon. Next, we introduce the diffusion regime, where stochastic fluctuations help the learning process to escape suboptimal local minima. We analyze the Mean Exit Time (MET) and prove upper and lower bounds of the MET. Finally, we address the asymptotic convergence of SGD, for a non-convex cost function and a degenerate diffusion matrix, that do not allow to use the standard approaches, and require new techniques. For this purpose, we exploit two different methods: duality and entropy methods. We provide new results about the dynamics and effectiveness of SGD, offering a deep connection between stochastic optimization and PDE theory, and some answers and insights to basic questions in the Machine Learning processes: How long does SGD take to escape from a bad minimum? Do neural network parameters converge using SGD? How do parameters evolve in the first stage of training with SGD?

**Comment:** The paper provides a PDE-based analysis of SGD, offering theoretical insights into its dynamics and effectiveness. This aligns with foundational research in optimization and training dynamics.

**Relevance:** 9
**Novelty:** 8

---

## 16. [A Similarity Measure Between Functions with Applications to Statistical Learning and Optimization](https://arxiv.org/abs/2501.08317) <a id="link16"></a>

**ArXiv ID:** 2501.08317

**Authors:** Chengpiao Huang, Kaizheng Wang

**Abstract:** In this note, we present a novel measure of similarity between two functions. It quantifies how the sub-optimality gaps of two functions convert to each other, and unifies several existing notions of functional similarity. We show that it has convenient operation rules, and illustrate its use in empirical risk minimization and non-stationary online optimization.

**Comment:** The paper introduces a novel similarity measure between functions with applications in statistical learning and optimization. This is a foundational contribution with potential broad impact.

**Relevance:** 9
**Novelty:** 8

---

## 17. [MoE$^2$: Optimizing Collaborative Inference for Edge Large Language Models](https://arxiv.org/abs/2501.09410) <a id="link17"></a>

**ArXiv ID:** 2501.09410

**Authors:** Lyudong Jin, Yanning Zhang, Yanhan Li, Shurong Wang, Howard H. Yang, Jian Wu, Meng Zhang

**Abstract:** Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks. Exploiting the heterogeneous capabilities of edge LLMs is crucial for diverse emerging applications, as it enables greater cost-effectiveness and reduced latency. In this work, we introduce \textit{Mixture-of-Edge-Experts (MoE$^2$)}, a novel collaborative inference framework for edge LLMs. We formulate the joint gating and expert selection problem to optimize inference performance under energy and latency constraints. Unlike conventional MoE problems, LLM expert selection is significantly more challenging due to the combinatorial nature and the heterogeneity of edge LLMs across various attributes. To this end, we propose a two-level expert selection mechanism through which we uncover an optimality-preserving property of gating parameters across expert selections. This property enables the decomposition of the training and selection processes, significantly reducing complexity. Furthermore, we leverage the objective's monotonicity and design a discrete monotonic optimization algorithm for optimal expert selection. We implement edge servers with NVIDIA Jetson AGX Orins and NVIDIA RTX 4090 GPUs, and perform extensive experiments. Our results validate that performance improvements of various LLM models and show that our MoE$^2$ method can achieve optimal trade-offs among different delay and energy budgets, and outperforms baselines under various system resource constraints.

**Comment:** The paper proposes MoE$^2$, a novel framework for collaborative inference in edge LLMs, which aligns with the Mixture-of-Experts (MoE) criterion and provides architectural insights.

**Relevance:** 9
**Novelty:** 8

---

## 18. [Task Vectors in In-Context Learning: Emergence, Formation, and Benefit](https://arxiv.org/abs/2501.09240) <a id="link18"></a>

**ArXiv ID:** 2501.09240

**Authors:** Liu Yang, Ziqian Lin, Kangwook Lee, Dimitris Papailiopoulos, Robert Nowak

**Abstract:** In-context learning is a remarkable capability of transformers, referring to their ability to adapt to specific tasks based on a short history or context. Previous research has found that task-specific information is locally encoded within models, though their emergence and functionality remain unclear due to opaque pre-training processes. In this work, we investigate the formation of task vectors in a controlled setting, using models trained from scratch on synthetic datasets. Our findings confirm that task vectors naturally emerge under certain conditions, but the tasks may be relatively weakly and/or non-locally encoded within the model. To promote strong task vectors encoded at a prescribed location within the model, we propose an auxiliary training mechanism based on a task vector prompting loss (TVP-loss). This method eliminates the need to search for task-correlated encodings within the trained model and demonstrably improves robustness and generalization.

**Comment:** The paper investigates task vectors in in-context learning, providing insights into representation learning and training dynamics in transformers, which aligns well with the core topics.

**Relevance:** 9
**Novelty:** 8

---

## 19. [SWSC: Shared Weight for Similar Channel in LLM](https://arxiv.org/abs/2501.08631) <a id="link19"></a>

**ArXiv ID:** 2501.08631

**Authors:** Binrui Zeng, Yongtao Tang, Xiaodong Liu, Xiaopeng Li

**Abstract:** Large language models (LLMs) have spurred development in multiple industries. However, the growing number of their parameters brings substantial storage and computing burdens, making it essential to explore model compression techniques for parameter reduction and easier deployment. We propose SWSC, an LLM compression method based on the concept of Shared Weight for Similar Channel. It uses the K-Means clustering algorithm to cluster model weights channel-by-channel, generating clusters with highly similar vectors within each. A representative vector from each cluster is selected to approximately replace all vectors in the cluster, significantly reducing the number of model weight parameters. However, approximate restoration will inevitably cause damage to the performance of the model. To tackle this issue, we perform singular value decomposition on the weight error values before and after compression and retain the larger singular values and their corresponding singular vectors to compensate for the accuracy. The experimental results show that our method can effectively ensure the performance of the compressed LLM even under low-precision conditions.

**Comment:** The paper proposes SWSC, a novel compression method for LLMs, which aligns with the model compression criterion and introduces a unique clustering-based approach.

**Relevance:** 9
**Novelty:** 8

---

## 20. [FASP: Fast and Accurate Structured Pruning of Large Language Models](https://arxiv.org/abs/2501.09412) <a id="link20"></a>

**ArXiv ID:** 2501.09412

**Authors:** Hanyu Hu, Pengxiang Zhao, Ping Li, Yi Zheng, Zhefeng Wang, Xiaoming Yuan

**Abstract:** The rapid increase in the size of large language models (LLMs) has significantly escalated their computational and memory demands, posing challenges for efficient deployment, especially on resource-constrained devices. Structured pruning has emerged as an effective model compression method that can reduce these demands while preserving performance. In this paper, we introduce FASP (Fast and Accurate Structured Pruning), a novel structured pruning framework for LLMs that emphasizes both speed and accuracy. FASP employs a distinctive pruning structure that interlinks sequential layers, allowing for the removal of columns in one layer while simultaneously eliminating corresponding rows in the preceding layer without incurring additional performance loss. The pruning metric, inspired by Wanda, is computationally efficient and effectively selects components to prune. Additionally, we propose a restoration mechanism that enhances model fidelity by adjusting the remaining weights post-pruning. We evaluate FASP on the OPT and LLaMA model families, demonstrating superior performance in terms of perplexity and accuracy on downstream tasks compared to state-of-the-art methods. Our approach achieves significant speed-ups, pruning models such as OPT-125M in 17 seconds and LLaMA-30B in 15 minutes on a single NVIDIA RTX 4090 GPU, making it a highly practical solution for optimizing LLMs.

**Comment:** The paper introduces FASP, a structured pruning framework for LLMs, which aligns with the model compression criterion and provides significant efficiency improvements.

**Relevance:** 9
**Novelty:** 8

---

## 21. [Geometry-Preserving Encoder/Decoder in Latent Generative Models](https://arxiv.org/abs/2501.09876) <a id="link21"></a>

**ArXiv ID:** 2501.09876

**Authors:** Wonjun Lee, Riley C. W. O'Neill, Dongmian Zou, Jeff Calder, Gilad Lerman

**Abstract:** Generative modeling aims to generate new data samples that resemble a given dataset, with diffusion models recently becoming the most popular generative model. One of the main challenges of diffusion models is solving the problem in the input space, which tends to be very high-dimensional. Recently, solving diffusion models in the latent space through an encoder that maps from the data space to a lower-dimensional latent space has been considered to make the training process more efficient and has shown state-of-the-art results. The variational autoencoder (VAE) is the most commonly used encoder/decoder framework in this domain, known for its ability to learn latent representations and generate data samples. In this paper, we introduce a novel encoder/decoder framework with theoretical properties distinct from those of the VAE, specifically designed to preserve the geometric structure of the data distribution. We demonstrate the significant advantages of this geometry-preserving encoder in the training process of both the encoder and decoder. Additionally, we provide theoretical results proving convergence of the training process, including convergence guarantees for encoder training, and results showing faster convergence of decoder training when using the geometry-preserving encoder.

**Comment:** The paper introduces a geometry-preserving encoder/decoder framework, which aligns with foundational research in representation learning and encoder-decoder architectures.

**Relevance:** 9
**Novelty:** 8

---

## 22. [PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM Serving](https://arxiv.org/abs/2501.08192) <a id="link22"></a>

**ArXiv ID:** 2501.08192

**Authors:** Ahmet Caner Yüzügüler, Jiawei Zhuang, Lukas Cavigelli

**Abstract:** Large language models (LLMs) are widely used across various applications, but their substantial computational requirements pose significant challenges, particularly in terms of HBM bandwidth bottlenecks and inter-device communication overhead. In this paper, we present PRESERVE, a novel prefetching framework designed to optimize LLM inference by overlapping memory reads for model weights and KV-cache with collective communication operations. Through extensive experiments conducted on commercial AI accelerators, we demonstrate up to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs. Additionally, we perform a design space exploration that identifies the optimal hardware configuration for the proposed method, showing a further 1.25x improvement in performance per cost by selecting the optimal L2 cache size. Our results show that PRESERVE has the potential to mitigate the memory bottlenecks and communication overheads, offering a solution to improve the performance and scalability of the LLM inference systems.

**Comment:** The paper introduces a prefetching framework for optimizing LLM inference, which aligns with model compression and efficiency improvements, particularly focusing on KV-cache optimization.

**Relevance:** 9
**Novelty:** 7

---

## 23. [Transformed Low-rank Adaptation via Tensor Decomposition and Its Applications to Text-to-image Models](https://arxiv.org/abs/2501.08727) <a id="link23"></a>

**ArXiv ID:** 2501.08727

**Authors:** Zerui Tao, Yuhta Takida, Naoki Murata, Qibin Zhao, Yuki Mitsufuji

**Abstract:** Parameter-Efficient Fine-Tuning (PEFT) of text-to-image models has become an increasingly popular technique with many applications. Among the various PEFT methods, Low-Rank Adaptation (LoRA) and its variants have gained significant attention due to their effectiveness, enabling users to fine-tune models with limited computational resources. However, the approximation gap between the low-rank assumption and desired fine-tuning weights prevents the simultaneous acquisition of ultra-parameter-efficiency and better performance. To reduce this gap and further improve the power of LoRA, we propose a new PEFT method that combines two classes of adaptations, namely, transform and residual adaptations. In specific, we first apply a full-rank and dense transform to the pre-trained weight. This learnable transform is expected to align the pre-trained weight as closely as possible to the desired weight, thereby reducing the rank of the residual weight. Then, the residual part can be effectively approximated by more compact and parameter-efficient structures, with a smaller approximation error. To achieve ultra-parameter-efficiency in practice, we design highly flexible and effective tensor decompositions for both the transform and residual adaptations. Additionally, popular PEFT methods such as DoRA can be summarized under this transform plus residual adaptation scheme. Experiments are conducted on fine-tuning Stable Diffusion models in subject-driven and controllable generation. The results manifest that our method can achieve better performances and parameter efficiency compared to LoRA and several baselines.

**Comment:** The paper proposes a novel parameter-efficient fine-tuning method combining tensor decomposition, which aligns with model compression and efficiency breakthroughs.

**Relevance:** 8
**Novelty:** 8

---

## 24. [Mono-Forward: Backpropagation-Free Algorithm for Efficient Neural Network Training Harnessing Local Errors](https://arxiv.org/abs/2501.09238) <a id="link24"></a>

**ArXiv ID:** 2501.09238

**Authors:** James Gong, Bruce Li, Waleed Abdulla

**Abstract:** Backpropagation is the standard method for achieving state-of-the-art accuracy in neural network training, but it often imposes high memory costs and lacks biological plausibility. In this paper, we introduce the Mono-Forward algorithm, a purely local layerwise learning method inspired by Hinton's Forward-Forward framework. Unlike backpropagation, Mono-Forward optimizes each layer solely with locally available information, eliminating the reliance on global error signals. We evaluated Mono-Forward on multi-layer perceptrons and convolutional neural networks across multiple benchmarks, including MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100. The test results show that Mono-Forward consistently matches or surpasses the accuracy of backpropagation across all tasks, with significantly reduced and more even memory usage, better parallelizability, and a comparable convergence rate.

**Comment:** The paper introduces a backpropagation-free training algorithm inspired by local learning, which aligns with emerging trends in training dynamics.

**Relevance:** 8
**Novelty:** 8

---

## 25. [Training Hybrid Neural Networks with Multimode Optical Nonlinearities Using Digital Twins](https://arxiv.org/abs/2501.07991) <a id="link25"></a>

**ArXiv ID:** 2501.07991

**Authors:** Ilker Oguz, Louis J. E. Suter, Jih-Liang Hsieh, Mustafa Yildirim, Niyazi Ulas Dinc, Christophe Moser, Demetri Psaltis

**Abstract:** The ability to train ever-larger neural networks brings artificial intelligence to the forefront of scientific and technical discoveries. However, their exponentially increasing size creates a proportionally greater demand for energy and computational hardware. Incorporating complex physical events in networks as fixed, efficient computation modules can address this demand by decreasing the complexity of trainable layers. Here, we utilize ultrashort pulse propagation in multimode fibers, which perform large-scale nonlinear transformations, for this purpose. Training the hybrid architecture is achieved through a neural model that differentiably approximates the optical system. The training algorithm updates the neural simulator and backpropagates the error signal over this proxy to optimize layers preceding the optical one. Our experimental results achieve state-of-the-art image classification accuracies and simulation fidelity. Moreover, the framework demonstrates exceptional resilience to experimental drifts. By integrating low-energy physical systems into neural networks, this approach enables scalable, energy-efficient AI models with significantly reduced computational demands.

**Comment:** The paper explores hybrid neural networks with physical systems for energy-efficient AI, which aligns with model compression and efficiency breakthroughs.

**Relevance:** 8
**Novelty:** 8

---

## 26. [Diagonal Over-parameterization in Reproducing Kernel Hilbert Spaces as an Adaptive Feature Model: Generalization and Adaptivity](https://arxiv.org/abs/2501.08679) <a id="link26"></a>

**ArXiv ID:** 2501.08679

**Authors:** Yicheng Li, Qian Lin

**Abstract:** This paper introduces a diagonal adaptive kernel model that dynamically learns kernel eigenvalues and output coefficients simultaneously during training. Unlike fixed-kernel methods tied to the neural tangent kernel theory, the diagonal adaptive kernel model adapts to the structure of the truth function, significantly improving generalization over fixed-kernel methods, especially when the initial kernel is misaligned with the target. Moreover, we show that the adaptivity comes from learning the right eigenvalues during training, showing a feature learning behavior. By extending to deeper parameterization, we further show how extra depth enhances adaptability and generalization. This study combines the insights from feature learning and implicit regularization and provides new perspective into the adaptivity and generalization potential of neural networks beyond the kernel regime.

**Comment:** The paper introduces a diagonal adaptive kernel model that exhibits feature learning behavior, aligning with representation learning and feature learning criteria.

**Relevance:** 8
**Novelty:** 8

---

## 27. [Practical Continual Forgetting for Pre-trained Vision Models](https://arxiv.org/abs/2501.09705) <a id="link27"></a>

**ArXiv ID:** 2501.09705

**Authors:** Hongbo Zhao, Fei Zhu, Bolin Ni, Feng Zhu, Gaofeng Meng, Zhaoxiang Zhang

**Abstract:** For privacy and security concerns, the need to erase unwanted information from pre-trained vision models is becoming evident nowadays. In real-world scenarios, erasure requests originate at any time from both users and model owners, and these requests usually form a sequence. Therefore, under such a setting, selective information is expected to be continuously removed from a pre-trained model while maintaining the rest. We define this problem as continual forgetting and identify three key challenges. (i) For unwanted knowledge, efficient and effective deleting is crucial. (ii) For remaining knowledge, the impact brought by the forgetting procedure should be minimal. (iii) In real-world scenarios, the training samples may be scarce or partially missing during the process of forgetting. To address them, we first propose Group Sparse LoRA (GS-LoRA). Specifically, towards (i), we introduce LoRA modules to fine-tune the FFN layers in Transformer blocks for each forgetting task independently, and towards (ii), a simple group sparse regularization is adopted, enabling automatic selection of specific LoRA groups and zeroing out the others. To further extend GS-LoRA to more practical scenarios, we incorporate prototype information as additional supervision and introduce a more practical approach, GS-LoRA++. For each forgotten class, we move the logits away from its original prototype. For the remaining classes, we pull the logits closer to their respective prototypes. We conduct extensive experiments on face recognition, object detection and image classification and demonstrate that our method manages to forget specific classes with minimal impact on other classes. Codes have been released on https://github.com/bjzhb666/GS-LoRA.

**Comment:** The paper addresses continual forgetting in vision models with novel methods like GS-LoRA, which aligns with foundational research in model compression and sparsity.

**Relevance:** 8
**Novelty:** 8

---

## 28. [Decoding Interpretable Logic Rules from Neural Networks](https://arxiv.org/abs/2501.08281) <a id="link28"></a>

**ArXiv ID:** 2501.08281

**Authors:** Chuqin Geng, Xiaojie Xu, Zhaoyue Wang, Ziyu Zhao, Xujie Si

**Abstract:** As deep neural networks continue to excel across various domains, their black-box nature has raised concerns about transparency and trust. In particular, interpretability has become increasingly essential for applications that demand high safety and knowledge rigor, such as drug discovery, autonomous driving, and genomics. However, progress in understanding even the simplest deep neural networks - such as fully connected networks - has been limited, despite their role as foundational elements in state-of-the-art models like ResNet and Transformer. In this paper, we address this challenge by introducing NeuroLogic, a novel approach for decoding interpretable logic rules from neural networks. NeuroLogic leverages neural activation patterns to capture the model's critical decision-making processes, translating them into logical rules represented by hidden predicates. Thanks to its flexible design in the grounding phase, NeuroLogic can be adapted to a wide range of neural networks. For simple fully connected neural networks, hidden predicates can be grounded in certain split patterns of original input features to derive decision-tree-like rules. For large, complex vision neural networks, NeuroLogic grounds hidden predicates into high-level visual concepts that are understandable to humans. Our empirical study demonstrates that NeuroLogic can extract global and interpretable rules from state-of-the-art models such as ResNet, a task at which existing work struggles. We believe NeuroLogic can help pave the way for understanding the black-box nature of neural networks.

**Comment:** The paper introduces NeuroLogic for decoding interpretable logic rules from neural networks, which aligns with foundational research in interpretability and representation learning.

**Relevance:** 8
**Novelty:** 8

---

## 29. [MatrixNet: Learning over symmetry groups using learned group representations](https://arxiv.org/abs/2501.09571) <a id="link29"></a>

**ArXiv ID:** 2501.09571

**Authors:** Lucas Laird, Circe Hsu, Asilata Bapat, Robin Walters

**Abstract:** Group theory has been used in machine learning to provide a theoretically grounded approach for incorporating known symmetry transformations in tasks from robotics to protein modeling. In these applications, equivariant neural networks use known symmetry groups with predefined representations to learn over geometric input data. We propose MatrixNet, a neural network architecture that learns matrix representations of group element inputs instead of using predefined representations. MatrixNet achieves higher sample efficiency and generalization over several standard baselines in prediction tasks over the several finite groups and the Artin braid group. We also show that MatrixNet respects group relations allowing generalization to group elements of greater word length than in the training set.

**Comment:** The paper introduces MatrixNet, which learns group representations and aligns with representation learning and emerging trends in symmetry-based methods.

**Relevance:** 8
**Novelty:** 8

---

## 30. [Globally Convergent Variational Inference](https://arxiv.org/abs/2501.08201) <a id="link30"></a>

**ArXiv ID:** 2501.08201

**Authors:** Declan McNamara, Jackson Loper, Jeffrey Regier

**Abstract:** In variational inference (VI), an approximation of the posterior distribution is selected from a family of distributions through numerical optimization. With the most common variational objective function, known as the evidence lower bound (ELBO), only convergence to a local optimum can be guaranteed. In this work, we instead establish the global convergence of a particular VI method. This VI method, which may be considered an instance of neural posterior estimation (NPE), minimizes an expectation of the inclusive (forward) KL divergence to fit a variational distribution that is parameterized by a neural network. Our convergence result relies on the neural tangent kernel (NTK) to characterize the gradient dynamics that arise from considering the variational objective in function space. In the asymptotic regime of a fixed, positive-definite neural tangent kernel, we establish conditions under which the variational objective admits a unique solution in a reproducing kernel Hilbert space (RKHS). Then, we show that the gradient descent dynamics in function space converge to this unique function. In ablation studies and practical problems, we demonstrate that our results explain the behavior of NPE in non-asymptotic finite-neuron settings, and show that NPE outperforms ELBO-based optimization, which often converges to shallow local optima.

**Comment:** This paper provides a theoretical contribution to variational inference by establishing global convergence using neural tangent kernels. It aligns with foundational research in representation learning and optimization.

**Relevance:** 8
**Novelty:** 8

---

## 31. [Self-supervised Transformation Learning for Equivariant Representations](https://arxiv.org/abs/2501.08712) <a id="link31"></a>

**ArXiv ID:** 2501.08712

**Authors:** Jaemyung Yu, Jaehyun Choi, Dong-Jae Lee, HyeongGwon Hong, Junmo Kim

**Abstract:** Unsupervised representation learning has significantly advanced various machine learning tasks. In the computer vision domain, state-of-the-art approaches utilize transformations like random crop and color jitter to achieve invariant representations, embedding semantically the same inputs despite transformations. However, this can degrade performance in tasks requiring precise features, such as localization or flower classification. To address this, recent research incorporates equivariant representation learning, which captures transformation-sensitive information. However, current methods depend on transformation labels and thus struggle with interdependency and complex transformations. We propose Self-supervised Transformation Learning (STL), replacing transformation labels with transformation representations derived from image pairs. The proposed method ensures transformation representation is image-invariant and learns corresponding equivariant transformations, enhancing performance without increased batch complexity. We demonstrate the approach's effectiveness across diverse classification and detection tasks, outperforming existing methods in 7 out of 11 benchmarks and excelling in detection. By integrating complex transformations like AugMix, unusable by prior equivariant methods, this approach enhances performance across tasks, underscoring its adaptability and resilience. Additionally, its compatibility with various base models highlights its flexibility and broad applicability. The code is available at https://github.com/jaemyung-u/stl.

**Comment:** The paper introduces a self-supervised method for learning equivariant representations, which is relevant to representation learning. Its focus on transformation learning adds novelty to the field.

**Relevance:** 8
**Novelty:** 8

---

## 32. [On the Statistical Capacity of Deep Generative Models](https://arxiv.org/abs/2501.07763) <a id="link32"></a>

**ArXiv ID:** 2501.07763

**Authors:** Edric Tam, David B. Dunson

**Abstract:** Deep generative models are routinely used in generating samples from complex, high-dimensional distributions. Despite their apparent successes, their statistical properties are not well understood. A common assumption is that with enough training data and sufficiently large neural networks, deep generative model samples will have arbitrarily small errors in sampling from any continuous target distribution. We set up a unifying framework that debunks this belief. We demonstrate that broad classes of deep generative models, including variational autoencoders and generative adversarial networks, are not universal generators. Under the predominant case of Gaussian latent variables, these models can only generate concentrated samples that exhibit light tails. Using tools from concentration of measure and convex geometry, we give analogous results for more general log-concave and strongly log-concave latent variable distributions. We extend our results to diffusion models via a reduction argument. We use the Gromov--Levy inequality to give similar guarantees when the latent variables lie on manifolds with positive Ricci curvature. These results shed light on the limited capacity of common deep generative models to handle heavy tails. We illustrate the empirical relevance of our work with simulations and financial data.

**Comment:** The paper provides theoretical insights into the statistical capacity of deep generative models, which aligns with foundational research in representation learning.

**Relevance:** 8
**Novelty:** 8

---

## 33. [Lossless Compression of Vector IDs for Approximate Nearest Neighbor Search](https://arxiv.org/abs/2501.10479) <a id="link33"></a>

**ArXiv ID:** 2501.10479

**Authors:** Daniel Severo, Giuseppe Ottaviano, Matthew Muckley, Karen Ullrich, Matthijs Douze

**Abstract:** Approximate nearest neighbor search for vectors relies on indexes that are most often accessed from RAM. Therefore, storage is the factor limiting the size of the database that can be served from a machine. Lossy vector compression, i.e., embedding quantization, has been applied extensively to reduce the size of indexes. However, for inverted file and graph-based indices, auxiliary data such as vector ids and links (edges) can represent most of the storage cost. We introduce and evaluate lossless compression schemes for these cases. These approaches are based on asymmetric numeral systems or wavelet trees that exploit the fact that the ordering of ids is irrelevant within the data structures. In some settings, we are able to compress the vector ids by a factor 7, with no impact on accuracy or search runtime. On billion-scale datasets, this results in a reduction of 30% of the index size. Furthermore, we show that for some datasets, these methods can also compress the quantized vector codes losslessly, by exploiting sub-optimalities in the original quantization algorithm. The source code for our approach available at https://github.com/facebookresearch/vector_db_id_compression.

**Comment:** The paper focuses on lossless compression techniques for vector IDs in approximate nearest neighbor search, which aligns with the model compression criterion, particularly in terms of efficiency breakthroughs.

**Relevance:** 8
**Novelty:** 7

---

## 34. [Random Subspace Cubic-Regularization Methods, with Applications to Low-Rank Functions](https://arxiv.org/abs/2501.09734) <a id="link34"></a>

**ArXiv ID:** 2501.09734

**Authors:** Coralia Cartis, Zhen Shao, Edward Tansley

**Abstract:** We propose and analyze random subspace variants of the second-order Adaptive Regularization using Cubics (ARC) algorithm. These methods iteratively restrict the search space to some random subspace of the parameters, constructing and minimizing a local model only within this subspace. Thus, our variants only require access to (small-dimensional) projections of first- and second-order problem derivatives and calculate a reduced step inexpensively. Under suitable assumptions, the ensuing methods maintain the optimal first-order, and second-order, global rates of convergence of (full-dimensional) cubic regularization, while showing improved scalability both theoretically and numerically, particularly when applied to low-rank functions. When applied to the latter, our adaptive variant naturally adapts the subspace size to the true rank of the function, without knowing it a priori.

**Comment:** The paper proposes random subspace cubic-regularization methods, which align with model compression and efficiency breakthroughs.

**Relevance:** 8
**Novelty:** 7

---

## 35. [Disentangled Interleaving Variational Encoding](https://arxiv.org/abs/2501.08710) <a id="link35"></a>

**ArXiv ID:** 2501.08710

**Authors:** Noelle Y. L. Wong, Eng Yeow Cheu, Zhonglin Chiam, Dipti Srinivasan

**Abstract:** Conflicting objectives present a considerable challenge in interleaving multi-task learning, necessitating the need for meticulous design and balance to ensure effective learning of a representative latent data space across all tasks without mutual negative impact. Drawing inspiration from the concept of marginal and conditional probability distributions in probability theory, we design a principled and well-founded approach to disentangle the original input into marginal and conditional probability distributions in the latent space of a variational autoencoder. Our proposed model, Deep Disentangled Interleaving Variational Encoding (DeepDIVE) learns disentangled features from the original input to form clusters in the embedding space and unifies these features via the cross-attention mechanism in the fusion stage. We theoretically prove that combining the objectives for reconstruction and forecasting fully captures the lower bound and mathematically derive a loss function for disentanglement using Na\"ive Bayes. Under the assumption that the prior is a mixture of log-concave distributions, we also establish that the Kullback-Leibler divergence between the prior and the posterior is upper bounded by a function minimized by the minimizer of the cross entropy loss, informing our adoption of radial basis functions (RBF) and cross entropy with interleaving training for DeepDIVE to provide a justified basis for convergence. Experiments on two public datasets show that DeepDIVE disentangles the original input and yields forecast accuracies better than the original VAE and comparable to existing state-of-the-art baselines.

**Comment:** The paper introduces a novel disentangled representation learning method using variational autoencoders, which aligns with the representation learning criterion.

**Relevance:** 8
**Novelty:** 7

---

## 36. [Convergence Analysis of Real-time Recurrent Learning (RTRL) for a class of Recurrent Neural Networks](https://arxiv.org/abs/2501.08040) <a id="link36"></a>

**ArXiv ID:** 2501.08040

**Authors:** Samuel Chun-Hei Lam, Justin Sirignano, Konstantinos Spiliopoulos

**Abstract:** Recurrent neural networks (RNNs) are commonly trained with the truncated backpropagation-through-time (TBPTT) algorithm. For the purposes of computational tractability, the TBPTT algorithm truncates the chain rule and calculates the gradient on a finite block of the overall data sequence. Such approximation could lead to significant inaccuracies, as the block length for the truncated backpropagation is typically limited to be much smaller than the overall sequence length. In contrast, Real-time recurrent learning (RTRL) is an online optimization algorithm which asymptotically follows the true gradient of the loss on the data sequence as the number of sequence time steps $t \rightarrow \infty$. RTRL forward propagates the derivatives of the RNN hidden/memory units with respect to the parameters and, using the forward derivatives, performs online updates of the parameters at each time step in the data sequence. RTRL's online forward propagation allows for exact optimization over extremely long data sequences, although it can be computationally costly for models with large numbers of parameters. We prove convergence of the RTRL algorithm for a class of RNNs. The convergence analysis establishes a fixed point for the joint distribution of the data sequence, RNN hidden layer, and the RNN hidden layer forward derivatives as the number of data samples from the sequence and the number of training steps tend to infinity. We prove convergence of the RTRL algorithm to a stationary point of the loss. Numerical studies illustrate our theoretical results. One potential application area for RTRL is the analysis of financial data, which typically involve long time series and models with small to medium numbers of parameters. This makes RTRL computationally tractable and a potentially appealing optimization method for training models. Thus, we include an example of RTRL applied to limit order book data.

**Comment:** The paper provides a convergence analysis of RTRL for RNNs, offering theoretical insights into training dynamics, which aligns with representation learning and training dynamics criteria.

**Relevance:** 8
**Novelty:** 7

---

## 37. [Enhancing Graph Representation Learning with Localized Topological Features](https://arxiv.org/abs/2501.09178) <a id="link37"></a>

**ArXiv ID:** 2501.09178

**Authors:** Zuoyu Yan, Qi Zhao, Ze Ye, Tengfei Ma, Liangcai Gao, Zhi Tang, Yusu Wang, Chao Chen

**Abstract:** Representation learning on graphs is a fundamental problem that can be crucial in various tasks. Graph neural networks, the dominant approach for graph representation learning, are limited in their representation power. Therefore, it can be beneficial to explicitly extract and incorporate high-order topological and geometric information into these models. In this paper, we propose a principled approach to extract the rich connectivity information of graphs based on the theory of persistent homology. Our method utilizes the topological features to enhance the representation learning of graph neural networks and achieve state-of-the-art performance on various node classification and link prediction benchmarks. We also explore the option of end-to-end learning of the topological features, i.e., treating topological computation as a differentiable operator during learning. Our theoretical analysis and empirical study provide insights and potential guidelines for employing topological features in graph learning tasks.

**Comment:** The paper enhances graph representation learning with topological features, aligning with representation learning and offering insights into graph neural networks.

**Relevance:** 8
**Novelty:** 7

---

## 38. [PolyLUT: Ultra-low Latency Polynomial Inference with Hardware-Aware Structured Pruning](https://arxiv.org/abs/2501.08043) <a id="link38"></a>

**ArXiv ID:** 2501.08043

**Authors:** Marta Andronic, Jiawen Li, George A. Constantinides

**Abstract:** Standard deep neural network inference involves the computation of interleaved linear maps and nonlinear activation functions. Prior work for ultra-low latency implementations has hardcoded these operations inside FPGA lookup tables (LUTs). However, FPGA LUTs can implement a much greater variety of functions. In this paper, we propose a novel approach to training DNNs for FPGA deployment using multivariate polynomials as the basic building block. Our method takes advantage of the flexibility offered by the soft logic, hiding the polynomial evaluation inside the LUTs with minimal overhead. By using polynomial building blocks, we achieve the same accuracy using considerably fewer layers of soft logic than by using linear functions, leading to significant latency and area improvements. LUT-based implementations also face a significant challenge: the LUT size grows exponentially with the number of inputs. Prior work relies on a priori fixed sparsity, with results heavily dependent on seed selection. To address this, we propose a structured pruning strategy using a bespoke hardware-aware group regularizer that encourages a particular sparsity pattern that leads to a small number of inputs per neuron. We demonstrate the effectiveness of PolyLUT on three tasks: network intrusion detection, jet identification at the CERN Large Hadron Collider, and MNIST.

**Comment:** The paper introduces a novel structured pruning strategy for FPGA deployment, which aligns with the model compression criterion, particularly in sparsity and hardware-aware pruning.

**Relevance:** 8
**Novelty:** 7

---

## 39. [Gradient Descent Converges Linearly to Flatter Minima than Gradient Flow in Shallow Linear Networks](https://arxiv.org/abs/2501.09137) <a id="link39"></a>

**ArXiv ID:** 2501.09137

**Authors:** Pierfrancesco Beneventano, Blake Woodworth

**Abstract:** We study the gradient descent (GD) dynamics of a depth-2 linear neural network with a single input and output. We show that GD converges at an explicit linear rate to a global minimum of the training loss, even with a large stepsize -- about $2/\textrm{sharpness}$. It still converges for even larger stepsizes, but may do so very slowly. We also characterize the solution to which GD converges, which has lower norm and sharpness than the gradient flow solution. Our analysis reveals a trade off between the speed of convergence and the magnitude of implicit regularization. This sheds light on the benefits of training at the ``Edge of Stability'', which induces additional regularization by delaying convergence and may have implications for training more complex models.

**Comment:** The paper analyzes gradient descent dynamics and implicit regularization, which aligns with training dynamics in neural networks and representation learning.

**Relevance:** 8
**Novelty:** 7

---

## 40. [Towards Spectral Convergence of Locally Linear Embedding on Manifolds with Boundary](https://arxiv.org/abs/2501.09572) <a id="link40"></a>

**ArXiv ID:** 2501.09572

**Authors:** Andrew Lyons

**Abstract:** We study the eigenvalues and eigenfunctions of a differential operator that governs the asymptotic behavior of the unsupervised learning algorithm known as Locally Linear Embedding when a large data set is sampled from an interval or disc. In particular, the differential operator is of second order, mixed-type, and degenerates near the boundary. We show that a natural regularity condition on the eigenfunctions imposes a consistent boundary condition and use the Frobenius method to estimate pointwise behavior. We then determine the limiting sequence of eigenvalues analytically and compare them to numerical predictions. Finally, we propose a variational framework for determining eigenvalues on other compact manifolds.

**Comment:** The paper provides theoretical insights into Locally Linear Embedding on manifolds, which aligns with representation learning and foundational research on training dynamics.

**Relevance:** 8
**Novelty:** 7

---

## 41. [Free-Knots Kolmogorov-Arnold Network: On the Analysis of Spline Knots and Advancing Stability](https://arxiv.org/abs/2501.09283) <a id="link41"></a>

**ArXiv ID:** 2501.09283

**Authors:** Liangwewi Nathan Zheng, Wei Emma Zhang, Lin Yue, Miao Xu, Olaf Maennel, Weitong Chen

**Abstract:** Kolmogorov-Arnold Neural Networks (KANs) have gained significant attention in the machine learning community. However, their implementation often suffers from poor training stability and heavy trainable parameter. Furthermore, there is limited understanding of the behavior of the learned activation functions derived from B-splines. In this work, we analyze the behavior of KANs through the lens of spline knots and derive the lower and upper bound for the number of knots in B-spline-based KANs. To address existing limitations, we propose a novel Free Knots KAN that enhances the performance of the original KAN while reducing the number of trainable parameters to match the trainable parameter scale of standard Multi-Layer Perceptrons (MLPs). Additionally, we introduce new a training strategy to ensure $C^2$ continuity of the learnable spline, resulting in smoother activation compared to the original KAN and improve the training stability by range expansion. The proposed method is comprehensively evaluated on 8 datasets spanning various domains, including image, text, time series, multimodal, and function approximation tasks. The promising results demonstrates the feasibility of KAN-based network and the effectiveness of proposed method.

**Comment:** The paper proposes improvements to Kolmogorov-Arnold Networks, which aligns with model architecture innovations and foundational research.

**Relevance:** 8
**Novelty:** 7

---

## 42. [ARMAX identification of low rank graphical models](https://arxiv.org/abs/2501.09616) <a id="link42"></a>

**ArXiv ID:** 2501.09616

**Authors:** Wenqi Cao, Aming Li

**Abstract:** In large-scale systems, complex internal relationships are often present. Such interconnected systems can be effectively described by low rank stochastic processes. When identifying a predictive model of low rank processes from sampling data, the rank-deficient property of spectral densities is often obscured by the inevitable measurement noise in practice. However, existing low rank identification approaches often did not take noise into explicit consideration, leading to non-negligible inaccuracies even under weak noise. In this paper, we address the identification issue of low rank processes under measurement noise. We find that the noisy measurement model admits a sparse plus low rank structure in latent-variable graphical models. Specifically, we first decompose the problem into a maximum entropy covariance extension problem, and a low rank graphical estimation problem based on an autoregressive moving-average with exogenous input (ARMAX) model. To identify the ARMAX low rank graphical models, we propose an estimation approach based on maximum likelihood. The identifiability and consistency of this approach are proven under certain conditions. Simulation results confirm the reliable performance of the entire algorithm in both the parameter estimation and noisy data filtering.

**Comment:** The paper addresses low-rank graphical models and introduces a novel estimation approach, which aligns with the model compression topic through low-rank methods.

**Relevance:** 8
**Novelty:** 7

---

## 43. [ASCENT-ViT: Attention-based Scale-aware Concept Learning Framework for Enhanced Alignment in Vision Transformers](https://arxiv.org/abs/2501.09221) <a id="link43"></a>

**ArXiv ID:** 2501.09221

**Authors:** Sanchit Sinha, Guangzhi Xiong, Aidong Zhang

**Abstract:** As Vision Transformers (ViTs) are increasingly adopted in sensitive vision applications, there is a growing demand for improved interpretability. This has led to efforts to forward-align these models with carefully annotated abstract, human-understandable semantic entities - concepts. Concepts provide global rationales to the model predictions and can be quickly understood/intervened on by domain experts. Most current research focuses on designing model-agnostic, plug-and-play generic concept-based explainability modules that do not incorporate the inner workings of foundation models (e.g., inductive biases, scale invariance, etc.) during training. To alleviate this issue for ViTs, in this paper, we propose ASCENT-ViT, an attention-based, concept learning framework that effectively composes scale and position-aware representations from multiscale feature pyramids and ViT patch representations, respectively. Further, these representations are aligned with concept annotations through attention matrices - which incorporate spatial and global (semantic) concepts. ASCENT-ViT can be utilized as a classification head on top of standard ViT backbones for improved predictive performance and accurate and robust concept explanations as demonstrated on five datasets, including three widely used benchmarks (CUB, Pascal APY, Concept-MNIST) and 2 real-world datasets (AWA2, KITS).

**Comment:** The paper introduces a concept learning framework for Vision Transformers, which aligns with architectural innovations in transformers.

**Relevance:** 8
**Novelty:** 7

---

## 44. [Symmetry-Aware Generative Modeling through Learned Canonicalization](https://arxiv.org/abs/2501.07773) <a id="link44"></a>

**ArXiv ID:** 2501.07773

**Authors:** Kusha Sareen, Daniel Levy, Arnab Kumar Mondal, Sékou-Oumar Kaba, Tara Akhound-Sadegh, Siamak Ravanbakhsh

**Abstract:** Generative modeling of symmetric densities has a range of applications in AI for science, from drug discovery to physics simulations. The existing generative modeling paradigm for invariant densities combines an invariant prior with an equivariant generative process. However, we observe that this technique is not necessary and has several drawbacks resulting from the limitations of equivariant networks. Instead, we propose to model a learned slice of the density so that only one representative element per orbit is learned. To accomplish this, we learn a group-equivariant canonicalization network that maps training samples to a canonical pose and train a non-equivariant generative model over these canonicalized samples. We implement this idea in the context of diffusion models. Our preliminary experimental results on molecular modeling are promising, demonstrating improved sample quality and faster inference time.

**Comment:** The paper proposes a symmetry-aware generative modeling approach, which aligns with emerging trends in foundational AI research.

**Relevance:** 8
**Novelty:** 7

---

## 45. [Learnings from Scaling Visual Tokenizers for Reconstruction and Generation](https://arxiv.org/abs/2501.09755) <a id="link45"></a>

**ArXiv ID:** 2501.09755

**Authors:** Philippe Hansen-Estruch, David Yan, Ching-Yao Chung, Orr Zohar, Jialiang Wang, Tingbo Hou, Tao Xu, Sriram Vishwanath, Peter Vajda, Xinlei Chen

**Abstract:** Visual tokenization via auto-encoding empowers state-of-the-art image and video generative models by compressing pixels into a latent space. Although scaling Transformer-based generators has been central to recent advances, the tokenizer component itself is rarely scaled, leaving open questions about how auto-encoder design choices influence both its objective of reconstruction and downstream generative performance. Our work aims to conduct an exploration of scaling in auto-encoders to fill in this blank. To facilitate this exploration, we replace the typical convolutional backbone with an enhanced Vision Transformer architecture for Tokenization (ViTok). We train ViTok on large-scale image and video datasets far exceeding ImageNet-1K, removing data constraints on tokenizer scaling. We first study how scaling the auto-encoder bottleneck affects both reconstruction and generation -- and find that while it is highly correlated with reconstruction, its relationship with generation is more complex. We next explored the effect of separately scaling the auto-encoders' encoder and decoder on reconstruction and generation performance. Crucially, we find that scaling the encoder yields minimal gains for either reconstruction or generation, while scaling the decoder boosts reconstruction but the benefits for generation are mixed. Building on our exploration, we design ViTok as a lightweight auto-encoder that achieves competitive performance with state-of-the-art auto-encoders on ImageNet-1K and COCO reconstruction tasks (256p and 512p) while outperforming existing auto-encoders on 16-frame 128p video reconstruction for UCF-101, all with 2-5x fewer FLOPs. When integrated with Diffusion Transformers, ViTok demonstrates competitive performance on image generation for ImageNet-1K and sets new state-of-the-art benchmarks for class-conditional video generation on UCF-101.

**Comment:** The paper explores scaling visual tokenizers in autoencoders, which aligns with foundational research in representation learning and architectural design.

**Relevance:** 8
**Novelty:** 7

---

## 46. [Complexity Control Facilitates Reasoning-Based Compositional Generalization in Transformers](https://arxiv.org/abs/2501.08537) <a id="link46"></a>

**ArXiv ID:** 2501.08537

**Authors:** Zhongwang Zhang, Pengxiao Lin, Zhiwei Wang, Yaoyu Zhang, Zhi-Qin John Xu

**Abstract:** Transformers have demonstrated impressive capabilities across various tasks, yet their performance on compositional problems remains a subject of debate. In this study, we investigate the internal mechanisms underlying Transformers' behavior in compositional tasks. We find that complexity control strategies significantly influence whether the model learns primitive-level rules that generalize out-of-distribution (reasoning-based solutions) or relies solely on memorized mappings (memory-based solutions). By applying masking strategies to the model's information circuits and employing multiple complexity metrics, we reveal distinct internal working mechanisms associated with different solution types. Further analysis reveals that reasoning-based solutions exhibit a lower complexity bias, which aligns with the well-studied neuron condensation phenomenon. This lower complexity bias is hypothesized to be the key factor enabling these solutions to learn reasoning rules. We validate these conclusions across multiple real-world datasets, including image generation and natural language processing tasks, confirming the broad applicability of our findings.

**Comment:** The paper investigates complexity control in Transformers for compositional generalization, which aligns with foundational research in understanding model behavior.

**Relevance:** 8
**Novelty:** 7

---

## 47. [Beyond Reward Hacking: Causal Rewards for Large Language Model Alignment](https://arxiv.org/abs/2501.09620) <a id="link47"></a>

**ArXiv ID:** 2501.09620

**Authors:** Chaoqi Wang, Zhuokai Zhao, Yibo Jiang, Zhaorun Chen, Chen Zhu, Yuxin Chen, Jiayi Liu, Lizhu Zhang, Xiangjun Fan, Hao Ma, Sinong Wang

**Abstract:** Recent advances in large language models (LLMs) have demonstrated significant progress in performing complex tasks. While Reinforcement Learning from Human Feedback (RLHF) has been effective in aligning LLMs with human preferences, it is susceptible to spurious correlations in reward modeling. Consequently, it often introduces biases-such as length bias, sycophancy, conceptual bias, and discrimination that hinder the model's ability to capture true causal relationships. To address this, we propose a novel causal reward modeling approach that integrates causal inference to mitigate these spurious correlations. Our method enforces counterfactual invariance, ensuring reward predictions remain consistent when irrelevant variables are altered. Through experiments on both synthetic and real-world datasets, we show that our approach mitigates various types of spurious correlations effectively, resulting in more reliable and fair alignment of LLMs with human preferences. As a drop-in enhancement to the existing RLHF workflow, our causal reward modeling provides a practical way to improve the trustworthiness and fairness of LLM finetuning.

**Comment:** The paper proposes a causal reward modeling approach for LLM alignment, which is relevant to foundational research in LLM behavior and interpretability.

**Relevance:** 8
**Novelty:** 7

---

## 48. [Exploring Narrative Clustering in Large Language Models: A Layerwise Analysis of BERT](https://arxiv.org/abs/2501.08053) <a id="link48"></a>

**ArXiv ID:** 2501.08053

**Authors:** Awritrojit Banerjee, Achim Schilling, Patrick Krauss

**Abstract:** This study investigates the internal mechanisms of BERT, a transformer-based large language model, with a focus on its ability to cluster narrative content and authorial style across its layers. Using a dataset of narratives developed via GPT-4, featuring diverse semantic content and stylistic variations, we analyze BERT's layerwise activations to uncover patterns of localized neural processing. Through dimensionality reduction techniques such as Principal Component Analysis (PCA) and Multidimensional Scaling (MDS), we reveal that BERT exhibits strong clustering based on narrative content in its later layers, with progressively compact and distinct clusters. While strong stylistic clustering might occur when narratives are rephrased into different text types (e.g., fables, sci-fi, kids' stories), minimal clustering is observed for authorial style specific to individual writers. These findings highlight BERT's prioritization of semantic content over stylistic features, offering insights into its representational capabilities and processing hierarchy. This study contributes to understanding how transformer models like BERT encode linguistic information, paving the way for future interdisciplinary research in artificial intelligence and cognitive neuroscience.

**Comment:** The paper analyzes BERT's layerwise clustering capabilities, which aligns with representation learning and insights into existing architectures.

**Relevance:** 8
**Novelty:** 7

---

## 49. [D$^2$-DPM: Dual Denoising for Quantized Diffusion Probabilistic Models](https://arxiv.org/abs/2501.08180) <a id="link49"></a>

**ArXiv ID:** 2501.08180

**Authors:** Qian Zeng, Jie Song, Han Zheng, Hao Jiang, Mingli Song

**Abstract:** Diffusion models have achieved cutting-edge performance in image generation. However, their lengthy denoising process and computationally intensive score estimation network impede their scalability in low-latency and resource-constrained scenarios. Post-training quantization (PTQ) compresses and accelerates diffusion models without retraining, but it inevitably introduces additional quantization noise, resulting in mean and variance deviations. In this work, we propose D2-DPM, a dual denoising mechanism aimed at precisely mitigating the adverse effects of quantization noise on the noise estimation network. Specifically, we first unravel the impact of quantization noise on the sampling equation into two components: the mean deviation and the variance deviation. The mean deviation alters the drift coefficient of the sampling equation, influencing the trajectory trend, while the variance deviation magnifies the diffusion coefficient, impacting the convergence of the sampling trajectory. The proposed D2-DPM is thus devised to denoise the quantization noise at each time step, and then denoise the noisy sample through the inverse diffusion iterations. Experimental results demonstrate that D2-DPM achieves superior generation quality, yielding a 1.42 lower FID than the full-precision model while achieving 3.99x compression and 11.67x bit-operation acceleration.

**Comment:** The paper proposes a dual denoising mechanism for quantized diffusion models, which is relevant to model compression and efficiency. The focus on mitigating quantization noise adds novelty.

**Relevance:** 8
**Novelty:** 7

---

## 50. [Increasing Batch Size Improves Convergence of Stochastic Gradient Descent with Momentum](https://arxiv.org/abs/2501.08883) <a id="link50"></a>

**ArXiv ID:** 2501.08883

**Authors:** Keisuke Kamo, Hideaki Iiduka

**Abstract:** Stochastic gradient descent with momentum (SGDM), which is defined by adding a momentum term to SGD, has been well studied in both theory and practice. Theoretically investigated results showed that the settings of the learning rate and momentum weight affect the convergence of SGDM. Meanwhile, practical results showed that the setting of batch size strongly depends on the performance of SGDM. In this paper, we focus on mini-batch SGDM with constant learning rate and constant momentum weight, which is frequently used to train deep neural networks in practice. The contribution of this paper is showing theoretically that using a constant batch size does not always minimize the expectation of the full gradient norm of the empirical loss in training a deep neural network, whereas using an increasing batch size definitely minimizes it, that is, increasing batch size improves convergence of mini-batch SGDM. We also provide numerical results supporting our analyses, indicating specifically that mini-batch SGDM with an increasing batch size converges to stationary points faster than with a constant batch size. Python implementations of the optimizers used in the numerical experiments are available at https://anonymous.4open.science/r/momentum-increasing-batch-size-888C/.

**Comment:** The paper provides theoretical insights into the convergence of SGDM with increasing batch sizes, which aligns with training dynamics in neural networks under representation learning.

**Relevance:** 7
**Novelty:** 7

---

## 51. [A Closer Look at the Learnability of Out-of-Distribution (OOD) Detection](https://arxiv.org/abs/2501.08821) <a id="link51"></a>

**ArXiv ID:** 2501.08821

**Authors:** Konstantin Garov, Kamalika Chaudhuri

**Abstract:** Machine learning algorithms often encounter different or "out-of-distribution" (OOD) data at deployment time, and OOD detection is frequently employed to detect these examples. While it works reasonably well in practice, existing theoretical results on OOD detection are highly pessimistic. In this work, we take a closer look at this problem, and make a distinction between uniform and non-uniform learnability, following PAC learning theory. We characterize under what conditions OOD detection is uniformly and non-uniformly learnable, and we show that in several cases, non-uniform learnability turns a number of negative results into positive. In all cases where OOD detection is learnable, we provide concrete learning algorithms and a sample-complexity analysis.

**Comment:** The paper provides theoretical insights into the learnability of OOD detection, which aligns with representation learning and foundational research.

**Relevance:** 7
**Novelty:** 7

---

## 52. [An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures](https://arxiv.org/abs/2501.07930) <a id="link52"></a>

**ArXiv ID:** 2501.07930

**Authors:** Thibaut Boissin, Franck Mamalet, Thomas Fel, Agustin Martin Picard, Thomas Massena, Mathieu Serrurier

**Abstract:** Orthogonal convolutional layers are the workhorse of multiple areas in machine learning, such as adversarial robustness, normalizing flows, GANs, and Lipschitzconstrained models. Their ability to preserve norms and ensure stable gradient propagation makes them valuable for a large range of problems. Despite their promise, the deployment of orthogonal convolution in large-scale applications is a significant challenge due to computational overhead and limited support for modern features like strides, dilations, group convolutions, and transposed convolutions.In this paper, we introduce AOC (Adaptative Orthogonal Convolution), a scalable method for constructing orthogonal convolutions, effectively overcoming these limitations. This advancement unlocks the construction of architectures that were previously considered impractical. We demonstrate through our experiments that our method produces expressive models that become increasingly efficient as they scale. To foster further advancement, we provide an open-source library implementing this method, available at https://github.com/thib-s/orthogonium.

**Comment:** The paper proposes a scalable method for orthogonal convolutions, which could be relevant to model architecture innovations, particularly in efficient CNN design.

**Relevance:** 7
**Novelty:** 7

---

## 53. [Linearly Convergent Mixup Learning](https://arxiv.org/abs/2501.07794) <a id="link53"></a>

**ArXiv ID:** 2501.07794

**Authors:** Gakuto Obi, Ayato Saito, Yuto Sasaki, Tsuyoshi Kato

**Abstract:** Learning in the reproducing kernel Hilbert space (RKHS) such as the support vector machine has been recognized as a promising technique. It continues to be highly effective and competitive in numerous prediction tasks, particularly in settings where there is a shortage of training data or computational limitations exist. These methods are especially valued for their ability to work with small datasets and their interpretability. To address the issue of limited training data, mixup data augmentation, widely used in deep learning, has remained challenging to apply to learning in RKHS due to the generation of intermediate class labels. Although gradient descent methods handle these labels effectively, dual optimization approaches are typically not directly applicable. In this study, we present two novel algorithms that extend to a broader range of binary classification models. Unlike gradient-based approaches, our algorithms do not require hyperparameters like learning rates, simplifying their implementation and optimization. Both the number of iterations to converge and the computational cost per iteration scale linearly with respect to the dataset size. The numerical experiments demonstrate that our algorithms achieve faster convergence to the optimal solution compared to gradient descent approaches, and that mixup data augmentation consistently improves the predictive performance across various loss functions.

**Comment:** The paper proposes linearly convergent mixup learning algorithms, which aligns with foundational research in representation learning.

**Relevance:** 7
**Novelty:** 7

---

## 54. [Merging Models on the Fly Without Retraining: A Sequential Approach to Scalable Continual Model Merging](https://arxiv.org/abs/2501.09522) <a id="link54"></a>

**ArXiv ID:** 2501.09522

**Authors:** Anke Tang, Enneng Yang, Li Shen, Yong Luo, Han Hu, Bo Du, Dacheng Tao

**Abstract:** Deep model merging represents an emerging research direction that combines multiple fine-tuned models to harness their specialized capabilities across different tasks and domains. Current model merging techniques focus on merging all available models simultaneously, with weight interpolation-based methods being the predominant approaches. However, these conventional approaches are not well-suited for scenarios where models become available sequentially, and they often suffer from high memory requirements and potential interference between tasks. In this study, we propose a training-free projection-based continual merging method that processes models sequentially through orthogonal projections of weight matrices and adaptive scaling mechanisms. Our method operates by projecting new parameter updates onto subspaces orthogonal to existing merged parameter updates while using an adaptive scaling mechanism to maintain stable parameter distances, enabling efficient sequential integration of task-specific knowledge. Our approach maintains constant memory complexity to the number of models, minimizes interference between tasks through orthogonal projections, and retains the performance of previously merged models through adaptive task vector scaling. Extensive experiments on CLIP-ViT models demonstrate that our method achieves a 5-8% average accuracy improvement while maintaining robust performance in different task orderings.

**Comment:** The paper proposes a sequential model merging method, which aligns with model compression and efficiency but focuses on task-specific merging scenarios.

**Relevance:** 7
**Novelty:** 7

---

## 55. [Weight Averaging for Out-of-Distribution Generalization and Few-Shot Domain Adaptation](https://arxiv.org/abs/2501.08361) <a id="link55"></a>

**ArXiv ID:** 2501.08361

**Authors:** Shijian Xu

**Abstract:** Empirical risk minimization (ERM) is not robust to changes in the distribution of data. When the distribution of test data is different from that of training data, the problem is known as out-of-distribution generalization. Recently, two techniques have been developed for addressing out-of-distribution generalization in computer vision: weight averaging (WA) and sharpness-aware minimization (SAM). WA involves training multiple models with different hyperparameters and then averaging the weights of these models, which can significantly improve out-of-distribution generalization performance. SAM optimizes a neural network to find minima in flat regions, which have been proven to perform well under distribution shifts. While these techniques have made great progress, there is still room for improvement and further exploration. In this thesis, we propose increasing the model diversity in WA explicitly by introducing gradient similarity as a loss regularizer to further improve out-of-distribution generalization performance. We also propose combining WA and SAM to solve the problem of few-shot domain adaptation. Our extensive experiments on digits datasets (MNIST, SVHN, USPS, MNIST-M) and other domain adaptation datasets (VLCS, PACS) show that combining WA and SAM leads to improved out-of-distribution generalization performance and significantly increases few-shot domain adaptation accuracy.

**Comment:** The paper explores weight averaging and sharpness-aware minimization for OOD generalization and domain adaptation. While it provides incremental improvements, it lacks groundbreaking insights.

**Relevance:** 7
**Novelty:** 6

---

# Paper Selection Prompt

You are a helpful paper reading assistant whose job is to read daily posts from ArXiv and identify a few papers that your friend will enjoy reading.
Your job is to carefully read the paper titles and abstracts below and find the ones that match the criteria below.

## Instructions

Write the response in JSONL format with {ARXIVID, COMMENT, RELEVANCE, NOVELTY} on each line, one for each paper.

- ARXIVID: should be the ArXiv ID.
- COMMENT: should identify whether there is a criteria that match the paper very closely. These matches should not be based on general terms like "language modeling" or "advancements" and should specifically refer to a criterion. No need to mention the non-matching criteria.
- RELEVANCE: should be a score from 1-10.
- NOVELTY: should be a score from 1-10.

## Scoring Criteria

> The "Relevance" score measures how closely the paper aligns with the core topics of the prompt.
> The "Novelty" score assesses the originality and impact of the paper.
> They are two **ORTHONORMAL** axes and **SHOULD NOT** be confused with each other.

### Relevance Scoring

- Relevance 9-10 (Completely Relevant)
  - Focus: Fully aligned with core topics with no deviation, score the highest if contains relevant keywords in it.
  - Examples: Papers focused on foundational methods or theoretical research, whose titles contain topic keywords like "MoE".

- Relevance 7-8 (Relevant)
  - Focus: Retain a solid link to the main research area, though may touch on peripheral elements.
  - Examples: Papers research on the fundamental part of MoE through a less critical aspect like its behavior in GNN.

- Relevance 5-6 (Borderline)
  - Focus: Maintains a link to the core topic but also extends into at least one other domain/area beyond the primary focus.
  - Examples: Work referencing MoE centered on reinforcement learning.

- Relevance 3-4 (Irrelevant)
  - Focus: Largely outside our interests with no association to our topics.
  - Examples: Application-focused papers like using MoE to solve a problem in the real world.

- Relevance 1-2 (Ignore)
  - Focus: Purely unrelated to our topics. Completely a different domain.
  - **Exception**: If the paper hints at a cutting-edge, radically new direction that could eventually transform the primary domain, consider a score of 9–10 despite initial appearances. (Usually a very rare concept that belongs to the fundamental research)

### Novelty Scoring

- Novelty 9-10 (Breakthrough)
  - Definition: Groundbreaking methods/theory introducing new directions or solving major challenges.
  - Examples: Entirely new paradigm for foundational models; a novel theory transforming representation learning.

- Novelty 7-8 (Improvements)
  - Definition: Substantial insights/enhancements, though not a full paradigm shift.
  - Examples: Modifications on existing methods yielding significantly better results.

- Novelty 5-6 (Borderline)
  - Definition: Incremental contributions with possible long-term benefits, not immediately transformative.
  - Examples: Moderately novel extension to an existing architecture; refining current methods without fundamentally altering them.

- Novelty 3-4 (Tangential)
  - Definition: Minor or domain-specific improvements with limited broader impact.
  - Examples: Slight modifications to known methods with strange motivation; purely engineering jobs like a new benchmark/dataset.

- Novelty 1-2 (Low)
  - Definition: Minimal originality, applying standard approaches without real innovation.
  - Examples: Using an off-the-shelf model without adding new insights; purely application-driven studies like finetuning a pretrained model using existing methods.

## Papers

[PAPER LIST HERE]

## Relevant Topics

Use the following relevance criteria to focus on foundational research. Keep **relevant** papers and filter out **irrelevant** ones. Avoid purely **application-driven** work.

1. Representation Learning
   - Relevant: Insights into how deep networks encode information, feature/dictionary learning, sparse/contrastive methods, training dynamics in neural networks.
   - Irrelevant: Standard applications of known techniques lacking new theoretical or methodological contributions.

2. Model Architecture
   - Relevant: Mixture-of-Experts (MoE), Transformers, Conditional/Dynamic Networks, Autoencoders, analysis on existing architectures (like encoder-decoder), or other architectural innovations.
   - Irrelevant: Merely using existing architectures for a certain task without insights into the structure themselves.

3. Model Compression
   - Relevant: Sparsity, pruning, quantization, low-rank approaches, KV cache, or other algorithmic/theoretical efficiency breakthroughs.
   - Irrelevant: Straightforward applications of existing compression methods to new tasks.

4. Large Language Models (LLMs)
   - Relevant: Major breakthroughs in pretraining or architecture, theoretical insights into LLM behavior/interpretability.
   - Irrelevant: Domain-specific usage (e.g., translation, jail-breaking), finetuning or inference tricks (e.g., instruction tuning, chain-of-thoughts, data mixing), or empirical dataset/benchmark studies and text-level analysis (e.g. hallucination, reasoning, safety).

5. AI for Science
   - Relevant: Foundational research in molecular/protein modeling, new generative paradigms, or significant architecture-level innovations.
   - Irrelevant: Conventional, domain-specific applications without new theoretical perspectives.

6. Emerging Trends
   - Relevant: Cutting-edge theoretical work challenging established assumptions or introducing broad new paradigms.
   - Irrelevant: Incremental improvements or trend-following without novel insights.

**Keywords:**

- Relevant: Mixture of Experts (MoE), Representation Learning, Compression/Efficiency, Sparse/Sparsity, Pruning, Quantization, Low-rank, Foundation Model, etc.
- Irrelevant: Reinforcement Learning, Transfer Learning, Federated Learning, Online Learning, Diffusion Models, etc.
- Application: Image Segmentation, Medical Imaging, 3D Vision, Video Understanding, Information Retrieval, Summarization, Recommendation Systems, Machine Translation, Speech Recognition, Signal Processing, Spatial/Temporal Modeling, Time Series, Knowledge Graph, etc.