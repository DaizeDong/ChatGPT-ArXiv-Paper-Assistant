{
    "2508.11145": {
        "SCORE": 17,
        "ARXIVID": "2508.11145",
        "COMMENT": "The paper proposes a novel paradigm for Bayesian network classifiers by learning distributional representations, which aligns with representation learning. It introduces a new neural network architecture to capture high-order feature dependencies.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Huan Zhang",
            "Daokun Zhang",
            "Kexin Meng",
            "Geoffrey I. Webb"
        ],
        "title": "Towards the Next-generation Bayesian Network Classifiers",
        "abstract": "Bayesian network classifiers provide a feasible solution to tabular data classification, with a number of merits like high time and memory efficiency, and great explainability. However, due to the parameter explosion and data sparsity issues, Bayesian network classifiers are restricted to low-order feature dependency modeling, making them struggle in extrapolating the occurrence probabilities of complex real-world data. In this paper, we propose a novel paradigm to design high-order Bayesian network classifiers, by learning distributional representations for feature values, as what has been done in word embedding and graph representation learning. The learned distributional representations are encoded with the semantic relatedness between different features through their observed co-occurrence patterns in training data, which then serve as a hallmark to extrapolate the occurrence probabilities of new test samples. As a classifier design realization, we remake the K-dependence Bayesian classifier (KDB) by extending it into a neural version, i.e., NeuralKDB, where a novel neural network architecture is designed to learn distributional representations of feature values and parameterize the conditional probabilities between interdependent features. A stochastic gradient descent based algorithm is designed to train the NeuralKDB model efficiently. Extensive classification experiments on 60 UCI datasets demonstrate that the proposed NeuralKDB classifier excels in capturing high-order feature dependencies and significantly outperforms the conventional Bayesian network classifiers, as well as other competitive classifiers, including two neural network based classifiers without distributional representation learning.",
        "arxiv_id": "2508.11145"
    },
    "2508.10975": {
        "SCORE": 17,
        "ARXIVID": "2508.10975",
        "COMMENT": "The paper discusses synthetic data generation for LLM pretraining, providing insights into pretraining paradigms and data quality, which is relevant to foundational research in LLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Pratyush Maini",
            "Vineeth Dorna",
            "Parth Doshi",
            "Aldo Carranza",
            "Fan Pan",
            "Jack Urbanek",
            "Paul Burstein",
            "Alex Fang",
            "Alvin Deng",
            "Amro Abbas",
            "Brett Larsen",
            "Cody Blakeney",
            "Charvi Bannur",
            "Christina Baek",
            "Darren Teh",
            "David Schwab",
            "Haakon Mongstad",
            "Haoli Yin",
            "Josh Wills",
            "Kaleigh Mentzer",
            "Luke Merrick",
            "Ricardo Monti",
            "Rishabh Adiga",
            "Siddharth Joshi",
            "Spandan Das",
            "Zhengping Wang",
            "Bogdan Gaza",
            "Ari Morcos",
            "Matthew Leavitt"
        ],
        "title": "BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining",
        "abstract": "Recent advances in large language model (LLM) pretraining have shown that simply scaling data quantity eventually leads to diminishing returns, hitting a data wall. In response, the use of synthetic data for pretraining has emerged as a promising paradigm for pushing the frontier of performance. Despite this, the factors affecting synthetic data quality remain poorly understood. In this work, we introduce BeyondWeb, a synthetic data generation framework that produces high-quality synthetic data for pretraining. BeyondWeb significantly extends the capabilities of traditional web-scale datasets, outperforming state-of-the-art synthetic pretraining datasets such as Cosmopedia and Nemotron-CC's high-quality synthetic subset (Nemotron-Synth) by up to 5.1 percentage points (pp) and 2.6pp, respectively, when averaged across a suite of 14 benchmark evaluations. It delivers up to 7.7x faster training than open web data and 2.7x faster than Nemotron-Synth. Remarkably, a 3B model trained for 180B tokens on BeyondWeb outperforms an 8B model trained for the same token budget on Cosmopedia. We also present several insights from BeyondWeb on synthetic data for pretraining: what drives its benefits, which data to rephrase and how, and the impact of model size and family on data quality. Overall, our work shows that there's no silver bullet for generating high-quality synthetic pretraining data. The best outcomes require jointly optimizing many factors, a challenging task that requires rigorous science and practical expertise. Naive approaches can yield modest improvements, potentially at great cost, while well-executed methods can yield transformative improvements, as exemplified by BeyondWeb.",
        "arxiv_id": "2508.10975"
    },
    "2508.11112": {
        "SCORE": 17,
        "ARXIVID": "2508.11112",
        "COMMENT": "The paper discusses quantization through piecewise-affine regularization, which is relevant to model compression techniques.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Jianhao Ma",
            "Lin Xiao"
        ],
        "title": "Quantization through Piecewise-Affine Regularization: Optimization and Statistical Guarantees",
        "abstract": "Optimization problems over discrete or quantized variables are very challenging in general due to the combinatorial nature of their search space. Piecewise-affine regularization (PAR) provides a flexible modeling and computational framework for quantization based on continuous optimization. In this work, we focus on the setting of supervised learning and investigate the theoretical foundations of PAR from optimization and statistical perspectives. First, we show that in the overparameterized regime, where the number of parameters exceeds the number of samples, every critical point of the PAR-regularized loss function exhibits a high degree of quantization. Second, we derive closed-form proximal mappings for various (convex, quasi-convex, and non-convex) PARs and show how to solve PAR-regularized problems using the proximal gradient method, its accelerated variant, and the Alternating Direction Method of Multipliers. Third, we study statistical guarantees of PAR-regularized linear regression problems; specifically, we can approximate classical formulations of $\\ell_1$-, squared $\\ell_2$-, and nonconvex regularizations using PAR and obtain similar statistical guarantees with quantized solutions.",
        "arxiv_id": "2508.11112"
    },
    "2508.11441": {
        "SCORE": 17,
        "ARXIVID": "2508.11441",
        "COMMENT": "The paper provides a theoretical framework for understanding the informativeness of post-hoc explanations, which aligns with foundational research in representation learning by challenging assumptions about model interpretability.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Eric G\\\"unther",
            "Bal\\'azs Szabados",
            "Robi Bhattacharjee",
            "Sebastian Bordt",
            "Ulrike von Luxburg"
        ],
        "title": "Informative Post-Hoc Explanations Only Exist for Simple Functions",
        "abstract": "Many researchers have suggested that local post-hoc explanation algorithms can be used to gain insights into the behavior of complex machine learning models. However, theoretical guarantees about such algorithms only exist for simple decision functions, and it is unclear whether and under which assumptions similar results might exist for complex models. In this paper, we introduce a general, learning-theory-based framework for what it means for an explanation to provide information about a decision function. We call an explanation informative if it serves to reduce the complexity of the space of plausible decision functions. With this approach, we show that many popular explanation algorithms are not informative when applied to complex decision functions, providing a rigorous mathematical rejection of the idea that it should be possible to explain any model. We then derive conditions under which different explanation algorithms become informative. These are often stronger than what one might expect. For example, gradient explanations and counterfactual explanations are non-informative with respect to the space of differentiable functions, and SHAP and anchor explanations are not informative with respect to the space of decision trees. Based on these results, we discuss how explanation algorithms can be modified to become informative. While the proposed analysis of explanation algorithms is mathematical, we argue that it holds strong implications for the practical applicability of these algorithms, particularly for auditing, regulation, and high-risk applications of AI.",
        "arxiv_id": "2508.11441"
    },
    "2508.11190": {
        "SCORE": 17,
        "ARXIVID": "2508.11190",
        "COMMENT": "The paper introduces a hybrid quantum-classical architecture for deep learning, which is a significant architectural innovation. It leverages quantum computing to improve the expressiveness of deep generative models, aligning with the model architecture criterion.",
        "RELEVANCE": 8,
        "NOVELTY": 9,
        "authors": [
            "Feng-ao Wang",
            "Shaobo Chen",
            "Yao Xuan",
            "Junwei Liu",
            "Qi Gao",
            "Hongdong Zhu",
            "Junjie Hou",
            "Lixin Yuan",
            "Jinyu Cheng",
            "Chenxin Yi",
            "Hai Wei",
            "Yin Ma",
            "Tao Xu",
            "Kai Wen",
            "Yixue Li"
        ],
        "title": "Quantum-Boosted High-Fidelity Deep Learning",
        "abstract": "A fundamental limitation of probabilistic deep learning is its predominant reliance on Gaussian priors. This simplistic assumption prevents models from accurately capturing the complex, non-Gaussian landscapes of natural data, particularly in demanding domains like complex biological data, severely hindering the fidelity of the model for scientific discovery. The physically-grounded Boltzmann distribution offers a more expressive alternative, but it is computationally intractable on classical computers. To date, quantum approaches have been hampered by the insufficient qubit scale and operational stability required for the iterative demands of deep learning. Here, we bridge this gap by introducing the Quantum Boltzmann Machine-Variational Autoencoder (QBM-VAE), a large-scale and long-time stable hybrid quantum-classical architecture. Our framework leverages a quantum processor for efficient sampling from the Boltzmann distribution, enabling its use as a powerful prior within a deep generative model. Applied to million-scale single-cell datasets from multiple sources, the QBM-VAE generates a latent space that better preserves complex biological structures, consistently outperforming conventional Gaussian-based deep learning models like VAE and SCVI in essential tasks such as omics data integration, cell-type classification, and trajectory inference. It also provides a typical example of introducing a physics priori into deep learning to drive the model to acquire scientific discovery capabilities that breaks through data limitations. This work provides the demonstration of a practical quantum advantage in deep learning on a large-scale scientific problem and offers a transferable blueprint for developing hybrid quantum AI models.",
        "arxiv_id": "2508.11190"
    },
    "2508.11307": {
        "SCORE": 16,
        "ARXIVID": "2508.11307",
        "COMMENT": "The paper explores sparse regression techniques with orthogonal polynomials, which aligns with representation learning through sparse methods.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Sabin Roman",
            "Gregor Skok",
            "Ljupco Todorovski",
            "Saso Dzeroski"
        ],
        "title": "Approximating the universal thermal climate index using sparse regression with orthogonal polynomials",
        "abstract": "This article explores novel data-driven modeling approaches for analyzing and approximating the Universal Thermal Climate Index (UTCI), a physiologically-based metric integrating multiple atmospheric variables to assess thermal comfort. Given the nonlinear, multivariate structure of UTCI, we investigate symbolic and sparse regression techniques as tools for interpretable and efficient function approximation. In particular, we highlight the benefits of using orthogonal polynomial bases-such as Legendre polynomials-in sparse regression frameworks, demonstrating their advantages in stability, convergence, and hierarchical interpretability compared to standard polynomial expansions. We demonstrate that our models achieve significantly lower root-mean squared losses than the widely used sixth-degree polynomial benchmark-while using the same or fewer parameters. By leveraging Legendre polynomial bases, we construct models that efficiently populate a Pareto front of accuracy versus complexity and exhibit stable, hierarchical coefficient structures across varying model capacities. Training on just 20% of the data, our models generalize robustly to the remaining 80%, with consistent performance under bootstrapping. The decomposition effectively approximates the UTCI as a Fourier-like expansion in an orthogonal basis, yielding results near the theoretical optimum in the L2 (least squares) sense. We also connect these findings to the broader context of equation discovery in environmental modeling, referencing probabilistic grammar-based methods that enforce domain consistency and compactness in symbolic expressions. Taken together, these results illustrate how combining sparsity, orthogonality, and symbolic structure enables robust, interpretable modeling of complex environmental indices like UTCI - and significantly outperforms the state-of-the-art approximation in both accuracy and efficiency.",
        "arxiv_id": "2508.11307"
    },
    "2508.11513": {
        "SCORE": 15,
        "ARXIVID": "2508.11513",
        "COMMENT": "The paper presents a novel self-explainable GNN framework for class-level explanations, which aligns with the core topic of model architecture analysis and interpretability in graph neural networks.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Fanzhen Liu",
            "Xiaoxiao Ma",
            "Jian Yang",
            "Alsharif Abuadbba",
            "Kristen Moore",
            "Surya Nepal",
            "Cecile Paris",
            "Quan Z. Sheng",
            "Jia Wu"
        ],
        "title": "Towards Faithful Class-level Self-explainability in Graph Neural Networks by Subgraph Dependencies",
        "abstract": "Enhancing the interpretability of graph neural networks (GNNs) is crucial to ensure their safe and fair deployment. Recent work has introduced self-explainable GNNs that generate explanations as part of training, improving both faithfulness and efficiency. Some of these models, such as ProtGNN and PGIB, learn class-specific prototypes, offering a potential pathway toward class-level explanations. However, their evaluations focus solely on instance-level explanations, leaving open the question of whether these prototypes meaningfully generalize across instances of the same class. In this paper, we introduce GraphOracle, a novel self-explainable GNN framework designed to generate and evaluate class-level explanations for GNNs. Our model jointly learns a GNN classifier and a set of structured, sparse subgraphs that are discriminative for each class. We propose a novel integrated training that captures graph$\\unicode{x2013}$subgraph$\\unicode{x2013}$prediction dependencies efficiently and faithfully, validated through a masking-based evaluation strategy. This strategy enables us to retroactively assess whether prior methods like ProtGNN and PGIB deliver effective class-level explanations. Our results show that they do not. In contrast, GraphOracle achieves superior fidelity, explainability, and scalability across a range of graph classification tasks. We further demonstrate that GraphOracle avoids the computational bottlenecks of previous methods$\\unicode{x2014}$like Monte Carlo Tree Search$\\unicode{x2014}$by using entropy-regularized subgraph selection and lightweight random walk extraction, enabling faster and more scalable training. These findings position GraphOracle as a practical and principled solution for faithful class-level self-explainability in GNNs.",
        "arxiv_id": "2508.11513"
    },
    "2508.11017": {
        "SCORE": 15,
        "ARXIVID": "2508.11017",
        "COMMENT": "The paper provides insights into the pre-training dynamics of LLMs, focusing on cross-lingual knowledge transfer, which is relevant to understanding LLM behavior.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Carter Blum",
            "Katja Filipova",
            "Ann Yuan",
            "Asma Ghandeharioun",
            "Julian Zimmert",
            "Fred Zhang",
            "Jessica Hoffmann",
            "Tal Linzen",
            "Martin Wattenberg",
            "Lucas Dixon",
            "Mor Geva"
        ],
        "title": "Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics",
        "abstract": "Large language models (LLMs) struggle with cross-lingual knowledge transfer: they hallucinate when asked in one language about facts expressed in a different language during training. This work introduces a controlled setting to study the causes and dynamics of this phenomenon by training small Transformer models from scratch on synthetic multilingual datasets. We identify a learning phase wherein a model develops either separate or unified representations of the same facts across languages, and show that unification is essential for cross-lingual transfer. We also show that the degree of unification depends on mutual information between facts and training data language, and on how easy it is to extract that language. Based on these insights, we develop methods to modulate the level of cross-lingual transfer by manipulating data distribution and tokenization, and we introduce metrics and visualizations to formally characterize their effects on unification. Our work shows how controlled settings can shed light on pre-training dynamics and suggests new directions for improving cross-lingual transfer in LLMs.",
        "arxiv_id": "2508.11017"
    },
    "2508.11348": {
        "SCORE": 15,
        "ARXIVID": "2508.11348",
        "COMMENT": "The paper introduces a neuron-level modularizing-while-training approach for DNNs, which is relevant to model architecture and representation learning by proposing a new method for modularization applicable to Transformers and CNNs.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Xiaohan Bi",
            "Binhang Qi",
            "Hailong Sun",
            "Xiang Gao",
            "Yue Yu",
            "Xiaojun Liang"
        ],
        "title": "NeMo: A Neuron-Level Modularizing-While-Training Approach for Decomposing DNN Models",
        "abstract": "With the growing incorporation of deep neural network (DNN) models into modern software systems, the prohibitive construction costs have become a significant challenge. Model reuse has been widely applied to reduce training costs, but indiscriminately reusing entire models may incur significant inference overhead. Consequently, DNN modularization has gained attention, enabling module reuse by decomposing DNN models. The emerging modularizing-while-training (MwT) paradigm, which incorporates modularization into training, outperforms modularizing-after-training approaches. However, existing MwT methods focus on small-scale CNN models at the convolutional kernel level and struggle with diverse DNNs and large-scale models, particularly Transformer-based models. To address these limitations, we propose NeMo, a scalable and generalizable MwT approach. NeMo operates at the neuron level fundamental component common to all DNNs-ensuring applicability to Transformers and various architectures. We design a contrastive learning-based modular training method with an effective composite loss function, enabling scalability to large-scale models. Comprehensive experiments on two Transformer-based models and four CNN models across two classification datasets demonstrate NeMo's superiority over state-of-the-art MwT methods. Results show average gains of 1.72% in module classification accuracy and 58.10% reduction in module size, demonstrating efficacy across both CNN and large-scale Transformer-based models. A case study on open-source projects shows NeMo's potential benefits in practical scenarios, offering a promising approach for scalable and generalizable DNN modularization.",
        "arxiv_id": "2508.11348"
    },
    "2508.11214": {
        "SCORE": 15,
        "ARXIVID": "2508.11214",
        "COMMENT": "The paper discusses causal abstraction in computational explanations, which relates to foundational research in representation learning and theoretical insights into model behavior.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Atticus Geiger",
            "Jacqueline Harding",
            "Thomas Icard"
        ],
        "title": "How Causal Abstraction Underpins Computational Explanation",
        "abstract": "Explanations of cognitive behavior often appeal to computations over representations. What does it take for a system to implement a given computation over suitable representational vehicles within that system? We argue that the language of causality -- and specifically the theory of causal abstraction -- provides a fruitful lens on this topic. Drawing on current discussions in deep learning with artificial neural networks, we illustrate how classical themes in the philosophy of computation and cognition resurface in contemporary machine learning. We offer an account of computational implementation grounded in causal abstraction, and examine the role for representation in the resulting picture. We argue that these issues are most profitably explored in connection with generalization and prediction.",
        "arxiv_id": "2508.11214"
    }
}