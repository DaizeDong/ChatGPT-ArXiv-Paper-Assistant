# Personalized Daily Arxiv Papers 01/21/2025
Total cost: $0.297825

Total relevant papers: 2

Paper selection prompt and criteria at the bottom

Table of contents with paper titles:

0. [Evolving Deeper LLM Thinking](#user-content-link0)
**Authors:** Kuang-Huei Lee, Ian Fischer, Yueh-Hua Wu, Dave Marwood, Shumeet Baluja, Dale Schuurmans, Xinyun Chen

1. [Topology-Driven Attribute Recovery for Attribute Missing Graph Learning in Social Internet of Things](#user-content-link1)
**Authors:** Mengran Li, Junzhou Chen, Chenyun Yu, Guanying Jiang, Ronghui Zhang, Yanming Shen, Houbing Herbert Song

---
## 0. [Evolving Deeper LLM Thinking](https://arxiv.org/abs/2501.09891) <a id="link0"></a>
**ArXiv ID:** 2501.09891
**Authors:** Kuang-Huei Lee, Ian Fischer, Yueh-Hua Wu, Dave Marwood, Shumeet Baluja, Dale Schuurmans, Xinyun Chen

**Abstract:** arXiv:2501.09891v1 Announce Type: new  Abstract: We explore an evolutionary search strategy for scaling inference time compute in Large Language Models. The proposed approach, Mind Evolution, uses a language model to generate, recombine and refine candidate responses. The proposed approach avoids the need to formalize the underlying inference problem whenever a solution evaluator is available. Controlling for inference cost, we find that Mind Evolution significantly outperforms other inference strategies such as Best-of-N and Sequential Revision in natural language planning tasks. In the TravelPlanner and Natural Plan benchmarks, Mind Evolution solves more than 98% of the problem instances using Gemini 1.5 Pro without the use of a formal solver.

**Comment:** The paper introduces Mind Evolution, a novel evolutionary search strategy for scaling inference in LLMs. This directly addresses scalability challenges in model inference and represents potential advancements for efficient LLM usage.
**Relevance:** 9
**Novelty:** 8

---

## 1. [Topology-Driven Attribute Recovery for Attribute Missing Graph Learning in Social Internet of Things](https://arxiv.org/abs/2501.10151) <a id="link1"></a>
**ArXiv ID:** 2501.10151
**Authors:** Mengran Li, Junzhou Chen, Chenyun Yu, Guanying Jiang, Ronghui Zhang, Yanming Shen, Houbing Herbert Song

**Abstract:** arXiv:2501.10151v1 Announce Type: new  Abstract: With the advancement of information technology, the Social Internet of Things (SIoT) has fostered the integration of physical devices and social networks, deepening the study of complex interaction patterns. Text Attribute Graphs (TAGs) capture both topological structures and semantic attributes, enhancing the analysis of complex interactions within the SIoT. However, existing graph learning methods are typically designed for complete attributed graphs, and the common issue of missing attributes in Attribute Missing Graphs (AMGs) increases the difficulty of analysis tasks. To address this, we propose the Topology-Driven Attribute Recovery (TDAR) framework, which leverages topological data for AMG learning. TDAR introduces an improved pre-filling method for initial attribute recovery using native graph topology. Additionally, it dynamically adjusts propagation weights and incorporates homogeneity strategies within the embedding space to suit AMGs' unique topological structures, effectively reducing noise during information propagation. Extensive experiments on public datasets demonstrate that TDAR significantly outperforms state-of-the-art methods in attribute reconstruction and downstream tasks, offering a robust solution to the challenges posed by AMGs. The code is available at https://github.com/limengran98/TDAR.

**Comment:** The proposed Topology-Driven Attribute Recovery (TDAR) leverages graph topology for incomplete attribute recovery in graph learning, aligning with representation learning. However, its contribution is more applied and lacks broader theoretical impact.
**Relevance:** 7
**Novelty:** 6

---


---

# Paper selection prompt
You are a helpful paper reading assistant whose job is to read daily posts from ArXiv and identify a few papers that your friend will enjoy reading.
Your job is to carefully read the paper titles and abstracts below and find the ones that match the criteria below.

## Relevant Topics

1. Representation Learning
   - Relevant: Feature learning, sparse/contrastive learning, dictionary learning, or theoretical insights into how deep networks encode information.
   - Irrelevant: Application-only work using standard representation learning without innovative insights.

2. Model Architecture
   - Relevant: Mixture-of-Experts (MoE), Transformers, Conditional/Dynamic Networks, Autoencoders, and other foundational structures.
   - Irrelevant: Simply applying existing architectures to new tasks without structural/theoretical innovation.

3. Model Compression
   - Relevant: Sparsity, pruning, quantization, low-rank, KV cache, or theoretical/algorithmic innovations for efficiency, etc.
   - Irrelevant: Simply applying existing compression to new tasks.

4. Large Language Models (LLMs)
   - Relevant: Strong theoretical insights on LLM behavior, architecture/training breakthroughs (e.g., MoE).
   - Irrelevant: Domain-specific usage or small tweaks (e.g., instruction tuning), lack of theoretical advancement (e.g., benchmarks/datasets, inference tricks like RAG).

5. AI for Science
   - Relevant: Foundational research in molecule/protein modeling (e.g., new training paradigms, advanced generative methods, or theoretical perspectives), or major architecture-level innovation.
   - Irrelevant: Conventional, domain-limited applications lacking insights on the foundational side.

6. Emerging Trends
   - Relevant: Cutting-edge theoretical work challenging assumptions, or broad new paradigms/concepts in AI research.
   - Irrelevant: Trend-following or incremental extensions on existing methods.

## Papers

[PAPER LIST HERE]

## Scoring Criteria

> The "Relevance" score measures how closely the paper aligns with the core topics of the prompt.
> The "Novelty" score assesses the originality and impact of the paper.
> They are two **ORTHONORMAL** axes and **SHOULD NOT** be confused with each other. E.g., a paper with high relevance can be of low novelty, or vice versa.

### Relevance Scoring

- Relevance 9-10 (Completely Relevant)
  - Focus: Fully aligned with core topics, score the highest if also contains keywords in it.
  - Keywords: “Mixture of Experts (MoE),” “Representation Learning,” “Compression,” “Sparse,” “Pruning,” “Quantization,” “Low-rank,” “Theoretical,” “Scalability,” “Foundation Models,” etc.
  - Examples: Papers focused on foundational methods or theoretical research, whose titles contain topic keywords (like "MoE").

- Relevance 7-8 (Relevant)
  - Focus: Clearly tied to our main topics, may not fully hit the interest in foundational methods.
  - Examples: Pure research on representation/architecture with no other domain focus; significant overlap with MoE.

- Relevance 5-6 (Optional)
  - Focus: Link to our topics—covers relevant ideas but also includes another area of interest.
  - Examples: Work referencing MoE in a broader context or centered on another domain like federated learning, online learning, etc.

- Relevance 3-4 (Irrelevant)
  - Focus: Largely outside our interests, with little/no association to our topics.
  - Examples: application-level tasks like using MoE as a method for medical image segmentation, etc.

- Relevance 1-2 (Ignore)
  - Focus: Purely unrelated to our topics.
  - Examples: Entirely different fields like reinforcement learning, 3D vision learning, etc.

### Novelty Scoring

> *Note: Foundation vs. Application*
> - Foundational/theoretical papers (new theorems, architectures, or strong methodological insights) are of **high novelty**.
> - Subdomain papers  and application-focused papers (e.g., "methods for xxx") are **lower** in novelty.

- Novelty 9-10 (Breakthrough)
  - Definition: Groundbreaking methods/theory introducing new directions or solving major challenges.
  - Examples: Entirely new paradigm for MoE routing; a novel theoretical result transforming representation learning.

- Novelty 7-8 (Improvements)
  - Definition: Substantial insights/enhancements, though not a full paradigm shift.
  - Examples: Modifications on existing methods yielding significantly better results.

- Novelty 5-6 (Moderate)
  - Definition: Incremental contributions with possible long-term benefits, not immediately transformative.
  - Examples: Moderately novel extension to an existing architecture; refining current methods without fundamentally altering them.

- Novelty 3-4 (Tangential)
  - Definition: Minor or domain-specific improvements with limited broader impact.
  - Examples: Slight modifications to known methods that don’t change the broader landscape (e.g., a standard LLM fine-tuned on a new dataset).

- Novelty 1-2 (Low)
  - Definition: Minimal originality, applying standard approaches without real innovation.
  - Examples: Using an off-the-shelf model without adding new insights; purely application-driven studies with no methodological advancement.

## Instructions

Write the response in JSONL format with {ARXIVID, COMMENT, RELEVANCE, NOVELTY} on each line, one for each paper.

- ARXIVID: should be the ArXiv ID.
- COMMENT: should identify whether there is a criteria that match the paper very closely. These matches should not be based on general terms like "language modeling" or "advancements" and should specifically refer to a criterion. No need to mention the non-matching criteria.
- RELEVANCE: should be a score from 1-10.
- NOVELTY: should be a score from 1-10.