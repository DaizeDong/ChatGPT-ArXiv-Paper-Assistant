{
    "2509.03733": {
        "SCORE": 17,
        "ARXIVID": "2509.03733",
        "COMMENT": "The paper introduces a differentiable estimator of range-partition entropy and applies it to neural networks, contributing to representation learning and model efficiency.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Ibne Farabi Shihab",
            "Sanjeda Akter",
            "Anuj Sharma"
        ],
        "title": "Differentiable Entropy Regularization for Geometry and Neural Networks",
        "abstract": "We introduce a differentiable estimator of range-partition entropy, a recent concept from computational geometry that enables algorithms to adapt to the \"sortedness\" of their input. While range-partition entropy provides strong guarantees in algorithm design, it has not yet been made accessible to deep learning. In this work, we (i) propose the first differentiable approximation of range-partition entropy, enabling its use as a trainable loss or regularizer; (ii) design EntropyNet, a neural module that restructures data into low-entropy forms to accelerate downstream instance-optimal algorithms; and (iii) extend this principle beyond geometry by applying entropy regularization directly to Transformer attention. Across tasks, we demonstrate that differentiable entropy improves efficiency without degrading correctness: in geometry, our method achieves up to $4.1\\times$ runtime speedups with negligible error ($<0.2%$); in deep learning, it induces structured attention patterns that yield 6% higher accuracy at 80% sparsity compared to L1 baselines. Our theoretical analysis provides approximation bounds for the estimator, and extensive ablations validate design choices. These results suggest that entropy-bounded computation is not only theoretically elegant but also a practical mechanism for adaptive learning, efficiency, and structured representation.",
        "arxiv_id": "2509.03733"
    },
    "2509.04419": {
        "SCORE": 17,
        "ARXIVID": "2509.04419",
        "COMMENT": "The paper provides a unified theoretical framework for post-training LLMs, which is a significant contribution to foundational research on LLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Xingtai Lv",
            "Yuxin Zuo",
            "Youbang Sun",
            "Hongyi Liu",
            "Yuntian Wei",
            "Zhekai Chen",
            "Lixuan He",
            "Xuekai Zhu",
            "Kaiyan Zhang",
            "Bingning Wang",
            "Ning Ding",
            "Bowen Zhou"
        ],
        "title": "Towards a Unified View of Large Language Model Post-Training",
        "abstract": "Two major sources of training data exist for post-training modern language models: online (model-generated rollouts) data, and offline (human or other-model demonstrations) data. These two types of data are typically used by approaches like Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT), respectively. In this paper, we show that these approaches are not in contradiction, but are instances of a single optimization process. We derive a Unified Policy Gradient Estimator, and present the calculations of a wide spectrum of post-training approaches as the gradient of a common objective under different data distribution assumptions and various bias-variance tradeoffs. The gradient estimator is constructed with four interchangeable parts: stabilization mask, reference policy denominator, advantage estimate, and likelihood gradient. Motivated by our theoretical findings, we propose Hybrid Post-Training (HPT), an algorithm that dynamically selects different training signals. HPT is designed to yield both effective exploitation of demonstration and stable exploration without sacrificing learned reasoning patterns. We provide extensive experiments and ablation studies to verify the effectiveness of our unified theoretical framework and HPT. Across six mathematical reasoning benchmarks and two out-of-distribution suites, HPT consistently surpasses strong baselines across models of varying scales and families.",
        "arxiv_id": "2509.04419"
    },
    "2509.03780": {
        "SCORE": 17,
        "ARXIVID": "2509.03780",
        "COMMENT": "The paper discusses conditions under which latent variables can be translated between different generative models, contributing to foundational research in representation learning.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "John Wentworth",
            "David Lorell"
        ],
        "title": "Natural Latents: Latent Variables Stable Across Ontologies",
        "abstract": "Suppose two Bayesian agents each learn a generative model of the same environment. We will assume the two have converged on the predictive distribution, i.e. distribution over some observables in the environment, but may have different generative models containing different latent variables. Under what conditions can one agent guarantee that their latents are a function of the other agents latents?   We give simple conditions under which such translation is guaranteed to be possible: the natural latent conditions. We also show that, absent further constraints, these are the most general conditions under which translatability is guaranteed. Crucially for practical application, our theorems are robust to approximation error in the natural latent conditions.",
        "arxiv_id": "2509.03780"
    },
    "2509.03738": {
        "SCORE": 17,
        "ARXIVID": "2509.03738",
        "COMMENT": "The paper extends sparse autoencoders to function spaces, contributing to foundational research in representation learning and model recovery.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Bahareh Tolooshams",
            "Ailsa Shen",
            "Anima Anandkumar"
        ],
        "title": "Sparse Autoencoder Neural Operators: Model Recovery in Function Spaces",
        "abstract": "We frame the problem of unifying representations in neural models as one of sparse model recovery and introduce a framework that extends sparse autoencoders (SAEs) to lifted spaces and infinite-dimensional function spaces, enabling mechanistic interpretability of large neural operators (NO). While the Platonic Representation Hypothesis suggests that neural networks converge to similar representations across architectures, the representational properties of neural operators remain underexplored despite their growing importance in scientific computing. We compare the inference and training dynamics of SAEs, lifted-SAE, and SAE neural operators. We highlight how lifting and operator modules introduce beneficial inductive biases, enabling faster recovery, improved recovery of smooth concepts, and robust inference across varying resolutions, a property unique to neural operators.",
        "arxiv_id": "2509.03738"
    },
    "2509.03540": {
        "SCORE": 17,
        "ARXIVID": "2509.03540",
        "COMMENT": "The paper introduces a novel framework for improving factuality in LLMs by constructing knowledge graphs during inference, which aligns with foundational research in LLM behavior and interpretability.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Shanglin Wu",
            "Lihui Liu",
            "Jinho D. Choi",
            "Kai Shu"
        ],
        "title": "Improving Factuality in LLMs via Inference-Time Knowledge Graph Construction",
        "abstract": "Large Language Models (LLMs) often struggle with producing factually consistent answers due to limitations in their parametric memory. Retrieval-Augmented Generation (RAG) methods address this issue by incorporating external knowledge from trusted sources at inference time. However, such methods typically treat knowledge as unstructured text, which limits their ability to support compositional reasoning and identify factual inconsistencies. To overcome these limitations, we propose a novel framework that dynamically constructs and expands knowledge graphs (KGs) during inference, integrating both internal knowledge extracted from LLMs and external information retrieved from external sources. Our method begins by extracting a seed KG from the question via prompting, followed by iterative expansion using the LLM's latent knowledge. The graph is then selectively refined through external retrieval, enhancing factual coverage and correcting inaccuracies. We evaluate our approach on three diverse factual QA benchmarks, demonstrating consistent improvements in factual accuracy, answer precision, and interpretability over baseline prompting and static KG-augmented methods. Our findings suggest that inference-time KG construction is a promising direction for enhancing LLM factuality in a structured, interpretable, and scalable manner.",
        "arxiv_id": "2509.03540"
    },
    "2509.04377": {
        "SCORE": 17,
        "ARXIVID": "2509.04377",
        "COMMENT": "The paper introduces PagedEviction, a novel KV cache pruning strategy for LLM inference, which aligns with the model compression criterion focusing on efficiency breakthroughs.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Krishna Teja Chitty-Venkata",
            "Jie Ye",
            "Xian-He Sun",
            "Anthony Kougkas",
            "Murali Emani",
            "Venkatram Vishwanath",
            "Bogdan Nicolae"
        ],
        "title": "PagedEviction: Structured Block-wise KV Cache Pruning for Efficient Large Language Model Inference",
        "abstract": "KV caching significantly improves the efficiency of Large Language Model (LLM) inference by storing attention states from previously processed tokens, enabling faster generation of subsequent tokens. However, as sequence length increases, the KV cache quickly becomes a major memory bottleneck. To address this, we propose PagedEviction, a novel fine-grained, structured KV cache pruning strategy that enhances the memory efficiency of vLLM's PagedAttention. Unlike existing approaches that rely on attention-based token importance or evict tokens across different vLLM pages, PagedEviction introduces an efficient block-wise eviction algorithm tailored for paged memory layouts. Our method integrates seamlessly with PagedAttention without requiring any modifications to its CUDA attention kernels. We evaluate PagedEviction across Llama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models on the LongBench benchmark suite, demonstrating improved memory usage with better accuracy than baselines on long context tasks.",
        "arxiv_id": "2509.04377"
    },
    "2509.04398": {
        "SCORE": 17,
        "ARXIVID": "2509.04398",
        "COMMENT": "The paper proposes a new framework for efficient foundation model adaptation, which aligns with model compression and efficiency improvements.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Yuan Yin",
            "Shashanka Venkataramanan",
            "Tuan-Hung Vu",
            "Andrei Bursuc",
            "Matthieu Cord"
        ],
        "title": "IPA: An Information-Preserving Input Projection Framework for Efficient Foundation Model Adaptation",
        "abstract": "Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, reduce adaptation cost by injecting low-rank updates into pretrained weights. However, LoRA's down-projection is randomly initialized and data-agnostic, discarding potentially useful information. Prior analyses show that this projection changes little during training, while the up-projection carries most of the adaptation, making the random input compression a performance bottleneck. We propose IPA, a feature-aware projection framework that explicitly preserves information in the reduced hidden space. In the linear case, we instantiate IPA with algorithms approximating top principal components, enabling efficient projector pretraining with negligible inference overhead. Across language and vision benchmarks, IPA consistently improves over LoRA and DoRA, achieving on average 1.5 points higher accuracy on commonsense reasoning and 2.3 points on VTAB-1k, while matching full LoRA performance with roughly half the trainable parameters when the projection is frozen.",
        "arxiv_id": "2509.04398"
    },
    "2509.04226": {
        "SCORE": 17,
        "ARXIVID": "2509.04226",
        "COMMENT": "The paper provides a theoretical analysis of long-range dependency in SSM and transformer models, which is relevant to model architecture and offers insights into the behavior of these models.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Cong Ma",
            "Kayvan Najarian"
        ],
        "title": "Rethinking the long-range dependency in Mamba/SSM and transformer models",
        "abstract": "Long-range dependency is one of the most desired properties of recent sequence models such as state-space models (particularly Mamba) and transformer models. New model architectures are being actively developed and benchmarked for prediction tasks requiring long-range dependency. However, the capability of modeling long-range dependencies of these models has not been investigated from a theoretical perspective, which hinders a systematic improvement on this aspect. In this work, we mathematically define long-range dependency using the derivative of hidden states with respect to past inputs and compare the capability of SSM and transformer models of modeling long-range dependency based on this definition. We showed that the long-range dependency of SSM decays exponentially with the sequence length, which aligns with the exponential decay of memory function in RNN. But the attention mechanism used in transformers is more flexible and is not constrained to exponential decay, which could in theory perform better at modeling long-range dependency with sufficient training data, computing resources, and proper training. To combine the flexibility of long-range dependency of attention mechanism and computation efficiency of SSM, we propose a new formulation for hidden state update in SSM and prove its stability under a standard Gaussian distribution of the input data.",
        "arxiv_id": "2509.04226"
    },
    "2509.03730": {
        "SCORE": 16,
        "ARXIVID": "2509.03730",
        "COMMENT": "The paper provides insights into LLM behavior and interpretability, which aligns with the foundational research on LLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Pengrui Han",
            "Rafal Kocielnik",
            "Peiyang Song",
            "Ramit Debnath",
            "Dean Mobbs",
            "Anima Anandkumar",
            "R. Michael Alvarez"
        ],
        "title": "The Personality Illusion: Revealing Dissociation Between Self-Reports & Behavior in LLMs",
        "abstract": "Personality traits have long been studied as predictors of human behavior.Recent advances in Large Language Models (LLMs) suggest similar patterns may emerge in artificial systems, with advanced LLMs displaying consistent behavioral tendencies resembling human traits like agreeableness and self-regulation. Understanding these patterns is crucial, yet prior work primarily relied on simplified self-reports and heuristic prompting, with little behavioral validation. In this study, we systematically characterize LLM personality across three dimensions: (1) the dynamic emergence and evolution of trait profiles throughout training stages; (2) the predictive validity of self-reported traits in behavioral tasks; and (3) the impact of targeted interventions, such as persona injection, on both self-reports and behavior. Our findings reveal that instructional alignment (e.g., RLHF, instruction tuning) significantly stabilizes trait expression and strengthens trait correlations in ways that mirror human data. However, these self-reported traits do not reliably predict behavior, and observed associations often diverge from human patterns. While persona injection successfully steers self-reports in the intended direction, it exerts little or inconsistent effect on actual behavior. By distinguishing surface-level trait expression from behavioral consistency, our findings challenge assumptions about LLM personality and underscore the need for deeper evaluation in alignment and interpretability.",
        "arxiv_id": "2509.03730"
    },
    "2509.04126": {
        "SCORE": 16,
        "ARXIVID": "2509.04126",
        "COMMENT": "The paper proposes a Multi-Expert Planning and Generation Framework, which involves mixture-of-experts, relevant to model architecture.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Yuan Zhao",
            "Liu Lin"
        ],
        "title": "MEPG:Multi-Expert Planning and Generation for Compositionally-Rich Image Generation",
        "abstract": "Text-to-image diffusion models have achieved remarkable image quality, but they still struggle with complex, multiele ment prompts, and limited stylistic diversity. To address these limitations, we propose a Multi-Expert Planning and Gen eration Framework (MEPG) that synergistically integrates position- and style-aware large language models (LLMs) with spatial-semantic expert modules. The framework comprises two core components: (1) a Position-Style-Aware (PSA) module that utilizes a supervised fine-tuned LLM to decom pose input prompts into precise spatial coordinates and style encoded semantic instructions; and (2) a Multi-Expert Dif fusion (MED) module that implements cross-region genera tion through dynamic expert routing across both local regions and global areas. During the generation process for each lo cal region, specialized models (e.g., realism experts, styliza tion specialists) are selectively activated for each spatial par tition via attention-based gating mechanisms. The architec ture supports lightweight integration and replacement of ex pert models, providing strong extensibility. Additionally, an interactive interface enables real-time spatial layout editing and per-region style selection from a portfolio of experts. Ex periments show that MEPG significantly outperforms base line models with the same backbone in both image quality   and style diversity.",
        "arxiv_id": "2509.04126"
    },
    "2509.04394": {
        "SCORE": 16,
        "ARXIVID": "2509.04394",
        "COMMENT": "The paper introduces a novel generative paradigm, Transition Models, which is a significant contribution to generative modeling.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Zidong Wang",
            "Yiyuan Zhang",
            "Xiaoyu Yue",
            "Xiangyu Yue",
            "Yangguang Li",
            "Wanli Ouyang",
            "Lei Bai"
        ],
        "title": "Transition Models: Rethinking the Generative Learning Objective",
        "abstract": "A fundamental dilemma in generative modeling persists: iterative diffusion models achieve outstanding fidelity, but at a significant computational cost, while efficient few-step alternatives are constrained by a hard quality ceiling. This conflict between generation steps and output quality arises from restrictive training objectives that focus exclusively on either infinitesimal dynamics (PF-ODEs) or direct endpoint prediction. We address this challenge by introducing an exact, continuous-time dynamics equation that analytically defines state transitions across any finite time interval. This leads to a novel generative paradigm, Transition Models (TiM), which adapt to arbitrary-step transitions, seamlessly traversing the generative trajectory from single leaps to fine-grained refinement with more steps. Despite having only 865M parameters, TiM achieves state-of-the-art performance, surpassing leading models such as SD3.5 (8B parameters) and FLUX.1 (12B parameters) across all evaluated step counts. Importantly, unlike previous few-step generators, TiM demonstrates monotonic quality improvement as the sampling budget increases. Additionally, when employing our native-resolution strategy, TiM delivers exceptional fidelity at resolutions up to 4096x4096.",
        "arxiv_id": "2509.04394"
    },
    "2509.03652": {
        "SCORE": 15,
        "ARXIVID": "2509.03652",
        "COMMENT": "The paper explores the relationship between nonnegative matrix factorization and the principle of the common cause, contributing to representation learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "E. Khalafyan",
            "A. E. Allahverdyan",
            "A. Hovhannisyan"
        ],
        "title": "Nonnegative matrix factorization and the principle of the common cause",
        "abstract": "Nonnegative matrix factorization (NMF) is a known unsupervised data-reduction method. The principle of the common cause (PCC) is a basic methodological approach in probabilistic causality, which seeks an independent mixture model for the joint probability of two dependent random variables. It turns out that these two concepts are closely related. This relationship is explored reciprocally for several datasets of gray-scale images, which are conveniently mapped into probability models. On one hand, PCC provides a predictability tool that leads to a robust estimation of the effective rank of NMF. Unlike other estimates (e.g., those based on the Bayesian Information Criteria), our estimate of the rank is stable against weak noise. We show that NMF implemented around this rank produces features (basis images) that are also stable against noise and against seeds of local optimization, thereby effectively resolving the NMF nonidentifiability problem. On the other hand, NMF provides an interesting possibility of implementing PCC in an approximate way, where larger and positively correlated joint probabilities tend to be explained better via the independent mixture model. We work out a clustering method, where data points with the same common cause are grouped into the same cluster. We also show how NMF can be employed for data denoising.",
        "arxiv_id": "2509.03652"
    },
    "2509.03647": {
        "SCORE": 15,
        "ARXIVID": "2509.03647",
        "COMMENT": "The paper addresses self-preference bias in LLM evaluators, which is relevant to LLM behavior and interpretability.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Dani Roytburg",
            "Matthew Bozoukov",
            "Matthew Nguyen",
            "Jou Barzdukas",
            "Simon Fu",
            "Narmeen Oozeer"
        ],
        "title": "Breaking the Mirror: Activation-Based Mitigation of Self-Preference in LLM Evaluators",
        "abstract": "Large language models (LLMs) increasingly serve as automated evaluators, yet they suffer from \"self-preference bias\": a tendency to favor their own outputs over those of other models. This bias undermines fairness and reliability in evaluation pipelines, particularly for tasks like preference tuning and model routing. We investigate whether lightweight steering vectors can mitigate this problem at inference time without retraining. We introduce a curated dataset that distinguishes self-preference bias into justified examples of self-preference and unjustified examples of self-preference, and we construct steering vectors using two methods: Contrastive Activation Addition (CAA) and an optimization-based approach. Our results show that steering vectors can reduce unjustified self-preference bias by up to 97\\%, substantially outperforming prompting and direct preference optimization baselines. Yet steering vectors are unstable on legitimate self-preference and unbiased agreement, implying self-preference spans multiple or nonlinear directions. This underscores both their promise and limits as safeguards for LLM-as-judges and motivates more robust interventions.",
        "arxiv_id": "2509.03647"
    },
    "2509.04009": {
        "SCORE": 15,
        "ARXIVID": "2509.04009",
        "COMMENT": "The paper focuses on detecting spurious correlations in vision transformers, which aligns with the interest in understanding model architectures and their training dynamics.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Solha Kang",
            "Esla Timothy Anzaku",
            "Wesley De Neve",
            "Arnout Van Messem",
            "Joris Vankerschaver",
            "Francois Rameau",
            "Utku Ozbulak"
        ],
        "title": "Detecting Regional Spurious Correlations in Vision Transformers via Token Discarding",
        "abstract": "Due to their powerful feature association capabilities, neural network-based computer vision models have the ability to detect and exploit unintended patterns within the data, potentially leading to correct predictions based on incorrect or unintended but statistically relevant signals. These clues may vary from simple color aberrations to small texts within the image. In situations where these unintended signals align with the predictive task, models can mistakenly link these features with the task and rely on them for making predictions. This phenomenon is referred to as spurious correlations, where patterns appear to be associated with the task but are actually coincidental. As a result, detection and mitigation of spurious correlations have become crucial tasks for building trustworthy, reliable, and generalizable machine learning models. In this work, we present a novel method to detect spurious correlations in vision transformers, a type of neural network architecture that gained significant popularity in recent years. Using both supervised and self-supervised trained models, we present large-scale experiments on the ImageNet dataset demonstrating the ability of the proposed method to identify spurious correlations. We also find that, even if the same architecture is used, the training methodology has a significant impact on the model's reliance on spurious correlations. Furthermore, we show that certain classes in the ImageNet dataset contain spurious signals that are easily detected by the models and discuss the underlying reasons for those spurious signals. In light of our findings, we provide an exhaustive list of the aforementioned images and call for caution in their use in future research efforts. Lastly, we present a case study investigating spurious signals in invasive breast mass classification, grounding our work in real-world scenarios.",
        "arxiv_id": "2509.04009"
    },
    "2509.04083": {
        "SCORE": 15,
        "ARXIVID": "2509.04083",
        "COMMENT": "The paper explores the impact of formal language choice on neurosymbolic reasoning with LLMs, contributing to foundational insights into LLM behavior.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Alexander Beiser",
            "David Penz",
            "Nysret Musliu"
        ],
        "title": "Intermediate Languages Matter: Formal Languages and LLMs affect Neurosymbolic Reasoning",
        "abstract": "Large language models (LLMs) achieve astonishing results on a wide range of tasks. However, their formal reasoning ability still lags behind. A promising approach is Neurosymbolic LLM reasoning. It works by using LLMs as translators from natural to formal languages and symbolic solvers for deriving correct results. Still, the contributing factors to the success of Neurosymbolic LLM reasoning remain unclear. This paper demonstrates that one previously overlooked factor is the choice of the formal language. We introduce the intermediate language challenge: selecting a suitable formal language for neurosymbolic reasoning. By comparing four formal languages across three datasets and seven LLMs, we show that the choice of formal language affects both syntactic and semantic reasoning capabilities. We also discuss the varying effects across different LLMs.",
        "arxiv_id": "2509.04083"
    },
    "2509.03695": {
        "SCORE": 15,
        "ARXIVID": "2509.03695",
        "COMMENT": "The paper discusses hierarchical federated foundation models with a focus on mixture-of-experts (MoE) and architectural innovations, which aligns with the model architecture criterion.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Payam Abdisarabshali",
            "Fardis Nadimi",
            "Kasra Borazjani",
            "Naji Khosravan",
            "Minghui Liwang",
            "Wei Ni",
            "Dusit Niyato",
            "Michael Langberg",
            "Seyyedali Hosseinalipour"
        ],
        "title": "Hierarchical Federated Foundation Models over Wireless Networks for Multi-Modal Multi-Task Intelligence: Integration of Edge Learning with D2D/P2P-Enabled Fog Learning Architectures",
        "abstract": "The rise of foundation models (FMs) has reshaped the landscape of machine learning. As these models continued to grow, leveraging geo-distributed data from wireless devices has become increasingly critical, giving rise to federated foundation models (FFMs). More recently, FMs have evolved into multi-modal multi-task (M3T) FMs (e.g., GPT-4) capable of processing diverse modalities across multiple tasks, which motivates a new underexplored paradigm: M3T FFMs. In this paper, we unveil an unexplored variation of M3T FFMs by proposing hierarchical federated foundation models (HF-FMs), which in turn expose two overlooked heterogeneity dimensions to fog/edge networks that have a direct impact on these emerging models: (i) heterogeneity in collected modalities and (ii) heterogeneity in executed tasks across fog/edge nodes. HF-FMs strategically align the modular structure of M3T FMs, comprising modality encoders, prompts, mixture-of-experts (MoEs), adapters, and task heads, with the hierarchical nature of fog/edge infrastructures. Moreover, HF-FMs enable the optional usage of device-to-device (D2D) communications, enabling horizontal module relaying and localized cooperative training among nodes when feasible. Through delving into the architectural design of HF-FMs, we highlight their unique capabilities along with a series of tailored future research directions. Finally, to demonstrate their potential, we prototype HF-FMs in a wireless network setting and release the open-source code for the development of HF-FMs with the goal of fostering exploration in this untapped field (GitHub: https://github.com/payamsiabd/M3T-FFM).",
        "arxiv_id": "2509.03695"
    },
    "2509.04415": {
        "SCORE": 15,
        "ARXIVID": "2509.04415",
        "COMMENT": "The paper presents a framework for interpretable clustering with causal structure learning, which is relevant to representation learning and emerging trends in foundational research.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Wenrui Li",
            "Qinghao Zhang",
            "Xiaowo Wang"
        ],
        "title": "Interpretable Clustering with Adaptive Heterogeneous Causal Structure Learning in Mixed Observational Data",
        "abstract": "Understanding causal heterogeneity is essential for scientific discovery in domains such as biology and medicine. However, existing methods lack causal awareness, with insufficient modeling of heterogeneity, confounding, and observational constraints, leading to poor interpretability and difficulty distinguishing true causal heterogeneity from spurious associations. We propose an unsupervised framework, HCL (Interpretable Causal Mechanism-Aware Clustering with Adaptive Heterogeneous Causal Structure Learning), that jointly infers latent clusters and their associated causal structures from mixed-type observational data without requiring temporal ordering, environment labels, interventions or other prior knowledge. HCL relaxes the homogeneity and sufficiency assumptions by introducing an equivalent representation that encodes both structural heterogeneity and confounding. It further develops a bi-directional iterative strategy to alternately refine causal clustering and structure learning, along with a self-supervised regularization that balance cross-cluster universality and specificity. Together, these components enable convergence toward interpretable, heterogeneous causal patterns. Theoretically, we show identifiability of heterogeneous causal structures under mild conditions. Empirically, HCL achieves superior performance in both clustering and structure learning tasks, and recovers biologically meaningful mechanisms in real-world single-cell perturbation data, demonstrating its utility for discovering interpretable, mechanism-level causal heterogeneity.",
        "arxiv_id": "2509.04415"
    },
    "2509.04439": {
        "SCORE": 15,
        "ARXIVID": "2509.04439",
        "COMMENT": "The paper introduces a novel approach to LLM memory management, which aligns with the interest in theoretical insights into LLM behavior.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Matthew Ho",
            "Chen Si",
            "Zhaoxiang Feng",
            "Fangxu Yu",
            "Zhijian Liu",
            "Zhiting Hu",
            "Lianhui Qin"
        ],
        "title": "ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory",
        "abstract": "While inference-time scaling enables LLMs to carry out increasingly long and capable reasoning traces, the patterns and insights uncovered during these traces are immediately discarded once the context window is reset for a new query. External memory is a natural way to persist these discoveries, and recent work has shown clear benefits for reasoning-intensive tasks. We see an opportunity to make such memories more broadly reusable and scalable by moving beyond instance-based memory entries (e.g. exact query/response pairs, or summaries tightly coupled with the original problem context) toward concept-level memory: reusable, modular abstractions distilled from solution traces and stored in natural language. For future queries, relevant concepts are selectively retrieved and integrated into the prompt, enabling test-time continual learning without weight updates. Our design introduces new strategies for abstracting takeaways from rollouts and retrieving entries for new queries, promoting reuse and allowing memory to expand with additional experiences. On the challenging ARC-AGI benchmark, our method yields a 7.5% relative gain over a strong no-memory baseline with performance continuing to scale with inference compute. We find abstract concepts to be the most consistent memory design, outscoring the baseline at all tested inference compute scales. Moreover, we confirm that dynamically updating memory during test-time outperforms an otherwise identical fixed memory setting with additional attempts, supporting the hypothesis that solving more problems and abstracting more patterns to memory enables further solutions in a form of self-improvement. Code available at https://github.com/matt-seb-ho/arc_memo.",
        "arxiv_id": "2509.04439"
    },
    "2509.04442": {
        "SCORE": 15,
        "ARXIVID": "2509.04442",
        "COMMENT": "The paper introduces Delta Activations, a method for representing finetuned LLMs, which is relevant to representation learning and insights into LLM behavior.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Zhiqiu Xu",
            "Amish Sethi",
            "Mayur Naik",
            "Ser-Nam Lim"
        ],
        "title": "Delta Activations: A Representation for Finetuned Large Language Models",
        "abstract": "The success of powerful open source Large Language Models (LLMs) has enabled the community to create a vast collection of post-trained models adapted to specific tasks and domains. However, navigating and understanding these models remains challenging due to inconsistent metadata and unstructured repositories. We introduce Delta Activations, a method to represent finetuned models as vector embeddings by measuring shifts in their internal activations relative to a base model. This representation allows for effective clustering by domain and task, revealing structure in the model landscape. Delta Activations also demonstrate desirable properties: it is robust across finetuning settings and exhibits an additive property when finetuning datasets are mixed. In addition, we show that Delta Activations can embed tasks via few-shot finetuning, and further explore its use for model selection and merging. We hope Delta Activations can facilitate the practice of reusing publicly available models. Code is available at https://github.com/OscarXZQ/delta_activations.",
        "arxiv_id": "2509.04442"
    },
    "2509.03918": {
        "SCORE": 15,
        "ARXIVID": "2509.03918",
        "COMMENT": "The paper proposes a novel thought structure for enhancing reasoning in LLMs, which aligns with the core topic of large language models and their theoretical insights.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Fengxiao Tang",
            "Yufeng Li",
            "Zongzong Wu",
            "Ming Zhao"
        ],
        "title": "MTQA:Matrix of Thought for Enhanced Reasoning in Complex Question Answering",
        "abstract": "Complex Question Answering (QA) is a fundamental and challenging task in NLP. While large language models (LLMs) exhibit impressive performance in QA, they suffer from significant performance degradation when facing complex and abstract QA tasks due to insufficient reasoning capabilities. Works such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT) aim to enhance LLMs' reasoning abilities, but they face issues such as in-layer redundancy in tree structures and single paths in chain structures. Although some studies utilize Retrieval-Augmented Generation (RAG) methods to assist LLMs in reasoning, the challenge of effectively utilizing large amounts of information involving multiple entities and hops remains critical. To address this, we propose the Matrix of Thought (MoT), a novel and efficient LLM thought structure. MoT explores the problem in both horizontal and vertical dimensions through the \"column-cell communication\" mechanism, enabling LLMs to actively engage in multi-strategy and deep-level thinking, reducing redundancy within the column cells and enhancing reasoning capabilities. Furthermore, we develop a fact-correction mechanism by constructing knowledge units from retrieved knowledge graph triples and raw text to enhance the initial knowledge for LLM reasoning and correct erroneous answers. This leads to the development of an efficient and accurate QA framework (MTQA). Experimental results show that our framework outperforms state-of-the-art methods on four widely-used datasets in terms of F1 and EM scores, with reasoning time only 14.4\\% of the baseline methods, demonstrating both its efficiency and accuracy. The code for this framework is available at https://github.com/lyfiter/mtqa.",
        "arxiv_id": "2509.03918"
    },
    "2509.03643": {
        "SCORE": 15,
        "ARXIVID": "2509.03643",
        "COMMENT": "The paper presents CEHR-GPT, a foundation model for EHRs, focusing on feature representation and temporal reasoning, which aligns with foundational research in representation learning and model architecture.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Chao Pang",
            "Jiheum Park",
            "Xinzhuo Jiang",
            "Nishanth Parameshwar Pavinkurve",
            "Krishna S. Kalluri",
            "Shalmali Joshi",
            "No\\'emie Elhadad",
            "Karthik Natarajan"
        ],
        "title": "CEHR-GPT: A Scalable Multi-Task Foundation Model for Electronic Health Records",
        "abstract": "Electronic Health Records (EHRs) provide a rich, longitudinal view of patient health and hold significant potential for advancing clinical decision support, risk prediction, and data-driven healthcare research. However, most artificial intelligence (AI) models for EHRs are designed for narrow, single-purpose tasks, limiting their generalizability and utility in real-world settings. Here, we present CEHR-GPT, a general-purpose foundation model for EHR data that unifies three essential capabilities - feature representation, zero-shot prediction, and synthetic data generation - within a single architecture. To support temporal reasoning over clinical sequences, \\cehrgpt{} incorporates a novel time-token-based learning framework that explicitly encodes patients' dynamic timelines into the model structure. CEHR-GPT demonstrates strong performance across all three tasks and generalizes effectively to external datasets through vocabulary expansion and fine-tuning. Its versatility enables rapid model development, cohort discovery, and patient outcome forecasting without the need for task-specific retraining.",
        "arxiv_id": "2509.03643"
    }
}