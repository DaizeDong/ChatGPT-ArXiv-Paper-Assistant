[SELECTION]
# ==== author matching ====
# this will take a lot of time...
run_author_match = false
# the score assigned to papers filtered by author matching, this influences the final ranking of papers (20 means always rank top)
author_match_score = 20

# ==== gpt matching ====
run_openai = true
run_title_filter = true
run_abstract_filter = true
# use gpt-4o if you are using the api key from `GitHub Copilot Free plan`
model = gpt-4o

# number of calls to gpt per minute (-1 denotes no limit)
# use 10 if you are using the api key from `GitHub Copilot Free plan`
limit_per_minute = 10

# cost quality tradeoff - larger batches are cheaper but less accurate.
title_batch_size = 8
abstract_batch_size = 4
# whether to adjust the batch size according to the number of papers.
# If true, the batch size will scale logarithmically with the number of papers by `adaptive_threshold`.
# For example, if `batch_size=1` and `adaptive_threshold=10`, the real batch size will be: 1 for <=10 papers, 2 for 11-20 papers, 3 for 21-40 papers, 4 for 41-80 papers, etc.
adaptive_batch_size = true
adaptive_threshold = 32

# number of retries for papers failed to be filtered/selected by gpt
title_retry = 3
abstract_retry = 3

[FILTERING]
# https://arxiv.org/category_taxonomy
# e.g., cs.LG,cs.AI
arxiv_category =
# type of papers on arxiv to preserve: new, cross, replace, replace-cross. Details in https://info.arxiv.org/help/rss_specifications.html
announce_type = new,cross
# force_primary ignores papers that are only cross-listed into the arxiv_category
force_primary = false
# Filter out any papers that have no authors with h-index above `h_cutoff`
h_cutoff = 0
relevance_cutoff = 6
novelty_cutoff = 6

[OUTPUT]
debug_messages = false
output_path = out/
dump_debug_file = false
dump_json = true
dump_md = true
push_to_slack = false