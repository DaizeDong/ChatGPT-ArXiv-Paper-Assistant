{
    "2507.02550": {
        "SCORE": 17,
        "ARXIVID": "2507.02550",
        "COMMENT": "The paper discusses the role of compositional sparsity in deep learning, which aligns with the representation learning criterion, focusing on how deep networks encode information.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "David A. Danhofer",
            "Davide D'Ascenzo",
            "Rafael Dubach",
            "Tomaso Poggio"
        ],
        "title": "Position: A Theory of Deep Learning Must Include Compositional Sparsity",
        "abstract": "Overparametrized Deep Neural Networks (DNNs) have demonstrated remarkable success in a wide variety of domains too high-dimensional for classical shallow networks subject to the curse of dimensionality. However, open questions about fundamental principles, that govern the learning dynamics of DNNs, remain. In this position paper we argue that it is the ability of DNNs to exploit the compositionally sparse structure of the target function driving their success. As such, DNNs can leverage the property that most practically relevant functions can be composed from a small set of constituent functions, each of which relies only on a low-dimensional subset of all inputs. We show that this property is shared by all efficiently Turing-computable functions and is therefore highly likely present in all current learning problems. While some promising theoretical insights on questions concerned with approximation and generalization exist in the setting of compositionally sparse functions, several important questions on the learnability and optimization of DNNs remain. Completing the picture of the role of compositional sparsity in deep learning is essential to a comprehensive theory of artificial, and even general, intelligence.",
        "arxiv_id": "2507.02550"
    },
    "2507.02754": {
        "SCORE": 17,
        "ARXIVID": "2507.02754",
        "COMMENT": "The paper investigates the use of 2-simplicial Transformer, aligning with the model architecture criterion by exploring a novel attention mechanism.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Aurko Roy",
            "Timothy Chou",
            "Sai Surya Duvvuri",
            "Sijia Chen",
            "Jiecao Yu",
            "Xiaodong Wang",
            "Manzil Zaheer",
            "Rohan Anil"
        ],
        "title": "Fast and Simplex: 2-Simplicial Attention in Triton",
        "abstract": "Recent work has shown that training loss scales as a power law with both model size and the number of tokens, and that achieving compute-optimal models requires scaling model size and token count together. However, these scaling laws assume an infinite supply of data and apply primarily in compute-bound settings. As modern large language models increasingly rely on massive internet-scale datasets, the assumption that they are compute-bound is becoming less valid. This shift highlights the need for architectures that prioritize token efficiency.   In this work, we investigate the use of the 2-simplicial Transformer, an architecture that generalizes standard dot-product attention to trilinear functions through an efficient Triton kernel implementation. We demonstrate that the 2-simplicial Transformer achieves better token efficiency than standard Transformers: for a fixed token budget, similarly sized models outperform their dot-product counterparts on tasks involving mathematics, coding, reasoning, and logic. We quantify these gains by demonstrating that $2$-simplicial attention changes the exponent in the scaling laws for knowledge and reasoning tasks compared to dot product attention.",
        "arxiv_id": "2507.02754"
    },
    "2507.02760": {
        "SCORE": 17,
        "ARXIVID": "2507.02760",
        "COMMENT": "The paper introduces Knowledge Protocol Engineering, a new paradigm for AI in domain-specific knowledge work, which could be considered an emerging trend with potential foundational impact.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Guangwei Zhang"
        ],
        "title": "Knowledge Protocol Engineering: A New Paradigm for AI in Domain-Specific Knowledge Work",
        "abstract": "The capabilities of Large Language Models (LLMs) have opened new frontiers for interacting with complex, domain-specific knowledge. However, prevailing methods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic AI, while powerful, often struggle with tasks that demand deep, procedural, and methodological reasoning inherent to expert domains. RAG provides factual context but fails to convey logical frameworks; autonomous agents can be inefficient and unpredictable without domain-specific heuristics. To bridge this gap, we introduce Knowledge Protocol Engineering (KPE), a new paradigm focused on systematically translating human expert knowledge, often expressed in natural language documents, into a machine-executable Knowledge Protocol (KP). KPE shifts the focus from merely augmenting LLMs with fragmented information to endowing them with a domain's intrinsic logic, operational strategies, and methodological principles. We argue that a well-engineered Knowledge Protocol allows a generalist LLM to function as a specialist, capable of decomposing abstract queries and executing complex, multi-step tasks. This position paper defines the core principles of KPE, differentiates it from related concepts, and illustrates its potential applicability across diverse fields such as law and bioinformatics, positing it as a foundational methodology for the future of human-AI collaboration.",
        "arxiv_id": "2507.02760"
    },
    "2507.02377": {
        "SCORE": 17,
        "ARXIVID": "2507.02377",
        "COMMENT": "The paper discusses structured approximations in sparse Gaussian processes, which is relevant to representation learning and model compression through sparsity and efficiency improvements.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Thang D. Bui",
            "Michalis K. Titsias"
        ],
        "title": "Sparse Gaussian Processes: Structured Approximations and Power-EP Revisited",
        "abstract": "Inducing-point-based sparse variational Gaussian processes have become the standard workhorse for scaling up GP models. Recent advances show that these methods can be improved by introducing a diagonal scaling matrix to the conditional posterior density given the inducing points. This paper first considers an extension that employs a block-diagonal structure for the scaling matrix, provably tightening the variational lower bound. We then revisit the unifying framework of sparse GPs based on Power Expectation Propagation (PEP) and show that it can leverage and benefit from the new structured approximate posteriors. Through extensive regression experiments, we show that the proposed block-diagonal approximation consistently performs similarly to or better than existing diagonal approximations while maintaining comparable computational costs. Furthermore, the new PEP framework with structured posteriors provides competitive performance across various power hyperparameter settings, offering practitioners flexible alternatives to standard variational approaches.",
        "arxiv_id": "2507.02377"
    },
    "2507.02436": {
        "SCORE": 17,
        "ARXIVID": "2507.02436",
        "COMMENT": "The paper introduces a Bayesian transformer-based foundation model for metamaterials, aligning with the AI for Science criterion by offering a new generative paradigm for material design.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Namjung Kim",
            "Dongseok Lee",
            "Jongbin Yu",
            "Sung Woong Cho",
            "Dosung Lee",
            "Yesol Park",
            "Youngjoon Hong"
        ],
        "title": "Toward a Robust and Generalizable Metamaterial Foundation Model",
        "abstract": "Advances in material functionalities drive innovations across various fields, where metamaterials-defined by structure rather than composition-are leading the way. Despite the rise of artificial intelligence (AI)-driven design strategies, their impact is limited by task-specific retraining, poor out-of-distribution(OOD) generalization, and the need for separate models for forward and inverse design. To address these limitations, we introduce the Metamaterial Foundation Model (MetaFO), a Bayesian transformer-based foundation model inspired by large language models. MetaFO learns the underlying mechanics of metamaterials, enabling probabilistic, zero-shot predictions across diverse, unseen combinations of material properties and structural responses. It also excels in nonlinear inverse design, even under OOD conditions. By treating metamaterials as an operator that maps material properties to structural responses, MetaFO uncovers intricate structure-property relationships and significantly expands the design space. This scalable and generalizable framework marks a paradigm shift in AI-driven metamaterial discovery, paving the way for next-generation innovations.",
        "arxiv_id": "2507.02436"
    },
    "2507.00884": {
        "SCORE": 17,
        "ARXIVID": "2507.00884",
        "COMMENT": "The paper introduces a novel neural network for biomolecular simulations, aligning with the AI for Science criterion by providing a new architecture-level innovation for biomolecular modeling.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Qun Su",
            "Kai Zhu",
            "Qiaolin Gou",
            "Jintu Zhang",
            "Renling Hu",
            "Yurong Li",
            "Yongze Wang",
            "Hui Zhang",
            "Ziyi You",
            "Linlong Jiang",
            "Yu Kang",
            "Jike Wang",
            "Chang-Yu Hsieh",
            "Tingjun Hou"
        ],
        "title": "A Scalable and Quantum-Accurate Foundation Model for Biomolecular Force Field via Linearly Tensorized Quadrangle Attention",
        "abstract": "Accurate atomistic biomolecular simulations are vital for disease mechanism understanding, drug discovery, and biomaterial design, but existing simulation methods exhibit significant limitations. Classical force fields are efficient but lack accuracy for transition states and fine conformational details critical in many chemical and biological processes. Quantum Mechanics (QM) methods are highly accurate but computationally infeasible for large-scale or long-time simulations. AI-based force fields (AIFFs) aim to achieve QM-level accuracy with efficiency but struggle to balance many-body modeling complexity, accuracy, and speed, often constrained by limited training data and insufficient validation for generalizability. To overcome these challenges, we introduce LiTEN, a novel equivariant neural network with Tensorized Quadrangle Attention (TQA). TQA efficiently models three- and four-body interactions with linear complexity by reparameterizing high-order tensor features via vector operations, avoiding costly spherical harmonics. Building on LiTEN, LiTEN-FF is a robust AIFF foundation model, pre-trained on the extensive nablaDFT dataset for broad chemical generalization and fine-tuned on SPICE for accurate solvated system simulations. LiTEN achieves state-of-the-art (SOTA) performance across most evaluation subsets of rMD17, MD22, and Chignolin, outperforming leading models such as MACE, NequIP, and EquiFormer. LiTEN-FF enables the most comprehensive suite of downstream biomolecular modeling tasks to date, including QM-level conformer searches, geometry optimization, and free energy surface construction, while offering 10x faster inference than MACE-OFF for large biomolecules (~1000 atoms). In summary, we present a physically grounded, highly efficient framework that advances complex biomolecular modeling, providing a versatile foundation for drug discovery and related applications.",
        "arxiv_id": "2507.00884"
    },
    "2507.02119": {
        "SCORE": 17,
        "ARXIVID": "2507.02119",
        "COMMENT": "The paper discusses universal dynamics in compute-optimally trained neural networks, providing insights into training dynamics, which is relevant to representation learning.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Shikai Qiu",
            "Lechao Xiao",
            "Andrew Gordon Wilson",
            "Jeffrey Pennington",
            "Atish Agarwala"
        ],
        "title": "Scaling Collapse Reveals Universal Dynamics in Compute-Optimally Trained Neural Networks",
        "abstract": "What scaling limits govern neural network training dynamics when model size and training time grow in tandem? We show that despite the complex interactions between architecture, training algorithms, and data, compute-optimally trained models exhibit a remarkably precise universality. Specifically, loss curves from models of varying sizes collapse onto a single universal curve when training compute and loss are normalized to unity at the end of training. With learning rate decay, the collapse becomes so tight that differences in the normalized curves across models fall below the noise floor of individual loss curves across random seeds, a phenomenon we term supercollapse. We observe supercollapse across learning rate schedules, datasets, and architectures, including transformers trained on next-token prediction, and find it breaks down when hyperparameters are scaled suboptimally, providing a precise and practical indicator of good scaling. We explain these phenomena by connecting collapse to the power-law structure in typical neural scaling laws, and analyzing a simple yet surprisingly effective model of SGD noise dynamics that accurately predicts loss curves across various learning rate schedules and quantitatively explains the origin of supercollapse.",
        "arxiv_id": "2507.02119"
    },
    "2507.02092": {
        "SCORE": 17,
        "ARXIVID": "2507.02092",
        "COMMENT": "The paper introduces Energy-Based Transformers, a new class of models that improve learning and inference, relevant to model architecture innovations.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Alexi Gladstone",
            "Ganesh Nanduru",
            "Md Mofijul Islam",
            "Peixuan Han",
            "Hyeonjeong Ha",
            "Aman Chadha",
            "Yilun Du",
            "Heng Ji",
            "Jundong Li",
            "Tariq Iqbal"
        ],
        "title": "Energy-Based Transformers are Scalable Learners and Thinkers",
        "abstract": "Inference-time computation techniques, analogous to human System 2 Thinking, have recently become popular for improving model performances. However, most existing approaches suffer from several limitations: they are modality-specific (e.g., working only in text), problem-specific (e.g., verifiable domains like math and coding), or require additional supervision/training on top of unsupervised pretraining (e.g., verifiers or verifiable rewards). In this paper, we ask the question \"Is it possible to generalize these System 2 Thinking approaches, and develop models that learn to think solely from unsupervised learning?\" Interestingly, we find the answer is yes, by learning to explicitly verify the compatibility between inputs and candidate-predictions, and then re-framing prediction problems as optimization with respect to this verifier. Specifically, we train Energy-Based Transformers (EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy value to every input and candidate-prediction pair, enabling predictions through gradient descent-based energy minimization until convergence. Across both discrete (text) and continuous (visual) modalities, we find EBTs scale faster than the dominant Transformer++ approach during training, achieving an up to 35% higher scaling rate with respect to data, batch size, parameters, FLOPs, and depth. During inference, EBTs improve performance with System 2 Thinking by 29% more than the Transformer++ on language tasks, and EBTs outperform Diffusion Transformers on image denoising while using fewer forward passes. Further, we find that EBTs achieve better results than existing models on most downstream tasks given the same or worse pretraining performance, suggesting that EBTs generalize better than existing approaches. Consequently, EBTs are a promising new paradigm for scaling both the learning and thinking capabilities of models.",
        "arxiv_id": "2507.02092"
    },
    "2507.02199": {
        "SCORE": 16,
        "ARXIVID": "2507.02199",
        "COMMENT": "The paper investigates latent chain-of-thought reasoning in transformers, aligning with the Representation Learning criterion by exploring how reasoning structures emerge in models.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Wenquan Lu",
            "Yuechuan Yang",
            "Kyle Lee",
            "Yanshu Li",
            "Enqi Liu"
        ],
        "title": "Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer",
        "abstract": "Chain-of-thought (CoT) reasoning has enabled transformer-based language models to excel at complex mathematics and multi-step planning. However, in standard decoder-only architectures, these reasoning steps are externalized in natural language, improving interpretability at the cost of efficiency. To capture reasoning that is not easily represented in words, many works have explored recurrent architectures that aim to internalize reasoning in latent space, potentially supporting latent CoT. In this paper, we investigate whether such reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer that reuses layers at inference time without increasing parameter count. We examine the model's internal behavior on arithmetic tasks using a suite of probing techniques including the Logit Lens and Coda Lens. Our findings reveal limited evidence of interpretable latent CoT by tracking rank trajectories of final and intermediate result tokens. Furthermore, we uncover significant probing inconsistencies across recurrent blocks, where the interpretability of hidden states depends heavily on both the layer index and the decoding method. Finally, we empirically show that increasing recurrence depth yields only marginal gains and falls well short of models that explicitly externalize reasoning steps. The code is available at https://github.com/wenquanlu/huginn-latent-cot.",
        "arxiv_id": "2507.02199"
    },
    "2507.02619": {
        "SCORE": 16,
        "ARXIVID": "2507.02619",
        "COMMENT": "The paper proposes L-VAE, a novel model for disentangled representation learning, which is relevant to representation learning and autoencoders.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Hazal Mogultay Ozcan",
            "Sinan Kalkan",
            "Fatos T. Yarman-Vural"
        ],
        "title": "L-VAE: Variational Auto-Encoder with Learnable Beta for Disentangled Representation",
        "abstract": "In this paper, we propose a novel model called Learnable VAE (L-VAE), which learns a disentangled representation together with the hyperparameters of the cost function. L-VAE can be considered as an extension of \\b{eta}-VAE, wherein the hyperparameter, \\b{eta}, is empirically adjusted. L-VAE mitigates the limitations of \\b{eta}-VAE by learning the relative weights of the terms in the loss function to control the dynamic trade-off between disentanglement and reconstruction losses. In the proposed model, the weight of the loss terms and the parameters of the model architecture are learned concurrently. An additional regularization term is added to the loss function to prevent bias towards either reconstruction or disentanglement losses. Experimental analyses show that the proposed L-VAE finds an effective balance between reconstruction fidelity and disentangling the latent dimensions. Comparisons of the proposed L-VAE against \\b{eta}-VAE, VAE, ControlVAE, DynamicVAE, and {\\sigma}-VAE on datasets, such as dSprites, MPI3D-complex, Falcor3D, and Isaac3D reveals that L-VAE consistently provides the best or the second best performances measured by a set of disentanglement metrics. Moreover, qualitative experiments on CelebA dataset, confirm the success of the L-VAE model for disentangling the facial attributes.",
        "arxiv_id": "2507.02619"
    },
    "2507.02644": {
        "SCORE": 16,
        "ARXIVID": "2507.02644",
        "COMMENT": "The paper uses transformer-based architectures for neural quantum states, providing insights into representation learning and model architecture in quantum systems.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Yuntian Gu",
            "Wenrui Li",
            "Heng Lin",
            "Bo Zhan",
            "Ruichen Li",
            "Yifei Huang",
            "Di He",
            "Yantao Wu",
            "Tao Xiang",
            "Mingpu Qin",
            "Liwei Wang",
            "Dingshun Lv"
        ],
        "title": "Solving the Hubbard model with Neural Quantum States",
        "abstract": "The rapid development of neural quantum states (NQS) has established it as a promising framework for studying quantum many-body systems. In this work, by leveraging the cutting-edge transformer-based architectures and developing highly efficient optimization algorithms, we achieve the state-of-the-art results for the doped two-dimensional (2D) Hubbard model, arguably the minimum model for high-Tc superconductivity. Interestingly, we find different attention heads in the NQS ansatz can directly encode correlations at different scales, making it capable of capturing long-range correlations and entanglements in strongly correlated systems. With these advances, we establish the half-filled stripe in the ground state of 2D Hubbard model with the next nearest neighboring hoppings, consistent with experimental observations in cuprates. Our work establishes NQS as a powerful tool for solving challenging many-fermions systems.",
        "arxiv_id": "2507.02644"
    },
    "2507.02503": {
        "SCORE": 15,
        "ARXIVID": "2507.02503",
        "COMMENT": "The paper presents a novel training strategy for continual learning in LLMs using low-rank projection, aligning with the model compression criterion.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Chenxu Wang",
            "Yilin Lyu",
            "Zicheng Sun",
            "Liping Jing"
        ],
        "title": "Continual Gradient Low-Rank Projection Fine-Tuning for LLMs",
        "abstract": "Continual fine-tuning of Large Language Models (LLMs) is hampered by the trade-off between efficiency and expressiveness. Low-Rank Adaptation (LoRA) offers efficiency but constrains the model's ability to learn new tasks and transfer knowledge due to its low-rank nature and reliance on explicit parameter constraints. We propose GORP (Gradient LOw Rank Projection) for Continual Learning, a novel training strategy that overcomes these limitations by synergistically combining full and low-rank parameters and jointly updating within a unified low-rank gradient subspace. GORP expands the optimization space while preserving efficiency and mitigating catastrophic forgetting. Extensive experiments on continual learning benchmarks demonstrate GORP's superior performance compared to existing state-of-the-art approaches. Code is available at https://github.com/Wcxwcxw/GORP.",
        "arxiv_id": "2507.02503"
    },
    "2507.02686": {
        "SCORE": 15,
        "ARXIVID": "2507.02686",
        "COMMENT": "The paper proposes a novel framework integrating deep unfolding and model distillation for diffusion models, which aligns with foundational research in model architecture and efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Charlesquin Kemajou Mbakam",
            "Jonathan Spence",
            "Marcelo Pereyra"
        ],
        "title": "Learning few-step posterior samplers by unfolding and distillation of diffusion models",
        "abstract": "Diffusion models (DMs) have emerged as powerful image priors in Bayesian computational imaging. Two primary strategies have been proposed for leveraging DMs in this context: Plug-and-Play methods, which are zero-shot and highly flexible but rely on approximations; and specialized conditional DMs, which achieve higher accuracy and faster inference for specific tasks through supervised training. In this work, we introduce a novel framework that integrates deep unfolding and model distillation to transform a DM image prior into a few-step conditional model for posterior sampling. A central innovation of our approach is the unfolding of a Markov chain Monte Carlo (MCMC) algorithm - specifically, the recently proposed LATINO Langevin sampler (Spagnoletti et al., 2025) - representing the first known instance of deep unfolding applied to a Monte Carlo sampling scheme. We demonstrate our proposed unfolded and distilled samplers through extensive experiments and comparisons with the state of the art, where they achieve excellent accuracy and computational efficiency, while retaining the flexibility to adapt to variations in the forward model at inference time.",
        "arxiv_id": "2507.02686"
    },
    "2507.02259": {
        "SCORE": 15,
        "ARXIVID": "2507.02259",
        "COMMENT": "The paper introduces a novel agent workflow, MemAgent, for handling long-text tasks with linear complexity, which aligns with the interest in foundational research on LLMs and architecture-level innovations.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Hongli Yu",
            "Tinghong Chen",
            "Jiangtao Feng",
            "Jiangjie Chen",
            "Weinan Dai",
            "Qiying Yu",
            "Ya-Qin Zhang",
            "Wei-Ying Ma",
            "Jingjing Liu",
            "Mingxuan Wang",
            "Hao Zhou"
        ],
        "title": "MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent",
        "abstract": "Despite improvements by length extrapolation, efficient attention and memory modules, handling infinitely long documents with linear complexity without performance degradation during extrapolation remains the ultimate challenge in long-text processing. We directly optimize for long-text tasks in an end-to-end fashion and introduce a novel agent workflow, MemAgent, which reads text in segments and updates the memory using an overwrite strategy. We extend the DAPO algorithm to facilitate training via independent-context multi-conversation generation. MemAgent has demonstrated superb long-context capabilities, being able to extrapolate from an 8K context trained on 32K text to a 3.5M QA task with performance loss < 5% and achieves 95%+ in 512K RULER test.",
        "arxiv_id": "2507.02259"
    },
    "2507.02541": {
        "SCORE": 15,
        "ARXIVID": "2507.02541",
        "COMMENT": "The paper explores improving task clarity to enhance reasoning in LLMs, which aligns with foundational research in LLM behavior and interpretability.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Yanzhen Lu",
            "Hanbin Yang",
            "Xiaodie Wang",
            "Ge Zhang",
            "Biao Li",
            "Chenxu Fu",
            "Chao Li",
            "Yang Yuan",
            "Andrew Chi-Chih Yao"
        ],
        "title": "Clarifying Before Reasoning: A Coq Prover with Structural Context",
        "abstract": "In this work, we investigate whether improving task clarity can enhance reasoning ability of large language models, focusing on theorem proving in Coq. We introduce a concept-level metric to evaluate task clarity and show that adding structured semantic context to the standard input used by modern LLMs, leads to a 1.85$\\times$ improvement in clarity score (44.5\\%~$\\rightarrow$~82.3\\%). Using the general-purpose model \\texttt{DeepSeek-V3}, our approach leads to a 2.1$\\times$ improvement in proof success (21.8\\%~$\\rightarrow$~45.8\\%) and outperforms the previous state-of-the-art \\texttt{Graph2Tac} (33.2\\%). We evaluate this on 1,386 theorems randomly sampled from 15 standard Coq packages, following the same evaluation protocol as \\texttt{Graph2Tac}. Furthermore, fine-tuning smaller models on our structured data can achieve even higher performance (48.6\\%). Our method uses selective concept unfolding to enrich task descriptions, and employs a Planner--Executor architecture. These findings highlight the value of structured task representations in bridging the gap between understanding and reasoning.",
        "arxiv_id": "2507.02541"
    },
    "2507.02103": {
        "SCORE": 15,
        "ARXIVID": "2507.02103",
        "COMMENT": "The paper explores insights from neuroscience for AI learning, which aligns with the Emerging Trends criterion by challenging established assumptions in AI learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Daniel Durstewitz",
            "Bruno Averbeck",
            "Georgia Koppe"
        ],
        "title": "What Neuroscience Can Teach AI About Learning in Continuously Changing Environments",
        "abstract": "Modern AI models, such as large language models, are usually trained once on a huge corpus of data, potentially fine-tuned for a specific task, and then deployed with fixed parameters. Their training is costly, slow, and gradual, requiring billions of repetitions. In stark contrast, animals continuously adapt to the ever-changing contingencies in their environments. This is particularly important for social species, where behavioral policies and reward outcomes may frequently change in interaction with peers. The underlying computational processes are often marked by rapid shifts in an animal's behaviour and rather sudden transitions in neuronal population activity. Such computational capacities are of growing importance for AI systems operating in the real world, like those guiding robots or autonomous vehicles, or for agentic AI interacting with humans online. Can AI learn from neuroscience? This Perspective explores this question, integrating the literature on continual and in-context learning in AI with the neuroscience of learning on behavioral tasks with shifting rules, reward probabilities, or outcomes. We will outline an agenda for how specifically insights from neuroscience may inform current developments in AI in this area, and - vice versa - what neuroscience may learn from AI, contributing to the evolving field of NeuroAI.",
        "arxiv_id": "2507.02103"
    },
    "2507.02748": {
        "SCORE": 15,
        "ARXIVID": "2507.02748",
        "COMMENT": "The paper introduces a new attention mechanism, Multipole Attention Neural Operator (MANO), which is relevant to model architecture innovations.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Alex Colagrande",
            "Paul Caillon",
            "Eva Feillet",
            "Alexandre Allauzen"
        ],
        "title": "Linear Attention with Global Context: A Multipole Attention Mechanism for Vision and Physics",
        "abstract": "Transformers have become the de facto standard for a wide range of tasks, from image classification to physics simulations. Despite their impressive performance, the quadratic complexity of standard Transformers in both memory and time with respect to the input length makes them impractical for processing high-resolution inputs. Therefore, several variants have been proposed, the most successful relying on patchification, downsampling, or coarsening techniques, often at the cost of losing the finest-scale details. In this work, we take a different approach. Inspired by state-of-the-art techniques in $n$-body numerical simulations, we cast attention as an interaction problem between grid points. We introduce the Multipole Attention Neural Operator (MANO), which computes attention in a distance-based multiscale fashion. MANO maintains, in each attention head, a global receptive field and achieves linear time and memory complexity with respect to the number of grid points. Empirical results on image classification and Darcy flows demonstrate that MANO rivals state-of-the-art models such as ViT and Swin Transformer, while reducing runtime and peak memory usage by orders of magnitude. We open source our code for reproducibility at https://github.com/AlexColagrande/MANO.",
        "arxiv_id": "2507.02748"
    },
    "2507.02084": {
        "SCORE": 15,
        "ARXIVID": "2507.02084",
        "COMMENT": "The paper provides a theoretical analysis of the adaptive Iterative Soft-Thresholding Algorithm, which is relevant to model compression techniques like sparsity and thresholding.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Yining Feng",
            "Ivan Selesnick"
        ],
        "title": "Adaptive Iterative Soft-Thresholding Algorithm with the Median Absolute Deviation",
        "abstract": "The adaptive Iterative Soft-Thresholding Algorithm (ISTA) has been a popular algorithm for finding a desirable solution to the LASSO problem without explicitly tuning the regularization parameter $\\lambda$. Despite that the adaptive ISTA is a successful practical algorithm, few theoretical results exist. In this paper, we present the theoretical analysis on the adaptive ISTA with the thresholding strategy of estimating noise level by median absolute deviation. We show properties of the fixed points of the algorithm, including scale equivariance, non-uniqueness, and local stability, prove the local linear convergence guarantee, and show its global convergence behavior.",
        "arxiv_id": "2507.02084"
    }
}