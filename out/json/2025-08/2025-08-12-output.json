{
    "2508.07397": {
        "SCORE": 17,
        "ARXIVID": "2508.07397",
        "COMMENT": "The paper provides a novel statistical mechanics characterization of neural networks, offering insights into their structure and properties beyond conventional metrics.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Jun Li"
        ],
        "title": "A Spin Glass Characterization of Neural Networks",
        "abstract": "This work presents a statistical mechanics characterization of neural networks, motivated by the replica symmetry breaking (RSB) phenomenon in spin glasses. A Hopfield-type spin glass model is constructed from a given feedforward neural network (FNN). Overlaps between simulated replica samples serve as a characteristic descriptor of the FNN. The connection between the spin-glass description and commonly studied properties of the FNN -- such as data fitting, capacity, generalization, and robustness -- has been investigated and empirically demonstrated. Unlike prior analytical studies that focus on model ensembles, this method provides a computable descriptor for individual network instances, which reveals nontrivial structural properties that are not captured by conventional metrics such as loss or accuracy. Preliminary results suggests its potential for practical applications such as model inspection, safety verification, and detection of hidden vulnerabilities.",
        "arxiv_id": "2508.07397"
    },
    "2508.06953": {
        "SCORE": 17,
        "ARXIVID": "2508.06953",
        "COMMENT": "The paper proposes BoRA, a novel method for low-rank adaptation in LLMs, enhancing parameter efficiency and model performance, relevant to model compression.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Shiwei Li",
            "Xiandi Luo",
            "Haozhao Wang",
            "Xing Tang",
            "Ziqiang Cui",
            "Dugang Liu",
            "Yuhua Li",
            "Xiuqiang He",
            "Ruixuan Li"
        ],
        "title": "BoRA: Towards More Expressive Low-Rank Adaptation with Block Diversity",
        "abstract": "Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) method widely used in large language models (LLMs). It approximates the update of a pretrained weight matrix $W\\in\\mathbb{R}^{m\\times n}$ by the product of two low-rank matrices, $BA$, where $A \\in\\mathbb{R}^{r\\times n}$ and $B\\in\\mathbb{R}^{m\\times r} (r\\ll\\min\\{m,n\\})$. Increasing the dimension $r$ can raise the rank of LoRA weights (i.e., $BA$), which typically improves fine-tuning performance but also significantly increases the number of trainable parameters. In this paper, we propose Block Diversified Low-Rank Adaptation (BoRA), which improves the rank of LoRA weights with a small number of additional parameters. Specifically, BoRA treats the product $BA$ as a block matrix multiplication, where $A$ and $B$ are partitioned into $b$ blocks along the columns and rows, respectively (i.e., $A=[A_1,\\dots,A_b]$ and $B=[B_1,\\dots,B_b]^\\top$). Consequently, the product $BA$ becomes the concatenation of the block products $B_iA_j$ for $i,j\\in[b]$. To enhance the diversity of different block products, BoRA introduces a unique diagonal matrix $\\Sigma_{i,j} \\in \\mathbb{R}^{r\\times r}$ for each block multiplication, resulting in $B_i \\Sigma_{i,j} A_j$. By leveraging these block-wise diagonal matrices, BoRA increases the rank of LoRA weights by a factor of $b$ while only requiring $b^2r$ additional parameters. Extensive experiments across multiple datasets and models demonstrate the superiority of BoRA, and ablation studies further validate its scalability.",
        "arxiv_id": "2508.06953"
    },
    "2508.07494": {
        "SCORE": 17,
        "ARXIVID": "2508.07494",
        "COMMENT": "The paper presents a novel solution to generalize the Koopman operator for nonlinear systems, which is foundational research in representation learning.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Mircea Lazar"
        ],
        "title": "From Product Hilbert Spaces to the Generalized Koopman Operator and the Nonlinear Fundamental Lemma",
        "abstract": "The generalization of the Koopman operator to systems with control input and the derivation of a nonlinear fundamental lemma are two open problems that play a key role in the development of data-driven control methods for nonlinear systems. Both problems hinge on the construction of observable or basis functions and their corresponding Hilbert space that enable an infinite-dimensional, linear system representation. In this paper we derive a novel solution to these problems based on orthonormal expansion in a product Hilbert space constructed as the tensor product between the Hilbert spaces of the state and input observable functions, respectively. We prove that there exists an infinite-dimensional linear operator, i.e. the generalized Koopman operator, from the constructed product Hilbert space to the Hilbert space corresponding to the lifted state propagated forward in time. A scalable data-driven method for computing finite-dimensional approximations of generalized Koopman operators and several choices of observable functions are also presented. Moreover, we derive a nonlinear fundamental lemma by exploiting the bilinear structure of the infinite-dimensional generalized Koopman model. The effectiveness of the developed generalized Koopman embedding is illustrated on the Van der Pol oscillator.",
        "arxiv_id": "2508.07494"
    },
    "2508.07101": {
        "SCORE": 17,
        "ARXIVID": "2508.07101",
        "COMMENT": "The paper introduces a training-free sparse attention mechanism, relevant to model compression and efficiency.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Lijie Yang",
            "Zhihao Zhang",
            "Arti Jain",
            "Shijie Cao",
            "Baihong Yuan",
            "Yiwei Chen",
            "Zhihao Jia",
            "Ravi Netravali"
        ],
        "title": "Less Is More: Training-Free Sparse Attention with Global Locality for Efficient Reasoning",
        "abstract": "Large reasoning models achieve strong performance through test-time scaling but incur substantial computational overhead, particularly from excessive token generation when processing short input prompts. While sparse attention mechanisms can reduce latency and memory usage, existing approaches suffer from significant accuracy degradation due to accumulated errors during long-generation reasoning. These methods generally require either high token retention rates or expensive retraining. We introduce LessIsMore, a training-free sparse attention mechanism for reasoning tasks, which leverages global attention patterns rather than relying on traditional head-specific local optimizations. LessIsMore aggregates token selections from local attention heads with recent contextual information, enabling unified cross-head token ranking for future decoding layers. This unified selection improves generalization and efficiency by avoiding the need to maintain separate token subsets per head. Evaluation across diverse reasoning tasks and benchmarks shows that LessIsMore preserves -- and in some cases improves -- accuracy while achieving a $1.1\\times$ average decoding speed-up compared to full attention. Moreover, LessIsMore attends to $2\\times$ fewer tokens without accuracy loss, achieving a $1.13\\times$ end-to-end speed-up compared to existing sparse attention methods.",
        "arxiv_id": "2508.07101"
    },
    "2508.06617": {
        "SCORE": 17,
        "ARXIVID": "2508.06617",
        "COMMENT": "The paper proposes a generalized scaling law for both dense and sparse LLMs, which is relevant to foundational research in LLMs and model efficiency.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Md Arafat Hossain",
            "Xingfu Wu",
            "Valerie Taylor",
            "Ali Jannesari"
        ],
        "title": "Generalizing Scaling Laws for Dense and Sparse Large Language Models",
        "abstract": "Over the past few years, the size of language models has grown exponentially, as has the computational cost to train these large models. This rapid growth has motivated researchers to develop new techniques aimed at enhancing the efficiency of the training process. Despite these advancements, optimally predicting the model size or allocating optimal resources remains a challenge. Several efforts have addressed the challenge by proposing different scaling laws, but almost all of them are architecture-specific (dense or sparse). In this work we revisit existing scaling laws and propose a generalized scaling law to provide a unified framework that is applicable to both dense and sparse large language models. We evaluate and compare our proposed scaling law with existing scaling laws to demonstrate its effectiveness.",
        "arxiv_id": "2508.06617"
    },
    "2508.07185": {
        "SCORE": 17,
        "ARXIVID": "2508.07185",
        "COMMENT": "The paper proposes a framework for real-time knowledge updating in LLMs using dynamic sparse knowledge attention, which is relevant to model architecture and efficiency.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Kabir Khan",
            "Priya Sharma",
            "Arjun Mehta",
            "Neha Gupta",
            "Ravi Narayanan"
        ],
        "title": "DySK-Attn: A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention",
        "abstract": "Large Language Models (LLMs) suffer from a critical limitation: their knowledge is static and quickly becomes outdated. Retraining these massive models is computationally prohibitive, while existing knowledge editing techniques can be slow and may introduce unforeseen side effects. To address this, we propose DySK-Attn, a novel framework that enables LLMs to efficiently integrate real-time knowledge from a dynamic external source. Our approach synergizes an LLM with a dynamic Knowledge Graph (KG) that can be updated instantaneously. The core of our framework is a sparse knowledge attention mechanism, which allows the LLM to perform a coarse-to-fine grained search, efficiently identifying and focusing on a small, highly relevant subset of facts from the vast KG. This mechanism avoids the high computational cost of dense attention over the entire knowledge base and mitigates noise from irrelevant information. We demonstrate through extensive experiments on time-sensitive question-answering tasks that DySK-Attn significantly outperforms strong baselines, including standard Retrieval-Augmented Generation (RAG) and model editing techniques, in both factual accuracy for updated knowledge and computational efficiency. Our framework offers a scalable and effective solution for building LLMs that can stay current with the ever-changing world.",
        "arxiv_id": "2508.07185"
    },
    "2508.07208": {
        "SCORE": 17,
        "ARXIVID": "2508.07208",
        "COMMENT": "The paper provides theoretical insights into the representation capabilities of two-layer transformers, which aligns with the interest in understanding model architectures and training dynamics.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Chanakya Ekbote",
            "Marco Bondaschi",
            "Nived Rajaraman",
            "Jason D. Lee",
            "Michael Gastpar",
            "Ashok Vardhan Makkuva",
            "Paul Pu Liang"
        ],
        "title": "What One Cannot, Two Can: Two-Layer Transformers Provably Represent Induction Heads on Any-Order Markov Chains",
        "abstract": "In-context learning (ICL) is a hallmark capability of transformers, through which trained models learn to adapt to new tasks by leveraging information from the input context. Prior work has shown that ICL emerges in transformers due to the presence of special circuits called induction heads. Given the equivalence between induction heads and conditional k-grams, a recent line of work modeling sequential inputs as Markov processes has revealed the fundamental impact of model depth on its ICL capabilities: while a two-layer transformer can efficiently represent a conditional 1-gram model, its single-layer counterpart cannot solve the task unless it is exponentially large. However, for higher order Markov sources, the best known constructions require at least three layers (each with a single attention head) - leaving open the question: can a two-layer single-head transformer represent any kth-order Markov process? In this paper, we precisely address this and theoretically show that a two-layer transformer with one head per layer can indeed represent any conditional k-gram. Thus, our result provides the tightest known characterization of the interplay between transformer depth and Markov order for ICL. Building on this, we further analyze the learning dynamics of our two-layer construction, focusing on a simplified variant for first-order Markov chains, illustrating how effective in-context representations emerge during training. Together, these results deepen our current understanding of transformer-based ICL and illustrate how even shallow architectures can surprisingly exhibit strong ICL capabilities on structured sequence modeling tasks.",
        "arxiv_id": "2508.07208"
    },
    "2508.08139": {
        "SCORE": 17,
        "ARXIVID": "2508.08139",
        "COMMENT": "The paper investigates reliability estimation in LLMs using uncertainty, which provides insights into LLM behavior and interpretability.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Tianyi Zhou",
            "Johanne Medina",
            "Sanjay Chawla"
        ],
        "title": "Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models",
        "abstract": "Large Language Models (LLMs) are prone to generating fluent but incorrect content, known as confabulation, which poses increasing risks in multi-turn or agentic applications where outputs may be reused as context. In this work, we investigate how in-context information influences model behavior and whether LLMs can identify their unreliable responses. We propose a reliability estimation that leverages token-level uncertainty to guide the aggregation of internal model representations. Specifically, we compute aleatoric and epistemic uncertainty from output logits to identify salient tokens and aggregate their hidden states into compact representations for response-level reliability prediction. Through controlled experiments on open QA benchmarks, we find that correct in-context information improves both answer accuracy and model confidence, while misleading context often induces confidently incorrect responses, revealing a misalignment between uncertainty and correctness. Our probing-based method captures these shifts in model behavior and improves the detection of unreliable outputs across multiple open-source LLMs. These results underscore the limitations of direct uncertainty signals and highlight the potential of uncertainty-guided probing for reliability-aware generation.",
        "arxiv_id": "2508.08139"
    },
    "2508.07370": {
        "SCORE": 17,
        "ARXIVID": "2508.07370",
        "COMMENT": "The paper provides insights into the intrinsic training dynamics of deep neural networks, focusing on dimensionality reduction and implicit bias, which aligns with representation learning.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Sibylle Marcotte",
            "Gabriel Peyr\\'e",
            "R\\'emi Gribonval"
        ],
        "title": "Intrinsic training dynamics of deep neural networks",
        "abstract": "A fundamental challenge in the theory of deep learning is to understand whether gradient-based training in high-dimensional parameter spaces can be captured by simpler, lower-dimensional structures, leading to so-called implicit bias. As a stepping stone, we study when a gradient flow on a high-dimensional variable $\\theta$ implies an intrinsic gradient flow on a lower-dimensional variable $z = \\phi(\\theta)$, for an architecture-related function $\\phi$. We express a so-called intrinsic dynamic property and show how it is related to the study of conservation laws associated with the factorization $\\phi$. This leads to a simple criterion based on the inclusion of kernels of linear maps which yields a necessary condition for this property to hold. We then apply our theory to general ReLU networks of arbitrary depth and show that, for any initialization, it is possible to rewrite the flow as an intrinsic dynamic in a lower dimension that depends only on $z$ and the initialization, when $\\phi$ is the so-called path-lifting. In the case of linear networks with $\\phi$ the product of weight matrices, so-called balanced initializations are also known to enable such a dimensionality reduction; we generalize this result to a broader class of {\\em relaxed balanced} initializations, showing that, in certain configurations, these are the \\emph{only} initializations that ensure the intrinsic dynamic property. Finally, for the linear neural ODE associated with the limit of infinitely deep linear networks, with relaxed balanced initialization, we explicitly express the corresponding intrinsic dynamics.",
        "arxiv_id": "2508.07370"
    },
    "2508.08222": {
        "SCORE": 17,
        "ARXIVID": "2508.08222",
        "COMMENT": "The paper provides theoretical insights into how transformers learn symbolic multi-step reasoning, which is relevant to model architecture and LLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Tong Yang",
            "Yu Huang",
            "Yingbin Liang",
            "Yuejie Chi"
        ],
        "title": "Multi-head Transformers Provably Learn Symbolic Multi-step Reasoning via Gradient Descent",
        "abstract": "Transformers have demonstrated remarkable capabilities in multi-step reasoning tasks. However, understandings of the underlying mechanisms by which they acquire these abilities through training remain limited, particularly from a theoretical standpoint. This work investigates how transformers learn to solve symbolic multi-step reasoning problems through chain-of-thought processes, focusing on path-finding in trees. We analyze two intertwined tasks: a backward reasoning task, where the model outputs a path from a goal node to the root, and a more complex forward reasoning task, where the model implements two-stage reasoning by first identifying the goal-to-root path and then reversing it to produce the root-to-goal path. Our theoretical analysis, grounded in the dynamics of gradient descent, shows that trained one-layer transformers can provably solve both tasks with generalization guarantees to unseen trees. In particular, our multi-phase training dynamics for forward reasoning elucidate how different attention heads learn to specialize and coordinate autonomously to solve the two subtasks in a single autoregressive path. These results provide a mechanistic explanation of how trained transformers can implement sequential algorithmic procedures. Moreover, they offer insights into the emergence of reasoning abilities, suggesting that when tasks are structured to take intermediate chain-of-thought steps, even shallow multi-head transformers can effectively solve problems that would otherwise require deeper architectures.",
        "arxiv_id": "2508.08222"
    },
    "2508.06601": {
        "SCORE": 16,
        "ARXIVID": "2508.06601",
        "COMMENT": "The paper explores pretraining data filtering as a method to build tamper-resistant safeguards into open-weight LLMs, contributing to foundational research in LLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Kyle O'Brien",
            "Stephen Casper",
            "Quentin Anthony",
            "Tomek Korbak",
            "Robert Kirk",
            "Xander Davies",
            "Ishan Mishra",
            "Geoffrey Irving",
            "Yarin Gal",
            "Stella Biderman"
        ],
        "title": "Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs",
        "abstract": "Open-weight AI systems offer unique benefits, including enhanced transparency, open research, and decentralized access. However, they are vulnerable to tampering attacks which can efficiently elicit harmful behaviors by modifying weights or activations. Currently, there is not yet a robust science of open-weight model risk management. Existing safety fine-tuning methods and other post-training techniques have struggled to make LLMs resistant to more than a few dozen steps of adversarial fine-tuning. In this paper, we investigate whether filtering text about dual-use topics from training data can prevent unwanted capabilities and serve as a more tamper-resistant safeguard. We introduce a multi-stage pipeline for scalable data filtering and show that it offers a tractable and effective method for minimizing biothreat proxy knowledge in LLMs. We pretrain multiple 6.9B-parameter models from scratch and find that they exhibit substantial resistance to adversarial fine-tuning attacks on up to 10,000 steps and 300M tokens of biothreat-related text -- outperforming existing post-training baselines by over an order of magnitude -- with no observed degradation to unrelated capabilities. However, while filtered models lack internalized dangerous knowledge, we find that they can still leverage such information when it is provided in context (e.g., via search tool augmentation), demonstrating a need for a defense-in-depth approach. Overall, these findings help to establish pretraining data curation as a promising layer of defense for open-weight AI systems.",
        "arxiv_id": "2508.06601"
    },
    "2508.07281": {
        "SCORE": 16,
        "ARXIVID": "2508.07281",
        "COMMENT": "The paper explores activation maximization for understanding feature representations in DNNs, which is relevant to representation learning and model interpretability.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Hongbo Zhu",
            "Angelo Cangelosi"
        ],
        "title": "Representation Understanding via Activation Maximization",
        "abstract": "Understanding internal feature representations of deep neural networks (DNNs) is a fundamental step toward model interpretability. Inspired by neuroscience methods that probe biological neurons using visual stimuli, recent deep learning studies have employed Activation Maximization (AM) to synthesize inputs that elicit strong responses from artificial neurons. In this work, we propose a unified feature visualization framework applicable to both Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). Unlike prior efforts that predominantly focus on the last output-layer neurons in CNNs, we extend feature visualization to intermediate layers as well, offering deeper insights into the hierarchical structure of learned feature representations. Furthermore, we investigate how activation maximization can be leveraged to generate adversarial examples, revealing potential vulnerabilities and decision boundaries of DNNs. Our experiments demonstrate the effectiveness of our approach in both traditional CNNs and modern ViT, highlighting its generalizability and interpretive value.",
        "arxiv_id": "2508.07281"
    },
    "2508.06526": {
        "SCORE": 16,
        "ARXIVID": "2508.06526",
        "COMMENT": "The paper introduces a KV cache management system for MoE architectures, focusing on compression and efficiency, which aligns with interests in model compression and MoE.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Dong Liu",
            "Yanxuan Yu",
            "Ben Lengerich",
            "Ying Nian Wu",
            "Xuhong Wang"
        ],
        "title": "PiKV: KV Cache Management System for Mixture of Experts",
        "abstract": "As large language models continue to scale up in both size and context length, the memory and communication cost of key-value (KV) cache storage has become a major bottleneck in multi-GPU and multi-node inference. While MoE-based architectures sparsify computation across experts, the corresponding KV caches remain dense and globally synchronized, resulting in significant overhead.   We introduce \\textbf{PiKV}, a parallel and distributed KV cache serving framework tailored for MoE architecture. PiKV leverages \\textit{expert-sharded KV storage} to partition caches across GPUs, \\textit{PiKV routing} to reduce token-to-KV access, and a \\textit{PiKV Scheduling} to adaptively retain query-relevant entries. To further reduce memory usage, PiKV integrates \\textit{PiKV Compression} modules the caching pipeline for acceleration.   PiKV is recently publicly available as an open-source software library: \\href{https://github.com/NoakLiu/PiKV}{https://github.com/NoakLiu/PiKV}. Experiments details is recorded at: \\href{https://github.com/NoakLiu/PiKV/blob/main/downstream_tasks/README.md}{https://github.com/NoakLiu/PiKV/Experimental\\_Results}. We also have PiKV integrated with Nvidia kvpress for acceleration, details see \\href{https://github.com/NoakLiu/PiKVpress}{https://github.com/NoakLiu/PiKVpress}. PiKV is still a living project, aiming to become a comprehesive KV Cache management system for MoE Architectures.",
        "arxiv_id": "2508.06526"
    },
    "2508.07102": {
        "SCORE": 16,
        "ARXIVID": "2508.07102",
        "COMMENT": "The paper introduces a theoretical study on Second-Order MeanFlow, which is relevant to emerging trends in generative modeling and foundational research.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Yang Cao",
            "Yubin Chen",
            "Zhao Song",
            "Jiahao Zhang"
        ],
        "title": "Towards High-Order Mean Flow Generative Models: Feasibility, Expressivity, and Provably Efficient Criteria",
        "abstract": "Generative modelling has seen significant advances through simulation-free paradigms such as Flow Matching, and in particular, the MeanFlow framework, which replaces instantaneous velocity fields with average velocities to enable efficient single-step sampling. In this work, we introduce a theoretical study on Second-Order MeanFlow, a novel extension that incorporates average acceleration fields into the MeanFlow objective. We first establish the feasibility of our approach by proving that the average acceleration satisfies a generalized consistency condition analogous to first-order MeanFlow, thereby supporting stable, one-step sampling and tractable loss functions. We then characterize its expressivity via circuit complexity analysis, showing that under mild assumptions, the Second-Order MeanFlow sampling process can be implemented by uniform threshold circuits within the $\\mathsf{TC}^0$ class. Finally, we derive provably efficient criteria for scalable implementation by leveraging fast approximate attention computations: we prove that attention operations within the Second-Order MeanFlow architecture can be approximated to within $1/\\mathrm{poly}(n)$ error in time $n^{2+o(1)}$. Together, these results lay the theoretical foundation for high-order flow matching models that combine rich dynamics with practical sampling efficiency.",
        "arxiv_id": "2508.07102"
    },
    "2508.07636": {
        "SCORE": 16,
        "ARXIVID": "2508.07636",
        "COMMENT": "The paper provides a theoretical perspective on attribution explanations for DNNs, which is relevant to understanding model interpretability and representation learning.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Huiqi Deng",
            "Hongbin Pei",
            "Quanshi Zhang",
            "Mengnan Du"
        ],
        "title": "Attribution Explanations for Deep Neural Networks: A Theoretical Perspective",
        "abstract": "Attribution explanation is a typical approach for explaining deep neural networks (DNNs), inferring an importance or contribution score for each input variable to the final output. In recent years, numerous attribution methods have been developed to explain DNNs. However, a persistent concern remains unresolved, i.e., whether and which attribution methods faithfully reflect the actual contribution of input variables to the decision-making process. The faithfulness issue undermines the reliability and practical utility of attribution explanations. We argue that these concerns stem from three core challenges. First, difficulties arise in comparing attribution methods due to their unstructured heterogeneity, differences in heuristics, formulations, and implementations that lack a unified organization. Second, most methods lack solid theoretical underpinnings, with their rationales remaining absent, ambiguous, or unverified. Third, empirically evaluating faithfulness is challenging without ground truth. Recent theoretical advances provide a promising way to tackle these challenges, attracting increasing attention. We summarize these developments, with emphasis on three key directions: (i) Theoretical unification, which uncovers commonalities and differences among methods, enabling systematic comparisons; (ii) Theoretical rationale, clarifying the foundations of existing methods; (iii) Theoretical evaluation, rigorously proving whether methods satisfy faithfulness principles. Beyond a comprehensive review, we provide insights into how these studies help deepen theoretical understanding, inform method selection, and inspire new attribution methods. We conclude with a discussion of promising open problems for further work.",
        "arxiv_id": "2508.07636"
    },
    "2508.07876": {
        "SCORE": 16,
        "ARXIVID": "2508.07876",
        "COMMENT": "The paper advances theoretical foundations of reservoir computing, which is relevant to emerging trends in model architecture and representation learning.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Juan-Pablo Ortega",
            "Florian Rossmannek"
        ],
        "title": "Stochastic dynamics learning with state-space systems",
        "abstract": "This work advances the theoretical foundations of reservoir computing (RC) by providing a unified treatment of fading memory and the echo state property (ESP) in both deterministic and stochastic settings. We investigate state-space systems, a central model class in time series learning, and establish that fading memory and solution stability hold generically -- even in the absence of the ESP -- offering a robust explanation for the empirical success of RC models without strict contractivity conditions. In the stochastic case, we critically assess stochastic echo states, proposing a novel distributional perspective rooted in attractor dynamics on the space of probability distributions, which leads to a rich and coherent theory. Our results extend and generalize previous work on non-autonomous dynamical systems, offering new insights into causality, stability, and memory in RC models. This lays the groundwork for reliable generative modeling of temporal data in both deterministic and stochastic regimes.",
        "arxiv_id": "2508.07876"
    },
    "2508.07710": {
        "SCORE": 16,
        "ARXIVID": "2508.07710",
        "COMMENT": "The paper proposes a training-free ANN-to-SNN conversion for Transformers, which is relevant to model architecture innovations and efficiency improvements.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Jingya Wang",
            "Xin Deng",
            "Wenjie Wei",
            "Dehao Zhang",
            "Shuai Wang",
            "Qian Sun",
            "Jieyuan Zhang",
            "Hanwen Liu",
            "Ning Xie",
            "Malu Zhang"
        ],
        "title": "Training-Free ANN-to-SNN Conversion for High-Performance Spiking Transformer",
        "abstract": "Leveraging the event-driven paradigm, Spiking Neural Networks (SNNs) offer a promising approach for constructing energy-efficient Transformer architectures. Compared to directly trained Spiking Transformers, ANN-to-SNN conversion methods bypass the high training costs. However, existing methods still suffer from notable limitations, failing to effectively handle nonlinear operations in Transformer architectures and requiring additional fine-tuning processes for pre-trained ANNs. To address these issues, we propose a high-performance and training-free ANN-to-SNN conversion framework tailored for Transformer architectures. Specifically, we introduce a Multi-basis Exponential Decay (MBE) neuron, which employs an exponential decay strategy and multi-basis encoding method to efficiently approximate various nonlinear operations. It removes the requirement for weight modifications in pre-trained ANNs. Extensive experiments across diverse tasks (CV, NLU, NLG) and mainstream Transformer architectures (ViT, RoBERTa, GPT-2) demonstrate that our method achieves near-lossless conversion accuracy with significantly lower latency. This provides a promising pathway for the efficient and scalable deployment of Spiking Transformers in real-world applications.",
        "arxiv_id": "2508.07710"
    },
    "2508.07395": {
        "SCORE": 15,
        "ARXIVID": "2508.07395",
        "COMMENT": "The paper investigates the theoretical limitations of structured state-space models (SSMs) and proposes conditions for their effectiveness, contributing to model architecture insights.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Behnoush Khavari",
            "Mehran Shakerinava",
            "Jayesh Khullar",
            "Jerry Huang",
            "Fran\\c{c}ois Rivest",
            "Siamak Ravanbakhsh",
            "Sarath Chandar"
        ],
        "title": "Parity Requires Unified Input Dependence and Negative Eigenvalues in SSMs",
        "abstract": "Recent work has shown that LRNN models such as S4D, Mamba, and DeltaNet lack state-tracking capability due to either time-invariant transition matrices or restricted eigenvalue ranges. To address this, input-dependent transition matrices, particularly those that are complex or non-triangular, have been proposed to enhance SSM performance on such tasks. While existing theorems demonstrate that both input-independent and non-negative SSMs are incapable of solving simple state-tracking tasks, such as parity, regardless of depth, they do not explore whether combining these two types in a multilayer SSM could help. We investigate this question for efficient SSMs with diagonal transition matrices and show that such combinations still fail to solve parity. This implies that a recurrence layer must both be input-dependent and include negative eigenvalues. Our experiments support this conclusion by analyzing an SSM model that combines S4D and Mamba layers.",
        "arxiv_id": "2508.07395"
    },
    "2508.06588": {
        "SCORE": 15,
        "ARXIVID": "2508.06588",
        "COMMENT": "The paper addresses the challenge of codebook collapse in graph representation learning, proposing a novel framework that enhances codebook utilization and token diversity.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Zian Zhai",
            "Fan Li",
            "Xingyu Tan",
            "Xiaoyang Wang",
            "Wenjie Zhang"
        ],
        "title": "Graph is a Natural Regularization: Revisiting Vector Quantization for Graph Representation Learning",
        "abstract": "Vector Quantization (VQ) has recently emerged as a promising approach for learning discrete representations of graph-structured data. However, a fundamental challenge, i.e., codebook collapse, remains underexplored in the graph domain, significantly limiting the expressiveness and generalization of graph tokens.In this paper, we present the first empirical study showing that codebook collapse consistently occurs when applying VQ to graph data, even with mitigation strategies proposed in vision or language domains. To understand why graph VQ is particularly vulnerable to collapse, we provide a theoretical analysis and identify two key factors: early assignment imbalances caused by redundancy in graph features and structural patterns, and self-reinforcing optimization loops in deterministic VQ. To address these issues, we propose RGVQ, a novel framework that integrates graph topology and feature similarity as explicit regularization signals to enhance codebook utilization and promote token diversity. RGVQ introduces soft assignments via Gumbel-Softmax reparameterization, ensuring that all codewords receive gradient updates. In addition, RGVQ incorporates a structure-aware contrastive regularization to penalize the token co-assignments among similar node pairs. Extensive experiments demonstrate that RGVQ substantially improves codebook utilization and consistently boosts the performance of state-of-the-art graph VQ backbones across multiple downstream tasks, enabling more expressive and transferable graph token representations.",
        "arxiv_id": "2508.06588"
    },
    "2508.07842": {
        "SCORE": 15,
        "ARXIVID": "2508.07842",
        "COMMENT": "The paper presents a cross-domain learning framework using a mixture of disentangled experts, relevant to model architecture.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Yutong Shen",
            "Hangxu Liu",
            "Penghui Liu",
            "Ruizhe Xia",
            "Tianyi Yao",
            "Yitong Sun",
            "Tongtong Feng"
        ],
        "title": "DETACH: Cross-domain Learning for Long-Horizon Tasks via Mixture of Disentangled Experts",
        "abstract": "Long-Horizon (LH) tasks in Human-Scene Interaction (HSI) are complex multi-step tasks that require continuous planning, sequential decision-making, and extended execution across domains to achieve the final goal. However, existing methods heavily rely on skill chaining by concatenating pre-trained subtasks, with environment observations and self-state tightly coupled, lacking the ability to generalize to new combinations of environments and skills, failing to complete various LH tasks across domains. To solve this problem, this paper presents DETACH, a cross-domain learning framework for LH tasks via biologically inspired dual-stream disentanglement. Inspired by the brain's \"where-what\" dual pathway mechanism, DETACH comprises two core modules: i) an environment learning module for spatial understanding, which captures object functions, spatial relationships, and scene semantics, achieving cross-domain transfer through complete environment-self disentanglement; ii) a skill learning module for task execution, which processes self-state information including joint degrees of freedom and motor patterns, enabling cross-skill transfer through independent motor pattern encoding. We conducted extensive experiments on various LH tasks in HSI scenes. Compared with existing methods, DETACH can achieve an average subtasks success rate improvement of 23% and average execution efficiency improvement of 29%.",
        "arxiv_id": "2508.07842"
    },
    "2508.06641": {
        "SCORE": 15,
        "ARXIVID": "2508.06641",
        "COMMENT": "The paper discusses fractal language modeling using Universal Sequence Maps, which is relevant to representation learning and foundational research in encoding procedures.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Jonas S Almeida",
            "Daniel E Russ",
            "Susana Vinga",
            "Ines Duarte",
            "Lee Mason",
            "Praphulla Bhawsar",
            "Aaron Ge",
            "Arlindo Oliveira",
            "Jeya Balaji Balasubramanian"
        ],
        "title": "Fractal Language Modelling by Universal Sequence Maps (USM)",
        "abstract": "Motivation: With the advent of Language Models using Transformers, popularized by ChatGPT, there is a renewed interest in exploring encoding procedures that numerically represent symbolic sequences at multiple scales and embedding dimensions. The challenge that encoding addresses is the need for mechanisms that uniquely retain contextual information about the succession of individual symbols, which can then be modeled by nonlinear formulations such as neural networks.   Context: Universal Sequence Maps(USM) are iterated functions that bijectively encode symbolic sequences onto embedded numerical spaces. USM is composed of two Chaos Game Representations (CGR), iterated forwardly and backwardly, that can be projected into the frequency domain (FCGR). The corresponding USM coordinates can be used to compute a Chebyshev distance metric as well as k-mer frequencies, without having to recompute the embedded numeric coordinates, and, paradoxically, allowing for non-integers values of k.   Results: This report advances the bijective fractal encoding by Universal Sequence Maps (USM) by resolving seeding biases affecting the iterated process. The resolution had two results, the first expected, the second an intriguing outcome: 1) full reconciliation of numeric positioning with sequence identity; and 2) uncovering the nature of USM as an efficient numeric process converging towards a steady state sequence embedding solution. We illustrate these results for genomic sequences because of the convenience of a planar representation defined by an alphabet with only 4 tokens (the 4 nucleotides). Nevertheless, the application to alphabet of arbitrary cardinality was found to be straightforward.",
        "arxiv_id": "2508.06641"
    },
    "2508.07675": {
        "SCORE": 15,
        "ARXIVID": "2508.07675",
        "COMMENT": "The paper presents a framework for semantic caching in LLMs, focusing on efficiency and theoretical foundations, which aligns with interests in model compression and efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Xutong Liu",
            "Baran Atalar",
            "Xiangxiang Dai",
            "Jinhang Zuo",
            "Siwei Wang",
            "John C. S. Lui",
            "Wei Chen",
            "Carlee Joe-Wong"
        ],
        "title": "Semantic Caching for Low-Cost LLM Serving: From Offline Learning to Online Adaptation",
        "abstract": "Large Language Models (LLMs) are revolutionizing how users interact with information systems, yet their high inference cost poses serious scalability and sustainability challenges. Caching inference responses, allowing them to be retrieved without another forward pass through the LLM, has emerged as one possible solution. Traditional exact-match caching, however, overlooks the semantic similarity between queries, leading to unnecessary recomputation. Semantic caching addresses this by retrieving responses based on semantic similarity, but introduces a fundamentally different cache eviction problem: one must account for mismatch costs between incoming queries and cached responses. Moreover, key system parameters, such as query arrival probabilities and serving costs, are often unknown and must be learned over time. Existing semantic caching methods are largely ad-hoc, lacking theoretical foundations and unable to adapt to real-world uncertainty. In this paper, we present a principled, learning-based framework for semantic cache eviction under unknown query and cost distributions. We formulate both offline optimization and online learning variants of the problem, and develop provably efficient algorithms with state-of-the-art guarantees. We also evaluate our framework on a synthetic dataset, showing that our proposed algorithms perform matching or superior performance compared with baselines.",
        "arxiv_id": "2508.07675"
    },
    "2508.06784": {
        "SCORE": 15,
        "ARXIVID": "2508.06784",
        "COMMENT": "The paper introduces a non-linear Tucker autoencoder for tensor-based learning, which aligns with interests in representation learning and model architecture.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Junjing Zheng",
            "Chengliang Song",
            "Weidong Jiang",
            "Xinyu Zhang"
        ],
        "title": "Mode-Aware Non-Linear Tucker Autoencoder for Tensor-based Unsupervised Learning",
        "abstract": "High-dimensional data, particularly in the form of high-order tensors, presents a major challenge in self-supervised learning. While MLP-based autoencoders (AE) are commonly employed, their dependence on flattening operations exacerbates the curse of dimensionality, leading to excessively large model sizes, high computational overhead, and challenging optimization for deep structural feature capture. Although existing tensor networks alleviate computational burdens through tensor decomposition techniques, most exhibit limited capability in learning non-linear relationships. To overcome these limitations, we introduce the Mode-Aware Non-linear Tucker Autoencoder (MA-NTAE). MA-NTAE generalized classical Tucker decomposition to a non-linear framework and employs a Pick-and-Unfold strategy, facilitating flexible per-mode encoding of high-order tensors via recursive unfold-encode-fold operations, effectively integrating tensor structural priors. Notably, MA-NTAE exhibits linear growth in computational complexity with tensor order and proportional growth with mode dimensions. Extensive experiments demonstrate MA-NTAE's performance advantages over standard AE and current tensor networks in compression and clustering tasks, which become increasingly pronounced for higher-order, higher-dimensional tensors.",
        "arxiv_id": "2508.06784"
    },
    "2508.07559": {
        "SCORE": 15,
        "ARXIVID": "2508.07559",
        "COMMENT": "The paper studies the approximation of high-dimensional PDEs using Barron spaces and shallow networks, which is relevant to foundational research in AI for Science.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Ziang Chen",
            "Liqiang Huang"
        ],
        "title": "Barron Space Representations for Elliptic PDEs with Homogeneous Boundary Conditions",
        "abstract": "We study the approximation complexity of high-dimensional second-order elliptic PDEs with homogeneous boundary conditions on the unit hypercube, within the framework of Barron spaces. Under the assumption that the coefficients belong to suitably defined Barron spaces, we prove that the solution can be efficiently approximated by two-layer neural networks, circumventing the curse of dimensionality. Our results demonstrate the expressive power of shallow networks in capturing high-dimensional PDE solutions under appropriate structural assumptions.",
        "arxiv_id": "2508.07559"
    },
    "2508.07329": {
        "SCORE": 15,
        "ARXIVID": "2508.07329",
        "COMMENT": "The paper addresses efficient deployment of MoE architectures using Hessian-Aware Quantization, which is relevant to model compression and efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Tuo Zhang",
            "Ning Li",
            "Xin Yuan",
            "Wenchao Xu",
            "Quan Chen",
            "Song Guo",
            "Haijun Zhang"
        ],
        "title": "Efficient Edge LLMs Deployment via HessianAware Quantization and CPU GPU Collaborative",
        "abstract": "With the breakthrough progress of large language models (LLMs) in natural language processing and multimodal tasks, efficiently deploying them on resource-constrained edge devices has become a critical challenge. The Mixture of Experts (MoE) architecture enhances model capacity through sparse activation, but faces two major difficulties in practical deployment: (1) The presence of numerous outliers in activation distributions leads to severe degradation in quantization accuracy for both activations and weights, significantly impairing inference performance; (2) Under limited memory, efficient offloading and collaborative inference of expert modules struggle to balance latency and throughput. To address these issues, this paper proposes an efficient MoE edge deployment scheme based on Hessian-Aware Quantization (HAQ) and CPU-GPU collaborative inference. First, by introducing smoothed Hessian matrix quantization, we achieve joint 8-bit quantization of activations and weights, which significantly alleviates the accuracy loss caused by outliers while ensuring efficient implementation on mainstream hardware. Second, we design an expert-level collaborative offloading and inference mechanism, which, combined with expert activation path statistics, enables efficient deployment and scheduling of expert modules between CPU and GPU, greatly reducing memory footprint and inference latency. Extensive experiments validate the effectiveness of our method on mainstream large models such as the OPT series and Mixtral 8*7B: on datasets like Wikitext2 and C4, the inference accuracy of the low-bit quantized model approaches that of the full-precision model, while GPU memory usage is reduced by about 60%, and inference latency is significantly improved.",
        "arxiv_id": "2508.07329"
    }
}