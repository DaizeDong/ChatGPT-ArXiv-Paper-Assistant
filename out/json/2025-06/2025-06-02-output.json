{
    "2505.24205": {
        "SCORE": 18,
        "ARXIVID": "2505.24205",
        "COMMENT": "The paper provides a theoretical study on the expressive power of Mixture-of-Experts (MoE), directly relevant to model architecture.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Mingze Wang",
            "Weinan E"
        ],
        "title": "On the Expressive Power of Mixture-of-Experts for Structured Complex Tasks",
        "abstract": "Mixture-of-experts networks (MoEs) have demonstrated remarkable efficiency in modern deep learning. Despite their empirical success, the theoretical foundations underlying their ability to model complex tasks remain poorly understood. In this work, we conduct a systematic study of the expressive power of MoEs in modeling complex tasks with two common structural priors: low-dimensionality and sparsity. For shallow MoEs, we prove that they can efficiently approximate functions supported on low-dimensional manifolds, overcoming the curse of dimensionality. For deep MoEs, we show that $\\cO(L)$-layer MoEs with $E$ experts per layer can approximate piecewise functions comprising $E^L$ pieces with compositional sparsity, i.e., they can exhibit an exponential number of structured tasks. Our analysis reveals the roles of critical architectural components and hyperparameters in MoEs, including the gating mechanism, expert networks, the number of experts, and the number of layers, and offers natural suggestions for MoE variants.",
        "arxiv_id": "2505.24205"
    },
    "2505.24665": {
        "SCORE": 18,
        "ARXIVID": "2505.24665",
        "COMMENT": "The paper introduces a method for learning geometry and topology via multi-chart flows, which is relevant to representation learning and emerging trends in foundational research.",
        "RELEVANCE": 9,
        "NOVELTY": 9,
        "authors": [
            "Hanlin Yu",
            "S{\\o}ren Hauberg",
            "Marcelo Hartmann",
            "Arto Klami",
            "Georgios Arvanitidis"
        ],
        "title": "Learning geometry and topology via multi-chart flows",
        "abstract": "Real world data often lie on low-dimensional Riemannian manifolds embedded in high-dimensional spaces. This motivates learning degenerate normalizing flows that map between the ambient space and a low-dimensional latent space. However, if the manifold has a non-trivial topology, it can never be correctly learned using a single flow. Instead multiple flows must be `glued together'. In this paper, we first propose the general training scheme for learning such a collection of flows, and secondly we develop the first numerical algorithms for computing geodesics on such manifolds. Empirically, we demonstrate that this leads to highly significant improvements in topology estimation.",
        "arxiv_id": "2505.24665"
    },
    "2505.24722": {
        "SCORE": 18,
        "ARXIVID": "2505.24722",
        "COMMENT": "The paper introduces HELM, a hyperbolic LLM with Mixture-of-Curvature Experts, relevant to LLM architecture innovations.",
        "RELEVANCE": 9,
        "NOVELTY": 9,
        "authors": [
            "Neil He",
            "Rishabh Anand",
            "Hiren Madhu",
            "Ali Maatouk",
            "Smita Krishnaswamy",
            "Leandros Tassiulas",
            "Menglin Yang",
            "Rex Ying"
        ],
        "title": "HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts",
        "abstract": "Large language models (LLMs) have shown great success in text modeling tasks across domains. However, natural language exhibits inherent semantic hierarchies and nuanced geometric structure, which current LLMs do not capture completely owing to their reliance on Euclidean operations. Recent studies have also shown that not respecting the geometry of token embeddings leads to training instabilities and degradation of generative capabilities. These findings suggest that shifting to non-Euclidean geometries can better align language models with the underlying geometry of text. We thus propose to operate fully in Hyperbolic space, known for its expansive, scale-free, and low-distortion properties. We thus introduce HELM, a family of HypErbolic Large Language Models, offering a geometric rethinking of the Transformer-based LLM that addresses the representational inflexibility, missing set of necessary operations, and poor scalability of existing hyperbolic LMs. We additionally introduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert operates in a distinct curvature space to encode more fine-grained geometric structure from text, as well as a dense model, HELM-D. For HELM-MICE, we further develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient, reduced-KV-cache training and inference. For both models, we develop essential hyperbolic equivalents of rotary positional encodings and RMS normalization. We are the first to train fully hyperbolic LLMs at billion-parameter scale, and evaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM problem-solving, general knowledge, and commonsense reasoning. Our results show consistent gains from our HELM architectures -- up to 4% -- over popular Euclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy and enhanced reasoning afforded by hyperbolic geometry in large-scale LM pretraining.",
        "arxiv_id": "2505.24722"
    },
    "2505.23917": {
        "SCORE": 17,
        "ARXIVID": "2505.23917",
        "COMMENT": "The paper introduces Representational Differences Explanations (RDX), a method for comparing learned representations, which aligns with representation learning by providing insights into how models encode information.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Neehar Kondapaneni",
            "Oisin Mac Aodha",
            "Pietro Perona"
        ],
        "title": "Representational Difference Explanations",
        "abstract": "We propose a method for discovering and visualizing the differences between two learned representations, enabling more direct and interpretable model comparisons. We validate our method, which we call Representational Differences Explanations (RDX), by using it to compare models with known conceptual differences and demonstrate that it recovers meaningful distinctions where existing explainable AI (XAI) techniques fail. Applied to state-of-the-art models on challenging subsets of the ImageNet and iNaturalist datasets, RDX reveals both insightful representational differences and subtle patterns in the data. Although comparison is a cornerstone of scientific analysis, current tools in machine learning, namely post hoc XAI methods, struggle to support model comparison effectively. Our work addresses this gap by introducing an effective and explainable tool for contrasting model representations.",
        "arxiv_id": "2505.23917"
    },
    "2505.24054": {
        "SCORE": 17,
        "ARXIVID": "2505.24054",
        "COMMENT": "The paper proposes a novel input-dependent gating mechanism for self-attention in Transformers, which is relevant to model architecture innovations.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Elpiniki Maria Lygizou",
            "M\\'onika Farsang",
            "Radu Grosu"
        ],
        "title": "Differential Gated Self-Attention",
        "abstract": "Transformers excel across a large variety of tasks but remain susceptible to corrupted inputs, since standard self-attention treats all query-key interactions uniformly. Inspired by lateral inhibition in biological neural circuits and building on the recent use by the Differential Transformer's use of two parallel softmax subtraction for noise cancellation, we propose Multihead Differential Gated Self-Attention (M-DGSA) that learns per-head input-dependent gating to dynamically suppress attention noise. Each head splits into excitatory and inhibitory branches whose dual softmax maps are fused by a sigmoid gate predicted from the token embedding, yielding a context-aware contrast enhancement. M-DGSA integrates seamlessly into existing Transformer stacks with minimal computational overhead. We evaluate on both vision and language benchmarks, demonstrating consistent robustness gains over vanilla Transformer, Vision Transformer, and Differential Transformer baselines. Our contributions are (i) a novel input-dependent gating mechanism for self-attention grounded in lateral inhibition, (ii) a principled synthesis of biological contrast-enhancement and self-attention theory, and (iii) comprehensive experiments demonstrating noise resilience and cross-domain applicability.",
        "arxiv_id": "2505.24054"
    },
    "2505.23808": {
        "SCORE": 17,
        "ARXIVID": "2505.23808",
        "COMMENT": "DenseLoRA enhances parameter efficiency in low-rank adaptation of LLMs, which is relevant to model compression and large language models.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Lin Mu",
            "Xiaoyu Wang",
            "Li Ni",
            "Yang Li",
            "Zhize Wu",
            "Peiquan Jin",
            "Yiwen Zhang"
        ],
        "title": "DenseLoRA: Dense Low-Rank Adaptation of Large Language Models",
        "abstract": "Low-rank adaptation (LoRA) has been developed as an efficient approach for adapting large language models (LLMs) by fine-tuning two low-rank matrices, thereby reducing the number of trainable parameters. However, prior research indicates that many of the weights in these matrices are redundant, leading to inefficiencies in parameter utilization. To address this limitation, we introduce Dense Low-Rank Adaptation (DenseLoRA), a novel approach that enhances parameter efficiency while achieving superior performance compared to LoRA. DenseLoRA builds upon the concept of representation fine-tuning, incorporating a single Encoder-Decoder to refine and compress hidden representations across all adaptation layers before applying adaptation. Instead of relying on two redundant low-rank matrices as in LoRA, DenseLoRA adapts LLMs through a dense low-rank matrix, improving parameter utilization and adaptation efficiency. We evaluate DenseLoRA on various benchmarks, showing that it achieves 83.8% accuracy with only 0.01% of trainable parameters, compared to LoRA's 80.8% accuracy with 0.70% of trainable parameters on LLaMA3-8B. Additionally, we conduct extensive experiments to systematically assess the impact of DenseLoRA's components on overall model performance. Code is available at https://github.com/mulin-ahu/DenseLoRA.",
        "arxiv_id": "2505.23808"
    },
    "2505.24623": {
        "SCORE": 17,
        "ARXIVID": "2505.24623",
        "COMMENT": "The paper introduces Hyperbolic Dataset Distillation (HDD), which is relevant to representation learning and model compression by addressing dataset distillation in hyperbolic space.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Wenyuan Li",
            "Guang Li",
            "Keisuke Maeda",
            "Takahiro Ogawa",
            "Miki Haseyama"
        ],
        "title": "Hyperbolic Dataset Distillation",
        "abstract": "To address the computational and storage challenges posed by large-scale datasets in deep learning, dataset distillation has been proposed to synthesize a compact dataset that replaces the original while maintaining comparable model performance. Unlike optimization-based approaches that require costly bi-level optimization, distribution matching (DM) methods improve efficiency by aligning the distributions of synthetic and original data, thereby eliminating nested optimization. DM achieves high computational efficiency and has emerged as a promising solution. However, existing DM methods, constrained to Euclidean space, treat data as independent and identically distributed points, overlooking complex geometric and hierarchical relationships. To overcome this limitation, we propose a novel hyperbolic dataset distillation method, termed HDD. Hyperbolic space, characterized by negative curvature and exponential volume growth with distance, naturally models hierarchical and tree-like structures. HDD embeds features extracted by a shallow network into the Lorentz hyperbolic space, where the discrepancy between synthetic and original data is measured by the hyperbolic (geodesic) distance between their centroids. By optimizing this distance, the hierarchical structure is explicitly integrated into the distillation process, guiding synthetic samples to gravitate towards the root-centric regions of the original data distribution while preserving their underlying geometric characteristics. Furthermore, we find that pruning in hyperbolic space requires only 20% of the distilled core set to retain model performance, while significantly improving training stability. Notably, HDD is seamlessly compatible with most existing DM methods, and extensive experiments on different datasets validate its effectiveness.",
        "arxiv_id": "2505.24623"
    },
    "2505.24060": {
        "SCORE": 17,
        "ARXIVID": "2505.24060",
        "COMMENT": "The paper provides an analytical case study on the inductive biases of neural networks on Boolean data, which is relevant to representation learning and emerging trends.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Chris Mingard",
            "Lukas Seier",
            "Niclas G\\\"oring",
            "Andrei-Vlad Badelita",
            "Charles London",
            "Ard Louis"
        ],
        "title": "Characterising the Inductive Biases of Neural Networks on Boolean Data",
        "abstract": "Deep neural networks are renowned for their ability to generalise well across diverse tasks, even when heavily overparameterized. Existing works offer only partial explanations (for example, the NTK-based task-model alignment explanation neglects feature learning). Here, we provide an end-to-end, analytically tractable case study that links a network's inductive prior, its training dynamics including feature learning, and its eventual generalisation. Specifically, we exploit the one-to-one correspondence between depth-2 discrete fully connected networks and disjunctive normal form (DNF) formulas by training on Boolean functions. Under a Monte Carlo learning algorithm, our model exhibits predictable training dynamics and the emergence of interpretable features. This framework allows us to trace, in detail, how inductive bias and feature formation drive generalisation.",
        "arxiv_id": "2505.24060"
    },
    "2505.23949": {
        "SCORE": 17,
        "ARXIVID": "2505.23949",
        "COMMENT": "The paper presents a novel approach to network pruning with transposable N:M sparsity, which is relevant to model compression.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Xiang Meng",
            "Mehdi Makni",
            "Rahul Mazumder"
        ],
        "title": "TSENOR: Highly-Efficient Algorithm for Finding Transposable N:M Sparse Masks",
        "abstract": "Network pruning reduces the computational requirements of large neural networks, with N:M sparsity -- retaining only N out of every M consecutive weights -- offering a compelling balance between compressed model quality and hardware acceleration. However, N:M sparsity only accelerates forward-pass computations, as N:M patterns are not preserved during matrix transposition, limiting efficiency during training where both passes are computationally intensive. While transposable N:M sparsity has been proposed to address this limitation, existing methods for finding transposable N:M sparse masks either fail to scale to large models or are restricted to M=4 which results in suboptimal compression-accuracy trade-off. We introduce an efficient solver for transposable N:M masks that scales to billion-parameter models. We formulate mask generation as optimal transport problems and solve through entropy regularization and Dykstra's algorithm, followed by a rounding procedure. Our tensor-based implementation exploits GPU parallelism, achieving up to 100x speedup with only 1-10% error compared to existing methods. Our approach can be integrated with layer-wise N:M pruning frameworks including Wanda, SparseGPT and ALPS to produce transposable N:M sparse models with arbitrary N:M values. Experiments show that LLaMA3.2-8B with transposable 16:32 sparsity maintains performance close to its standard N:M counterpart and outperforms standard 2:4 sparse model, showing the practical value of our approach.",
        "arxiv_id": "2505.23949"
    },
    "2505.23807": {
        "SCORE": 17,
        "ARXIVID": "2505.23807",
        "COMMENT": "The paper proposes Dynamic Layerwise Pruning for LLMs, relevant to model compression.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Yuli Chen",
            "Bo Cheng",
            "Jiale Han",
            "Yingying Zhang",
            "Yingting Li",
            "Shuhao Zhang"
        ],
        "title": "DLP: Dynamic Layerwise Pruning in Large Language Models",
        "abstract": "Pruning has recently been widely adopted to reduce the parameter scale and improve the inference efficiency of Large Language Models (LLMs). Mainstream pruning techniques often rely on uniform layerwise pruning strategies, which can lead to severe performance degradation at high sparsity levels. Recognizing the varying contributions of different layers in LLMs, recent studies have shifted their focus toward non-uniform layerwise pruning. However, these approaches often rely on pre-defined values, which can result in suboptimal performance. To overcome these limitations, we propose a novel method called Dynamic Layerwise Pruning (DLP). This approach adaptively determines the relative importance of each layer by integrating model weights with input activation information, assigning pruning rates accordingly. Experimental results show that DLP effectively preserves model performance at high sparsity levels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the perplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7% compared to state-of-the-art methods. Moreover, DLP is compatible with various existing LLM compression techniques and can be seamlessly integrated into Parameter-Efficient Fine-Tuning (PEFT). We release the code at https://github.com/ironartisan/DLP to facilitate future research.",
        "arxiv_id": "2505.23807"
    },
    "2505.24037": {
        "SCORE": 17,
        "ARXIVID": "2505.24037",
        "COMMENT": "The paper proposes a novel method, SEFT, for fine-tuning sparse LLMs, which is relevant to model compression and efficiency.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Qiao Xiao",
            "Alan Ansell",
            "Boqian Wu",
            "Lu Yin",
            "Mykola Pechenizkiy",
            "Shiwei Liu",
            "Decebal Constantin Mocanu"
        ],
        "title": "Leave it to the Specialist: Repair Sparse LLMs with Sparse Fine-Tuning via Sparsity Evolution",
        "abstract": "Large language models (LLMs) have achieved remarkable success across various tasks but face deployment challenges due to their massive computational demands. While post-training pruning methods like SparseGPT and Wanda can effectively reduce the model size, but struggle to maintain model performance at high sparsity levels, limiting their utility for downstream tasks. Existing fine-tuning methods, such as full fine-tuning and LoRA, fail to preserve sparsity as they require updating the whole dense metrics, not well-suited for sparse LLMs. In this paper, we propose Sparsity Evolution Fine-Tuning (SEFT), a novel method designed specifically for sparse LLMs. SEFT dynamically evolves the sparse topology of pruned models during fine-tuning, while preserving the overall sparsity throughout the process. The strengths of SEFT lie in its ability to perform task-specific adaptation through a weight drop-and-grow strategy, enabling the pruned model to self-adapt its sparse connectivity pattern based on the target dataset. Furthermore, a sensitivity-driven pruning criterion is employed to ensure that the desired sparsity level is consistently maintained throughout fine-tuning. Our experiments on various LLMs, including LLaMA families, DeepSeek, and Mistral, across a diverse set of benchmarks demonstrate that SEFT achieves stronger performance while offering superior memory and time efficiency compared to existing baselines. Our code is publicly available at: https://github.com/QiaoXiao7282/SEFT.",
        "arxiv_id": "2505.24037"
    },
    "2505.24275": {
        "SCORE": 17,
        "ARXIVID": "2505.24275",
        "COMMENT": "The paper introduces GradPower, a gradient-transformation technique for accelerating language model pre-training, which is relevant to efficiency improvements in LLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Mingze Wang",
            "Jinbo Wang",
            "Jiaqi Zhang",
            "Wei Wang",
            "Peng Pei",
            "Xunliang Cai",
            "Weinan E",
            "Lei Wu"
        ],
        "title": "GradPower: Powering Gradients for Faster Language Model Pre-Training",
        "abstract": "We propose GradPower, a lightweight gradient-transformation technique for accelerating language model pre-training. Given a gradient vector $g=(g_i)_i$, GradPower first applies the elementwise sign-power transformation: $\\varphi_p(g)=({\\rm sign}(g_i)|g_i|^p)_{i}$ for a fixed $p>0$, and then feeds the transformed gradient into a base optimizer. Notably, GradPower requires only a single-line code change and no modifications to the base optimizer's internal logic, including the hyperparameters. When applied to Adam (termed AdamPower), GradPower consistently achieves lower terminal loss across diverse architectures (LLaMA, Qwen2MoE), parameter scales (66M to 2B), datasets (C4, OpenWebText), and learning-rate schedules (cosine, warmup-stable-decay). The most pronounced gains are observed when training modern mixture-of-experts models with warmup-stable-decay schedules. GradPower also integrates seamlessly with other state-of-the-art optimizers, such as Muon, yielding further improvements. Finally, we provide theoretical analyses that reveal the underlying mechanism of GradPower and highlights the influence of gradient noise.",
        "arxiv_id": "2505.24275"
    },
    "2505.24179": {
        "SCORE": 17,
        "ARXIVID": "2505.24179",
        "COMMENT": "The paper proposes SALE, a sparse attention method for LLMs, focusing on efficiency improvements through quantization and sparse attention, aligning with the model compression criterion.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Xiaodong Ji",
            "Hailin Zhang",
            "Fangcheng Fu",
            "Bin Cui"
        ],
        "title": "SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling",
        "abstract": "Many advanced Large Language Model (LLM) applications require long-context processing, but the self-attention module becomes a bottleneck during the prefilling stage of inference due to its quadratic time complexity with respect to sequence length. Existing sparse attention methods accelerate attention computation by skipping less significant regions of the attention map. However, these approaches typically perform coarse-grained inspection of the attention map, rendering considerable loss in model accuracy. In this paper, we propose SALE, a fine-grained sparse attention method that accelerates the long-context prefilling stage of LLM with negligible loss in model accuracy. SALE achieves fast and accurate fine-grained attention weight estimation through 4-bit quantized query-key products, followed by block-sparse attention to accelerate prefilling computations. For importance evaluation for query-key pairs, we adopt our Relative Attention Score metric, which offers significantly higher efficiency within our framework. We implement a custom CUDA kernel optimized for our approach for hardware efficiency, reducing the additional overhead to approximately 11% of the full attention latency. Notably, SALE requires no parameter training and can be seamlessly integrated into existing systems with trivial code modifications. Experiments on long-context benchmarks demonstrate that our method outperforms existing approaches in accuracy-efficiency trade-offs, achieving at least 3.36x speedups on Llama-3.1-8B for sequences longer than 64K while maintaining model quality.",
        "arxiv_id": "2505.24179"
    },
    "2505.24593": {
        "SCORE": 17,
        "ARXIVID": "2505.24593",
        "COMMENT": "The paper explores the interpretability of Mixture-of-Experts (MoE) models, providing insights into their architecture and efficiency, which aligns with the model architecture criterion.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Junzhuo Li",
            "Bo Wang",
            "Xiuze Zhou",
            "Peijie Jiang",
            "Jia Liu",
            "Xuming Hu"
        ],
        "title": "Decoding Knowledge Attribution in Mixture-of-Experts: A Framework of Basic-Refinement Collaboration and Efficiency Analysis",
        "abstract": "The interpretability of Mixture-of-Experts (MoE) models, especially those with heterogeneous designs, remains underexplored. Existing attribution methods for dense models fail to capture dynamic routing-expert interactions in sparse MoE architectures. To address this issue, we propose a cross-level attribution algorithm to analyze sparse MoE architectures (Qwen 1.5-MoE, OLMoE, Mixtral-8x7B) against dense models (Qwen 1.5-7B, Llama-7B, Mixtral-7B). Results show MoE models achieve 37% higher per-layer efficiency via a \"mid-activation, late-amplification\" pattern: early layers screen experts, while late layers refine knowledge collaboratively. Ablation studies reveal a \"basic-refinement\" framework--shared experts handle general tasks (entity recognition), while routed experts specialize in domain-specific processing (geographic attributes). Semantic-driven routing is evidenced by strong correlations between attention heads and experts (r=0.68), enabling task-aware coordination. Notably, architectural depth dictates robustness: deep Qwen 1.5-MoE mitigates expert failures (e.g., 43% MRR drop in geographic tasks when blocking top-10 experts) through shared expert redundancy, whereas shallow OLMoE suffers severe degradation (76% drop). Task sensitivity further guides design: core-sensitive tasks (geography) require concentrated expertise, while distributed-tolerant tasks (object attributes) leverage broader participation. These insights advance MoE interpretability, offering principles to balance efficiency, specialization, and robustness.",
        "arxiv_id": "2505.24593"
    },
    "2505.24009": {
        "SCORE": 17,
        "ARXIVID": "2505.24009",
        "COMMENT": "The paper investigates the diversity of Transformer layers and their impact on parameter scaling laws, providing insights into model architecture and theoretical understanding of Transformers.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Hidetaka Kamigaito",
            "Ying Zhang",
            "Jingun Kwon",
            "Katsuhiko Hayashi",
            "Manabu Okumura",
            "Taro Watanabe"
        ],
        "title": "Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws",
        "abstract": "Transformers deliver outstanding performance across a wide range of tasks and are now a dominant backbone architecture for large language models (LLMs). Their task-solving performance is improved by increasing parameter size, as shown in the recent studies on parameter scaling laws. Although recent mechanistic-interpretability studies have deepened our understanding of the internal behavior of Transformers by analyzing their residual stream, the relationship between these internal mechanisms and the parameter scaling laws remains unclear. To bridge this gap, we focus on layers and their size, which mainly decide the parameter size of Transformers. For this purpose, we first theoretically investigate the layers within the residual stream through a bias-diversity decomposition. The decomposition separates (i) bias, the error of each layer's output from the ground truth, and (ii) diversity, which indicates how much the outputs of each layer differ from each other. Analyzing Transformers under this theory reveals that performance improves when individual layers make predictions close to the correct answer and remain mutually diverse. We show that diversity becomes especially critical when individual layers' outputs are far from the ground truth. Finally, we introduce an information-theoretic diversity and show our main findings that adding layers enhances performance only when those layers behave differently, i.e., are diverse. We also reveal the performance gains from increasing the number of layers exhibit submodularity: marginal improvements diminish as additional layers increase, mirroring the logarithmic convergence predicted by the parameter scaling laws. Experiments on multiple semantic-understanding tasks with various LLMs empirically confirm the theoretical properties derived in this study.",
        "arxiv_id": "2505.24009"
    },
    "2505.24357": {
        "SCORE": 17,
        "ARXIVID": "2505.24357",
        "COMMENT": "The paper focuses on KV cache compression, a relevant topic under model compression, introducing a novel method for low-rank compression with minimal performance loss.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Xianglong Yan",
            "Zhiteng Li",
            "Tianao Zhang",
            "Linghe Kong",
            "Yulun Zhang",
            "Xiaokang Yang"
        ],
        "title": "ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline Calibration",
        "abstract": "Large language models (LLMs) have achieved remarkable performance, yet their capability on long-context reasoning is often constrained by the excessive memory required to store the Key-Value (KV) cache. This makes KV cache compression an essential step toward enabling efficient long-context reasoning. Recent methods have explored reducing the hidden dimensions of the KV cache, but many introduce additional computation through projection layers or suffer from significant performance degradation under high compression ratios. To address these challenges, we propose ReCalKV, a post-training KV cache compression method that reduces the hidden dimensions of the KV cache. We develop distinct compression strategies for Keys and Values based on their different roles and varying importance in the attention mechanism. For Keys, we propose Head-wise Similarity-aware Reordering (HSR), which clusters similar heads and applies grouped SVD to the key projection matrix, reducing additional computation while preserving accuracy. For Values, we propose Offline Calibration and Matrix Fusion (OCMF) to preserve accuracy without extra computational overhead. Experiments show that ReCalKV outperforms existing low-rank compression methods, achieving high compression ratios with minimal performance loss. Code is available at: https://github.com/XIANGLONGYAN/ReCalKV.",
        "arxiv_id": "2505.24357"
    },
    "2505.24134": {
        "SCORE": 17,
        "ARXIVID": "2505.24134",
        "COMMENT": "The paper provides a mathematical perspective on contrastive learning, focusing on representation learning and introducing novel probabilistic loss functions and metrics.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Ricardo Baptista",
            "Andrew M. Stuart",
            "Son Tran"
        ],
        "title": "A Mathematical Perspective On Contrastive Learning",
        "abstract": "Multimodal contrastive learning is a methodology for linking different data modalities; the canonical example is linking image and text data. The methodology is typically framed as the identification of a set of encoders, one for each modality, that align representations within a common latent space. In this work, we focus on the bimodal setting and interpret contrastive learning as the optimization of (parameterized) encoders that define conditional probability distributions, for each modality conditioned on the other, consistent with the available data. This provides a framework for multimodal algorithms such as crossmodal retrieval, which identifies the mode of one of these conditional distributions, and crossmodal classification, which is similar to retrieval but includes a fine-tuning step to make it task specific.   The framework we adopt also gives rise to crossmodal generative models. This probabilistic perspective suggests two natural generalizations of contrastive learning: the introduction of novel probabilistic loss functions, and the use of alternative metrics for measuring alignment in the common latent space. We study these generalizations of the classical approach in the multivariate Gaussian setting. In this context we view the latent space identification as a low-rank matrix approximation problem. This allows us to characterize the capabilities of loss functions and alignment metrics to approximate natural statistics, such as conditional means and covariances; doing so yields novel variants on contrastive learning algorithms for specific mode-seeking and for generative tasks. The framework we introduce is also studied through numerical experiments on multivariate Gaussians, the labeled MNIST dataset, and on a data assimilation application arising in oceanography.",
        "arxiv_id": "2505.24134"
    },
    "2505.24684": {
        "SCORE": 17,
        "ARXIVID": "2505.24684",
        "COMMENT": "The paper explores implicit inductive bias in factorized VAEs, contributing to representation learning by uncovering disentangling granularity as a bias influencing disentanglement performance.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Zihao Chen",
            "Yu Xiang",
            "Wenyong Wang"
        ],
        "title": "Disentangling Granularity: An Implicit Inductive Bias in Factorized VAEs",
        "abstract": "Despite the success in learning semantically meaningful, unsupervised disentangled representations, variational autoencoders (VAEs) and their variants face a fundamental theoretical challenge: substantial evidence indicates that unsupervised disentanglement is unattainable without implicit inductive bias, yet such bias remains elusive. In this work, we focus on exploring the implicit inductive bias that drive disentanglement in VAEs with factorization priors. By analyzing the total correlation in \\b{eta}-TCVAE, we uncover a crucial implicit inductive bias called disentangling granularity, which leads to the discovery of an interesting \"V\"-shaped optimal Evidence Lower Bound (ELBO) trajectory within the parameter space. This finding is validated through over 100K experiments using factorized VAEs and our newly proposed model, \\b{eta}-STCVAE. Notably, experimental results reveal that conventional factorized VAEs, constrained by fixed disentangling granularity, inherently tend to disentangle low-complexity feature. Whereas, appropriately tuning disentangling granularity, as enabled by \\b{eta}-STCVAE, broadens the range of disentangled representations, allowing for the disentanglement of high-complexity features. Our findings unveil that disentangling granularity as an implicit inductive bias in factorized VAEs influence both disentanglement performance and the inference of the ELBO, offering fresh insights into the interpretability and inherent biases of VAEs.",
        "arxiv_id": "2505.24684"
    },
    "2505.24133": {
        "SCORE": 16,
        "ARXIVID": "2505.24133",
        "COMMENT": "The paper proposes a novel KV cache compression method for reasoning models, relevant to model compression.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Zefan Cai",
            "Wen Xiao",
            "Hanshi Sun",
            "Cheng Luo",
            "Yikai Zhang",
            "Ke Wan",
            "Yucheng Li",
            "Yeyang Zhou",
            "Li-Wen Chang",
            "Jiuxiang Gu",
            "Zhen Dong",
            "Anima Anandkumar",
            "Abedelkadir Asi",
            "Junjie Hu"
        ],
        "title": "R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration",
        "abstract": "Reasoning models have demonstrated impressive performance in self-reflection and chain-of-thought reasoning. However, they often produce excessively long outputs, leading to prohibitively large key-value (KV) caches during inference. While chain-of-thought inference significantly improves performance on complex reasoning tasks, it can also lead to reasoning failures when deployed with existing KV cache compression approaches. To address this, we propose Redundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel method specifically targeting redundant tokens in reasoning models. Our method preserves nearly 100% of the full KV cache performance using only 10% of the KV cache, substantially outperforming existing KV cache baselines, which reach only 60% of the performance. Remarkably, R-KV even achieves 105% of full KV cache performance with 16% of the KV cache. This KV-cache reduction also leads to a 90% memory saving and a 6.6X throughput over standard chain-of-thought reasoning inference. Experimental results show that R-KV consistently outperforms existing KV cache compression baselines across two mathematical reasoning datasets.",
        "arxiv_id": "2505.24133"
    },
    "2505.24843": {
        "SCORE": 16,
        "ARXIVID": "2505.24843",
        "COMMENT": "The paper introduces a method for robustness to spurious correlations using invariant data pairs, which relates to representation learning by focusing on invariant representations.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Ruqi Bai",
            "Yao Ji",
            "Zeyu Zhou",
            "David I. Inouye"
        ],
        "title": "From Invariant Representations to Invariant Data: Provable Robustness to Spurious Correlations via Noisy Counterfactual Matching",
        "abstract": "Spurious correlations can cause model performance to degrade in new environments. Prior causality-inspired works aim to learn invariant representations (e.g., IRM) but typically underperform empirical risk minimization (ERM). Recent alternatives improve robustness by leveraging test-time data, but such data may be unavailable in practice. To address these issues, we take a data-centric approach by leveraging invariant data pairs, pairs of samples that would have the same prediction with the optimally robust classifier. We prove that certain counterfactual pairs will naturally satisfy this invariance property and introduce noisy counterfactual matching (NCM), a simple constraint-based method for leveraging invariant pairs for enhanced robustness, even with a small set of noisy pairs-in the ideal case, each pair can eliminate one spurious feature. For linear causal models, we prove that the test domain error can be upper bounded by the in-domain error and a term that depends on the counterfactuals' diversity and quality. We validate on a synthetic dataset and demonstrate on real-world benchmarks that linear probing on a pretrained backbone improves robustness.",
        "arxiv_id": "2505.24843"
    },
    "2505.23866": {
        "SCORE": 15,
        "ARXIVID": "2505.23866",
        "COMMENT": "The paper discusses Sharpness-Aware Minimization (SAM) and its variant CSAM, which are relevant to representation learning as they provide insights into training dynamics and model calibration.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Chengli Tan",
            "Yubo Zhou",
            "Haishan Ye",
            "Guang Dai",
            "Junmin Liu",
            "Zengjie Song",
            "Jiangshe Zhang",
            "Zixiang Zhao",
            "Yunda Hao",
            "Yong Xu"
        ],
        "title": "Towards Understanding The Calibration Benefits of Sharpness-Aware Minimization",
        "abstract": "Deep neural networks have been increasingly used in safety-critical applications such as medical diagnosis and autonomous driving. However, many studies suggest that they are prone to being poorly calibrated and have a propensity for overconfidence, which may have disastrous consequences. In this paper, unlike standard training such as stochastic gradient descent, we show that the recently proposed sharpness-aware minimization (SAM) counteracts this tendency towards overconfidence. The theoretical analysis suggests that SAM allows us to learn models that are already well-calibrated by implicitly maximizing the entropy of the predictive distribution. Inspired by this finding, we further propose a variant of SAM, coined as CSAM, to ameliorate model calibration. Extensive experiments on various datasets, including ImageNet-1K, demonstrate the benefits of SAM in reducing calibration error. Meanwhile, CSAM performs even better than SAM and consistently achieves lower calibration error than other approaches",
        "arxiv_id": "2505.23866"
    },
    "2505.24048": {
        "SCORE": 15,
        "ARXIVID": "2505.24048",
        "COMMENT": "NeuronTune addresses spurious bias in neural networks by intervening in the model's internal decision process, which is relevant to representation learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Guangtao Zheng",
            "Wenqian Ye",
            "Aidong Zhang"
        ],
        "title": "NeuronTune: Towards Self-Guided Spurious Bias Mitigation",
        "abstract": "Deep neural networks often develop spurious bias, reliance on correlations between non-essential features and classes for predictions. For example, a model may identify objects based on frequently co-occurring backgrounds rather than intrinsic features, resulting in degraded performance on data lacking these correlations. Existing mitigation approaches typically depend on external annotations of spurious correlations, which may be difficult to obtain and are not relevant to the spurious bias in a model. In this paper, we take a step towards self-guided mitigation of spurious bias by proposing NeuronTune, a post hoc method that directly intervenes in a model's internal decision process. Our method probes in a model's latent embedding space to identify and regulate neurons that lead to spurious prediction behaviors. We theoretically justify our approach and show that it brings the model closer to an unbiased one. Unlike previous methods, NeuronTune operates without requiring spurious correlation annotations, making it a practical and effective tool for improving model robustness. Experiments across different architectures and data modalities demonstrate that our method significantly mitigates spurious bias in a self-guided way.",
        "arxiv_id": "2505.24048"
    },
    "2505.23854": {
        "SCORE": 15,
        "ARXIVID": "2505.23854",
        "COMMENT": "The paper evaluates uncertainty estimation in LLMs, focusing on Mixture-of-Experts architectures, which is relevant to large language models and model architecture.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Linwei Tao",
            "Yi-Fan Yeh",
            "Minjing Dong",
            "Tao Huang",
            "Philip Torr",
            "Chang Xu"
        ],
        "title": "Revisiting Uncertainty Estimation and Calibration of Large Language Models",
        "abstract": "As large language models (LLMs) are increasingly deployed in high-stakes applications, robust uncertainty estimation is essential for ensuring the safe and trustworthy deployment of LLMs. We present the most comprehensive study to date of uncertainty estimation in LLMs, evaluating 80 models spanning open- and closed-source families, dense and Mixture-of-Experts (MoE) architectures, reasoning and non-reasoning modes, quantization variants and parameter scales from 0.6B to 671B. Focusing on three representative black-box single-pass methods, including token probability-based uncertainty (TPU), numerical verbal uncertainty (NVU), and linguistic verbal uncertainty (LVU), we systematically evaluate uncertainty calibration and selective classification using the challenging MMLU-Pro benchmark, which covers both reasoning-intensive and knowledge-based tasks. Our results show that LVU consistently outperforms TPU and NVU, offering stronger calibration and discrimination while being more interpretable. We also find that high accuracy does not imply reliable uncertainty, and that model scale, post-training, reasoning ability and quantization all influence estimation performance. Notably, LLMs exhibit better uncertainty estimates on reasoning tasks than on knowledge-heavy ones, and good calibration does not necessarily translate to effective error ranking. These findings highlight the need for multi-perspective evaluation and position LVU as a practical tool for improving the reliability of LLMs in real-world settings.",
        "arxiv_id": "2505.23854"
    },
    "2505.24254": {
        "SCORE": 15,
        "ARXIVID": "2505.24254",
        "COMMENT": "The paper proposes Progressive Neural Collapse (ProNC) for continual learning, which is relevant to representation learning and model architecture.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Zheng Wang",
            "Wanhao Yu",
            "Li Yang",
            "Sen Lin"
        ],
        "title": "Rethinking Continual Learning with Progressive Neural Collapse",
        "abstract": "Continual Learning (CL) seeks to build an agent that can continuously learn a sequence of tasks, where a key challenge, namely Catastrophic Forgetting, persists due to the potential knowledge interference among different tasks. On the other hand, deep neural networks (DNNs) are shown to converge to a terminal state termed Neural Collapse during training, where all class prototypes geometrically form a static simplex equiangular tight frame (ETF). These maximally and equally separated class prototypes make the ETF an ideal target for model learning in CL to mitigate knowledge interference. Thus inspired, several studies have emerged very recently to leverage a fixed global ETF in CL, which however suffers from key drawbacks, such as impracticability and limited performance.To address these challenges and fully unlock the potential of ETF in CL, we propose Progressive Neural Collapse (ProNC), a novel framework that completely removes the need of a fixed global ETF in CL. Specifically, ProNC progressively expands the ETF target in a principled way by adding new class prototypes as vertices for new tasks, ensuring maximal separability across all encountered classes with minimal shifts from the previous ETF. We next develop a new CL framework by plugging ProNC into commonly used CL algorithm designs, where distillation is further leveraged to balance between target shifting for old classes and target aligning for new classes. Extensive experiments show that our approach significantly outperforms related baselines while maintaining superior flexibility, simplicity, and efficiency.",
        "arxiv_id": "2505.24254"
    },
    "2505.24244": {
        "SCORE": 15,
        "ARXIVID": "2505.24244",
        "COMMENT": "The paper explores information flow in Mamba SSM-based language models, relevant to understanding LLM behavior.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Nir Endy",
            "Idan Daniel Grosbard",
            "Yuval Ran-Milo",
            "Yonatan Slutzky",
            "Itay Tshuva",
            "Raja Giryes"
        ],
        "title": "Mamba Knockout for Unraveling Factual Information Flow",
        "abstract": "This paper investigates the flow of factual information in Mamba State-Space Model (SSM)-based language models. We rely on theoretical and empirical connections to Transformer-based architectures and their attention mechanisms. Exploiting this relationship, we adapt attentional interpretability techniques originally developed for Transformers--specifically, the Attention Knockout methodology--to both Mamba-1 and Mamba-2. Using them we trace how information is transmitted and localized across tokens and layers, revealing patterns of subject-token information emergence and layer-wise dynamics. Notably, some phenomena vary between mamba models and Transformer based models, while others appear universally across all models inspected--hinting that these may be inherent to LLMs in general. By further leveraging Mamba's structured factorization, we disentangle how distinct \"features\" either enable token-to-token information exchange or enrich individual tokens, thus offering a unified lens to understand Mamba internal operations.",
        "arxiv_id": "2505.24244"
    },
    "2505.24579": {
        "SCORE": 15,
        "ARXIVID": "2505.24579",
        "COMMENT": "The paper introduces an adaptive correction approach for Fourier Neural Operators, which is relevant to AI for Science as it addresses foundational issues in modeling physical systems.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Chaoyu Liu",
            "Yangming Li",
            "Zhongying Deng",
            "Chris Budd",
            "Carola-Bibiane Sch\\\"onlieb"
        ],
        "title": "Conservation-preserved Fourier Neural Operator through Adaptive Correction",
        "abstract": "Fourier Neural Operators (FNOs) have recently emerged as a promising and efficient approach for learning the numerical solutions to partial differential equations (PDEs) from data. However, standard FNO often fails to preserve key conservation laws, such as mass conservation, momentum conservation, norm conservation, etc., which are crucial for accurately modeling physical systems. Existing methods for incorporating these conservation laws into Fourier neural operators are achieved by designing related loss function or incorporating post-processing method at the training time. None of them can both exactly and adaptively correct the outputs to satisfy conservation laws, and our experiments show that these methods can lead to inferior performance while preserving conservation laws. In this work, we propose a novel adaptive correction approach to ensure the conservation of fundamental quantities. Our method introduces a learnable matrix to adaptively adjust the solution to satisfy the conservation law during training. It ensures that the outputs exactly satisfy the goal conservation law and allow for more flexibility and adaptivity for the model to correct the outputs. We theoretically show that applying our adaptive correction to an unconstrained FNO yields a solution with data loss no worse than that of the best conservation-satisfying FNO. We compare our approach with existing methods on a range of representative PDEs. Experiment results show that our method consistently outperform other methods.",
        "arxiv_id": "2505.24579"
    },
    "2505.24544": {
        "SCORE": 15,
        "ARXIVID": "2505.24544",
        "COMMENT": "The paper presents a cross-attention-based speculative decoding model, which is relevant to model architecture innovations in LLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Wei Zhong",
            "Manasa Bharadwaj",
            "Yixiao Wang",
            "Nikhil Verma",
            "Yipeng Ji",
            "Chul Lee"
        ],
        "title": "Cross-Attention Speculative Decoding",
        "abstract": "Speculative decoding (SD) is a widely adopted approach for accelerating inference in large language models (LLMs), particularly when the draft and target models are well aligned. However, state-of-the-art SD methods typically rely on tightly coupled, self-attention-based Transformer decoders, often augmented with auxiliary pooling or fusion layers. This coupling makes them increasingly complex and harder to generalize across different models. We present Budget EAGLE (Beagle), the first, to our knowledge, cross-attention-based Transformer decoder SD model that achieves performance on par with leading self-attention SD models (EAGLE-v2) while eliminating the need for pooling or auxiliary components, simplifying the architecture, improving training efficiency, and maintaining stable memory usage during training-time simulation. To enable effective training of this novel architecture, we propose Two-Stage Block-Attention Training, a new method that achieves training stability and convergence efficiency in block-level attention scenarios. Extensive experiments across multiple LLMs and datasets show that Beagle achieves competitive inference speedups and higher training efficiency than EAGLE-v2, offering a strong alternative for architectures in speculative decoding.",
        "arxiv_id": "2505.24544"
    },
    "2505.24541": {
        "SCORE": 15,
        "ARXIVID": "2505.24541",
        "COMMENT": "The paper introduces Mixpert, a mixture-of-vision-experts architecture, which is relevant to model architecture innovations, particularly in multimodal learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Xin He",
            "Xumeng Han",
            "Longhui Wei",
            "Lingxi Xie",
            "Qi Tian"
        ],
        "title": "Mixpert: Mitigating Multimodal Learning Conflicts with Efficient Mixture-of-Vision-Experts",
        "abstract": "Multimodal large language models (MLLMs) require a nuanced interpretation of complex image information, typically leveraging a vision encoder to perceive various visual scenarios. However, relying solely on a single vision encoder to handle diverse task domains proves difficult and inevitably leads to conflicts. Recent work enhances data perception by directly integrating multiple domain-specific vision encoders, yet this structure adds complexity and limits the potential for joint optimization. In this paper, we introduce Mixpert, an efficient mixture-of-vision-experts architecture that inherits the joint learning advantages from a single vision encoder while being restructured into a multi-expert paradigm for task-specific fine-tuning across different visual tasks. Additionally, we design a dynamic routing mechanism that allocates input images to the most suitable visual expert. Mixpert effectively alleviates domain conflicts encountered by a single vision encoder in multi-task learning with minimal additional computational cost, making it more efficient than multiple encoders. Furthermore, Mixpert integrates seamlessly into any MLLM, with experimental results demonstrating substantial performance gains across various tasks.",
        "arxiv_id": "2505.24541"
    },
    "2505.24428": {
        "SCORE": 15,
        "ARXIVID": "2505.24428",
        "COMMENT": "The paper introduces a novel framework for model unlearning using sparse autoencoder subspace guided projections, which is relevant to model compression and efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Xu Wang",
            "Zihao Li",
            "Benyou Wang",
            "Yan Hu",
            "Difan Zou"
        ],
        "title": "Model Unlearning via Sparse Autoencoder Subspace Guided Projections",
        "abstract": "Large language models (LLMs) store vast amounts of information, making them powerful yet raising privacy and safety concerns when selective knowledge removal is required. Existing unlearning strategies, ranging from gradient-based fine-tuning and model editing to sparse autoencoder (SAE) steering, either lack interpretability or fail to provide a robust defense against adversarial prompts. We propose SAE-Guided Subspace Projection Unlearning (SSPU), a novel framework that leverages SAE features to drive targeted updates in the model's parameter space, enabling precise, interpretable, and robust unlearning. SSPU's three-stage pipeline performs data-driven layer and feature selection, subspace construction via QR decomposition, and constrained optimization that controls activations into an \"irrelevant\" subspace while preserving retained knowledge. Overall, we use SAE features to construct a subspace that supervises unlearning, refining the loss and adding a regularization term to guide interpretable parameter updates. In experiments on the WMDP-Cyber forget set and three utility benchmarks (MMLU, TruthfulQA, GSM8K), SSPU reduces harmful knowledge accuracy by 3.22% compared to the strongest baseline. It also improves adversarial robustness, lowering malicious accuracy under jailbreak prompts compared to baselines. Our findings expose the limitations of prior unlearning methods and demonstrate how interpretable subspace-guided optimization can achieve robust, controllable model behavior.",
        "arxiv_id": "2505.24428"
    },
    "2505.24597": {
        "SCORE": 15,
        "ARXIVID": "2505.24597",
        "COMMENT": "The paper introduces a novel framework NextLocMoE using Mixture-of-Experts (MoE) for next location prediction, focusing on personalized and semantic-aware predictions. It provides insights into MoE architecture, which aligns with the model architecture criterion.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Shuai Liu",
            "Ning Cao",
            "Yile Chen",
            "Yue Jiang",
            "Gao Cong"
        ],
        "title": "Mixture-of-Experts for Personalized and Semantic-Aware Next Location Prediction",
        "abstract": "Next location prediction plays a critical role in understanding human mobility patterns. However, existing approaches face two core limitations: (1) they fall short in capturing the complex, multi-functional semantics of real-world locations; and (2) they lack the capacity to model heterogeneous behavioral dynamics across diverse user groups. To tackle these challenges, we introduce NextLocMoE, a novel framework built upon large language models (LLMs) and structured around a dual-level Mixture-of-Experts (MoE) design. Our architecture comprises two specialized modules: a Location Semantics MoE that operates at the embedding level to encode rich functional semantics of locations, and a Personalized MoE embedded within the Transformer backbone to dynamically adapt to individual user mobility patterns. In addition, we incorporate a history-aware routing mechanism that leverages long-term trajectory data to enhance expert selection and ensure prediction stability. Empirical evaluations across several real-world urban datasets show that NextLocMoE achieves superior performance in terms of predictive accuracy, cross-domain generalization, and interpretability",
        "arxiv_id": "2505.24597"
    },
    "2505.24874": {
        "SCORE": 15,
        "ARXIVID": "2505.24874",
        "COMMENT": "The paper discusses the role of foundation models in neuro-symbolic learning, offering a new perspective on integrating symbolic programs with foundation models, aligning with emerging trends.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Adam Stein",
            "Aaditya Naik",
            "Neelay Velingker",
            "Mayur Naik",
            "Eric Wong"
        ],
        "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models",
        "abstract": "Neuro-symbolic learning was proposed to address challenges with training neural networks for complex reasoning tasks with the added benefits of interpretability, reliability, and efficiency. Neuro-symbolic learning methods traditionally train neural models in conjunction with symbolic programs, but they face significant challenges that limit them to simplistic problems. On the other hand, purely-neural foundation models now reach state-of-the-art performance through prompting rather than training, but they are often unreliable and lack interpretability. Supplementing foundation models with symbolic programs, which we call neuro-symbolic prompting, provides a way to use these models for complex reasoning tasks. Doing so raises the question: What role does specialized model training as part of neuro-symbolic learning have in the age of foundation models? To explore this question, we highlight three pitfalls of traditional neuro-symbolic learning with respect to the compute, data, and programs leading to generalization problems. This position paper argues that foundation models enable generalizable neuro-symbolic solutions, offering a path towards achieving the original goals of neuro-symbolic learning without the downsides of training from scratch.",
        "arxiv_id": "2505.24874"
    },
    "2505.24791": {
        "SCORE": 15,
        "ARXIVID": "2505.24791",
        "COMMENT": "The paper proposes a method for accelerating inference in autoregressive normalizing flows, focusing on efficiency improvements, which aligns with model compression and efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Jiaru Zhang",
            "Juanwu Lu",
            "Ziran Wang",
            "Ruqi Zhang"
        ],
        "title": "Inference Acceleration of Autoregressive Normalizing Flows by Selective Jacobi Decoding",
        "abstract": "Normalizing flows are promising generative models with advantages such as theoretical rigor, analytical log-likelihood computation, and end-to-end training. However, the architectural constraints to ensure invertibility and tractable Jacobian computation limit their expressive power and practical usability. Recent advancements utilize autoregressive modeling, significantly enhancing expressive power and generation quality. However, such sequential modeling inherently restricts parallel computation during inference, leading to slow generation that impedes practical deployment. In this paper, we first identify that strict sequential dependency in inference is unnecessary to generate high-quality samples. We observe that patches in sequential modeling can also be approximated without strictly conditioning on all preceding patches. Moreover, the models tend to exhibit low dependency redundancy in the initial layer and higher redundancy in subsequent layers. Leveraging these observations, we propose a selective Jacobi decoding (SeJD) strategy that accelerates autoregressive inference through parallel iterative optimization. Theoretical analyses demonstrate the method's superlinear convergence rate and guarantee that the number of iterations required is no greater than the original sequential approach. Empirical evaluations across multiple datasets validate the generality and effectiveness of our acceleration technique. Experiments demonstrate substantial speed improvements up to 4.7 times faster inference while keeping the generation quality and fidelity.",
        "arxiv_id": "2505.24791"
    },
    "2505.23933": {
        "SCORE": 15,
        "ARXIVID": "2505.23933",
        "COMMENT": "The paper presents BIRD, a framework for transferring aligned behavior in models, focusing on representation learning and robustness, which aligns with representation learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Galen Pogoncheff",
            "Michael Beyeler"
        ],
        "title": "BIRD: Behavior Induction via Representation-structure Distillation",
        "abstract": "Human-aligned deep learning models exhibit behaviors consistent with human values, such as robustness, fairness, and honesty. Transferring these behavioral properties to models trained on different tasks or data distributions remains challenging: aligned behavior is easily forgotten during fine-tuning, and collecting task-specific data that preserves this behavior can be prohibitively costly. We introduce BIRD (Behavior Induction via Representation-structure Distillation), a flexible framework for transferring aligned behavior by matching the internal representation structure of a student model to that of a teacher. Applied to out-of-distribution robustness in image classification, BIRD outperforms fine-tuning, transfer learning, and continual learning methods, improving robust accuracy by up to 16% over the next strongest baseline. It remains effective even when the teacher is trained on a much simpler dataset and is $25 \\times$ smaller than the student. In a large-scale study of over 400 teacher-student pairs, we show that three interpretable and computable properties of the teacher's representations (i.e., task relevance, behavioral relevance, and complementary knowledge) explain up to 85% of the variance in transfer success. These insights offer practical guidance for teacher selection and design. BIRD turns small, well-aligned models into scalable alignment seeds, removing a key bottleneck in deploying safe AI systems in the wild.",
        "arxiv_id": "2505.23933"
    },
    "2505.24717": {
        "SCORE": 15,
        "ARXIVID": "2505.24717",
        "COMMENT": "The paper introduces a new transformer architecture for physics simulations, which aligns with model architecture innovations.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Benjamin Holzschuh",
            "Qiang Liu",
            "Georg Kohl",
            "Nils Thuerey"
        ],
        "title": "PDE-Transformer: Efficient and Versatile Transformers for Physics Simulations",
        "abstract": "We introduce PDE-Transformer, an improved transformer-based architecture for surrogate modeling of physics simulations on regular grids. We combine recent architectural improvements of diffusion transformers with adjustments specific for large-scale simulations to yield a more scalable and versatile general-purpose transformer architecture, which can be used as the backbone for building large-scale foundation models in physical sciences. We demonstrate that our proposed architecture outperforms state-of-the-art transformer architectures for computer vision on a large dataset of 16 different types of PDEs. We propose to embed different physical channels individually as spatio-temporal tokens, which interact via channel-wise self-attention. This helps to maintain a consistent information density of tokens when learning multiple types of PDEs simultaneously. We demonstrate that our pre-trained models achieve improved performance on several challenging downstream tasks compared to training from scratch and also beat other foundation model architectures for physics simulations.",
        "arxiv_id": "2505.24717"
    },
    "2505.24353": {
        "SCORE": 15,
        "ARXIVID": "2505.24353",
        "COMMENT": "The paper introduces a novel class of hyperbolic deep learning architectures, which is relevant to model architecture innovations.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Federico Milanesio",
            "Matteo Santoro",
            "Pietro G. Fr\\'e",
            "Guido Sanguinetti"
        ],
        "title": "Cartan Networks: Group theoretical Hyperbolic Deep Learning",
        "abstract": "Hyperbolic deep learning leverages the metric properties of hyperbolic spaces to develop efficient and informative embeddings of hierarchical data. Here, we focus on the solvable group structure of hyperbolic spaces, which follows naturally from their construction as symmetric spaces. This dual nature of Lie group and Riemannian manifold allows us to propose a new class of hyperbolic deep learning algorithms where group homomorphisms are interleaved with metric-preserving diffeomorphisms. The resulting algorithms, which we call Cartan networks, show promising results on various benchmark data sets and open the way to a novel class of hyperbolic deep learning architectures.",
        "arxiv_id": "2505.24353"
    },
    "2505.23786": {
        "SCORE": 15,
        "ARXIVID": "2505.23786",
        "COMMENT": "The paper discusses a practical attack on quantization methods, which is relevant to model compression.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Kazuki Egashira",
            "Robin Staab",
            "Mark Vero",
            "Jingxuan He",
            "Martin Vechev"
        ],
        "title": "Mind the Gap: A Practical Attack on GGUF Quantization",
        "abstract": "With the increasing size of frontier LLMs, post-training quantization has become the standard for memory-efficient deployment. Recent work has shown that basic rounding-based quantization schemes pose security risks, as they can be exploited to inject malicious behaviors into quantized models that remain hidden in full precision. However, existing attacks cannot be applied to more complex quantization methods, such as the GGUF family used in the popular ollama and llama.cpp frameworks. In this work, we address this gap by introducing the first attack on GGUF. Our key insight is that the quantization error -- the difference between the full-precision weights and their (de-)quantized version -- provides sufficient flexibility to construct malicious quantized models that appear benign in full precision. Leveraging this, we develop an attack that trains the target malicious LLM while constraining its weights based on quantization errors. We demonstrate the effectiveness of our attack on three popular LLMs across nine GGUF quantization data types on three diverse attack scenarios: insecure code generation ($\\Delta$=$88.7\\%$), targeted content injection ($\\Delta$=$85.0\\%$), and benign instruction refusal ($\\Delta$=$30.1\\%$). Our attack highlights that (1) the most widely used post-training quantization method is susceptible to adversarial interferences, and (2) the complexity of quantization schemes alone is insufficient as a defense.",
        "arxiv_id": "2505.23786"
    },
    "2505.24727": {
        "SCORE": 15,
        "ARXIVID": "2505.24727",
        "COMMENT": "The paper introduces a novel framework for compressive sensing with theoretical guarantees, relevant to model compression.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Xiaochen Zhang",
            "Haoyi Xiong"
        ],
        "title": "Knockoff-Guided Compressive Sensing: A Statistical Machine Learning Framework for Support-Assured Signal Recovery",
        "abstract": "This paper introduces a novel Knockoff-guided compressive sensing framework, referred to as \\TheName{}, which enhances signal recovery by leveraging precise false discovery rate (FDR) control during the support identification phase. Unlike LASSO, which jointly performs support selection and signal estimation without explicit error control, our method guarantees FDR control in finite samples, enabling more reliable identification of the true signal support. By separating and controlling the support recovery process through statistical Knockoff filters, our framework achieves more accurate signal reconstruction, especially in challenging scenarios where traditional methods fail. We establish theoretical guarantees demonstrating how FDR control directly ensures recovery performance under weaker conditions than traditional $\\ell_1$-based compressive sensing methods, while maintaining accurate signal reconstruction. Extensive numerical experiments demonstrate that our proposed Knockoff-based method consistently outperforms LASSO-based and other state-of-the-art compressive sensing techniques. In simulation studies, our method improves F1-score by up to 3.9x over baseline methods, attributed to principled false discovery rate (FDR) control and enhanced support recovery. The method also consistently yields lower reconstruction and relative errors. We further validate the framework on real-world datasets, where it achieves top downstream predictive performance across both regression and classification tasks, often narrowing or even surpassing the performance gap relative to uncompressed signals. These results establish \\TheName{} as a robust and practical alternative to existing approaches, offering both theoretical guarantees and strong empirical performance through statistically grounded support selection.",
        "arxiv_id": "2505.24727"
    },
    "2505.24022": {
        "SCORE": 15,
        "ARXIVID": "2505.24022",
        "COMMENT": "The paper investigates the implicit bias of Adam and SGD, which is relevant to representation learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Bhavya Vasudeva",
            "Jung Whan Lee",
            "Vatsal Sharan",
            "Mahdi Soltanolkotabi"
        ],
        "title": "The Rich and the Simple: On the Implicit Bias of Adam and SGD",
        "abstract": "Adam is the de facto optimization algorithm for several deep learning applications, but an understanding of its implicit bias and how it differs from other algorithms, particularly standard first-order methods such as (stochastic) gradient descent (GD), remains limited. In practice, neural networks trained with SGD are known to exhibit simplicity bias -- a tendency to find simple solutions. In contrast, we show that Adam is more resistant to such simplicity bias. To demystify this phenomenon, in this paper, we investigate the differences in the implicit biases of Adam and GD when training two-layer ReLU neural networks on a binary classification task involving synthetic data with Gaussian clusters. We find that GD exhibits a simplicity bias, resulting in a linear decision boundary with a suboptimal margin, whereas Adam leads to much richer and more diverse features, producing a nonlinear boundary that is closer to the Bayes' optimal predictor. This richer decision boundary also allows Adam to achieve higher test accuracy both in-distribution and under certain distribution shifts. We theoretically prove these results by analyzing the population gradients. To corroborate our theoretical findings, we present empirical results showing that this property of Adam leads to superior generalization across datasets with spurious correlations where neural networks trained with SGD are known to show simplicity bias and don't generalize well under certain distributional shifts.",
        "arxiv_id": "2505.24022"
    },
    "2505.24434": {
        "SCORE": 15,
        "ARXIVID": "2505.24434",
        "COMMENT": "The paper proposes Graph Flow Matching, enhancing image generation with a novel architecture that combines flow matching with graph neural networks, relevant to model architecture innovations.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Md Shahriar Rahim Siddiqui",
            "Moshe Eliasof",
            "Eldad Haber"
        ],
        "title": "Graph Flow Matching: Enhancing Image Generation with Neighbor-Aware Flow Fields",
        "abstract": "Flow matching casts sample generation as learning a continuous-time velocity field that transports noise to data. Existing flow matching networks typically predict each point's velocity independently, considering only its location and time along its flow trajectory, and ignoring neighboring points. However, this pointwise approach may overlook correlations between points along the generation trajectory that could enhance velocity predictions, thereby improving downstream generation quality. To address this, we propose Graph Flow Matching (GFM), a lightweight enhancement that decomposes the learned velocity into a reaction term -- any standard flow matching network -- and a diffusion term that aggregates neighbor information via a graph neural module. This reaction-diffusion formulation retains the scalability of deep flow models while enriching velocity predictions with local context, all at minimal additional computational cost. Operating in the latent space of a pretrained variational autoencoder, GFM consistently improves Fr\\'echet Inception Distance (FID) and recall across five image generation benchmarks (LSUN Church, LSUN Bedroom, FFHQ, AFHQ-Cat, and CelebA-HQ at $256\\times256$), demonstrating its effectiveness as a modular enhancement to existing flow matching architectures.",
        "arxiv_id": "2505.24434"
    },
    "2505.24438": {
        "SCORE": 15,
        "ARXIVID": "2505.24438",
        "COMMENT": "The paper introduces a novel message passing scheme for temporal graph neural networks, focusing on the expressive power of TGNNs, which is relevant to model architecture analysis.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Franziska Heeg",
            "Jonas Sauer",
            "Petra Mutzel",
            "Ingo Scholtes"
        ],
        "title": "Weisfeiler and Leman Follow the Arrow of Time: Expressive Power of Message Passing in Temporal Event Graphs",
        "abstract": "An important characteristic of temporal graphs is how the directed arrow of time influences their causal topology, i.e., which nodes can possibly influence each other causally via time-respecting paths. The resulting patterns are often neglected by temporal graph neural networks (TGNNs). To formally analyze the expressive power of TGNNs, we lack a generalization of graph isomorphism to temporal graphs that fully captures their causal topology. Addressing this gap, we introduce the notion of consistent event graph isomorphism, which utilizes a time-unfolded representation of time-respecting paths in temporal graphs. We compare this definition with existing notions of temporal graph isomorphisms. We illustrate and highlight the advantages of our approach and develop a temporal generalization of the Weisfeiler-Leman algorithm to heuristically distinguish non-isomorphic temporal graphs. Building on this theoretical foundation, we derive a novel message passing scheme for temporal graph neural networks that operates on the event graph representation of temporal graphs. An experimental evaluation shows that our approach performs well in a temporal graph classification experiment.",
        "arxiv_id": "2505.24438"
    }
}