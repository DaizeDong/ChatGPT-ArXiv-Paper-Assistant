{
    "2508.03613": {
        "SCORE": 18,
        "ARXIVID": "2508.03613",
        "COMMENT": "The paper introduces Goedel-Prover-V2, a new state-of-the-art in automated theorem proving, with innovations in data synthesis and self-correction, aligning with foundational research in AI for Science.",
        "RELEVANCE": 9,
        "NOVELTY": 9,
        "authors": [
            "Yong Lin",
            "Shange Tang",
            "Bohan Lyu",
            "Ziran Yang",
            "Jui-Hui Chung",
            "Haoyu Zhao",
            "Lai Jiang",
            "Yihan Geng",
            "Jiawei Ge",
            "Jingruo Sun",
            "Jiayun Wu",
            "Jiri Gesi",
            "Ximing Lu",
            "David Acuna",
            "Kaiyu Yang",
            "Hongzhou Lin",
            "Yejin Choi",
            "Danqi Chen",
            "Sanjeev Arora",
            "Chi Jin"
        ],
        "title": "Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data Synthesis and Self-Correction",
        "abstract": "We introduce Goedel-Prover-V2, a series of open-source language models that set a new state-of-the-art in automated theorem proving. Built on the standard expert iteration and reinforcement learning pipeline, our approach incorporates three key innovations: (1) Scaffolded data synthesis: We generate synthetic tasks of increasing difficulty to train the model to master increasingly complex theorems; (2) Verifier-guided self-correction: We enable the model to iteratively revise its proofs by leveraging feedback from the Lean compiler; (3) Model averaging: We merge model checkpoints to mitigate the decrease in model output diversity in later stages of training. Our small model, Goedel-Prover-V2-8B, reaches 84.6% pass@32 on MiniF2F and outperforms DeepSeek-Prover-V2-671B under the same metric, despite being 80X smaller. Our flagship model, Goedel-Prover-V2-32B, achieves 88.1% on MiniF2F at pass@32 in standard mode and 90.4% in self-correction mode, outperforming prior SOTA by a large margin. Additionally, our flagship model solves 86 problems on PutnamBench at pass@184, securing the first place among open-source models on the leaderboard, surpassing DeepSeek-Prover-V2-671B's record of solving 47 problems by pass@1024 with a significantly smaller model size and compute budget. At the time of its release (July-August 2025), Goedel-Prover-V2 achieves the strongest overall performance among all open-source theorem provers. It also ranks among the top-performing models--including closed-source systems with publicly reported performance--under a constrained test-time compute budget. Our models, code, and data are released at https://github.com/Goedel-LM/Goedel-Prover-V2.",
        "arxiv_id": "2508.03613"
    },
    "2508.03527": {
        "SCORE": 17,
        "ARXIVID": "2508.03527",
        "COMMENT": "The paper proposes a new generation of Kronecker adapters for parameter-efficient fine-tuning, which is relevant to model compression and architecture innovation.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Mohammadreza Sadeghi",
            "Mahsa Ghazvini Nejad",
            "MirHamed Jafarzadeh Asl",
            "Yu Gu",
            "Yuanhao Yu",
            "Masoud Asgharian",
            "Vahid Partovi Nia"
        ],
        "title": "MoKA: Mixture of Kronecker Adapters",
        "abstract": "Parameter-efficient fine-tuning (PEFT) is essential for reducing the computational overhead of large language models (LLMs). Low-rank family adapters are commonly used to control the parameter size efficiently while maintaining the generative power of LLMs. However, their limited expressiveness due to the rank constraint often restricts their performance on complex tasks. We propose Mixture of Kronecker Adapters (MoKA), a new generation of Kronecker adapters that addresses this limitation by modeling weight updates as a mixture of Kronecker products. Our proposed adapter leverages a gating mechanism that measures the importance of each Kronecker factor, enabling more expressive adaptation. Moreover, MoKA enables a rank flexibility that provides a better trade-off between parameter efficiency and accuracy. To ensure hardware efficiency, we reformulate Kronecker computations using standard matrix operations, allowing seamless deployment on GPU-optimized hardware. We conduct extensive experiments on instruction-tuning and commonsense reasoning tasks using low-bit quantized versions of LLaMA2-7B and LLaMA3-8B models. MoKA not only outperforms PEFT baselines, but also reduces the number of trainable parameters up to 27x, achieving state-of-the-art trade-offs between performance and parameter efficiency.",
        "arxiv_id": "2508.03527"
    },
    "2508.03688": {
        "SCORE": 17,
        "ARXIVID": "2508.03688",
        "COMMENT": "The paper provides a theoretical analysis of SGD dynamics in high-dimensional neural networks, which aligns with the Representation Learning criterion.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "G\\'erard Ben Arous",
            "Murat A. Erdogdu",
            "N. Mert Vural",
            "Denny Wu"
        ],
        "title": "Learning quadratic neural networks in high dimensions: SGD dynamics and scaling laws",
        "abstract": "We study the optimization and sample complexity of gradient-based training of a two-layer neural network with quadratic activation function in the high-dimensional regime, where the data is generated as $y \\propto \\sum_{j=1}^{r}\\lambda_j \\sigma\\left(\\langle \\boldsymbol{\\theta_j}, \\boldsymbol{x}\\rangle\\right), \\boldsymbol{x} \\sim N(0,\\boldsymbol{I}_d)$, $\\sigma$ is the 2nd Hermite polynomial, and $\\lbrace\\boldsymbol{\\theta}_j \\rbrace_{j=1}^{r} \\subset \\mathbb{R}^d$ are orthonormal signal directions. We consider the extensive-width regime $r \\asymp d^\\beta$ for $\\beta \\in [0, 1)$, and assume a power-law decay on the (non-negative) second-layer coefficients $\\lambda_j\\asymp j^{-\\alpha}$ for $\\alpha \\geq 0$. We present a sharp analysis of the SGD dynamics in the feature learning regime, for both the population limit and the finite-sample (online) discretization, and derive scaling laws for the prediction risk that highlight the power-law dependencies on the optimization time, sample size, and model width. Our analysis combines a precise characterization of the associated matrix Riccati differential equation with novel matrix monotonicity arguments to establish convergence guarantees for the infinite-dimensional effective dynamics.",
        "arxiv_id": "2508.03688"
    },
    "2508.03440": {
        "SCORE": 17,
        "ARXIVID": "2508.03440",
        "COMMENT": "The paper explores the 'Soft Thinking' capabilities of LLMs, providing theoretical insights into their behavior and interpretability.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Junhong Wu",
            "Jinliang Lu",
            "Zixuan Ren",
            "Ganqiang Hu",
            "Zhi Wu",
            "Dai Dai",
            "Hua Wu"
        ],
        "title": "LLMs Have a Heart of Stone: Demystifying the Soft Thinking Ability of Large Reasoning Models",
        "abstract": "Human cognition naturally engages with abstract and fluid concepts, whereas existing reasoning models often rely on generating discrete tokens, potentially constraining their expressive capabilities. Recent advancements aim to address this limitation by enabling large language models (LLMs) to generate soft, abstract tokens, thus facilitating reasoning within a continuous concept space. This paper explores the `Soft Thinking' capabilities of various LLMs by examining the models' internal behavior using a suite of probing techniques. Contrary to the common belief that Soft Thinking enables the simultaneous exploration of diverse reasoning paths, our findings reveal that LLMs predominantly rely on the most influential component of the soft inputs during subsequent decoding steps. This reliance hinders the exploration of different reasoning paths and reduces vanilla Soft Thinking to a form of greedy decoding, obscuring the advantage of transmitting more information through Soft Tokens. To tackle this issue, we explore sampling strategies to introduce \\emph{randomness}, employing methods such as Dirichlet resampling and the Gumbel-Softmax trick. Our experiments demonstrate that incorporating randomness can alleviate the limitations of vanilla approaches and unleash the potential of Soft Thinking. Notably, the Gumbel-Softmax trick provides adequate randomness with controlled smoothness, resulting in superior performance across eight reasoning benchmarks.",
        "arxiv_id": "2508.03440"
    },
    "2508.03682": {
        "SCORE": 17,
        "ARXIVID": "2508.03682",
        "COMMENT": "The paper proposes Self-Questioning Language Models, an innovative framework for improving LLMs without external data, aligning with foundational research in LLM behavior and training dynamics.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Lili Chen",
            "Mihir Prabhudesai",
            "Katerina Fragkiadaki",
            "Hao Liu",
            "Deepak Pathak"
        ],
        "title": "Self-Questioning Language Models",
        "abstract": "Can large language models improve without external data -- by generating their own questions and answers? We hypothesize that a pre-trained language model can improve its reasoning skills given only a single prompt specifying the topic (e.g., algebra word problems) and asking the model to generate its own questions. To do this, we propose Self-Questioning Language Models (SQLM): an asymmetric self-play framework where a proposer is given the topic and generates a question for a solver, who tries to answer it. Both the proposer and solver are trained via reinforcement learning. The proposer receives a reward if the problem is not too easy or too difficult, and the solver receives a reward based on majority voting, a proxy for correctness in the absence of ground-truth answers. For coding, the proposer can instead generate unit tests which are used for verification. We study this asymmetric self-play framework on three benchmarks: three-digit multiplication, algebra problems from the OMEGA benchmark, and programming problems from Codeforces. By continually generating more interesting problems and attempting to solve them, language models can improve on downstream benchmarks without access to any curated training datasets.",
        "arxiv_id": "2508.03682"
    },
    "2508.02834": {
        "SCORE": 17,
        "ARXIVID": "2508.02834",
        "COMMENT": "The paper introduces a biologically-motivated framework for antibody design using a multi-expert system, which aligns with the interest in Mixture-of-Experts (MoE) and representation learning.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Hanqi Feng",
            "Peng Qiu",
            "Mengchun Zhang",
            "Yiran Tao",
            "You Fan",
            "Jingtao Xu",
            "Barnabas Poczos"
        ],
        "title": "Learning from B Cell Evolution: Adaptive Multi-Expert Diffusion for Antibody Design via Online Optimization",
        "abstract": "Recent advances in diffusion models have shown remarkable potential for antibody design, yet existing approaches apply uniform generation strategies that cannot adapt to each antigen's unique requirements. Inspired by B cell affinity maturation, where antibodies evolve through multi-objective optimization balancing affinity, stability, and self-avoidance, we propose the first biologically-motivated framework that leverages physics-based domain knowledge within an online meta-learning system. Our method employs multiple specialized experts (van der Waals, molecular recognition, energy balance, and interface geometry) whose parameters evolve during generation based on iterative feedback, mimicking natural antibody refinement cycles. Instead of fixed protocols, this adaptive guidance discovers personalized optimization strategies for each target. Our experiments demonstrate that this approach: (1) discovers optimal SE(3)-equivariant guidance strategies for different antigen classes without pre-training, preserving molecular symmetries throughout optimization; (2) significantly enhances hotspot coverage and interface quality through target-specific adaptation, achieving balanced multi-objective optimization characteristic of therapeutic antibodies; (3) establishes a paradigm for iterative refinement where each antibody-antigen system learns its unique optimization profile through online evaluation; (4) generalizes effectively across diverse design challenges, from small epitopes to large protein interfaces, enabling precision-focused campaigns for individual targets.",
        "arxiv_id": "2508.02834"
    },
    "2508.03222": {
        "SCORE": 17,
        "ARXIVID": "2508.03222",
        "COMMENT": "The paper provides insights into how deep networks encode information by studying information propagation in finite-width neural networks, which aligns with representation learning.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Giuseppe Alessio D'Inverno",
            "Zhiyuan Hu",
            "Leo Davy",
            "Michael Unser",
            "Gianluigi Rozza",
            "Jonathan Dong"
        ],
        "title": "Revisiting Deep Information Propagation: Fractal Frontier and Finite-size Effects",
        "abstract": "Information propagation characterizes how input correlations evolve across layers in deep neural networks. This framework has been well studied using mean-field theory, which assumes infinitely wide networks. However, these assumptions break down for practical, finite-size networks. In this work, we study information propagation in randomly initialized neural networks with finite width and reveal that the boundary between ordered and chaotic regimes exhibits a fractal structure. This shows the fundamental complexity of neural network dynamics, in a setting that is independent of input data and optimization. To extend this analysis beyond multilayer perceptrons, we leverage recently introduced Fourier-based structured transforms, and show that information propagation in convolutional neural networks also follow the same behavior. Our investigation highlights the importance of finite network depth with respect to the tradeoff between separation and robustness.",
        "arxiv_id": "2508.03222"
    },
    "2508.03346": {
        "SCORE": 17,
        "ARXIVID": "2508.03346",
        "COMMENT": "The paper introduces a novel CoT compression framework for LLMs, focusing on efficiency improvements, which aligns with model compression.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Zeju Li",
            "Jianyuan Zhong",
            "Ziyang Zheng",
            "Xiangyu Wen",
            "Zhijian Xu",
            "Yingying Cheng",
            "Fan Zhang",
            "Qiang Xu"
        ],
        "title": "Compressing Chain-of-Thought in LLMs via Step Entropy",
        "abstract": "Large Language Models (LLMs) using Chain-of-Thought (CoT) prompting excel at complex reasoning but generate verbose thought processes with considerable redundancy, leading to increased inference costs and reduced efficiency. We introduce a novel CoT compression framework based on step entropy, a metric that quantifies the informational contribution of individual reasoning steps to identify redundancy. Through theoretical analysis and extensive empirical validation on mathematical reasoning benchmarks, we demonstrate that steps with low entropy are indeed highly redundant. Our experiments reveal that an astonishing 80\\% of low-entropy intermediate steps can be pruned with minor degradation in the final answer accuracy across DeepSeek-R1-7B, 14B and Qwen3-8B. This finding sharply contrasts with random or high-entropy pruning, which severely impairs reasoning performance. Building on this, we propose a novel two-stage training strategy combining Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO) reinforcement learning. This approach enables LLMs to autonomously learn to generate compressed COTs during inference by strategically incorporating [SKIP] tokens. Our method significantly enhances LLM inference efficiency while rigorously preserving accuracy, offering profound implications for practical LLM deployment and a deeper understanding of reasoning structures.",
        "arxiv_id": "2508.03346"
    },
    "2508.03148": {
        "SCORE": 16,
        "ARXIVID": "2508.03148",
        "COMMENT": "The paper discusses a simulator for LLM inference systems, focusing on Mixture-of-Experts (MoE) models and disaggregated architectures, which aligns with the Model Architecture criterion.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Yicheng Feng",
            "Xin Tan",
            "Kin Hang Sew",
            "Yimin Jiang",
            "Yibo Zhu",
            "Hong Xu"
        ],
        "title": "Frontier: Simulating the Next Generation of LLM Inference Systems",
        "abstract": "Large Language Model (LLM) inference is growing increasingly complex with the rise of Mixture-of-Experts (MoE) models and disaggregated architectures that decouple components like prefill/decode (PD) or attention/FFN (AF) for heterogeneous scaling. Existing simulators, architected for co-located, dense models, are unable to capture the intricate system dynamics of these emerging paradigms. We present Frontier, a high-fidelity simulator designed from the ground up for this new landscape. Frontier introduces a unified framework to model both co-located and disaggregated systems, providing native support for MoE inference with expert parallelism (EP). It enables the simulation of complex workflows like cross-cluster expert routing and advanced pipelining strategies for latency hiding. To ensure fidelity and usability, Frontier incorporates refined operator models for improved accuracy. Frontier empowers the community to design and optimize the future of LLM inference at scale.",
        "arxiv_id": "2508.03148"
    },
    "2508.02751": {
        "SCORE": 16,
        "ARXIVID": "2508.02751",
        "COMMENT": "The paper introduces SmallKV, a method for KV cache compression in LLMs, which aligns with model compression and efficiency breakthroughs.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Yi Zhao",
            "Yajuan Peng",
            "Cam-Tu Nguyen",
            "Zuchao Li",
            "Xiaoliang Wang",
            "Hai Zhao",
            "Xiaoming Fu"
        ],
        "title": "SmallKV: Small Model Assisted Compensation of KV Cache Compression for Efficient LLM Inference",
        "abstract": "KV cache eviction has emerged as an effective solution to alleviate resource constraints faced by LLMs in long-context scenarios. However, existing token-level eviction methods often overlook two critical aspects: (1) their irreversible eviction strategy fails to adapt to dynamic attention patterns during decoding (the saliency shift problem), and (2) they treat both marginally important tokens and truly unimportant tokens equally, despite the collective significance of marginal tokens to model performance (the marginal information over-compression problem). To address these issues, we design two compensation mechanisms based on the high similarity of attention matrices between LLMs of different scales. We propose SmallKV, a small model assisted compensation method for KV cache compression. SmallKV can maintain attention matching between different-scale LLMs to: 1) assist the larger model in perceiving globally important information of attention; and 2) use the smaller model's attention scores to approximate those of marginal tokens in the larger model. Extensive experiments on benchmarks including GSM8K, BBH, MT-Bench, and LongBench demonstrate the effectiveness of SmallKV. Moreover, efficiency evaluations show that SmallKV achieves 1.75 - 2.56 times higher throughput than baseline methods, highlighting its potential for efficient and performant LLM inference in resource constrained environments.",
        "arxiv_id": "2508.02751"
    },
    "2508.02789": {
        "SCORE": 16,
        "ARXIVID": "2508.02789",
        "COMMENT": "The paper introduces a cognitive loop for self-adaptive reasoning in LLMs, which is relevant to foundational research in AI for science and LLM behavior.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Newman Cheng",
            "Gordon Broadbent",
            "William Chappell"
        ],
        "title": "Cognitive Loop via In-Situ Optimization: Self-Adaptive Reasoning for Science",
        "abstract": "The capacity for artificial intelligence (AI) to formulate, evolve, and test altered thought patterns under dynamic conditions indicates advanced cognition that is crucial for scientific discovery. The existing AI development landscape falls into two categories: 1) frameworks over non-reasoning models that natively incorporate opinions on how humans think, and 2) reasoning models that abstract precise control of the reasoning intuition away from end users. While powerful, for scientists to maximize utility of AI in scientific discovery, they not only require accuracy and transparency in reasoning, but also steerability. Hence, we introduce an alternative approach that enables deep and precise control over the reasoning process called: a cognitive loop via in-situ optimization (CLIO). CLIO enables large language models (LLMs) to self-formulate ways of approaching a problem, adapt behavior when self-confidence is low, and ultimately provide scientists with a final belief or answer. Through CLIO's open design, scientists can observe uncertainty levels, understand how final belief states are formulated using graph structures, and interject corrections. Without any further post-training, OpenAI's GPT-4.1 with CLIO yields an accuracy of 22.37\\% in text-based biology and medicine questions on Humanity's Last Exam (HLE). This yields a 13.82\\% net or 161.64\\% relative increase when compared to the base GPT-4.1 model and surpasses OpenAI's o3 performance in high and low reasoning effort modes. We further discovered that oscillations within internal uncertainty measures are key in determining the accuracy of CLIO's results, revealing how its open design and internal mechanisms can provide insight and control into scientific decision-making processes.",
        "arxiv_id": "2508.02789"
    },
    "2508.03402": {
        "SCORE": 16,
        "ARXIVID": "2508.03402",
        "COMMENT": "SCFlow proposes a novel approach to disentangle style and content using flow models, which aligns with representation learning and architectural innovation.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Pingchuan Ma",
            "Xiaopei Yang",
            "Yusong Li",
            "Ming Gui",
            "Felix Krause",
            "Johannes Schusterbauer",
            "Bj\\\"orn Ommer"
        ],
        "title": "SCFlow: Implicitly Learning Style and Content Disentanglement with Flow Models",
        "abstract": "Explicitly disentangling style and content in vision models remains challenging due to their semantic overlap and the subjectivity of human perception. Existing methods propose separation through generative or discriminative objectives, but they still face the inherent ambiguity of disentangling intertwined concepts. Instead, we ask: Can we bypass explicit disentanglement by learning to merge style and content invertibly, allowing separation to emerge naturally? We propose SCFlow, a flow-matching framework that learns bidirectional mappings between entangled and disentangled representations. Our approach is built upon three key insights: 1) Training solely to merge style and content, a well-defined task, enables invertible disentanglement without explicit supervision; 2) flow matching bridges on arbitrary distributions, avoiding the restrictive Gaussian priors of diffusion models and normalizing flows; and 3) a synthetic dataset of 510,000 samples (51 styles $\\times$ 10,000 content samples) was curated to simulate disentanglement through systematic style-content pairing. Beyond controllable generation tasks, we demonstrate that SCFlow generalizes to ImageNet-1k and WikiArt in zero-shot settings and achieves competitive performance, highlighting that disentanglement naturally emerges from the invertible merging process.",
        "arxiv_id": "2508.03402"
    },
    "2508.03351": {
        "SCORE": 15,
        "ARXIVID": "2508.03351",
        "COMMENT": "The paper presents a novel post-training quantization framework for vision-language models, focusing on efficiency and compression, which is relevant to model compression.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Yufei Xue",
            "Yushi Huang",
            "Jiawei Shao",
            "Jun Zhang"
        ],
        "title": "VLMQ: Efficient Post-Training Quantization for Large Vision-Language Models via Hessian Augmentation",
        "abstract": "Post-training quantization (PTQ) has emerged as an effective approach for compressing large models and accelerating their inference without retraining. While PTQ has been extensively studied in the context of large language models (LLMs), its applicability to vision-language models (VLMs) remains underexplored. In this paper, we identify a modality discrepancy (\\emph{i.e.}, limited text tokens \\emph{vs.} excessive and redundant vision tokens) of VLMs. However, existing Hessian-based LLM PTQ methods treat all tokens equally during quantization, resulting in severe performance drops when applied to VLMs. Motivated by this observation, we propose a novel importance-aware PTQ framework tailored for VLMs, dubbed VLMQ. Specifically, to address vision token redundancy, VLMQ 1) optimizes an importance-aware objective that yields an enhanced Hessian with token-level importance factors, while retaining compatibility with parallelized weight updates, and 2) ensures efficiency and effectiveness by computing these factors via a single lightweight block-wise backward pass, guided by a theoretical connection to token-level perturbations. Extensive evaluations on 8 benchmarks across 0.5B$\\sim$32B VLMs demonstrate the state-of-the-art (SOTA) performance of our VLMQ, particularly under low-bit settings. For example, it achieves a substantial \\textbf{16.45\\%} improvement on MME-RealWorld under 2-bit quantization.",
        "arxiv_id": "2508.03351"
    },
    "2508.03332": {
        "SCORE": 15,
        "ARXIVID": "2508.03332",
        "COMMENT": "The paper presents a metric-driven post-training quantization framework for small language models, focusing on compression and efficiency, which is relevant to model compression.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "He Xiao",
            "Qingyao Yang",
            "Dirui Xie",
            "Wendong Xu",
            "Wenyong Zhou",
            "Haobo Liu",
            "Zhengwu Liu",
            "Ngai Wong"
        ],
        "title": "Exploring Layer-wise Information Effectiveness for Post-Training Quantization in Small Language Models",
        "abstract": "Large language models with billions of parameters are often over-provisioned: many layers contribute little unique information yet dominate the memory and energy footprint during inference. We present LieQ, a metric-driven post-training quantization framework that addresses the critical challenge of maintaining accuracy in sub-7B models under extreme low-bit compression. Our method introduces three complementary layer-wise diagnostics-Perplexity Drop, Representational Compactness, and Top-k Energy Gain -that reveal a canonical division of labour across layers, enabling automatic bit-width allocation without gradient updates. Unlike existing approaches that suffer severe accuracy degradation at 2-3 bits precision, LieQ achieves state-of-the-art compression-accuracy trade-offs: on Qwen3-4B, it recovers 95.9% of FP16 baseline performance at 2.05-bit quantization, outperforming GPTQ by 19.7% and AWQ by 18.1% on average across seven zero-shot reasoning tasks. Applied to LLaMA3.2-3B, LieQ maintains 98.2% of baseline accuracy at 2.07-bit precision while enabling 4x memory reduction, establishing new paradigms for deploying small language models on resource-constrained edge devices.",
        "arxiv_id": "2508.03332"
    },
    "2508.03002": {
        "SCORE": 15,
        "ARXIVID": "2508.03002",
        "COMMENT": "The paper proposes a Shapley-based method for mixed precision quantization, which is relevant to model compression and efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Haidong Kang",
            "Lianbo Ma",
            "Guo Yu",
            "Shangce Gao"
        ],
        "title": "Where and How to Enhance: Discovering Bit-Width Contribution for Mixed Precision Quantization",
        "abstract": "Mixed precision quantization (MPQ) is an effective quantization approach to achieve accuracy-complexity trade-off of neural network, through assigning different bit-widths to network activations and weights in each layer. The typical way of existing MPQ methods is to optimize quantization policies (i.e., bit-width allocation) in a gradient descent manner, termed as Differentiable (DMPQ). At the end of the search, the bit-width associated to the quantization parameters which has the largest value will be selected to form the final mixed precision quantization policy, with the implicit assumption that the values of quantization parameters reflect the operation contribution to the accuracy improvement. While much has been discussed about the MPQ improvement, the bit-width selection process has received little attention. We study this problem and argue that the magnitude of quantization parameters does not necessarily reflect the actual contribution of the bit-width to the task performance. Then, we propose a Shapley-based MPQ (SMPQ) method, which measures the bit-width operation direct contribution on the MPQ task. To reduce computation cost, a Monte Carlo sampling-based approximation strategy is proposed for Shapley computation. Extensive experiments on mainstream benchmarks demonstrate that our SMPQ consistently achieves state-of-the-art performance than gradient-based competitors.",
        "arxiv_id": "2508.03002"
    },
    "2508.02924": {
        "SCORE": 15,
        "ARXIVID": "2508.02924",
        "COMMENT": "The paper proposes a novel framework, BoostTransformer, which enhances transformer models with boosting principles, focusing on architectural innovation and efficiency improvements.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Biyi Fang",
            "Jean Utke",
            "Truong Vo",
            "Diego Klabjan"
        ],
        "title": "BoostTransformer: Enhancing Transformer Models with Subgrid Selection and Importance Sampling",
        "abstract": "Transformer architectures dominate modern NLP but often demand heavy computational resources and intricate hyperparameter tuning. To mitigate these challenges, we propose a novel framework, BoostTransformer, that augments transformers with boosting principles through subgrid token selection and importance-weighted sampling. Our method incorporates a least square boosting objective directly into the transformer pipeline, enabling more efficient training and improved performance. Across multiple fine-grained text classification benchmarks, BoostTransformer demonstrates both faster convergence and higher accuracy, surpassing standard transformers while minimizing architectural search overhead.",
        "arxiv_id": "2508.02924"
    },
    "2508.03587": {
        "SCORE": 15,
        "ARXIVID": "2508.03587",
        "COMMENT": "The paper proposes a method for zero-variance gradients in VAEs, which aligns with the interest in representation learning and autoencoders.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Zilei Shao",
            "Anji Liu",
            "Guy Van den Broeck"
        ],
        "title": "Zero-Variance Gradients for Variational Autoencoders",
        "abstract": "Training deep generative models like Variational Autoencoders (VAEs) is often hindered by the need to backpropagate gradients through the stochastic sampling of their latent variables, a process that inherently introduces estimation variance, which can slow convergence and degrade performance. In this paper, we propose a new perspective that sidesteps this problem, which we call Silent Gradients. Instead of improving stochastic estimators, we leverage specific decoder architectures to analytically compute the expected ELBO, yielding a gradient with zero variance. We first provide a theoretical foundation for this method and demonstrate its superiority over existing estimators in a controlled setting with a linear decoder. To generalize our approach for practical use with complex, expressive decoders, we introduce a novel training dynamic that uses the exact, zero-variance gradient to guide the early stages of encoder training before annealing to a standard stochastic estimator. Our experiments show that this technique consistently improves the performance of established baselines, including reparameterization, Gumbel-Softmax, and REINFORCE, across multiple datasets. This work opens a new direction for training generative models by combining the stability of analytical computation with the expressiveness of deep, nonlinear architecture.",
        "arxiv_id": "2508.03587"
    },
    "2508.03104": {
        "SCORE": 15,
        "ARXIVID": "2508.03104",
        "COMMENT": "The paper focuses on contrastive learning, a key aspect of representation learning, and introduces a novel hierarchical framework for hypergraph learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Mengting Pan",
            "Fan Li",
            "Xiaoyang Wang",
            "Wenjie Zhang",
            "Xuemin Lin"
        ],
        "title": "HiTeC: Hierarchical Contrastive Learning on Text-Attributed Hypergraph with Semantic-Aware Augmentation",
        "abstract": "Contrastive learning (CL) has become a dominant paradigm for self-supervised hypergraph learning, enabling effective training without costly labels. However, node entities in real-world hypergraphs are often associated with rich textual information, which is overlooked in prior works. Directly applying existing CL-based methods to such text-attributed hypergraphs (TAHGs) leads to three key limitations: (1) The common use of graph-agnostic text encoders overlooks the correlations between textual content and hypergraph topology, resulting in suboptimal representations. (2) Their reliance on random data augmentations introduces noise and weakens the contrastive objective. (3) The primary focus on node- and hyperedge-level contrastive signals limits the ability to capture long-range dependencies, which is essential for expressive representation learning. Although HyperBERT pioneers CL on TAHGs, its co-training paradigm suffers from poor scalability. To fill the research gap, we introduce HiTeC, a two-stage hierarchical contrastive learning framework with semantic-aware augmentation for scalable and effective self-supervised learning on TAHGs. In the first stage, we pre-train the text encoder with a structure-aware contrastive objective to overcome the graph-agnostic nature of conventional methods. In the second stage, we introduce two semantic-aware augmentation strategies, including prompt-enhanced text augmentation and semantic-aware hyperedge drop, to facilitate informative view generation. Furthermore, we propose a multi-scale contrastive loss that extends existing objectives with an $s$-walk-based subgraph-level contrast to better capture long-range dependencies. By decoupling text encoder pretraining from hypergraph contrastive learning, this two-stage design enhances scalability without compromising representation quality. Extensive experiments confirm the effectiveness of HiTeC.",
        "arxiv_id": "2508.03104"
    },
    "2508.03121": {
        "SCORE": 15,
        "ARXIVID": "2508.03121",
        "COMMENT": "The paper introduces RegMean++, which enhances model merging by considering intra- and cross-layer dependencies, relevant to model architecture and representation learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "The-Hai Nguyen",
            "Dang Huu-Tien",
            "Takeshi Suzuki",
            "Le-Minh Nguyen"
        ],
        "title": "RegMean++: Enhancing Effectiveness and Generalization of Regression Mean for Model Merging",
        "abstract": "Regression Mean (RegMean), an approach that formulates model merging as a linear regression problem, aims to find the optimal weights for each linear layer in the merge model by minimizing the discrepancy in predictions between the merge and candidate models. RegMean provides a precise closed-form solution for the merging problem; therefore, it offers explainability and computational efficiency. However, RegMean merges each linear layer independently, overlooking how the features and information in the earlier layers propagate through the layers and influence the final prediction in the merge model. In this paper, we introduce RegMean++, a simple yet effective alternative to RegMean, that explicitly incorporates both intra- and cross-layer dependencies between merge models' layers into RegMean's objective. By accounting for these dependencies, RegMean++ better captures the behaviors of the merge model. Extensive experiments demonstrate that RegMean++ consistently outperforms RegMean across diverse settings, including in-domain (ID) and out-of-domain (OOD) generalization, sequential merging, large-scale tasks, and robustness under several types of distribution shifts. Furthermore, RegMean++ achieves competitive or state-of-the-art performance compared to various recent advanced model merging methods. Our code is available at https://github.com/nthehai01/RegMean-plusplus.",
        "arxiv_id": "2508.03121"
    },
    "2508.02995": {
        "SCORE": 15,
        "ARXIVID": "2508.02995",
        "COMMENT": "VCNet introduces a novel architecture inspired by the primate visual cortex, aligning with model architecture innovation.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Brennen A. Hill",
            "Zhang Xinyu",
            "Timothy Putra Prasetio"
        ],
        "title": "VCNet: Recreating High-Level Visual Cortex Principles for Robust Artificial Vision",
        "abstract": "Despite their success in image classification, modern convolutional neural networks (CNNs) exhibit fundamental limitations, including data inefficiency, poor out-of-distribution generalization, and vulnerability to adversarial perturbations. The primate visual system, in contrast, demonstrates superior efficiency and robustness, suggesting that its architectural principles may offer a blueprint for more capable artificial vision systems. This paper introduces Visual Cortex Network (VCNet), a novel neural network architecture whose design is informed by the macro-scale organization of the primate visual cortex. VCNet emulates key biological mechanisms, including hierarchical processing across distinct cortical areas, dual-stream information segregation, and top-down predictive feedback. We evaluate VCNet on two specialized benchmarks: the Spots-10 animal pattern dataset and a light field image classification task. Our results show that VCNet achieves a classification accuracy of 92.1\\% on Spots-10 and 74.4\\% on the light field dataset, surpassing contemporary models of comparable size. This work demonstrates that integrating neuroscientific principles into network design can lead to more efficient and robust models, providing a promising direction for addressing long-standing challenges in machine learning.",
        "arxiv_id": "2508.02995"
    }
}