{
    "2510.05064": {
        "SCORE": 19,
        "ARXIVID": "2510.05064",
        "COMMENT": "Strongly matches Model Compression/Efficiency and Model Architecture: zero-shot model size interpolation by re-incorporating teacher blocks after distillation (no extra training).",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "Sara Kangaslahti",
            "Nihal V. Nayak",
            "Jonathan Geuter",
            "Marco Fumero",
            "Francesco Locatello",
            "David Alvarez-Melis"
        ],
        "title": "Boomerang Distillation Enables Zero-Shot Model Size Interpolation",
        "abstract": "Large language models (LLMs) are typically deployed under diverse memory and compute constraints. Existing approaches build model families by training each size independently, which is prohibitively expensive and provides only coarse-grained size options. In this work, we identify a novel phenomenon that we call boomerang distillation: starting from a large base model (the teacher), one first distills down to a small student and then progressively reconstructs intermediate-sized models by re-incorporating blocks of teacher layers into the student without any additional training. This process produces zero-shot interpolated models of many intermediate sizes whose performance scales smoothly between the student and teacher, often matching or surpassing pretrained or distilled models of the same size. We further analyze when this type of interpolation succeeds, showing that alignment between teacher and student through pruning and distillation is essential. Boomerang distillation thus provides a simple and efficient way to generate fine-grained model families, dramatically reducing training cost while enabling flexible adaptation across deployment environments. The code and models are available at https://github.com/dcml-lab/boomerang-distillation.",
        "arxiv_id": "2510.05064"
    },
    "2510.03638": {
        "SCORE": 19,
        "ARXIVID": "2510.03638",
        "COMMENT": "Model Architecture and Efficiency: theory for implicit (infinite-depth, weight-tied) models showing expressive power scales with test-time iterations and constant-memory training.",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "Jialin Liu",
            "Lisang Ding",
            "Stanley Osher",
            "Wotao Yin"
        ],
        "title": "Implicit Models: Expressive Power Scales with Test-Time Compute",
        "abstract": "Implicit models, an emerging model class, compute outputs by iterating a single parameter block to a fixed point. This architecture realizes an infinite-depth, weight-tied network that trains with constant memory, significantly reducing memory needs for the same level of performance compared to explicit models. While it is empirically known that these compact models can often match or even exceed larger explicit networks by allocating more test-time compute, the underlying mechanism remains poorly understood.   We study this gap through a nonparametric analysis of expressive power. We provide a strict mathematical characterization, showing that a simple and regular implicit operator can, through iteration, progressively express more complex mappings. We prove that for a broad class of implicit models, this process lets the model's expressive power scale with test-time compute, ultimately matching a much richer function class. The theory is validated across three domains: image reconstruction, scientific computing, and operations research, demonstrating that as test-time iterations increase, the complexity of the learned mapping rises, while the solution quality simultaneously improves and stabilizes.",
        "arxiv_id": "2510.03638"
    },
    "2510.04205": {
        "SCORE": 19,
        "ARXIVID": "2510.04205",
        "COMMENT": "Model Compression and Efficiency: provable, minimal KAN compression via polyhedral analysis and \u03b5-equivalent compression with an optimal DP algorithm.",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "Di Zhang"
        ],
        "title": "PolyKAN: A Polyhedral Analysis Framework for Provable and Minimal KAN Compression",
        "abstract": "Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to traditional Multi-Layer Perceptrons (MLPs), offering enhanced interpretability and a strong mathematical foundation. However, their parameter efficiency remains a significant challenge for practical deployment. This paper introduces PolyKAN, a novel theoretical framework for KAN compression that provides formal guarantees on both model size reduction and approximation error. By leveraging the inherent piecewise polynomial structure of KANs, we formulate the compression problem as one of optimal polyhedral region merging. We establish a rigorous polyhedral characterization of KANs, develop a complete theory of $\\epsilon$-equivalent compression, and design an optimal dynamic programming algorithm that guarantees minimal compression under specified error bounds. Our theoretical analysis demonstrates that PolyKAN achieves provably minimal compression while maintaining strict error control, with polynomial-time complexity in all network parameters. The framework provides the first formal foundation for KAN compression with mathematical guarantees, opening new directions for efficient deployment of interpretable neural architectures.",
        "arxiv_id": "2510.04205"
    },
    "2510.04008": {
        "SCORE": 19,
        "ARXIVID": "2510.04008",
        "COMMENT": "Matches Model Compression/Efficiency and HPC: replaces Softmax with linear-time RACE attention via sharpened angular similarity, randomized projections, and soft LSH; enables million-token contexts with reduced memory/runtime.",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "Sahil Joshi",
            "Agniva Chowdhury",
            "Amar Kanakamedala",
            "Ekam Singh",
            "Evan Tu",
            "Anshumali Shrivastava"
        ],
        "title": "Replacing Softmax Similarity with a Sharpened Angular Similarity: Theory and Practice of Scaling To Billion-Context Attention",
        "abstract": "Softmax Attention has a quadratic time complexity, which becomes prohibitive to run at long contexts, even with highly optimized GPU kernels. For example, FlashAttention (an exact, GPU-optimized implementation of Softmax Attention) cannot complete a single forward-backward pass of a multi-head attention layer once the context exceeds ~4 million tokens on an NVIDIA GH200 (96 GB). We introduce RACE Attention, a kernel-inspired alternative to Softmax Attention that is linear in sequence length and embedding dimension. RACE Attention replaces the exponential kernel with a sharpened angular (cosine) similarity, and approximates attention outputs via randomized projections and soft Locality-Sensitive Hashing (LSH). Across language modeling, masked language modeling, and text classification, RACE Attention matches the accuracy of strong baselines while reducing runtime and memory. In a controlled scale test, it processes up to 12 million tokens during a single forward-backward pass on an NVIDIA GH200 GPU and 75 million tokens on an Intel Xeon Gold 5220R CPU, well beyond the practical limits of the current state-of-the-art attention implementations. RACE Attention thus offers a practical, theoretically grounded mechanism for outrageously long context windows on today's hardware. We hope that it gets adopted in practice.",
        "arxiv_id": "2510.04008"
    },
    "2510.03784": {
        "SCORE": 18,
        "ARXIVID": "2510.03784",
        "COMMENT": "Strongly matches Model Architecture/Efficiency: theoretical allocation of attention heads and dimensions across Transformer layers with saturation analysis.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Ruoxi Yu",
            "Haotian Jiang",
            "Jingpu Cheng",
            "Penghao Yu",
            "Qianxiao Li",
            "Zhong Li"
        ],
        "title": "Allocation of Parameters in Transformers",
        "abstract": "Transformers have achieved remarkable successes across a wide range of applications, yet the theoretical foundation of their model efficiency remains underexplored. In this work, we investigate how the model parameters -- mainly attention heads and head dimensions -- should be allocated across layers to balance expressivity and efficiency. We first provide mathematical analysis on the role of early layers in information extraction from an approximation perspective, with a theoretical characterization on the trade-off between the number of heads and head dimension under a fixed parameter budget. In addition, we uncover and prove the \\emph{saturation} behavior of softmax activations: Continuously increasing head dimensions can lead to diminishing returns in learning errors, particularly for long sequences. Supported by both theory and experiments, this saturation pattern suggests that later layers can operate more efficiently with reduced parameters. Combining these insights, we propose principled strategies for allocating attention heads and dimensions across Transformers' layers, shedding light on theoretically-grounded model efficiency of Transformer-based architectures.",
        "arxiv_id": "2510.03784"
    },
    "2510.04476": {
        "SCORE": 18,
        "ARXIVID": "2510.04476",
        "COMMENT": "Model Compression and Efficiency: introduces compressed convolutional attention (CCA/CCGQA) reducing KV-cache and FLOPs with significant speedups; applicable to dense and MoE models.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Tomas Figliolia",
            "Nicholas Alonso",
            "Rishi Iyer",
            "Quentin Anthony",
            "Beren Millidge"
        ],
        "title": "Compressed Convolutional Attention: Efficient Attention in a Compressed Latent Space",
        "abstract": "Multi-headed Attention's (MHA) quadratic compute and linearly growing KV-cache make long-context transformers expensive to train and serve. Prior works such as Grouped Query Attention (GQA) and Multi-Latent Attention (MLA) shrink the cache, speeding decode, but leave compute, which determines prefill and training speed, largely unchanged. We introduce Compressed Convolutional Attention (CCA), a novel attention method which down-projects queries, keys, and values and performs the entire attention operation inside the shared latent space. This simple design dramatically cuts parameters, KV-cache, and FLOPs all at once by the desired compression factor. Because CCA is orthogonal to head-sharing, we combine the two to form Compressed Convolutional Grouped Query Attention (CCGQA), which further tightens the compute-bandwidth Pareto frontier so that users can tune compression toward either FLOP or memory limits without sacrificing quality. Experiments show that CCGQA consistently outperforms both GQA and MLA at equal KV-cache compression on dense and MoE models. Additionally, we show that CCGQA outperforms all other attention methods on MoE models with half the KV-cache of GQA and MLA, achieving an 8x KV-cache compression with no drop in performance compared to standard MHA. CCA and CCGQA also dramatically reduce the FLOP cost of attention which leads to substantially faster training and prefill than existing methods. On H100 GPUs, our fused CCA/CCGQA kernel reduces prefill latency by about 1.7x at a sequence length of 16k relative to MHA, and accelerates backward by about 1.3x.",
        "arxiv_id": "2510.04476"
    },
    "2510.03358": {
        "SCORE": 18,
        "ARXIVID": "2510.03358",
        "COMMENT": "Model Compression and Efficiency: establishes low-rank structure in time-series embeddings, proves compressibility of Q/K/V and attention, introduces flow-of-ranks; guides width/depth/head allocation and achieves large inference/memory reductions on a foundation TS model.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Annan Yu",
            "Danielle C. Maddix",
            "Boran Han",
            "Xiyuan Zhang",
            "Abdul Fatir Ansari",
            "Oleksandr Shchur",
            "Christos Faloutsos",
            "Andrew Gordon Wilson",
            "Michael W. Mahoney",
            "Yuyang Wang"
        ],
        "title": "Understanding Transformers for Time Series: Rank Structure, Flow-of-ranks, and Compressibility",
        "abstract": "Transformers are widely used across data modalities, and yet the principles distilled from text models often transfer imperfectly to models trained to other modalities. In this paper, we analyze Transformers through the lens of rank structure. Our focus is on the time series setting, where the structural properties of the data differ remarkably from those of text or vision. We show that time-series embeddings, unlike text or vision, exhibit sharply decaying singular value spectra: small patch sizes and smooth continuous mappings concentrate the data into low-rank subspaces. From this, we prove that the associated $Q/K/V$ projections admit accurate low-rank approximations, and that attention layers become compressible in proportion to the decay of the embedding spectrum. We introduce the concept of flow-of-ranks, a phenomenon by which nonlinear mixing across depth inflates the rank, explaining why early layers are most amenable to compression and why ranks grow with depth. Guided by these theoretical and empirical results, we use these insights to compress Chronos, a large time series foundation model, achieving a reduction of $65\\%$ in inference time and $81\\%$ in memory, without loss of accuracy. Our findings provide principled guidance for allocating width, depth, and heads in time series foundation models, and for exploiting their inherent compressibility.",
        "arxiv_id": "2510.03358"
    },
    "2508.04581": {
        "SCORE": 18,
        "ARXIVID": "2508.04581",
        "COMMENT": "Model Architecture and Efficiency: structured cross-layer weight sharing via matrix dictionary learning for attention projections, yielding 66.7% parameter reduction with strong performance.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Magauiya Zhussip",
            "Dmitriy Shopkhoev",
            "Ammar Ali",
            "Stamatios Lefkimmiatis"
        ],
        "title": "Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning",
        "abstract": "Large language models (LLMs) have revolutionized AI applications, yet their high computational and memory demands hinder their widespread deployment. Existing compression techniques focus on intra-block optimizations (e.g. low-rank approximation, attention head pruning), while the repetitive layered structure of transformers implies significant inter-block redundancy - a dimension largely unexplored beyond key-value (KV) caching. Inspired by dictionary learning in CNNs, we propose a framework for structured weight sharing across transformer layers. Our approach decomposes attention projection matrices into shared dictionary atoms, reducing the attention module's parameters by 66.7% while achieving on-par performance. Unlike complex methods requiring distillation or architectural changes, MASA (Matrix Atom Sharing in Attention) operates as a drop-in replacement - trained with standard optimizers - and represents each layer's weights as linear combinations of shared matrix atoms. Experiments across scales (100M-700M parameters) show that MASA achieves better benchmark accuracy and perplexity than grouped-query attention (GQA), low-rank baselines and recently proposed Repeat-all-over/Sequential sharing at comparable parameter budgets. Ablation studies confirm robustness to the dictionary size and the efficacy of shared representations in capturing cross-layer statistical regularities. Extending to Vision Transformers (ViT), MASA matches performance metrics on image classification and detection tasks with 66.7% fewer attention parameters. By combining dictionary learning strategies with transformer efficiency, MASA offers a scalable blueprint for parameter-efficient models without sacrificing performance. Finally, we investigate the possibility of employing MASA on pretrained LLMs to reduce their number of parameters without experiencing any significant drop in their performance.",
        "arxiv_id": "2508.04581"
    },
    "2505.02819": {
        "SCORE": 18,
        "ARXIVID": "2505.02819",
        "COMMENT": "Matches Model Compression and Efficiency: training-free depth pruning by replacing Transformer blocks with a linear operator using small calibration data; no retraining needed.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Dmitriy Shopkhoev",
            "Ammar Ali",
            "Magauiya Zhussip",
            "Valentin Malykh",
            "Stamatios Lefkimmiatis",
            "Nikos Komodakis",
            "Sergey Zagoruyko"
        ],
        "title": "ReplaceMe: Network Simplification via Depth Pruning and Transformer Block Linearization",
        "abstract": "We introduce ReplaceMe, a generalized training-free depth pruning method that effectively replaces transformer blocks with a linear operation, while maintaining high performance for low compression ratios. In contrast to conventional pruning approaches that require additional training or fine-tuning, our approach requires only a small calibration dataset that is used to estimate a linear transformation, which approximates the pruned blocks. The estimated linear mapping can be seamlessly merged with the remaining transformer blocks, eliminating the need for any additional network parameters. Our experiments show that ReplaceMe consistently outperforms other training-free approaches and remains highly competitive with state-of-the-art pruning methods that involve extensive retraining/fine-tuning and architectural modifications. Applied to several large language models (LLMs), ReplaceMe achieves up to 25% pruning while retaining approximately 90% of the original model's performance on open benchmarks - without any training or healing steps, resulting in minimal computational overhead (see Fig.1). We provide an open-source library implementing ReplaceMe alongside several state-of-the-art depth pruning techniques, available at https://github.com/mts-ai/ReplaceMe.",
        "arxiv_id": "2505.02819"
    },
    "2510.04212": {
        "SCORE": 18,
        "ARXIVID": "2510.04212",
        "COMMENT": "Matches Low-Precision Training and Efficiency: mechanistic analysis of flash attention failures under low precision and a minimal modification to mitigate biased rounding errors.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Haiquan Qiu",
            "Quanming Yao"
        ],
        "title": "Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention",
        "abstract": "The pursuit of computational efficiency has driven the adoption of low-precision formats for training transformer models. However, this progress is often hindered by notorious training instabilities. This paper provides the first mechanistic explanation for a long-standing and unresolved failure case where training with flash attention in low-precision settings leads to catastrophic loss explosions. Our in-depth analysis reveals that the failure is not a random artifact but caused by two intertwined phenomena: the emergence of similar low-rank representations within the attention mechanism and the compounding effect of biased rounding errors inherent in low-precision arithmetic. We demonstrate how these factors create a vicious cycle of error accumulation that corrupts weight updates, ultimately derailing the training dynamics. To validate our findings, we introduce a minimal modification to the flash attention that mitigates the bias in rounding errors. This simple change stabilizes the training process, confirming our analysis and offering a practical solution to this persistent problem.",
        "arxiv_id": "2510.04212"
    },
    "2510.03291": {
        "SCORE": 18,
        "ARXIVID": "2510.03291",
        "COMMENT": "Compression/Efficiency: unified post-training pruning with mirror descent combining local saliency and global coordination; supports unstructured and N:M sparsity with one-shot mask generation.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Yizhuo Ding",
            "Wanying Qu",
            "Jiawei Geng",
            "Wenqi Shao",
            "Yanwei Fu"
        ],
        "title": "UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs",
        "abstract": "Large Language Models (LLMs) achieve strong performance across diverse tasks but face prohibitive computational and memory costs. Pruning offers a promising path by inducing sparsity while preserving architectural flexibility. However, existing methods struggle to balance efficiency and robustness: local metric approaches prune layer by layer but often collapse under high sparsity, whereas global feedback methods enforce consistency at the cost of expensive weight updates or restrictive semi-structured formats. We present UniPruning, a unified post-training pruning framework that combines the speed of local saliency metrics with the stability of global coordination, enabled by a mirror descent based optimization, all without updating model weights. UniPruning leverages fast layer-wise scoring and a lightweight global controller to allocate a single sparsity budget, supporting both unstructured and semi-structured N :M pruning within one framework. After a brief calibration, it can generate pruning masks for arbitrary sparsity levels in one shot, and adapts seamlessly to hardware-aware constraints. Extensive experiments on multiple pretrained LLM families and standard benchmarks show that UniPruning consistently delivers competitive or superior perplexity and zero-shot accuracy. Ablation studies further highlight the importance of mirror descent and local saliency anchoring. Overall, UniPruning provides an efficient, principled, and scalable solution for sparsifying large-scale LLMs. Our code is available at: https://github.com/RainbowQTT/UniPruning.",
        "arxiv_id": "2510.03291"
    },
    "2510.03267": {
        "SCORE": 18,
        "ARXIVID": "2510.03267",
        "COMMENT": "Directly matches Model Compression and Efficiency: post-training ternarization (quantization) for LLMs with asymmetric ternary quantizer, iterative fitting, and activation-aware refinement.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Xianglong Yan",
            "Chengzhu Bao",
            "Zhiteng Li",
            "Tianao Zhang",
            "Kaicheng Yang",
            "Haotong Qin",
            "Ruobing Xie",
            "Xingwu Sun",
            "Yulun Zhang"
        ],
        "title": "PT$^2$-LLM: Post-Training Ternarization for Large Language Models",
        "abstract": "Large Language Models (LLMs) have shown impressive capabilities across diverse tasks, but their large memory and compute demands hinder deployment. Ternarization has gained attention as a promising compression technique, delivering substantial size reduction and high computational efficiency. However, its potential in the post-training quantization (PTQ) setting remains underexplored, due to the challenge of training-free parameter optimization and the quantization difficulty posed by outliers and dispersed weights. To address these issues, we propose PT$^2$-LLM, a post-training ternarization framework tailored for LLMs. At its core is an Asymmetric Ternary Quantizer equipped with a two-stage refinement pipeline: (1) Iterative Ternary Fitting (ITF), which alternates between optimal ternary grid construction and flexible rounding to minimize quantization error, and (2) Activation-aware Grid Alignment (AGA), which further refines the ternary grid to better match full-precision outputs. In addition, we propose a plug-and-play Structural Similarity-based Reordering (SSR) strategy that leverages inter-column structural similarity to ease quantization and mitigate outlier effects, further enhancing overall performance. Extensive experiments demonstrate that PT$^2$-LLM delivers competitive performance against state-of-the-art (SOTA) 2-bit PTQ methods with lower memory cost, while also accelerating both prefill and decoding to achieve end-to-end speedup. The code and models will be available at https://github.com/XIANGLONGYAN/PT2-LLM.",
        "arxiv_id": "2510.03267"
    },
    "2510.03279": {
        "SCORE": 18,
        "ARXIVID": "2510.03279",
        "COMMENT": "Strongly matches Model Architecture: theoretical analysis of Mamba\u2019s memory decay and a new MemMamba architecture adding state summarization and cross-layer/token attention with linear complexity.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Youjin Wang",
            "Yangjingyi Chen",
            "Jiahao Yan",
            "Jiaxuan Lu",
            "Xiao Sun"
        ],
        "title": "MemMamba: Rethinking Memory Patterns in State Space Model",
        "abstract": "With the explosive growth of data, long-sequence modeling has become increasingly important in tasks such as natural language processing and bioinformatics. However, existing methods face inherent trade-offs between efficiency and memory. Recurrent neural networks suffer from gradient vanishing and explosion, making them hard to scale. Transformers can model global dependencies but are constrained by quadratic complexity. Recently, selective state-space models such as Mamba have demonstrated high efficiency with O(n) time and O(1) recurrent inference, yet their long-range memory decays exponentially. In this work, we conduct mathematical derivations and information-theoretic analysis to systematically uncover the memory decay mechanism of Mamba, answering a fundamental question: what is the nature of Mamba's long-range memory and how does it retain information? To quantify key information loss, we further introduce horizontal-vertical memory fidelity metrics that capture degradation both within and across layers. Inspired by how humans distill and retain salient information when reading long documents, we propose MemMamba, a novel architectural framework that integrates state summarization mechanism together with cross-layer and cross-token attention, which alleviates long-range forgetting while preserving linear complexity. MemMamba achieves significant improvements over existing Mamba variants and Transformers on long-sequence benchmarks such as PG19 and Passkey Retrieval, while delivering a 48% speedup in inference efficiency. Both theoretical analysis and empirical results demonstrate that MemMamba achieves a breakthrough in the complexity-memory trade-off, offering a new paradigm for ultra-long sequence modeling.",
        "arxiv_id": "2510.03279"
    },
    "2510.03989": {
        "SCORE": 18,
        "ARXIVID": "2510.03989",
        "COMMENT": "Model Architecture\u2014provides a continuous operator-theoretic formulation of Transformers (self-attention as integral operator, layer norm as projection), deepening theoretical foundations.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Xue-Cheng Tai",
            "Hao Liu",
            "Lingfeng Li",
            "Raymond H. Chan"
        ],
        "title": "A Mathematical Explanation of Transformers for Large Language Models and GPTs",
        "abstract": "The Transformer architecture has revolutionized the field of sequence modeling and underpins the recent breakthroughs in large language models (LLMs). However, a comprehensive mathematical theory that explains its structure and operations remains elusive. In this work, we propose a novel continuous framework that rigorously interprets the Transformer as a discretization of a structured integro-differential equation. Within this formulation, the self-attention mechanism emerges naturally as a non-local integral operator, and layer normalization is characterized as a projection to a time-dependent constraint. This operator-theoretic and variational perspective offers a unified and interpretable foundation for understanding the architecture's core components, including attention, feedforward layers, and normalization. Our approach extends beyond previous theoretical analyses by embedding the entire Transformer operation in continuous domains for both token indices and feature dimensions. This leads to a principled and flexible framework that not only deepens theoretical insight but also offers new directions for architecture design, analysis, and control-based interpretations. This new interpretation provides a step toward bridging the gap between deep learning architectures and continuous mathematical modeling, and contributes a foundational perspective to the ongoing development of interpretable and theoretically grounded neural network models.",
        "arxiv_id": "2510.03989"
    },
    "2510.04547": {
        "SCORE": 18,
        "ARXIVID": "2510.04547",
        "COMMENT": "Model Compression and Efficiency\u2014training-free post-training quantization for vision encoders via prefix registers (RegCache) to suppress activation outliers.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Seunghyeon Kim",
            "Jinho Kim",
            "Taesun Yeom",
            "Wonpyo Park",
            "Kyuyeun Kim",
            "Jaeho Lee"
        ],
        "title": "Post-training quantization of vision encoders needs prefixing registers",
        "abstract": "Transformer-based vision encoders -- such as CLIP -- are central to multimodal intelligence, powering applications from autonomous web agents to robotic control. Since these applications often demand real-time processing of massive visual data, reducing the inference cost of vision encoders is critical. Post-training quantization offers a practical path, but remains challenging even at 8-bit precision due to massive-scale activations (i.e., outliers). In this work, we propose $\\textit{RegCache}$, a training-free algorithm to mitigate outliers in vision encoders, enabling quantization with significantly smaller accuracy drops. The proposed RegCache introduces outlier-prone yet semantically meaningless prefix tokens to the target vision encoder, which prevents other tokens from having outliers. Notably, we observe that outliers in vision encoders behave differently from those in language models, motivating two technical innovations: middle-layer prefixing and token deletion. Experiments show that our method consistently improves the accuracy of quantized models across both text-supervised and self-supervised vision encoders.",
        "arxiv_id": "2510.04547"
    },
    "2510.03246": {
        "SCORE": 18,
        "ARXIVID": "2510.03246",
        "COMMENT": "Model Compression/Efficiency\u2014structured global pruning with O(sqrt(N)) memory via ADMM and derived layer-wise sparsity allocation.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Xinyuan Song",
            "Guangji Bai",
            "Liang Zhao"
        ],
        "title": "StructPrune: Structured Global Pruning asymptotics with $\\mathcal{O}(\\sqrt{N})$ GPU Memory",
        "abstract": "Pruning is critical for scaling large language models (LLMs). Global pruning achieves strong performance but requires $\\mathcal{O}(N)$ memory, which is infeasible for billion-parameter models. Local pruning reduces GPU memory usage to that of a single layer by pruning layers independently, but it neglects inter-layer dependencies and often leads to suboptimal performance in high-sparsity regimes. Unlike unstructured pruning, structured pruning produces regular sparsity patterns that align well with GPU kernels and library optimizations, making it more hardware-efficient. However, structured pruning typically relies on global pruning, since structured patterns are more prone to severe performance degradation under local optimization. To jointly achieve structured pruning and the memory efficiency of local pruning, we propose a divide-and-conquer strategy that decomposes the global pruning problem into coordinated subproblems across different modules, each of which fits within limited GPU memory. Building on this idea, we design \\textbf{STRUPRUNE}, an ADMM-based framework that integrates structured sparsity into the pruning process, combining the memory efficiency of local pruning with the hardware compatibility of structured methods. We derive a closed-form analytical solution for structured pruning masks that provides an explicit rule for layer-wise sparsity allocation, and further develop an energy-based asymptotic framework yielding a softmax-form allocation scheme that simplifies optimization while adapting to heterogeneous layer importance. Experiments demonstrate that STRUPRUNE matches the perplexity of global structured pruning while reducing memory cost from $\\mathcal{O}(N)$ to $\\mathcal{O}(\\sqrt{N})$, enabling practical deployment at the billion-parameter scale.",
        "arxiv_id": "2510.03246"
    },
    "2510.03275": {
        "SCORE": 18,
        "ARXIVID": "2510.03275",
        "COMMENT": "Model Compression and Efficiency: introduces Sigma-Delta 1-bit/1.58-bit quantization for LLMs with adjustable and fine-grained OSR allocation plus Hadamard-based weight smoothing.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Junhao Xia",
            "Ming Zhao",
            "Limin Xiao",
            "Xiujun Zhang"
        ],
        "title": "SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size",
        "abstract": "Large language models (LLMs) face significant computational and memory challenges, making extremely low-bit quantization crucial for their efficient deployment. In this work, we introduce SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size, a novel framework that enables extremely low-bit quantization of LLMs while preserving their linguistic reasoning capabilities. A distinctive feature of SDQ-LLM is the continuous adjustability of the Over-Sampling Ratio (OSR), enabling dynamic adaptation to memory or VRAM constraints by selecting fractional OSR (e.g. 2.5 times) for an optimal trade-off between model size and accuracy. SDQ-LLM uses upsampling combined with Sigma-Delta Quantizer to binarize or ternarize LLMs weights, encoding high-precision parameters into 1-bit or 1.58-bit representations, replacing the multiplication operations within linear layers with addition. This approach significantly enhances inference efficiency under extremely low-bit quantization. To further reduce the loss of quantization precision, we incorporate Hadamard-based weight smoothing prior to quantization, improving the stability and robustness of the weight representations. Furthermore, to fully leverage the continuity of the OSR and reduce precision loss, recognizing the correlation between quantization sensitivity and weight variance, we propose a fine-grained, layer- and linear-wise OSR allocation strategy, MultiOSR. This strategy distributes OSR both across layers and within each layer, based on weight variance and parameter scale. Finally, extensive experiments on OPT and LLaMA model families demonstrate that SDQ-LLM achieves a more efficient and high-precision performance even under highly aggressive low-OSR settings. Our code is available at https://github.com/Dreamlittlecat/LLM-Quant-Factory.",
        "arxiv_id": "2510.03275"
    },
    "2510.04944": {
        "SCORE": 18,
        "ARXIVID": "2510.04944",
        "COMMENT": "Matches Model Architecture: formalizes and generalizes the SSM\u2013masked-attention duality, providing necessary/sufficient conditions and training complexity bounds; expands efficient Transformer/SSM design space.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Jerry Yao-Chieh Hu",
            "Xiwen Zhang",
            "Weimin Wu",
            "Han Liu"
        ],
        "title": "On Structured State-Space Duality",
        "abstract": "Structured State-Space Duality (SSD) [Dao & Gu, ICML 2024] is an equivalence between a simple Structured State-Space Model (SSM) and a masked attention mechanism. In particular, a state-space model with a scalar-times-identity state matrix is equivalent to a masked self-attention with a $1$-semiseparable causal mask. Consequently, the same sequence transformation (model) has two algorithmic realizations: as a linear-time $O(T)$ recurrence or as a quadratic-time $O(T^2)$ attention. In this note, we formalize and generalize this duality: (i) we extend SSD from the scalar-identity case to general diagonal SSMs (diagonal state matrices); (ii) we show that these diagonal SSMs match the scalar case's training complexity lower bounds while supporting richer dynamics; (iii) we establish a necessary and sufficient condition under which an SSM is equivalent to $1$-semiseparable masked attention; and (iv) we show that such duality fails to extend to standard softmax attention due to rank explosion. Together, these results tighten bridge between recurrent SSMs and Transformers, and widen the design space for expressive yet efficient sequence models.",
        "arxiv_id": "2510.04944"
    },
    "2510.03470": {
        "SCORE": 18,
        "ARXIVID": "2510.03470",
        "COMMENT": "Model Architecture: Residual Expansion Theorem giving first-principles analysis of depth in residual networks and principled scaling to control combinatorial path growth.",
        "RELEVANCE": 9,
        "NOVELTY": 9,
        "authors": [
            "Benoit Dherin",
            "Michael Munn"
        ],
        "title": "On residual network depth",
        "abstract": "Deep residual architectures, such as ResNet and the Transformer, have enabled models of unprecedented depth, yet a formal understanding of why depth is so effective remains an open question. A popular intuition, following Veit et al. (2016), is that these residual networks behave like ensembles of many shallower models. Our key finding is an explicit analytical formula that verifies this ensemble perspective, proving that increasing network depth is mathematically equivalent to expanding the size of this implicit ensemble. Furthermore, our expansion reveals a hierarchical ensemble structure in which the combinatorial growth of computation paths leads to an explosion in the output signal, explaining the historical necessity of normalization layers in training deep models. This insight offers a first principles explanation for the historical dependence on normalization layers and sheds new light on a family of successful normalization-free techniques like SkipInit and Fixup. However, while these previous approaches infer scaling factors through optimizer analysis or a heuristic analogy to Batch Normalization, our work offers the first explanation derived directly from the network's inherent functional structure. Specifically, our Residual Expansion Theorem reveals that scaling each residual module provides a principled solution to taming the combinatorial explosion inherent to these architectures. We further show that this scaling acts as a capacity controls that also implicitly regularizes the model's complexity.",
        "arxiv_id": "2510.03470"
    },
    "2510.04694": {
        "SCORE": 17,
        "ARXIVID": "2510.04694",
        "COMMENT": "Mixture-of-Experts: analysis of multilingual routing dynamics with inference-time router steering to enhance cross-lingual expert utilization.",
        "RELEVANCE": 10,
        "NOVELTY": 7,
        "authors": [
            "Lucas Bandarkar",
            "Chenyuan Yang",
            "Mohsen Fayyaz",
            "Junlin Hu",
            "Nanyun Peng"
        ],
        "title": "Multilingual Routing in Mixture-of-Experts",
        "abstract": "Mixture-of-Experts (MoE) architectures have become the key to scaling modern LLMs, yet little is understood about how their sparse routing dynamics respond to multilingual data. In this work, we analyze expert routing patterns using parallel multilingual datasets and present highly interpretable layer-wise phenomena. We find that MoE models route tokens in language-specific ways in the early and late decoder layers but exhibit significant cross-lingual routing alignment in middle layers, mirroring parameter-sharing trends observed in dense LLMs. In particular, we reveal a clear, strong correlation between a model's performance in a given language and how similarly its tokens are routed to English in these layers. Extending beyond correlation, we explore inference-time interventions that induce higher cross-lingual routing alignment. We introduce a method that steers the router by promoting middle-layer task experts frequently activated in English, and it successfully increases multilingual performance. These 1-2% gains are remarkably consistent across two evaluation tasks, three models, and 15+ languages, especially given that these simple interventions override routers of extensively trained, state-of-the-art LLMs. In comparison, interventions outside of the middle layers or targeting multilingual-specialized experts only yield performance degradation. Altogether, we present numerous findings that explain how MoEs process non-English text and demonstrate that generalization is limited by the model's ability to leverage language-universal experts in all languages.",
        "arxiv_id": "2510.04694"
    },
    "2510.04044": {
        "SCORE": 17,
        "ARXIVID": "2510.04044",
        "COMMENT": "Strongly matches Model Compression/Efficiency: post-training quantization with provable local convexity and efficient range search.",
        "RELEVANCE": 10,
        "NOVELTY": 7,
        "authors": [
            "Bingtao Yang",
            "Yujia Wang",
            "Mengzhi Jiao",
            "Hongwei Huo"
        ],
        "title": "Quantization Range Estimation for Convolutional Neural Networks",
        "abstract": "Post-training quantization for reducing the storage of deep neural network models has been demonstrated to be an effective way in various tasks. However, low-bit quantization while maintaining model accuracy is a challenging problem. In this paper, we present a range estimation method to improve the quantization performance for post-training quantization. We model the range estimation into an optimization problem of minimizing quantization errors by layer-wise local minima. We prove this problem is locally convex and present an efficient search algorithm to find the optimal solution. We propose the application of the above search algorithm to the transformed weights space to do further improvement in practice. Our experiments demonstrate that our method outperforms state-of-the-art performance generally on top-1 accuracy for image classification tasks on the ResNet series models and Inception-v3 model. The experimental results show that the proposed method has almost no loss of top-1 accuracy in 8-bit and 6-bit settings for image classifications, and the accuracy of 4-bit quantization is also significantly improved. The code is available at https://github.com/codeiscommitting/REQuant.",
        "arxiv_id": "2510.04044"
    },
    "2510.03293": {
        "SCORE": 17,
        "ARXIVID": "2510.03293",
        "COMMENT": "Model Architecture + HPC/Efficiency: MoE inference-time routing that adapts to gate score distributions to balance expert load and reduce latency without retraining.",
        "RELEVANCE": 10,
        "NOVELTY": 7,
        "authors": [
            "Rana Shahout",
            "Colin Cai",
            "Yilun Du",
            "Minlan Yu",
            "Michael Mitzenmacher"
        ],
        "title": "From Score Distributions to Balance: Plug-and-Play Mixture-of-Experts Routing",
        "abstract": "Mixture-of-Experts (MoE) models can scale parameter capacity by routing each token to a subset of experts through a learned gate function. While conditional routing reduces training costs, it shifts the burden on inference memory: expert parameters and activations consume memory, limiting the number of experts per device. As tokens are routed, some experts become overloaded while others are underutilized. Because experts are mapped to GPUs, this imbalance translates directly into degraded system performance in terms of latency, throughput, and cost. We present LASER, a plug-and-play, inference-time routing algorithm that balances load while preserving accuracy. LASER adapts to the shape of the gate's score distribution. When scores provide a clear preference, it routes to the strongest experts; when scores are more uniform, it broadens the set of viable experts and routes to the least-loaded among them. Because LASER relies only on gate scores from a trained model, it integrates directly into existing MoE inference pipelines without retraining or finetuning. We evaluate LASER on Mixtral-8x7B and DeepSeek-MoE-16b-chat across four datasets (ARC-Easy, ARC-Challenge, MMLU, and GSM8K). LASER improves load balancing, translating into lower latency and higher throughput, while keeping the accuracy changes negligible.",
        "arxiv_id": "2510.03293"
    },
    "2510.04130": {
        "SCORE": 17,
        "ARXIVID": "2510.04130",
        "COMMENT": "Matches Model Architecture (Transformers): theoretical analysis of position embeddings for length generalization (LRC/SRC) plus a learning-based PE framework and scale hints.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Yang Chen",
            "Yitao Liang",
            "Zhouchen Lin"
        ],
        "title": "On the Limitations and Capabilities of Position Embeddings for Length Generalization",
        "abstract": "In Transformers, Position Embeddings (PEs) significantly influence Length Generalization (LG) performance, yet their fundamental role remains unclear. In this work, we investigate the limitations and capabilities of PEs in achieving LG. We theoretically analyze PEs in Position-Only Linear Attentions (POLAs), introducing Linear Representation Complexity (LRC) to characterize when PEs enable LG. Our analysis shows that PEs do not expand computational capabilities but structure learned computations across positions. Extending to practical Transformers, we propose Sequential Representation Complexity (SRC) and conjecture that LG is possible if and only if SRC remains invariant across scales. We support this hypothesis with empirical evidence in various reasoning tasks. To enhance LG, we introduce Scale Hint, allowing flexible instance scaling, and a Learning-Based Position Embedding framework that automatically learns positional relations. Our work provides theoretical insights and practical strategies for improving LG in Transformers.",
        "arxiv_id": "2510.04130"
    },
    "2510.03272": {
        "SCORE": 17,
        "ARXIVID": "2510.03272",
        "COMMENT": "Model Architecture: PDE-based continuous dynamical system analysis of Transformer components (attention, FFN, residuals, layer norm) as stabilizers.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Yukun Zhang",
            "Xueqing Zhou"
        ],
        "title": "PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling",
        "abstract": "The Transformer architecture has revolutionized artificial intelligence, yet a principled theoretical understanding of its internal mechanisms remains elusive. This paper introduces a novel analytical framework that reconceptualizes the Transformer's discrete, layered structure as a continuous spatiotemporal dynamical system governed by a master Partial Differential Equation (PDE). Within this paradigm, we map core architectural components to distinct mathematical operators: self-attention as a non-local interaction, the feed-forward network as a local reaction, and, critically, residual connections and layer normalization as indispensable stabilization mechanisms. We do not propose a new model, but rather employ the PDE system as a theoretical probe to analyze the mathematical necessity of these components. By comparing a standard Transformer with a PDE simulator that lacks explicit stabilizers, our experiments provide compelling empirical evidence for our central thesis. We demonstrate that without residual connections, the system suffers from catastrophic representational drift, while the absence of layer normalization leads to unstable, explosive training dynamics. Our findings reveal that these seemingly heuristic \"tricks\" are, in fact, fundamental mathematical stabilizers required to tame an otherwise powerful but inherently unstable continuous system. This work offers a first-principles explanation for the Transformer's design and establishes a new paradigm for analyzing deep neural networks through the lens of continuous dynamics.",
        "arxiv_id": "2510.03272"
    },
    "2510.03273": {
        "SCORE": 17,
        "ARXIVID": "2510.03273",
        "COMMENT": "High Performance Computing: training without global backprop via local synergistic distillation to remove update locking and reduce activation memory, enabling parallel module updates.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Chenhao Ye",
            "Ming Tang"
        ],
        "title": "Learning without Global Backpropagation via Synergistic Information Distillation",
        "abstract": "Backpropagation (BP), while foundational to deep learning, imposes two critical scalability bottlenecks: update locking, where network modules remain idle until the entire backward pass completes, and high memory consumption due to storing activations for gradient computation. To address these limitations, we introduce Synergistic Information Distillation (SID), a novel training framework that reframes deep learning as a cascade of local cooperative refinement problems. In SID, a deep network is structured as a pipeline of modules, each imposed with a local objective to refine a probabilistic belief about the ground-truth target. This objective balances fidelity to the target with consistency to the belief from its preceding module. By decoupling the backward dependencies between modules, SID enables parallel training and hence eliminates update locking and drastically reduces memory requirements. Meanwhile, this design preserves the standard feed-forward inference pass, making SID a versatile drop-in replacement for BP. We provide a theoretical foundation, proving that SID guarantees monotonic performance improvement with network depth. Empirically, SID consistently matches or surpasses the classification accuracy of BP, exhibiting superior scalability and pronounced robustness to label noise.Code is available at: https://github.com/ychAlbert/sid-bp",
        "arxiv_id": "2510.03273"
    },
    "2510.03434": {
        "SCORE": 17,
        "ARXIVID": "2510.03434",
        "COMMENT": "Matches Model Architecture (MoE) and High Performance/Systems: decentralized training of independent experts with a router, eliminating synchronization.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Zhiying Jiang",
            "Raihan Seraj",
            "Marcos Villagra",
            "Bidhan Roy"
        ],
        "title": "Paris: A Decentralized Trained Open-Weight Diffusion Model",
        "abstract": "We present Paris, the first publicly released diffusion model pre-trained entirely through decentralized computation. Paris demonstrates that high-quality text-to-image generation can be achieved without centrally coordinated infrastructure. Paris is open for research and commercial use. Paris required implementing our Distributed Diffusion Training framework from scratch. The model consists of 8 expert diffusion models (129M-605M parameters each) trained in complete isolation with no gradient, parameter, or intermediate activation synchronization. Rather than requiring synchronized gradient updates across thousands of GPUs, we partition data into semantically coherent clusters where each expert independently optimizes its subset while collectively approximating the full distribution. A lightweight transformer router dynamically selects appropriate experts at inference, achieving generation quality comparable to centrally coordinated baselines. Eliminating synchronization enables training on heterogeneous hardware without specialized interconnects. Empirical validation confirms that Paris's decentralized training maintains generation quality while removing the dedicated GPU cluster requirement for large-scale diffusion models. Paris achieves this using 14$\\times$ less training data and 16$\\times$ less compute than the prior decentralized baseline.",
        "arxiv_id": "2510.03434"
    },
    "2510.03507": {
        "SCORE": 17,
        "ARXIVID": "2510.03507",
        "COMMENT": "Matches Model Compression and Efficiency and High-Performance Computing: communication-efficient distributed training with compression via a new EF\u2013Dual Averaging method and convergence analysis for composite objectives.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Yuan Gao",
            "Anton Rodomanov",
            "Jeremy Rack",
            "Sebastian Stich"
        ],
        "title": "Composite Optimization with Error Feedback: the Dual Averaging Approach",
        "abstract": "Communication efficiency is a central challenge in distributed machine learning training, and message compression is a widely used solution. However, standard Error Feedback (EF) methods (Seide et al., 2014), though effective for smooth unconstrained optimization with compression (Karimireddy et al., 2019), fail in the broader and practically important setting of composite optimization, which captures, e.g., objectives consisting of a smooth loss combined with a non-smooth regularizer or constraints. The theoretical foundation and behavior of EF in the context of the general composite setting remain largely unexplored. In this work, we consider composite optimization with EF. We point out that the basic EF mechanism and its analysis no longer stand when a composite part is involved. We argue that this is because of a fundamental limitation in the method and its analysis technique. We propose a novel method that combines Dual Averaging with EControl (Gao et al., 2024), a state-of-the-art variant of the EF mechanism, and achieves for the first time a strong convergence analysis for composite optimization with error feedback. Along with our new algorithm, we also provide a new and novel analysis template for inexact dual averaging method, which might be of independent interest. We also provide experimental results to complement our theoretical findings.",
        "arxiv_id": "2510.03507"
    },
    "2510.05040": {
        "SCORE": 17,
        "ARXIVID": "2510.05040",
        "COMMENT": "Matches Model Architecture and Efficiency: reveals implicit Mixture-of-Experts\u2013like specialization in diffusion LLMs and proposes a training-free test-time ensembling method (HEX) across generation schedules.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Jihoon Lee",
            "Hoyeon Moon",
            "Kevin Zhai",
            "Arun Kumar Chithanar",
            "Anit Kumar Sahu",
            "Soummya Kar",
            "Chul Lee",
            "Souradip Chakraborty",
            "Amrit Singh Bedi"
        ],
        "title": "Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive Experts",
        "abstract": "Diffusion-based large language models (dLLMs) are trained flexibly to model extreme dependence in the data distribution; however, how to best utilize this information at inference time remains an open problem. In this work, we uncover an interesting property of these models: dLLMs trained on textual data implicitly learn a mixture of semi-autoregressive experts, where different generation orders reveal different specialized behaviors. We show that committing to any single, fixed inference time schedule, a common practice, collapses performance by failing to leverage this latent ensemble. To address this, we introduce HEX (Hidden semiautoregressive EXperts for test-time scaling), a training-free inference method that ensembles across heterogeneous block schedules. By doing a majority vote over diverse block-sized generation paths, HEX robustly avoids failure modes associated with any single fixed schedule. On reasoning benchmarks such as GSM8K, it boosts accuracy by up to 3.56X (from 24.72% to 88.10%), outperforming top-K margin inference and specialized fine-tuned methods like GRPO, without additional training. HEX even yields significant gains on MATH benchmark from 16.40% to 40.00%, scientific reasoning on ARC-C from 54.18% to 87.80%, and TruthfulQA from 28.36% to 57.46%. Our results establish a new paradigm for test-time scaling in diffusion-based LLMs (dLLMs), revealing that the sequence in which masking is performed plays a critical role in determining performance during inference.",
        "arxiv_id": "2510.05040"
    },
    "2510.04327": {
        "SCORE": 17,
        "ARXIVID": "2510.04327",
        "COMMENT": "Matches Training Dynamics/Initialization and scaling laws: introduces AM-\u03bcP with provable learning-rate depth scaling (L^{-3/2}) for CNNs/ResNets enabling zero-shot LR transfer.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Haosong Zhang",
            "Shenxi Wu",
            "Yichi Zhang",
            "Wei Lin"
        ],
        "title": "Arithmetic-Mean $\\mu$P for Modern Architectures: A Unified Learning-Rate Scale for CNNs and ResNets",
        "abstract": "Choosing an appropriate learning rate remains a key challenge in scaling depth of modern deep networks. The classical maximal update parameterization ($\\mu$P) enforces a fixed per-layer update magnitude, which is well suited to homogeneous multilayer perceptrons (MLPs) but becomes ill-posed in heterogeneous architectures where residual accumulation and convolutions introduce imbalance across layers. We introduce Arithmetic-Mean $\\mu$P (AM-$\\mu$P), which constrains not each individual layer but the network-wide average one-step pre-activation second moment to a constant scale. Combined with a residual-aware He fan-in initialization - scaling residual-branch weights by the number of blocks ($\\mathrm{Var}[W]=c/(K\\cdot \\mathrm{fan\\text{-}in})$) - AM-$\\mu$P yields width-robust depth laws that transfer consistently across depths. We prove that, for one- and two-dimensional convolutional networks, the maximal-update learning rate satisfies $\\eta^\\star(L)\\propto L^{-3/2}$; with zero padding, boundary effects are constant-level as $N\\gg k$. For standard residual networks with general conv+MLP blocks, we establish $\\eta^\\star(L)=\\Theta(L^{-3/2})$, with $L$ the minimal depth. Empirical results across a range of depths confirm the $-3/2$ scaling law and enable zero-shot learning-rate transfer, providing a unified and practical LR principle for convolutional and deep residual networks without additional tuning overhead.",
        "arxiv_id": "2510.04327"
    },
    "2510.03511": {
        "SCORE": 17,
        "ARXIVID": "2510.03511",
        "COMMENT": "Model Architecture: introduces an equivariant Transformer via Platonic group\u2013based attention and weight sharing; formally equivalent to dynamic group convolution and includes a linear-time convolutional variant (Efficiency).",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Mohammad Mohaiminul Islam",
            "Rishabh Anand",
            "David R. Wessels",
            "Friso de Kruiff",
            "Thijs P. Kuipers",
            "Rex Ying",
            "Clara I. S\\'anchez",
            "Sharvaree Vadgama",
            "Georg B\\\"okman",
            "Erik J. Bekkers"
        ],
        "title": "Platonic Transformers: A Solid Choice For Equivariance",
        "abstract": "While widespread, Transformers lack inductive biases for geometric symmetries common in science and computer vision. Existing equivariant methods often sacrifice the efficiency and flexibility that make Transformers so effective through complex, computationally intensive designs. We introduce the Platonic Transformer to resolve this trade-off. By defining attention relative to reference frames from the Platonic solid symmetry groups, our method induces a principled weight-sharing scheme. This enables combined equivariance to continuous translations and Platonic symmetries, while preserving the exact architecture and computational cost of a standard Transformer. Furthermore, we show that this attention is formally equivalent to a dynamic group convolution, which reveals that the model learns adaptive geometric filters and enables a highly scalable, linear-time convolutional variant. Across diverse benchmarks in computer vision (CIFAR-10), 3D point clouds (ScanObjectNN), and molecular property prediction (QM9, OMol25), the Platonic Transformer achieves competitive performance by leveraging these geometric constraints at no additional cost.",
        "arxiv_id": "2510.03511"
    },
    "2510.03268": {
        "SCORE": 17,
        "ARXIVID": "2510.03268",
        "COMMENT": "Matches Representation Learning: first theoretical framework explaining modality gap in multimodal contrastive learning via dimension collapse and alignment theory.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Lingjie Yi",
            "Raphael Douady",
            "Chao Chen"
        ],
        "title": "Decrypt Modality Gap in Multimodal Contrastive Learning: From Convergent Representation to Pair Alignment",
        "abstract": "Multimodal contrastive learning (MCL) aims to embed data from different modalities in a shared embedding space. However, empirical evidence shows that representations from different modalities occupy completely separate regions of embedding space, a phenomenon referred to as the modality gap. Moreover, experimental findings on how the size of the modality gap influences downstream performance are inconsistent. These observations raise two key questions: (1) What causes the modality gap? (2) How does it affect downstream tasks? To address these questions, this paper introduces the first theoretical framework for analyzing the convergent optimal representations of MCL and the modality alignment when training is optimized. Specifically, we prove that without any constraint or under the cone constraint, the modality gap converges to zero. Under the subspace constraint (i.e., representations of two modalities fall into two distinct hyperplanes due to dimension collapse), the modality gap converges to the smallest angle between the two hyperplanes. This result identifies \\emph{dimension collapse} as the fundamental origin of the modality gap. Furthermore, our theorems demonstrate that paired samples cannot be perfectly aligned under the subspace constraint. The modality gap influences downstream performance by affecting the alignment between sample pairs. We prove that, in this case, perfect alignment between two modalities can still be achieved via two ways: hyperplane rotation and shared space projection.",
        "arxiv_id": "2510.03268"
    },
    "2510.04758": {
        "SCORE": 17,
        "ARXIVID": "2510.04758",
        "COMMENT": "Representation Learning: proves affine identifiability for nonlinear CCA under latent priors, with whitening necessity and finite-sample convergence guarantees.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Zhiwei Han",
            "Stefan Matthes",
            "Hao Shen"
        ],
        "title": "Provable Affine Identifiability of Nonlinear CCA under Latent Distributional Priors",
        "abstract": "In this work, we establish conditions under which nonlinear CCA recovers the ground-truth latent factors up to an orthogonal transform after whitening. Building on the classical result that linear mappings maximize canonical correlations under Gaussian priors, we prove affine identifiability for a broad class of latent distributions in the population setting. Central to our proof is a reparameterization result that transports the analysis from observation space to source space, where identifiability becomes tractable. We further show that whitening is essential for ensuring boundedness and well-conditioning, thereby underpinning identifiability. Beyond the population setting, we prove that ridge-regularized empirical CCA converges to its population counterpart, transferring these guarantees to the finite-sample regime. Experiments on a controlled synthetic dataset and a rendered image dataset validate our theory and demonstrate the necessity of its assumptions through systematic ablations.",
        "arxiv_id": "2510.04758"
    },
    "2510.04500": {
        "SCORE": 17,
        "ARXIVID": "2510.04500",
        "COMMENT": "Model Architecture and Efficiency\u2014Fixed Parameter Expansion widens networks at constant non-zero parameters to reduce polysemanticity and improve accuracy.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Linghao Kong",
            "Inimai Subramanian",
            "Yonadav Shavit",
            "Micah Adler",
            "Dan Alistarh",
            "Nir Shavit"
        ],
        "title": "Expand Neurons, Not Parameters",
        "abstract": "This work demonstrates how increasing the number of neurons in a network without increasing its number of non-zero parameters improves performance. We show that this gain corresponds with a decrease in interference between multiple features that would otherwise share the same neurons. To reduce such entanglement at a fixed non-zero parameter count, we introduce Fixed Parameter Expansion (FPE): replace a neuron with multiple children and partition the parent's weights disjointly across them, so that each child inherits a non-overlapping subset of connections. On symbolic tasks, specifically Boolean code problems, clause-aligned FPE systematically reduces polysemanticity metrics and yields higher task accuracy. Notably, random splits of neuron weights approximate these gains, indicating that reduced collisions, not precise assignment, are a primary driver. Consistent with the superposition hypothesis, the benefits of FPE grow with increasing interference: when polysemantic load is high, accuracy improvements are the largest. Transferring these insights to real models (classifiers over CLIP embeddings and deeper multilayer networks) we find that widening networks while maintaining a constant non-zero parameter count consistently increases accuracy. These results identify an interpretability-grounded mechanism to leverage width against superposition, improving performance without increasing the number of non-zero parameters. Such a direction is well matched to modern accelerators, where memory movement of non-zero parameters, rather than raw compute, is the dominant bottleneck.",
        "arxiv_id": "2510.04500"
    },
    "2510.04067": {
        "SCORE": 17,
        "ARXIVID": "2510.04067",
        "COMMENT": "Representation Learning/Training Dynamics: theoretical decomposition of cross-entropy into error-entropy/self-alignment/confidence, identifying error-entropy as the true scaling component.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Junxi Yan",
            "Zixi Wei",
            "Jingtao Zhan",
            "Qingyao Ai",
            "Yiqun Liu"
        ],
        "title": "What Scales in Cross-Entropy Scaling Law?",
        "abstract": "The cross-entropy scaling law has long served as a key tool for guiding the development of large language models. It shows that cross-entropy loss decreases in a predictable power-law rate as the model size increases. However, recent evidence indicates that this law breaks down at very large scales: the loss decreases more slowly than expected, which causes significant trouble for developing large language models. In this paper, we hypothesize that the root cause lies in the fact that cross-entropy itself does not truly scale; instead, only one of its hidden components does. To investigate this, we introduce a novel decomposition of cross-entropy into three parts: Error-Entropy, Self-Alignment, and Confidence. We show both theoretically and empirically that this decomposition precisely captures the training dynamics and optimization objectives. Through extensive experiments on multiple datasets and 32 models spanning five orders of magnitude in size, we find that only error-entropy follows a robust power-law scaling, while the other two terms remain largely invariant. Moreover, error-entropy constitutes the dominant share of cross-entropy in small models but diminishes in proportion as models grow larger. This explains why the cross-entropy scaling law appears accurate at small scales but fails at very large ones. Our findings establish the error-entropy scaling law as a more accurate description of model behavior. We believe it will have wide applications in the training, understanding, and future development of large language models.",
        "arxiv_id": "2510.04067"
    },
    "2510.03605": {
        "SCORE": 17,
        "ARXIVID": "2510.03605",
        "COMMENT": "Representation Learning/Training Dynamics: theoretical analysis of test-time scaling for transformers, linking training data properties to benefits of long chain-of-thought.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Adel Javanmard",
            "Baharan Mirzasoleiman",
            "Vahab Mirrokni"
        ],
        "title": "Understanding the Role of Training Data in Test-Time Scaling",
        "abstract": "Test-time scaling improves the reasoning capabilities of large language models (LLMs) by allocating extra compute to generate longer Chains-of-Thoughts (CoTs). This enables models to tackle more complex problem by breaking them down into additional steps, backtracking, and correcting mistakes. Despite its strong performance--demonstrated by OpenAI's o1 and DeepSeek R1, the conditions in the training data under which long CoTs emerge, and when such long CoTs improve the performance, remain unclear. In this paper, we study the performance of test-time scaling for transformers trained on an in-context weight prediction task for linear regression. Our analysis provides a theoretical explanation for several intriguing observations: First, at any fixed test error, increasing test-time compute allows us to reduce the number of in-context examples (context length) in training prompts. Second, if the skills required to solve a downstream task are not sufficiently present in the training data, increasing test-time compute can harm performance. Finally, we characterize task hardness via the smallest eigenvalue of its feature covariance matrix and show that training on a diverse, relevant, and hard set of tasks results in best performance for test-time scaling. We confirm our findings with experiments on large, nonlinear transformer architectures.",
        "arxiv_id": "2510.03605"
    },
    "2510.04295": {
        "SCORE": 17,
        "ARXIVID": "2510.04295",
        "COMMENT": "PEFT/Low-Rank: cross-head shared low-rank adapters generated by joint hypernetworks; theoretical sample-efficiency gains via a hierarchical MoE perspective.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Nghiem T. Diep",
            "Dung Le",
            "Tuan Truong",
            "Tan Dinh",
            "Huy Nguyen",
            "Nhat Ho"
        ],
        "title": "HoRA: Cross-Head Low-Rank Adaptation with Joint Hypernetworks",
        "abstract": "Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) technique that adapts large pre-trained models by adding low-rank matrices to their weight updates. However, in the context of fine-tuning multi-head self-attention (MHA), LoRA has been employed to adapt each attention head separately, thereby overlooking potential synergies across different heads. To mitigate this issue, we propose a novel Hyper-shared Low-Rank Adaptation (HoRA) method, which utilizes joint hypernetworks to generate low-rank matrices across attention heads. By coupling their adaptation through a shared generator, HoRA encourages cross-head information sharing, and thus directly addresses the aforementioned limitation of LoRA. By comparing LoRA and HoRA through the lens of hierarchical mixture of experts, our theoretical findings reveal that the latter achieves superior sample efficiency to the former. Furthermore, through extensive experiments across diverse language and vision benchmarks, we demonstrate that HoRA outperforms LoRA and other PEFT methods while requiring only a marginal increase in the number of trainable parameters.",
        "arxiv_id": "2510.04295"
    },
    "2510.03371": {
        "SCORE": 16,
        "ARXIVID": "2510.03371",
        "COMMENT": "High Performance Computing: reduces communication via decoupled momentum optimization and DCT-based momentum compression with infrequent syncs for distributed training.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Sasho Nedelkoski",
            "Alexander Acker",
            "Odej Kao",
            "Soeren Becker",
            "Dominik Scheinert"
        ],
        "title": "Distributed Low-Communication Training with Decoupled Momentum Optimization",
        "abstract": "The training of large models demands substantial computational resources, typically available only in data centers with high-bandwidth interconnects. However, reducing the reliance on high-bandwidth interconnects between nodes enables the use of distributed compute resources as an alternative to centralized data center training. Building on recent advances in distributed model training, we propose an approach that further reduces communication by combining infrequent synchronizations across distributed model replicas with gradient momentum compression. In particular, we treat the optimizer momentum as a signal and decompose the Nesterov momentum into high- and low-frequency components via the discrete cosine transform (DCT). Only the high-frequency components are synchronized across model replicas every $H$ steps. Empirically, our method achieves up to a $16\\times$ reduction in communication compared to the baseline DiLoCo, and it generalizes across architectures, including transformer-based language models and convolutional neural networks for images. Overall, this work advances the feasibility of training large models on distributed nodes with low-bandwidth interconnects.",
        "arxiv_id": "2510.03371"
    },
    "2510.04331": {
        "SCORE": 16,
        "ARXIVID": "2510.04331",
        "COMMENT": "Model Compression and Efficiency: stabilizes and enhances low-rank adaptation (DoRA) via noise injection and auxiliary networks that generate low-rank factors, improving PEFT.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Nghiem T. Diep",
            "Hien Dang",
            "Tuan Truong",
            "Tan Dinh",
            "Huy Nguyen",
            "Nhat Ho"
        ],
        "title": "DoRAN: Stabilizing Weight-Decomposed Low-Rank Adaptation via Noise Injection and Auxiliary Networks",
        "abstract": "Parameter-efficient fine-tuning (PEFT) methods have become the standard paradigm for adapting large-scale models. Among these techniques, Weight-Decomposed Low-Rank Adaptation (DoRA) has been shown to improve both the learning capacity and training stability of the vanilla Low-Rank Adaptation (LoRA) method by explicitly decomposing pre-trained weights into magnitude and directional components. In this work, we propose DoRAN, a new variant of DoRA designed to further stabilize training and boost the sample efficiency of DoRA. Our approach includes two key stages: (i) injecting noise into the denominator of DoRA's weight decomposition, which serves as an adaptive regularizer to mitigate instabilities; and (ii) replacing static low-rank matrices with auxiliary networks that generate them dynamically, enabling parameter coupling across layers and yielding better sample efficiency in both theory and practice. Comprehensive experiments on vision and language benchmarks show that DoRAN consistently outperforms LoRA, DoRA, and other PEFT baselines. These results underscore the effectiveness of combining stabilization through noise-based regularization with network-based parameter generation, offering a promising direction for robust and efficient fine-tuning of foundation models.",
        "arxiv_id": "2510.04331"
    },
    "2510.03659": {
        "SCORE": 16,
        "ARXIVID": "2510.03659",
        "COMMENT": "Matches Representation Learning and Sparse methods: analyzes Sparse Autoencoders\u2019 interpretability vs. steering utility and proposes Delta Token Confidence for feature selection.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Xu Wang",
            "Yan Hu",
            "Benyou Wang",
            "Difan Zou"
        ],
        "title": "Does higher interpretability imply better utility? A Pairwise Analysis on Sparse Autoencoders",
        "abstract": "Sparse Autoencoders (SAEs) are widely used to steer large language models (LLMs), based on the assumption that their interpretable features naturally enable effective model behavior steering. Yet, a fundamental question remains unanswered: does higher interpretability indeed imply better steering utility? To answer this question, we train 90 SAEs across three LLMs (Gemma-2-2B, Qwen-2.5-3B, Gemma-2-9B), spanning five architectures and six sparsity levels, and evaluate their interpretability and steering utility based on SAEBench (arXiv:2501.12345) and AxBench (arXiv:2502.23456) respectively, and perform a rank-agreement analysis via Kendall's rank coefficients (tau b). Our analysis reveals only a relatively weak positive association (tau b approx 0.298), indicating that interpretability is an insufficient proxy for steering performance. We conjecture the interpretability utility gap may stem from the selection of SAE features, as not all of them are equally effective for steering. To further find features that truly steer the behavior of LLMs, we propose a novel selection criterion called Delta Token Confidence, which measures how much amplifying a feature changes the next token distribution. We show that our method improves the steering performance of three LLMs by 52.52 percent compared to the current best output score based criterion (arXiv:2503.34567). Strikingly, after selecting features with high Delta Token Confidence, the correlation between interpretability and utility vanishes (tau b approx 0), and can even become negative. This further highlights the divergence between interpretability and utility for the most effective steering features.",
        "arxiv_id": "2510.03659"
    },
    "2510.03274": {
        "SCORE": 16,
        "ARXIVID": "2510.03274",
        "COMMENT": "Matches Model Compression and Efficiency: proposes ultra-low-bit (2-bit) post-training quantization tailored to diffusion LLMs with masked calibration simulation and adaptive blockwise mixed precision.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Tianao Zhang",
            "Zhiteng Li",
            "Xianglong Yan",
            "Haotong Qin",
            "Yong Guo",
            "Yulun Zhang"
        ],
        "title": "Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models",
        "abstract": "Diffusion large language models (dLLMs), which offer bidirectional context and flexible masked-denoising generation, are emerging as a compelling alternative to autoregressive (AR) LLMs. However, like AR LLMs, their model sizes continue to grow, motivating weight compression for deployment. Although post-training quantization (PTQ) is effective for AR LLMs, directly transferring it to dLLMs at 2-bit leads to unsatisfactory performance. To tackle these challenges, we propose Quant-dLLM, an ultra-low-bit PTQ framework tailored to dLLMs. Since masked-denoising activations in dLLMs differ from the fully visible signals assumed by standard PTQ methods, we introduce Masked Calibration Simulation (MCS) to align calibration with the timestep-dependent masking, which yields more reliable calibrations. Moreover, we propose a Data-aware Any-order Quantizer (DAQ) that learns ultra-low-bit weight representations via an optimization algorithm. It performs iterative approximation guided by our simulated calibration data. In addition, under a strict 2-bit budget, we introduce Adaptive Blockwise Mixed Precision (ABMP), a sensitivity-based precision allocation scheme that adaptively assigns bit width across channel groups. When restricted to 2-bit precision, Quant-dLLM consistently achieves higher accuracy than state-of-the-art (SOTA) AR-transfer PTQ methods on dLLMs. The code and models will be available at: https://github.com/ZTA2785/Quant-dLLM.",
        "arxiv_id": "2510.03274"
    },
    "2510.03339": {
        "SCORE": 16,
        "ARXIVID": "2510.03339",
        "COMMENT": "Strongly matches Model Architecture: theoretical expressivity bounds and analysis of pooling mechanisms in Transformers, offering principled architectural guidance.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Sofiane Ennadir",
            "Levente Z\\'olyomi",
            "Oleg Smirnov",
            "Tianze Wang",
            "John Pertoft",
            "Filip Cornell",
            "Lele Cao"
        ],
        "title": "Pool Me Wisely: On the Effect of Pooling in Transformer-Based Models",
        "abstract": "Transformer models have become the dominant backbone for sequence modeling, leveraging self-attention to produce contextualized token representations. These are typically aggregated into fixed-size vectors via pooling operations for downstream tasks. While much of the literature has focused on attention mechanisms, the role of pooling remains underexplored despite its critical impact on model behavior. In this paper, we introduce a theoretical framework that rigorously characterizes the expressivity of Transformer-based models equipped with widely used pooling methods by deriving closed-form bounds on their representational capacity and the ability to distinguish similar inputs. Our analysis extends to different variations of attention formulations, demonstrating that these bounds hold across diverse architectural variants. We empirically evaluate pooling strategies across tasks requiring both global and local contextual understanding, spanning three major modalities: computer vision, natural language processing, and time-series analysis. Results reveal consistent trends in how pooling choices affect accuracy, sensitivity, and optimization behavior. Our findings unify theoretical and empirical perspectives, providing practical guidance for selecting or designing pooling mechanisms suited to specific tasks. This work positions pooling as a key architectural component in Transformer models and lays the foundation for more principled model design beyond attention alone.",
        "arxiv_id": "2510.03339"
    },
    "2510.04285": {
        "SCORE": 16,
        "ARXIVID": "2510.04285",
        "COMMENT": "Representation Learning: introduces cumulant-expansion probes of softmax entropy to quantify higher-order feature-learning dynamics across layers and training.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Karthik Viswanathan",
            "Sang Eon Park"
        ],
        "title": "Probing Geometry of Next Token Prediction Using Cumulant Expansion of the Softmax Entropy",
        "abstract": "We introduce a cumulant-expansion framework for quantifying how large language models (LLMs) internalize higher-order statistical structure during next-token prediction. By treating the softmax entropy of each layer's logit distribution as a perturbation around its \"center\" distribution, we derive closed-form cumulant observables that isolate successively higher-order correlations. Empirically, we track these cumulants in GPT-2 and Pythia models on Pile-10K prompts. (i) Structured prompts exhibit a characteristic rise-and-plateau profile across layers, whereas token-shuffled prompts remain flat, revealing the dependence of the cumulant profile on meaningful context. (ii) During training, all cumulants increase monotonically before saturating, directly visualizing the model's progression from capturing variance to learning skew, kurtosis, and higher-order statistical structures. (iii) Mathematical prompts show distinct cumulant signatures compared to general text, quantifying how models employ fundamentally different processing mechanisms for mathematical versus linguistic content. Together, these results establish cumulant analysis as a lightweight, mathematically grounded probe of feature-learning dynamics in high-dimensional neural networks.",
        "arxiv_id": "2510.04285"
    },
    "2510.03425": {
        "SCORE": 16,
        "ARXIVID": "2510.03425",
        "COMMENT": "High Performance Computing: memory-efficient backpropagation enabling on-device fine-tuning of LLMs (<1GB), a systems-level memory optimization for training.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Congzheng Song",
            "Xinyu Tang"
        ],
        "title": "Memory-Efficient Backpropagation for Fine-Tuning LLMs on Resource-Constrained Mobile Devices",
        "abstract": "Fine-tuning large language models (LLMs) with backpropagation\\textemdash even for a subset of parameters such as LoRA\\textemdash can be much more memory-consuming than inference and is often deemed impractical for resource-constrained mobile devices. Alternative methods, such as zeroth-order optimization (ZO), can greatly reduce the memory footprint but come at the cost of significantly slower model convergence (10$\\times$ to 100$\\times$ more steps than backpropagation). We propose a memory-efficient implementation of backpropagation (MeBP) on mobile devices that provides better trade-off between memory usage and compute time, while converging faster and achieving better performance than the ZO baseline. We verify the effectiveness of MeBP on an iPhone 15 Pro Max and show that various LLMs, ranging from 0.5B to 4B parameters, can be fine-tuned using less than 1GB of memory. We release an example of the MeBP implementation at https://github.com/apple/ml-mebp.",
        "arxiv_id": "2510.03425"
    },
    "2510.03346": {
        "SCORE": 16,
        "ARXIVID": "2510.03346",
        "COMMENT": "Compression/Efficiency and Systems: selective KV sharing based on attention-importance with Gaussian prior reduces inter-LLM communication while retaining performance.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Xiangyu Shi",
            "Marco Chiesa",
            "Gerald Q. Maguire Jr.",
            "Dejan Kostic"
        ],
        "title": "KVComm: Enabling Efficient LLM Communication through Selective KV Sharing",
        "abstract": "Large Language Models (LLMs) are increasingly deployed in multi-agent systems, where effective inter-model communication is crucial. Existing communication protocols either rely on natural language, incurring high inference costs and information loss, or on hidden states, which suffer from information concentration bias and inefficiency. To address these limitations, we propose KVComm, a novel communication framework that enables efficient communication between LLMs through selective sharing of KV pairs. KVComm leverages the rich information encoded in the KV pairs while avoiding the pitfalls of hidden states. We introduce a KV layer-wise selection strategy based on attention importance scores with a Gaussian prior to identify the most informative KV pairs for communication. Extensive experiments across diverse tasks and model pairs demonstrate that KVComm achieves comparable performance to the upper-bound method, which directly merges inputs to one model without any communication, while transmitting as few as 30\\% of layers' KV pairs. Our study highlights the potential of KV pairs as an effective medium for inter-LLM communication, paving the way for scalable and efficient multi-agent systems.",
        "arxiv_id": "2510.03346"
    },
    "2510.03871": {
        "SCORE": 16,
        "ARXIVID": "2510.03871",
        "COMMENT": "Matches High Performance Computing/training dynamics: discovers an operator-norm invariance governing optimal LR/batch scaling for LLM training and reports distributed Scion implementation and large-scale scaling rules.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Oleg Filatov",
            "Jiangtao Wang",
            "Jan Ebert",
            "Stefan Kesselheim"
        ],
        "title": "Optimal Scaling Needs Optimal Norm",
        "abstract": "Despite recent progress in optimal hyperparameter transfer under model and dataset scaling, no unifying explanatory principle has been established. Using the Scion optimizer, we discover that joint optimal scaling across model and dataset sizes is governed by a single invariant: the operator norm of the output layer. Across models with up to 1.3B parameters trained on up to 138B tokens, the optimal learning rate/batch size pair $(\\eta^{\\ast}, B^{\\ast})$ consistently has the same operator norm value - a phenomenon we term norm transfer. This constant norm condition is necessary but not sufficient: while for each dataset size, multiple $(\\eta, B)$ reach the optimal norm, only a unique $(\\eta^{\\ast}, B^{\\ast})$ achieves the best loss. As a sufficient condition, we provide the first measurement of $(\\eta^{\\ast}, B^{\\ast})$ scaling with dataset size for Scion, and find that the scaling rules are consistent with those of the Adam optimizer. Tuning per-layer-group learning rates also improves model performance, with the output layer being the most sensitive and hidden layers benefiting from lower learning rates. We provide practical insights on norm-guided optimal scaling and release our Distributed Scion (Disco) implementation with logs from over two thousand runs to support research on LLM training dynamics at scale.",
        "arxiv_id": "2510.03871"
    },
    "2510.04060": {
        "SCORE": 16,
        "ARXIVID": "2510.04060",
        "COMMENT": "Model Architecture / Representation Learning: theoretical saturation bounds for linearized shallow ReLU^k networks, analyzing approximation capacity of the architecture.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Tong Mao",
            "Jinchao Xu"
        ],
        "title": "Sharp Lower Bounds for Linearized ReLU^k Approximation on the Sphere",
        "abstract": "We prove a saturation theorem for linearized shallow ReLU$^k$ neural networks on the unit sphere $\\mathbb S^d$. For any antipodally quasi-uniform set of centers, if the target function has smoothness $r>\\tfrac{d+2k+1}{2}$, then the best $\\mathcal{L}^2(\\mathbb S^d)$ approximation cannot converge faster than order $n^{-\\frac{d+2k+1}{2d}}$. This lower bound matches existing upper bounds, thereby establishing the exact saturation order $\\tfrac{d+2k+1}{2d}$ for such networks. Our results place linearized neural-network approximation firmly within the classical saturation framework and show that, although ReLU$^k$ networks outperform finite elements under equal degrees $k$, this advantage is intrinsically limited.",
        "arxiv_id": "2510.04060"
    },
    "2510.04686": {
        "SCORE": 16,
        "ARXIVID": "2510.04686",
        "COMMENT": "Representation Learning/Training Dynamics: shows how optimizer-induced effective noise shapes the global loss landscape and predicts model merging success.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Chenxiang Zhang",
            "Alexander Theus",
            "Damien Teney",
            "Antonio Orvieto",
            "Jun Pang",
            "Sjouke Mauw"
        ],
        "title": "How does the optimizer implicitly bias the model merging loss landscape?",
        "abstract": "Model merging methods combine models with different capabilities into a single one while maintaining the same inference cost. Two popular approaches are linear interpolation, which linearly interpolates between model weights, and task arithmetic, which combines task vectors obtained by the difference between finetuned and base models. While useful in practice, what properties make merging effective are poorly understood. This paper explores how the optimization process affects the loss landscape geometry and its impact on merging success. We show that a single quantity -- the effective noise scale -- unifies the impact of optimizer and data choices on model merging. Across architectures and datasets, the effectiveness of merging success is a non-monotonic function of effective noise, with a distinct optimum. Decomposing this quantity, we find that larger learning rates, stronger weight decay, smaller batch sizes, and data augmentation all independently modulate the effective noise scale, exhibiting the same qualitative trend. Unlike prior work that connects optimizer noise to the flatness or generalization of individual minima, we show that it also affects the global loss landscape, predicting when independently trained solutions can be merged. Our findings broaden the understanding of how optimization shapes the loss landscape geometry and its downstream consequences for model merging, suggesting the possibility of further manipulating the training dynamics to improve merging effectiveness.",
        "arxiv_id": "2510.04686"
    },
    "2510.03271": {
        "SCORE": 16,
        "ARXIVID": "2510.03271",
        "COMMENT": "Representation Learning: introduces Decision Potential Surface to approximate LLM decision boundaries with provable error bounds via K-sampling.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Zi Liang",
            "Zhiyao Wu",
            "Haoyang Shang",
            "Yulin Jin",
            "Qingqing Ye",
            "Huadi Zheng",
            "Peizhao Hu",
            "Haibo Hu"
        ],
        "title": "Decision Potential Surface: A Theoretical and Practical Approximation of LLM's Decision Boundary",
        "abstract": "Decision boundary, the subspace of inputs where a machine learning model assigns equal classification probabilities to two classes, is pivotal in revealing core model properties and interpreting behaviors. While analyzing the decision boundary of large language models (LLMs) has raised increasing attention recently, constructing it for mainstream LLMs remains computationally infeasible due to the enormous vocabulary-sequence sizes and the auto-regressive nature of LLMs. To address this issue, in this paper we propose Decision Potential Surface (DPS), a new notion for analyzing LLM decision boundary. DPS is defined on the confidences in distinguishing different sampling sequences for each input, which naturally captures the potential of decision boundary. We prove that the zero-height isohypse in DPS is equivalent to the decision boundary of an LLM, with enclosed regions representing decision regions. By leveraging DPS, for the first time in the literature, we propose an approximate decision boundary construction algorithm, namely $K$-DPS, which only requires K-finite times of sequence sampling to approximate an LLM's decision boundary with negligible error. We theoretically derive the upper bounds for the absolute error, expected error, and the error concentration between K-DPS and the ideal DPS, demonstrating that such errors can be trade-off with sampling times. Our results are empirically validated by extensive experiments across various LLMs and corpora.",
        "arxiv_id": "2510.03271"
    },
    "2510.05095": {
        "SCORE": 16,
        "ARXIVID": "2510.05095",
        "COMMENT": "Matches Representation Learning/training dynamics: a variance-optimized preference optimization method with theory for aligning large reasoning models.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Mingkang Zhu",
            "Xi Chen",
            "Bei Yu",
            "Hengshuang Zhao",
            "Jiaya Jia"
        ],
        "title": "From Noisy Traces to Stable Gradients: Bias-Variance Optimized Preference Optimization for Aligning Large Reasoning Models",
        "abstract": "Large reasoning models (LRMs) generate intermediate reasoning traces before producing final answers, yielding strong gains on multi-step and mathematical tasks. Yet aligning LRMs with human preferences, a crucial prerequisite for model deployment, remains underexplored. The statistically correct objective for preference alignment requires marginalizing over reasoning traces, but this computation is intractable in practice. A common workaround optimizes a single sampled trajectory, which introduces substantial gradient variance from stochastic trace sampling. To address this challenge, we frame preference optimization for LRMs through the lens of the bias--variance trade-off and propose Bias--Variance Optimized Preference Optimization (BVPO), a simple, drop-in method that mixes two gradient estimators: a high-variance trace-based estimator and a low-variance empty-trace estimator obtained by disabling reasoning trace generation. Our theory shows that BVPO strictly reduces trace-induced variance for any nontrivial mixture, provides a closed-form choice of the mixing weight that minimizes mean-squared error relative to the true marginal gradient, and under standard smoothness and step-size conditions, tightens classical convergence bounds for stochastic gradient descent. Empirically, BVPO improves alignment over the best baseline by up to 7.8 points on AlpacaEval~2 and 6.8 points on Arena-Hard. Despite being trained only on general conversational data, BVPO also boosts reasoning performance for base models by up to 4.0 points on the average of six math reasoning benchmarks. These results identify variance from trace sampling as a key bottleneck and demonstrate that directly optimizing the bias--variance trade-off yields more stable training and stronger overall performance.",
        "arxiv_id": "2510.05095"
    },
    "2510.04128": {
        "SCORE": 15,
        "ARXIVID": "2510.04128",
        "COMMENT": "Matches Representation Learning/mechanistic interpretability: identifies latent features that modulate \u2018wait\u2019 tokens and causally links them to reasoning patterns in transformers.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Dmitrii Troitskii",
            "Koyena Pal",
            "Chris Wendler",
            "Callum Stuart McDougall",
            "Neel Nanda"
        ],
        "title": "Internal states before wait modulate reasoning patterns",
        "abstract": "Prior work has shown that a significant driver of performance in reasoning models is their ability to reason and self-correct. A distinctive marker in these reasoning traces is the token wait, which often signals reasoning behavior such as backtracking. Despite being such a complex behavior, little is understood of exactly why models do or do not decide to reason in this particular manner, which limits our understanding of what makes a reasoning model so effective. In this work, we address the question whether model's latents preceding wait tokens contain relevant information for modulating the subsequent reasoning process. We train crosscoders at multiple layers of DeepSeek-R1-Distill-Llama-8B and its base version, and introduce a latent attribution technique in the crosscoder setting. We locate a small set of features relevant for promoting/suppressing wait tokens' probabilities. Finally, through a targeted series of experiments analyzing max activating examples and causal interventions, we show that many of our identified features indeed are relevant for the reasoning process and give rise to different types of reasoning patterns such as restarting from the beginning, recalling prior knowledge, expressing uncertainty, and double-checking.",
        "arxiv_id": "2510.04128"
    },
    "2510.05092": {
        "SCORE": 15,
        "ARXIVID": "2510.05092",
        "COMMENT": "Matches Representation Learning/interpretability: trains models to describe finetuning-induced weight diffs via adapters, enabling natural-language explanations of parameter changes.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Avichal Goel",
            "Yoon Kim",
            "Nir Shavit",
            "Tony T. Wang"
        ],
        "title": "Learning to Interpret Weight Differences in Language Models",
        "abstract": "Finetuning (pretrained) language models is a standard approach for updating their internal parametric knowledge and specializing them to new tasks and domains. However, the corresponding model weight changes (\"weight diffs\") are not generally interpretable. While inspecting the finetuning dataset can give a sense of how the model might have changed, these datasets are often not publicly available or are too large to work with directly. Towards the goal of comprehensively understanding weight diffs in natural language, we introduce Diff Interpretation Tuning (DIT), a method that trains models to describe their own finetuning-induced modifications. Our approach uses synthetic, labeled weight diffs to train a DIT adapter, which can be applied to a compatible finetuned model to make it describe how it has changed. We demonstrate in two proof-of-concept settings (reporting hidden behaviors and summarizing finetuned knowledge) that our method enables models to describe their finetuning-induced modifications using accurate natural language descriptions.",
        "arxiv_id": "2510.05092"
    },
    "2510.03313": {
        "SCORE": 15,
        "ARXIVID": "2510.03313",
        "COMMENT": "High Performance/Scaling: quality-aware scaling law extending Chinchilla to jointly model data quality, dataset size, and model size for compute-efficient pretraining.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Anirudh Subramanyam",
            "Yuxin Chen",
            "Robert L. Grossman"
        ],
        "title": "Scaling Laws Revisited: Modeling the Role of Data Quality in Language Model Pretraining",
        "abstract": "Scaling laws for language model training traditionally characterize how performance scales with model size and dataset volume. Prior work has explored architecture variants and data treatments such as dataset filtering and noise injection in language model pretraining; however, these studies have not formalized data quality within a principled scaling law. We introduce a dimensionless data-quality parameter Q, and propose a quality-aware scaling law extending the Chinchilla framework to predict loss as a joint function of model size, data volume, and data quality. The law is motivated by an effective-sample-size and information-theoretic view of noisy or redundant corpora, and it admits two practical estimators for Q: (i) a corruption rate proxy and (ii) a deficiency measure. Through synthetic experiments in neural machine translation and autoregressive modeling -- where we systematically control data quality via multiple levels of noise injection and coverage variation -- we show that loss scales predictably with data quality and that higher-quality data can substantially reduce model size and hence compute requirements. Our results demonstrate a sublinear decay of effective data with quality and robustness to moderate data corruption; out-of-sample evaluations further validate the predictive form of the law. Unlike prior empirical analyses, our work establishes an explicit, generalizable law for data quality, offering concrete guidance for balancing data curation effort and model scale in large-scale pretraining.",
        "arxiv_id": "2510.03313"
    },
    "2510.03276": {
        "SCORE": 15,
        "ARXIVID": "2510.03276",
        "COMMENT": "Matches Model Architecture and Efficiency: introduces quadratic transformations with low-rankness, weight sharing, and sparsification as a lightweight enhancer.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Qian Chen",
            "Linxin Yang",
            "Akang Wang",
            "Xiaodong Luo",
            "Yin Zhang"
        ],
        "title": "QuadEnhancer: Leveraging Quadratic Transformations to Enhance Deep Neural Networks",
        "abstract": "The combination of linear transformations and non-linear activation functions forms the foundation of most modern deep neural networks, enabling them to approximate highly complex functions. This paper explores the introduction of quadratic transformations to further increase nonlinearity in neural networks, with the aim of enhancing the performance of existing architectures. To reduce parameter complexity and computational complexity, we propose a lightweight quadratic enhancer that uses low-rankness, weight sharing, and sparsification techniques. For a fixed architecture, the proposed approach introduces quadratic interactions between features at every layer, while only adding negligible amounts of additional model parameters and forward computations. We conduct a set of proof-of-concept experiments for the proposed method across three tasks: image classification, text classification, and fine-tuning large-language models. In all tasks, the proposed approach demonstrates clear and substantial performance gains.",
        "arxiv_id": "2510.03276"
    },
    "2510.03315": {
        "SCORE": 15,
        "ARXIVID": "2510.03315",
        "COMMENT": "Matches Representation Learning/Interpretability: decomposes attention to uncover context-sensitive neurons from weights using a calibration text.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Alex Gibson"
        ],
        "title": "Decomposing Attention To Find Context-Sensitive Neurons",
        "abstract": "We study transformer language models, analyzing attention heads whose attention patterns are spread out, and whose attention scores depend weakly on content. We argue that the softmax denominators of these heads are stable when the underlying token distribution is fixed. By sampling softmax denominators from a \"calibration text\", we can combine together the outputs of multiple such stable heads in the first layer of GPT2-Small, approximating their combined output by a linear summary of the surrounding text. This approximation enables a procedure where from the weights alone - and a single calibration text - we can uncover hundreds of first layer neurons that respond to high-level contextual properties of the surrounding text, including neurons that didn't activate on the calibration text.",
        "arxiv_id": "2510.03315"
    },
    "2510.04548": {
        "SCORE": 15,
        "ARXIVID": "2510.04548",
        "COMMENT": "Representation Learning: theoretical analysis of in-context learning with a linear attention model on low-rank task distributions, characterizing prediction distributions, implicit regularization, and phase transitions in generalization.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Kaito Takanami",
            "Takashi Takahashi",
            "Yoshiyuki Kabashima"
        ],
        "title": "Learning Linear Regression with Low-Rank Tasks in-Context",
        "abstract": "In-context learning (ICL) is a key building block of modern large language models, yet its theoretical mechanisms remain poorly understood. It is particularly mysterious how ICL operates in real-world applications where tasks have a common structure. In this work, we address this problem by analyzing a linear attention model trained on low-rank regression tasks. Within this setting, we precisely characterize the distribution of predictions and the generalization error in the high-dimensional limit. Moreover, we find that statistical fluctuations in finite pre-training data induce an implicit regularization. Finally, we identify a sharp phase transition of the generalization error governed by task structure. These results provide a framework for understanding how transformers learn to learn the task structure.",
        "arxiv_id": "2510.04548"
    },
    "2510.04098": {
        "SCORE": 15,
        "ARXIVID": "2510.04098",
        "COMMENT": "Model Compression and Efficiency: spike-aware data pruning that approximates gradient-norm sampling via an efficient upper bound, cutting SNN training time while maintaining accuracy.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Chenxiang Ma",
            "Xinyi Chen",
            "Yujie Wu",
            "Kay Chen Tan",
            "Jibin Wu"
        ],
        "title": "Efficient Training of Spiking Neural Networks by Spike-aware Data Pruning",
        "abstract": "Spiking neural networks (SNNs), recognized as an energy-efficient alternative to traditional artificial neural networks (ANNs), have advanced rapidly through the scaling of models and datasets. However, such scaling incurs considerable training overhead, posing challenges for researchers with limited computational resources and hindering the sustained development of SNNs. Data pruning is a promising strategy for accelerating training by retaining the most informative examples and discarding redundant ones, but it remains largely unexplored in SNNs. Directly applying ANN-based data pruning methods to SNNs fails to capture the intrinsic importance of examples and suffers from high gradient variance. To address these challenges, we propose a novel spike-aware data pruning (SADP) method. SADP reduces gradient variance by determining each example's selection probability to be proportional to its gradient norm, while avoiding the high cost of direct gradient computation through an efficient upper bound, termed spike-aware importance score. This score accounts for the influence of all-or-nothing spikes on the gradient norm and can be computed with negligible overhead. Extensive experiments across diverse datasets and architectures demonstrate that SADP consistently outperforms data pruning baselines and achieves training speedups close to the theoretical maxima at different pruning ratios. Notably, SADP reduces training time by 35% on ImageNet while maintaining accuracy comparable to that of full-data training. This work, therefore, establishes a data-centric paradigm for efficient SNN training and paves the way for scaling SNNs to larger models and datasets. The source code will be released publicly after the review process.",
        "arxiv_id": "2510.04098"
    },
    "2510.04930": {
        "SCORE": 15,
        "ARXIVID": "2510.04930",
        "COMMENT": "Matches Representation Learning/Training Dynamics: proposes egalitarian gradient descent to equalize learning across principal directions, offering insights into grokking dynamics.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Ali Saheb Pasand",
            "Elvis Dohmatob"
        ],
        "title": "Egalitarian Gradient Descent: A Simple Approach to Accelerated Grokking",
        "abstract": "Grokking is the phenomenon whereby, unlike the training performance, which peaks early in the training process, the test/generalization performance of a model stagnates over arbitrarily many epochs and then suddenly jumps to usually close to perfect levels. In practice, it is desirable to reduce the length of such plateaus, that is to make the learning process \"grok\" faster. In this work, we provide new insights into grokking. First, we show both empirically and theoretically that grokking can be induced by asymmetric speeds of (stochastic) gradient descent, along different principal (i.e singular directions) of the gradients. We then propose a simple modification that normalizes the gradients so that dynamics along all the principal directions evolves at exactly the same speed. Then, we establish that this modified method, which we call egalitarian gradient descent (EGD) and can be seen as a carefully modified form of natural gradient descent, groks much faster. In fact, in some cases the stagnation is completely removed. Finally, we empirically show that on classical arithmetic problems such as modular addition and sparse parity problem which this stagnation has been widely observed and intensively studied, that our proposed method eliminates the plateaus.",
        "arxiv_id": "2510.04930"
    },
    "2510.04309": {
        "SCORE": 15,
        "ARXIVID": "2510.04309",
        "COMMENT": "Matches Model Architecture/Activation Steering foundations: frames activation steering as PID control with theoretical stability and a principled closed-loop mechanism.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Dung V. Nguyen",
            "Hieu M. Vu",
            "Nhi Y. Pham",
            "Lei Zhang",
            "Tan M. Nguyen"
        ],
        "title": "Activation Steering with a Feedback Controller",
        "abstract": "Controlling the behaviors of large language models (LLM) is fundamental to their safety alignment and reliable deployment. However, existing steering methods are primarily driven by empirical insights and lack theoretical performance guarantees. In this work, we develop a control-theoretic foundation for activation steering by showing that popular steering methods correspond to the proportional (P) controllers, with the steering vector serving as the feedback signal. Building on this finding, we propose Proportional-Integral-Derivative (PID) Steering, a principled framework that leverages the full PID controller for activation steering in LLMs. The proportional (P) term aligns activations with target semantic directions, the integral (I) term accumulates errors to enforce persistent corrections across layers, and the derivative (D) term mitigates overshoot by counteracting rapid activation changes. This closed-loop design yields interpretable error dynamics and connects activation steering to classical stability guarantees in control theory. Moreover, PID Steering is lightweight, modular, and readily integrates with state-of-the-art steering methods. Extensive experiments across multiple LLM families and benchmarks demonstrate that PID Steering consistently outperforms existing approaches, achieving more robust and reliable behavioral control.",
        "arxiv_id": "2510.04309"
    },
    "2510.03691": {
        "SCORE": 15,
        "ARXIVID": "2510.03691",
        "COMMENT": "Matches Training Dynamics/Optimization for large models: introduces a structure-aware optimizer (RACS) replacing Muon\u2019s matrix sign to stabilize and regularize updates.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Zehua Liu",
            "Han Wu",
            "Xiaojin Fu",
            "Shuqi Liu",
            "Xiongwei Han",
            "Tao Zhong",
            "Mingxuan Yuan"
        ],
        "title": "REG: A Regularization Optimizer for Robust Training Dynamics",
        "abstract": "Optimizers are crucial for the efficient training of Large Language Models (LLMs). While AdamW is the de facto standard, recent structure-aware optimizers like Muon have emerged, which regularize gradient updates by operating on entire weight matrices. The Muon optimizer balances the gradient updates along all the directions. However, Muon's reliance on the matrix sign function can lead to training instability, exhibits incompatibility when fine-tuning models pre-trained with AdamW. To address these limitations, we propose \\textbf{REG}, a novel optimizer that replaces Muon's aggressive matrix sign operator with the Row-and-Column-Scaling (RACS) operator. Theoretically grounded in balancing a matrix, the RACS operator regularizes the update steps in a less drastic manner, making it simpler to implement and more compatible with established training dynamics. Through extensive empirical experiments on LLM training, we demonstrate that our REG optimizer not only achieves superior performance and stability over AdamW, but also maintains consistency with the AdamW training paradigm. This consistency is particularly evident during the fine-tuning stage, where REG optimizer avoids the performance degradation observed with Muon.",
        "arxiv_id": "2510.03691"
    },
    "2510.04102": {
        "SCORE": 15,
        "ARXIVID": "2510.04102",
        "COMMENT": "Matches Representation Learning theory: formal analysis of extrapolation limits in neural networks with implications for designing models with better out-of-domain behavior.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Ramzi Dakhmouche",
            "Hossein Gorji"
        ],
        "title": "Why Cannot Neural Networks Master Extrapolation? Insights from Physical Laws",
        "abstract": "Motivated by the remarkable success of Foundation Models (FMs) in language modeling, there has been growing interest in developing FMs for time series prediction, given the transformative power such models hold for science and engineering. This culminated in significant success of FMs in short-range forecasting settings. However, extrapolation or long-range forecasting remains elusive for FMs, which struggle to outperform even simple baselines. This contrasts with physical laws which have strong extrapolation properties, and raises the question of the fundamental difference between the structure of neural networks and physical laws. In this work, we identify and formalize a fundamental property characterizing the ability of statistical learning models to predict more accurately outside of their training domain, hence explaining performance deterioration for deep learning models in extrapolation settings. In addition to a theoretical analysis, we present empirical results showcasing the implications of this property on current deep learning architectures. Our results not only clarify the root causes of the extrapolation gap but also suggest directions for designing next-generation forecasting models capable of mastering extrapolation.",
        "arxiv_id": "2510.04102"
    },
    "2510.03262": {
        "SCORE": 15,
        "ARXIVID": "2510.03262",
        "COMMENT": "Model Architecture: orthogonality-preserving adapter (LoRA) merging via Orthogonal Monte Carlo Dropout with analysis on compositionality/semantic interference.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Andi Zhang",
            "Xuan Ding",
            "Haofan Wang",
            "Steven McDonagh",
            "Samuel Kaski"
        ],
        "title": "Rethinking Inter-LoRA Orthogonality in Adapter Merging: Insights from Orthogonal Monte Carlo Dropout",
        "abstract": "We propose Orthogonal Monte Carlo Dropout, a mechanism that enforces strict orthogonality when combining sparse semantic vectors without extra time complexity. LoRA, a popular fine-tuning method for large models, typically trains a module to represent a specific concept such as an object or a style. When multiple LoRAs are merged, for example to generate an object in a particular style, their semantic vectors may interfere with each other. Our method guarantees, at the theoretical and runtime levels, that merged LoRAs remain orthogonal and thus free from direct interference. However, empirical analysis reveals that such orthogonality does not lead to the semantic disentanglement or compositionality highlighted in prior work on compositional adaptation. This finding suggests that inter-LoRA orthogonality alone may be insufficient for achieving true semantic compositionality, prompting a re-examination of its role in adapter merging.",
        "arxiv_id": "2510.03262"
    },
    "2510.03763": {
        "SCORE": 15,
        "ARXIVID": "2510.03763",
        "COMMENT": "Efficiency/optimization: accelerates SAM by decomposing and selectively reusing gradient components while preserving flat-minima generalization.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Jiaxin Deng",
            "Junbiao Pang"
        ],
        "title": "Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness Aware Minimization",
        "abstract": "Sharpness-Aware Minimization (SAM) improves model generalization but doubles the computational cost of Stochastic Gradient Descent (SGD) by requiring twice the gradient calculations per optimization step. To mitigate this, we propose Adaptively sampling-Reusing-mixing decomposed gradients to significantly accelerate SAM (ARSAM). Concretely, we firstly discover that SAM's gradient can be decomposed into the SGD gradient and the Projection of the Second-order gradient onto the First-order gradient (PSF). Furthermore, we observe that the SGD gradient and PSF dynamically evolve during training, emphasizing the growing role of the PSF to achieve a flat minima. Therefore, ARSAM is proposed to the reused PSF and the timely updated PSF still maintain the model's generalization ability. Extensive experiments show that ARSAM achieves state-of-the-art accuracies comparable to SAM across diverse network architectures. On CIFAR-10/100, ARSAM is comparable to SAM while providing a speedup of about 40\\%. Moreover, ARSAM accelerates optimization for the various challenge tasks (\\textit{e.g.}, human pose estimation, and model quantization) without sacrificing performance, demonstrating its broad practicality.% The code is publicly accessible at: https://github.com/ajiaaa/ARSAM.",
        "arxiv_id": "2510.03763"
    },
    "2510.04567": {
        "SCORE": 15,
        "ARXIVID": "2510.04567",
        "COMMENT": "Matches Model Architecture: an LLM-free, tuning-free graph foundational model enabling in-context learning via a novel token-based framework across node/edge/graph tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Weishuo Ma",
            "Yanbo Wang",
            "Xiyuan Wang",
            "Lei Zou",
            "Muhan Zhang"
        ],
        "title": "GILT: An LLM-Free, Tuning-Free Graph Foundational Model for In-Context Learning",
        "abstract": "Graph Neural Networks (GNNs) are powerful tools for precessing relational data but often struggle to generalize to unseen graphs, giving rise to the development of Graph Foundational Models (GFMs). However, current GFMs are challenged by the extreme heterogeneity of graph data, where each graph can possess a unique feature space, label set, and topology. To address this, two main paradigms have emerged. The first leverages Large Language Models (LLMs), but is fundamentally text-dependent, thus struggles to handle the numerical features in vast graphs. The second pre-trains a structure-based model, but the adaptation to new tasks typically requires a costly, per-graph tuning stage, creating a critical efficiency bottleneck. In this work, we move beyond these limitations and introduce \\textbf{G}raph \\textbf{I}n-context \\textbf{L}earning \\textbf{T}ransformer (GILT), a framework built on an LLM-free and tuning-free architecture. GILT introduces a novel token-based framework for in-context learning (ICL) on graphs, reframing classification tasks spanning node, edge and graph levels in a unified framework. This mechanism is the key to handling heterogeneity, as it is designed to operate on generic numerical features. Further, its ability to understand class semantics dynamically from the context enables tuning-free adaptation. Comprehensive experiments show that GILT achieves stronger few-shot performance with significantly less time than LLM-based or tuning-based baselines, validating the effectiveness of our approach.",
        "arxiv_id": "2510.04567"
    },
    "2510.03282": {
        "SCORE": 15,
        "ARXIVID": "2510.03282",
        "COMMENT": "Model Compression/Efficiency\u2014pruning-based circuit extraction; Representation Learning\u2014mechanistic interpretability via sparse circuit discovery with a hybrid attribution+pruning framework.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Hao Gu",
            "Vibhas Nair",
            "Amrithaa Ashok Kumar",
            "Jayvart Sharma",
            "Ryan Lagasse"
        ],
        "title": "Discovering Transformer Circuits via a Hybrid Attribution and Pruning Framework",
        "abstract": "Interpreting language models often involves circuit analysis, which aims to identify sparse subnetworks, or circuits, that accomplish specific tasks. Existing circuit discovery algorithms face a fundamental trade-off: attribution patching is fast but unfaithful to the full model, while edge pruning is faithful but computationally expensive. This research proposes a hybrid attribution and pruning (HAP) framework that uses attribution patching to identify a high-potential subgraph, then applies edge pruning to extract a faithful circuit from it. We show that HAP is 46\\% faster than baseline algorithms without sacrificing circuit faithfulness. Furthermore, we present a case study on the Indirect Object Identification task, showing that our method preserves cooperative circuit components (e.g. S-inhibition heads) that attribution patching methods prune at high sparsity. Our results show that HAP could be an effective approach for improving the scalability of mechanistic interpretability research to larger models. Our code is available at https://anonymous.4open.science/r/HAP-circuit-discovery.",
        "arxiv_id": "2510.03282"
    },
    "2510.04626": {
        "SCORE": 15,
        "ARXIVID": "2510.04626",
        "COMMENT": "Compression/Efficiency: concatenation of small embedding models with a Matryoshka-trained decoder and quantization to achieve high compression while preserving retrieval performance.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Mohamed Ayoub Ben Ayad",
            "Michael Dinzinger",
            "Kanishka Ghosh Dastidar",
            "Jelena Mitrovic",
            "Michael Granitzer"
        ],
        "title": "Compressed Concatenation of Small Embedding Models",
        "abstract": "Embedding models are central to dense retrieval, semantic search, and recommendation systems, but their size often makes them impractical to deploy in resource-constrained environments such as browsers or edge devices. While smaller embedding models offer practical advantages, they typically underperform compared to their larger counterparts. To bridge this gap, we demonstrate that concatenating the raw embedding vectors of multiple small models can outperform a single larger baseline on standard retrieval benchmarks. To overcome the resulting high dimensionality of naive concatenation, we introduce a lightweight unified decoder trained with a Matryoshka Representation Learning (MRL) loss. This decoder maps the high-dimensional joint representation to a low-dimensional space, preserving most of the original performance without fine-tuning the base models. We also show that while concatenating more base models yields diminishing gains, the robustness of the decoder's representation under compression and quantization improves. Our experiments show that, on a subset of MTEB retrieval tasks, our concat-encode-quantize pipeline recovers 89\\% of the original performance with a 48x compression factor when the pipeline is applied to a concatenation of four small embedding models.",
        "arxiv_id": "2510.04626"
    },
    "2510.04506": {
        "SCORE": 15,
        "ARXIVID": "2510.04506",
        "COMMENT": "Representation Learning\u2014treats contrastive signals as rewards over generated rationales to train embedding-capable LLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Jiashuo Sun",
            "Shixuan Liu",
            "Zhaochen Su",
            "Xianrui Zhong",
            "Pengcheng Jiang",
            "Bowen Jin",
            "Peiran Li",
            "Weijia Shi",
            "Jiawei Han"
        ],
        "title": "GRACE: Generative Representation Learning via Contrastive Policy Optimization",
        "abstract": "Prevailing methods for training Large Language Models (LLMs) as text encoders rely on contrastive losses that treat the model as a black box function, discarding its generative and reasoning capabilities in favor of static embeddings. We introduce GRACE (Generative Representation Learning via Contrastive Policy Optimization), a novel framework that reimagines contrastive signals not as losses to be minimized, but as rewards that guide a generative policy. In GRACE, the LLM acts as a policy that produces explicit, human-interpretable rationales--structured natural language explanations of its semantic understanding. These rationales are then encoded into high-quality embeddings via mean pooling. Using policy gradient optimization, we train the model with a multi-component reward function that maximizes similarity between query positive pairs and minimizes similarity with negatives. This transforms the LLM from an opaque encoder into an interpretable agent whose reasoning process is transparent and inspectable. On MTEB benchmark, GRACE yields broad cross category gains: averaged over four backbones, the supervised setting improves overall score by 11.5% over base models, and the unsupervised variant adds 6.9%, while preserving general capabilities. This work treats contrastive objectives as rewards over rationales, unifying representation learning with generation to produce stronger embeddings and transparent rationales. The model, data and code are available at https://github.com/GasolSun36/GRACE.",
        "arxiv_id": "2510.04506"
    },
    "2510.03250": {
        "SCORE": 15,
        "ARXIVID": "2510.03250",
        "COMMENT": "Model Architecture/Efficiency\u2014reparametrization of differentiable logic gate neurons reduces parameter size and improves training stability.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Lukas R\\\"uttgers",
            "Till Aczel",
            "Andreas Plesner",
            "Roger Wattenhofer"
        ],
        "title": "Light Differentiable Logic Gate Networks",
        "abstract": "Differentiable logic gate networks (DLGNs) exhibit extraordinary efficiency at inference while sustaining competitive accuracy. But vanishing gradients, discretization errors, and high training cost impede scaling these networks. Even with dedicated parameter initialization schemes from subsequent works, increasing depth still harms accuracy. We show that the root cause of these issues lies in the underlying parametrization of logic gate neurons themselves. To overcome this issue, we propose a reparametrization that also shrinks the parameter size logarithmically in the number of inputs per gate. For binary inputs, this already reduces the model size by 4x, speeds up the backward pass by up to 1.86x, and converges in 8.5x fewer training steps. On top of that, we show that the accuracy on CIFAR-100 remains stable and sometimes superior to the original parametrization.",
        "arxiv_id": "2510.03250"
    },
    "2510.04727": {
        "SCORE": 15,
        "ARXIVID": "2510.04727",
        "COMMENT": "Model Architecture\u2014Directional Sheaf Hypergraph Networks with a directed sheaf Laplacian for learning on directed/undirected hypergraphs.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Emanuele Mule",
            "Stefano Fiorini",
            "Antonio Purificato",
            "Federico Siciliano",
            "Stefano Coniglio",
            "Fabrizio Silvestri"
        ],
        "title": "Directional Sheaf Hypergraph Networks: Unifying Learning on Directed and Undirected Hypergraphs",
        "abstract": "Hypergraphs provide a natural way to represent higher-order interactions among multiple entities. While undirected hypergraphs have been extensively studied, the case of directed hypergraphs, which can model oriented group interactions, remains largely under-explored despite its relevance for many applications. Recent approaches in this direction often exhibit an implicit bias toward homophily, which limits their effectiveness in heterophilic settings. Rooted in the algebraic topology notion of Cellular Sheaves, Sheaf Neural Networks (SNNs) were introduced as an effective solution to circumvent such a drawback. While a generalization to hypergraphs is known, it is only suitable for undirected hypergraphs, failing to tackle the directed case. In this work, we introduce Directional Sheaf Hypergraph Networks (DSHN), a framework integrating sheaf theory with a principled treatment of asymmetric relations within a hypergraph. From it, we construct the Directed Sheaf Hypergraph Laplacian, a complex-valued operator by which we unify and generalize many existing Laplacian matrices proposed in the graph- and hypergraph-learning literature. Across 7 real-world datasets and against 13 baselines, DSHN achieves relative accuracy gains from 2% up to 20%, showing how a principled treatment of directionality in hypergraphs, combined with the expressive power of sheaves, can substantially improve performance.",
        "arxiv_id": "2510.04727"
    },
    "2510.03690": {
        "SCORE": 15,
        "ARXIVID": "2510.03690",
        "COMMENT": "Representation Learning: model-aware contrastive learning and mixup via graphon mixture modeling with a theoretical bound linking cut distance to motif densities.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Ali Azizpour",
            "Reza Ramezanpour",
            "Ashutosh Sabharwal",
            "Santiago Segarra"
        ],
        "title": "From Moments to Models: Graphon Mixture-Aware Mixup and Contrastive Learning",
        "abstract": "Real-world graph datasets often consist of mixtures of populations, where graphs are generated from multiple distinct underlying distributions. However, modern representation learning approaches, such as graph contrastive learning (GCL) and augmentation methods like Mixup, typically overlook this mixture structure. In this work, we propose a unified framework that explicitly models data as a mixture of underlying probabilistic graph generative models represented by graphons. To characterize these graphons, we leverage graph moments (motif densities) to cluster graphs arising from the same model. This enables us to disentangle the mixture components and identify their distinct generative mechanisms. This model-aware partitioning benefits two key graph learning tasks: 1) It enables a graphon-mixture-aware mixup (GMAM), a data augmentation technique that interpolates in a semantically valid space guided by the estimated graphons, instead of assuming a single graphon per class. 2) For GCL, it enables model-adaptive and principled augmentations. Additionally, by introducing a new model-aware objective, our proposed approach (termed MGCL) improves negative sampling by restricting negatives to graphs from other models. We establish a key theoretical guarantee: a novel, tighter bound showing that graphs sampled from graphons with small cut distance will have similar motif densities with high probability. Extensive experiments on benchmark datasets demonstrate strong empirical performance. In unsupervised learning, MGCL achieves state-of-the-art results, obtaining the top average rank across eight datasets. In supervised learning, GMAM consistently outperforms existing strategies, achieving new state-of-the-art accuracy in 6 out of 7 datasets.",
        "arxiv_id": "2510.03690"
    },
    "2510.03283": {
        "SCORE": 15,
        "ARXIVID": "2510.03283",
        "COMMENT": "High Performance Computing/Systems: colocated inference and fine-tuning with iteration-level scheduling and memory management to meet SLOs on edge GPUs.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Yufei Li",
            "Yu Fu",
            "Yue Dong",
            "Cong Liu"
        ],
        "title": "MACE: A Hybrid LLM Serving System with Colocated SLO-aware Continuous Retraining Alignment",
        "abstract": "Large language models (LLMs) deployed on edge servers are increasingly used in latency-sensitive applications such as personalized assistants, recommendation, and content moderation. However, the non-stationary nature of user data necessitates frequent retraining, which introduces a fundamental tension between inference latency and model accuracy under constrained GPU resources. Existing retraining strategies either delay model updates, over-commit resources to retraining, or overlook iteration-level retraining granularity. In this paper, we identify that iteration-level scheduling is crucial for adapting retraining frequency to model drift without violating service-level objectives (SLOs). We propose MACE, a hybrid LLM system that colocates concurrent inference (prefill, decode) and fine-tuning, with intelligent memory management to maximize task performance while promising inference throughput. MACE leverages the insight that not all model updates equally affect output alignment and allocates GPU cycles accordingly to balance throughput, latency, and update freshness. Our trace-driven evaluation shows that MACE matches or exceeds continuous retraining while reducing inference latency by up to 63% and maintaining throughput under resource constraints. Compared to periodic retraining, MACE improves latency breakdown across prefill, decode, and finetune stages, and sustains GPU utilization above 85% in NVIDIA AGX Orin. These results demonstrate that iteration-level hybrid scheduling is a promising direction for deploying LLMs with continual learning capabilities on edge platforms.",
        "arxiv_id": "2510.03283"
    }
}