{
    "2509.03249": {
        "SCORE": 17,
        "ARXIVID": "2509.03249",
        "COMMENT": "The paper introduces a novel calculus for representation transformation across diverse representational systems, which aligns with the core topic of representation learning.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Daniel Raggi",
            "Gem Stapleton",
            "Mateja Jamnik",
            "Aaron Stockdill",
            "Grecia Garcia Garcia",
            "Peter C-H. Cheng"
        ],
        "title": "Structure Transfer: an Inference-Based Calculus for the Transformation of Representations",
        "abstract": "Representation choice is of fundamental importance to our ability to communicate and reason effectively. A major unsolved problem, addressed in this paper, is how to devise \\textit{representational-system (RS) agnostic} techniques that drive representation transformation and choice. We present a novel calculus, called \\textit{structure transfer}, that enables representation transformation across diverse RSs. Specifically, given a \\textit{source} representation drawn from a source RS, the rules of structure transfer allow us to generate a \\textit{target} representation for a target RS. The generality of structure transfer comes in part from its ability to ensure that the source representation and the generated target representation satisfy \\textit{any} specified relation (such as semantic equivalence). This is done by exploiting \\textit{schemas}, which encode knowledge about RSs. Specifically, schemas can express \\textit{preservation of information} across relations between any pair of RSs, and this knowledge is used by structure transfer to derive a structure for the target representation which ensures that the desired relation holds. We formalise this using Representational Systems Theory~\\cite{raggi2022rst}, building on the key concept of a \\textit{construction space}. The abstract nature of construction spaces grants them the generality to model RSs of diverse kinds, including formal languages, geometric figures and diagrams, as well as informal notations. Consequently, structure transfer is a system-agnostic calculus that can be used to identify alternative representations in a wide range of practical settings.",
        "arxiv_id": "2509.03249"
    },
    "2509.03054": {
        "SCORE": 17,
        "ARXIVID": "2509.03054",
        "COMMENT": "The paper presents a novel optimization objective for binary quantization in LLMs, which is relevant to model compression.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Xinzhe Zheng",
            "Zhen-Qun Yang",
            "Haoran Xie",
            "S. Joe Qin",
            "Arlene Chen",
            "Fangzhen Lin"
        ],
        "title": "Binary Quantization For LLMs Through Dynamic Grouping",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of Natural Language Processing (NLP) tasks, but require substantial memory and computational resources. Binary quantization, which compresses model weights from 16-bit Brain Float to 1-bit representations in {-1, 1}, offers significant reductions in storage and inference costs. However, such aggressive quantization often leads to notable performance degradation compared to more conservative 4-bit quantization methods. In this research, we propose a novel optimization objective tailored for binary quantization, along with three algorithms designed to realize it effectively. Our method enhances blocked quantization by dynamically identifying optimal unstructured sub-matrices through adaptive grouping strategies. Experimental results demonstrate that our approach achieves an average bit length of just 1.007 bits, while maintaining high model quality. Specifically, our quantized LLaMA 3.2 3B model attains a perplexity of 8.23, remarkably close to the original 7.81, and surpasses previous SOTA BiLLM with a perplexity of only 123.90. Furthermore, our method is competitive with SOTA 4-bit approaches such as GPTQ in both performance and efficiency. The compression process is highly efficient, requiring only 14 seconds to quantize the full LLaMA 3.2 3B weights on a single CPU core, with the entire process completing in under 100 minutes and exhibiting embarrassingly parallel properties.   Code - https://github.com/johnnyzheng0636/WGM_bi_quan",
        "arxiv_id": "2509.03054"
    },
    "2509.03084": {
        "SCORE": 17,
        "ARXIVID": "2509.03084",
        "COMMENT": "The paper presents a new modeling approach for MD-based representation learning, which aligns with the core topic of representation learning.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Derek Jones",
            "Yue Yang",
            "Felice C. Lightstone",
            "Niema Moshiri",
            "Jonathan E. Allen",
            "Tajana S. Rosing"
        ],
        "title": "SurGBSA: Learning Representations From Molecular Dynamics Simulations",
        "abstract": "Self-supervised pretraining from static structures of drug-like compounds and proteins enable powerful learned feature representations. Learned features demonstrate state of the art performance on a range of predictive tasks including molecular properties, structure generation, and protein-ligand interactions. The majority of approaches are limited by their use of static structures and it remains an open question, how best to use atomistic molecular dynamics (MD) simulations to develop more generalized models to improve prediction accuracy for novel molecular structures. We present SURrogate mmGBSA (SurGBSA) as a new modeling approach for MD-based representation learning, which learns a surrogate function of the Molecular Mechanics Generalized Born Surface Area (MMGBSA). We show for the first time the benefits of physics-informed pre-training to train a surrogate MMGBSA model on a collection of over 1.4 million 3D trajectories collected from MD simulations of the CASF-2016 benchmark. SurGBSA demonstrates a dramatic 6,497x speedup versus a traditional physics-based single-point MMGBSA calculation while nearly matching single-point MMGBSA accuracy on the challenging pose ranking problem for identification of the correct top pose (-0.4% difference). Our work advances the development of molecular foundation models by showing model improvements when training on MD simulations. Models, code and training data are made publicly available.",
        "arxiv_id": "2509.03084"
    },
    "2509.02753": {
        "SCORE": 17,
        "ARXIVID": "2509.02753",
        "COMMENT": "The paper introduces LExI, a novel optimization technique for MoE models, which aligns with the core topic of model architecture and efficiency improvements.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Krishna Teja Chitty-Venkata",
            "Sandeep Madireddy",
            "Murali Emani",
            "Venkatram Vishwanath"
        ],
        "title": "LExI: Layer-Adaptive Active Experts for Efficient MoE Model Inference",
        "abstract": "Mixture-of-Experts (MoE) models scale efficiently by activating only a subset of experts per token, offering a computationally sparse alternative to dense architectures. While prior post-training optimizations, such as inter- and intra-expert pruning, reduce memory usage they provide limited gains in inference-time compute efficiency. Moreover, existing MoE architectures typically activate a fixed number of experts uniformly across all layers, resulting in redundant computation and suboptimal performance. In this work, we first demonstrate that MoE pruning strategies improve only the memory footprint but do not significantly improve inference performance on GPU using optimized frameworks such as vLLM. To address this, we introduce LExI, a data-free optimization technique that determines the optimal number of active experts per layer in a pretrained MoE model. LExI leverages only the model weights to estimate the relative importance of each layer and adaptively assigns the number of active experts accordingly per layer. Experiments on state-of-the-art language and vision MoE benchmarks demonstrate that LExI significantly outperforms traditional MoE pruning approaches in terms of inference efficiency with negligible accuracy loss. For example, using LExI, Qwen1.5-MoE achieves the same throughput on Nvidia H100 GPU with 10% better accuracy than traditional expert pruning.",
        "arxiv_id": "2509.02753"
    },
    "2509.02840": {
        "SCORE": 17,
        "ARXIVID": "2509.02840",
        "COMMENT": "The paper presents new algorithms for efficient SVD-type updating, which is relevant to model compression through low-rank approaches.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Johannes J. Brust",
            "Michael A. Saunders"
        ],
        "title": "Fast and Accurate SVD-Type Updating in Streaming Data",
        "abstract": "For a datastream, the change over a short interval is often of low rank. For high throughput information arranged in matrix format, recomputing an optimal SVD approximation after each step is typically prohibitive. Instead, incremental and truncated updating strategies are used, which may not scale for large truncation ranks. Therefore, we propose a set of efficient new algorithms that update a bidiagonal factorization, and which are similarly accurate as the SVD methods. In particular, we develop a compact Householder-type algorithm that decouples a sparse part from a low-rank update and has about half the memory requirements of standard bidiagonalization methods. A second algorithm based on Givens rotations has only about 10 flops per rotation and scales quadratically with the problem size, compared to a typical cubic scaling. The algorithm is therefore effective for processing high-throughput updates, as we demonstrate in tracking large subspaces of recommendation systems and networks, and when compared to well known software such as LAPACK or the incremental SVD.",
        "arxiv_id": "2509.02840"
    },
    "2509.03472": {
        "SCORE": 16,
        "ARXIVID": "2509.03472",
        "COMMENT": "The paper presents a dynamic quantization framework for differentially-private model training, which is relevant to model compression and efficiency.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Yubo Gao",
            "Renbo Tu",
            "Gennady Pekhimenko",
            "Nandita Vijaykumar"
        ],
        "title": "DPQuant: Efficient and Differentially-Private Model Training via Dynamic Quantization Scheduling",
        "abstract": "Differentially-Private SGD (DP-SGD) is a powerful technique to protect user privacy when using sensitive data to train neural networks. During training, converting model weights and activations into low-precision formats, i.e., quantization, can drastically reduce training times, energy consumption, and cost, and is thus a widely used technique. In this work, we demonstrate that quantization causes significantly higher accuracy degradation in DP-SGD compared to regular SGD. We observe that this is caused by noise injection in DP-SGD, which amplifies quantization variance, leading to disproportionately large accuracy degradation. To address this challenge, we present QPQuant, a dynamic quantization framework that adaptively selects a changing subset of layers to quantize at each epoch. Our method combines two key ideas that effectively reduce quantization variance: (i) probabilistic sampling of the layers that rotates which layers are quantized every epoch, and (ii) loss-aware layer prioritization, which uses a differentially private loss sensitivity estimator to identify layers that can be quantized with minimal impact on model quality. This estimator consumes a negligible fraction of the overall privacy budget, preserving DP guarantees. Empirical evaluations on ResNet18, ResNet50, and DenseNet121 across a range of datasets demonstrate that DPQuant consistently outperforms static quantization baselines, achieving near Pareto-optimal accuracy-compute trade-offs and up to 2.21x theoretical throughput improvements on low-precision hardware, with less than 2% drop in validation accuracy.",
        "arxiv_id": "2509.03472"
    },
    "2509.02803": {
        "SCORE": 16,
        "ARXIVID": "2509.02803",
        "COMMENT": "The paper proposes a novel pre-training method for GNNs by learning Laplacian eigenvectors, which is relevant to representation learning.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Howard Dai",
            "Nyambura Njenga",
            "Benjamin Whitsett",
            "Catherine Ma",
            "Darwin Deng",
            "Sara de \\'Angel",
            "Alexandre Van Tassel",
            "Siddharth Viswanath",
            "Ryan Pellico",
            "Ian Adelstein",
            "Smita Krishnaswamy"
        ],
        "title": "Learning Laplacian Eigenvectors: a Pre-training Method for Graph Neural Networks",
        "abstract": "We propose a novel framework for pre-training Graph Neural Networks (GNNs) by inductively learning Laplacian eigenvectors. Traditional Message Passing Neural Networks (MPNNs) often struggle to capture global and regional graph structure due to over-smoothing risk as network depth increases. Because the low-frequency eigenvectors of the graph Laplacian matrix encode global information, pre-training GNNs to predict these eigenvectors encourages the network to naturally learn large-scale structural patterns over each graph. Empirically, we show that models pre-trained via our framework outperform baseline models on a variety of graph structure-based tasks. While most existing pre-training methods focus on domain-specific tasks like node or edge feature reconstruction, our self-supervised pre-training framework is structure-based and highly flexible. Eigenvector-learning can be applied to all graph-based datasets, and can be used with synthetic features when task-specific data is sparse.",
        "arxiv_id": "2509.02803"
    },
    "2509.03474": {
        "SCORE": 16,
        "ARXIVID": "2509.03474",
        "COMMENT": "The paper provides a theoretical foundation for Tuning without Forgetting in neural ODEs, which is relevant to emerging trends in model architecture.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Erkan Bayram",
            "Mohamed-Ali Belabbas",
            "Tamer Ba\\c{s}ar"
        ],
        "title": "Geometric Foundations of Tuning without Forgetting in Neural ODEs",
        "abstract": "In our earlier work, we introduced the principle of Tuning without Forgetting (TwF) for sequential training of neural ODEs, where training samples are added iteratively and parameters are updated within the subspace of control functions that preserves the end-point mapping at previously learned samples on the manifold of output labels in the first-order approximation sense. In this letter, we prove that this parameter subspace forms a Banach submanifold of finite codimension under nonsingular controls, and we characterize its tangent space. This reveals that TwF corresponds to a continuation/deformation of the control function along the tangent space of this Banach submanifold, providing a theoretical foundation for its mapping-preserving (not forgetting) during the sequential training exactly, beyond first-order approximation.",
        "arxiv_id": "2509.03474"
    },
    "2509.03244": {
        "SCORE": 15,
        "ARXIVID": "2509.03244",
        "COMMENT": "The paper introduces a new paradigm for foundation models in multi-objective optimization, which aligns with foundational research in AI for Science.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Yiming Yao",
            "Fei Liu",
            "Liang Zhao",
            "Xi Lin",
            "Qingfu Zhang"
        ],
        "title": "FoMEMO: Towards Foundation Models for Expensive Multi-objective Optimization",
        "abstract": "Expensive multi-objective optimization is a prevalent and crucial concern in many real-world scenarios, where sample-efficiency is vital due to the limited evaluations to recover the true Pareto front for decision making. Existing works either involve rebuilding Gaussian process surrogates from scratch for each objective in each new problem encountered, or rely on extensive past domain experiments for pre-training deep learning models, making them hard to generalize and impractical to cope with various emerging applications in the real world. To address this issue, we propose a new paradigm named FoMEMO (Foundation Models for Expensive Multi-objective Optimization), which enables the establishment of a foundation model conditioned on any domain trajectory and user preference, and facilitates fast in-context optimization based on the predicted preference-wise aggregation posteriors. Rather than accessing extensive domain experiments in the real world, we demonstrate that pre-training the foundation model with a diverse set of hundreds of millions of synthetic data can lead to superior adaptability to unknown problems, without necessitating any subsequent model training or updates in the optimization process. We evaluate our method across a variety of synthetic benchmarks and real-word applications, and demonstrate its superior generality and competitive performance compared to existing methods.",
        "arxiv_id": "2509.03244"
    },
    "2509.02792": {
        "SCORE": 15,
        "ARXIVID": "2509.02792",
        "COMMENT": "The paper introduces Structured Basis Function Networks, linking multi-hypothesis prediction and ensembling, which is relevant to representation learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Alejandro Rodriguez Dominguez",
            "Muhammad Shahzad",
            "Xia Hong"
        ],
        "title": "Structured Basis Function Networks: Loss-Centric Multi-Hypothesis Ensembles with Controllable Diversity",
        "abstract": "Existing approaches to predictive uncertainty rely either on multi-hypothesis prediction, which promotes diversity but lacks principled aggregation, or on ensemble learning, which improves accuracy but rarely captures the structured ambiguity. This implicitly means that a unified framework consistent with the loss geometry remains absent. The Structured Basis Function Network addresses this gap by linking multi-hypothesis prediction and ensembling through centroidal aggregation induced by Bregman divergences. The formulation applies across regression and classification by aligning predictions with the geometry of the loss, and supports both a closed-form least-squares estimator and a gradient-based procedure for general objectives. A tunable diversity mechanism provides parametric control of the bias-variance-diversity trade-off, connecting multi-hypothesis generalisation with loss-aware ensemble aggregation. Experiments validate this relation and use the mechanism to study the complexity-capacity-diversity trade-off across datasets of increasing difficulty with deep-learning predictors.",
        "arxiv_id": "2509.02792"
    },
    "2509.02751": {
        "SCORE": 15,
        "ARXIVID": "2509.02751",
        "COMMENT": "The paper proposes a contrastive clustering framework for influential node identification, which involves representation learning through contrastive methods.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Matthew Russo",
            "Tim Kraska"
        ],
        "title": "Deep Research is the New Analytics System: Towards Building the Runtime for AI-Driven Analytics",
        "abstract": "With advances in large language models (LLMs), researchers are creating new systems that can perform AI-driven analytics over large unstructured datasets. Recent work has explored executing such analytics queries using semantic operators -- a declarative set of AI-powered data transformations with natural language specifications. However, even when optimized, these operators can be expensive to execute on millions of records and their iterator execution semantics make them ill-suited for interactive data analytics tasks. In another line of work, Deep Research systems have demonstrated an ability to answer natural language question(s) over large datasets. These systems use one or more LLM agent(s) to plan their execution, process the dataset(s), and iteratively refine their answer. However, these systems do not explicitly optimize their query plans which can lead to poor plan execution. In order for AI-driven analytics to excel, we need a runtime which combines the optimized execution of semantic operators with the flexibility and more dynamic execution of Deep Research systems. As a first step towards this vision, we build a prototype which enables Deep Research agents to write and execute optimized semantic operator programs. We evaluate our prototype and demonstrate that it can outperform a handcrafted semantic operator program and open Deep Research systems on two basic queries. Compared to a standard open Deep Research agent, our prototype achieves up to 1.95x better F1-score. Furthermore, even if we give the agent access to semantic operators as tools, our prototype still achieves cost and runtime savings of up to 76.8% and 72.7% thanks to its optimized execution.",
        "arxiv_id": "2509.02751"
    },
    "2509.03505": {
        "SCORE": 15,
        "ARXIVID": "2509.03505",
        "COMMENT": "The paper introduces LimiX, a foundation model for structured data, which aligns with the core topic of foundational models and architecture-level innovations.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Xingxuan Zhang",
            "Gang Ren",
            "Han Yu",
            "Hao Yuan",
            "Hui Wang",
            "Jiansheng Li",
            "Jiayun Wu",
            "Lang Mo",
            "Li Mao",
            "Mingchao Hao",
            "Ningbo Dai",
            "Renzhe Xu",
            "Shuyang Li",
            "Tianyang Zhang",
            "Yue He",
            "Yuanrui Wang",
            "Yunjia Zhang",
            "Zijing Xu",
            "Dongzhe Li",
            "Fang Gao",
            "Hao Zou",
            "Jiandong Liu",
            "Jiashuo Liu",
            "Jiawei Xu",
            "Kaijie Cheng",
            "Kehan Li",
            "Linjun Zhou",
            "Qing Li",
            "Shaohua Fan",
            "Xiaoyu Lin",
            "Xinyan Han",
            "Xuanyue Li",
            "Yan Lu",
            "Yuan Xue",
            "Yuanyuan Jiang",
            "Zimu Wang",
            "Zhenlei Wang",
            "Peng Cui"
        ],
        "title": "LimiX: Unleashing Structured-Data Modeling Capability for Generalist Intelligence",
        "abstract": "We argue that progress toward general intelligence requires complementary foundation models grounded in language, the physical world, and structured data. This report presents LimiX, the first installment of our large structured-data models (LDMs). LimiX treats structured data as a joint distribution over variables and missingness, thus capable of addressing a wide range of tabular tasks through query-based conditional prediction via a single model. LimiX is pretrained using masked joint-distribution modeling with an episodic, context-conditional objective, where the model predicts for query subsets conditioned on dataset-specific contexts, supporting rapid, training-free adaptation at inference. We evaluate LimiX across 10 large structured-data benchmarks with broad regimes of sample size, feature dimensionality, class number, categorical-to-numerical feature ratio, missingness, and sample-to-feature ratios. With a single model and a unified interface, LimiX consistently surpasses strong baselines including gradient-boosting trees, deep tabular networks, recent tabular foundation models, and automated ensembles, as shown in Figure 1 and Figure 2. The superiority holds across a wide range of tasks, such as classification, regression, missing value imputation, and data generation, often by substantial margins, while avoiding task-specific architectures or bespoke training per task. All LimiX models are publicly accessible under Apache 2.0.",
        "arxiv_id": "2509.03505"
    },
    "2509.02820": {
        "SCORE": 15,
        "ARXIVID": "2509.02820",
        "COMMENT": "The paper introduces a new method for unlearning in LLMs, which is relevant to foundational research in LLM behavior and interpretability.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Naman Deep Singh",
            "Maximilian M\\\"uller",
            "Francesco Croce",
            "Matthias Hein"
        ],
        "title": "Unlearning That Lasts: Utility-Preserving, Robust, and Almost Irreversible Forgetting in LLMs",
        "abstract": "Unlearning in large language models (LLMs) involves precisely removing specific information from a pre-trained model. This is crucial to ensure safety of LLMs by deleting private data or harmful knowledge acquired during pre-training. However, existing unlearning methods often fall short when subjected to thorough evaluation. To overcome this, we introduce JensUn, where we leverage the Jensen-Shannon Divergence as the training objective for both forget and retain sets for more stable and effective unlearning dynamics compared to commonly used loss functions. In extensive experiments, JensUn achieves better forget-utility trade-off than competing methods, and even demonstrates strong resilience to benign relearning. Additionally, for a precise unlearning evaluation, we introduce LKF, a curated dataset of lesser-known facts that provides a realistic unlearning scenario. Finally, to comprehensively test unlearning methods, we propose (i) employing an LLM as semantic judge instead of the standard ROUGE score, and (ii) using worst-case unlearning evaluation over various paraphrases and input formats. Our improved evaluation framework reveals that many existing methods are less effective than previously thought.",
        "arxiv_id": "2509.02820"
    },
    "2509.02785": {
        "SCORE": 15,
        "ARXIVID": "2509.02785",
        "COMMENT": "The paper introduces a novel framework for long-text generation with a dynamic expert scheduling mechanism and hierarchical sparse attention, which aligns with the interest in model architecture innovations and efficiency improvements.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Jusheng Zhang",
            "Yijia Fan",
            "Kaitong Cai",
            "Zimeng Huang",
            "Xiaofei Sun",
            "Jian Wang",
            "Chengpei Tang",
            "Keze Wang"
        ],
        "title": "DrDiff: Dynamic Routing Diffusion with Hierarchical Attention for Breaking the Efficiency-Quality Trade-off",
        "abstract": "This paper introduces DrDiff, a novel framework for long-text generation that overcomes the efficiency-quality trade-off through three core technologies. First, we design a dynamic expert scheduling mechanism that intelligently allocates computational resources during the diffusion process based on text complexity, enabling more efficient handling of text generation tasks of varying difficulty. Second, we introduce a Hierarchical Sparse Attention (HSA) mechanism that adaptively adjusts attention patterns according to a variety of input lengths, reducing computational complexity from O($n^2$) to O($n$) while maintaining model performance. Finally, we propose a soft absorption guidance optimization strategy that combines with DPM-solver++ to reduce diffusion steps, significantly improving generation speed. Comprehensive experiments on various long-text generation benchmarks demonstrate the superiority of our DrDiff over the existing SOTA methods.",
        "arxiv_id": "2509.02785"
    },
    "2509.02609": {
        "SCORE": 15,
        "ARXIVID": "2509.02609",
        "COMMENT": "The paper presents a novel deep unsupervised framework for influential node identification using contrastive clustering, which aligns with the interest in representation learning and contrastive methods.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Yanmei Hu",
            "Yihang Wu",
            "Bing Sun",
            "Xue Yue",
            "Biao Cai",
            "Xiangtao Li",
            "Yang Chen"
        ],
        "title": "Contrastive clustering based on regular equivalence for influential node identification in complex networks",
        "abstract": "Identifying influential nodes in complex networks is a fundamental task in network analysis with wide-ranging applications across domains. While deep learning has advanced node influence detection, existing supervised approaches remain constrained by their reliance on labeled data, limiting their applicability in real-world scenarios where labels are scarce or unavailable. While contrastive learning demonstrates significant potential for performance enhancement, existing approaches predominantly rely on multiple-embedding generation to construct positive/negative sample pairs. To overcome these limitations, we propose ReCC (\\textit{r}egular \\textit{e}quivalence-based \\textit{c}ontrastive \\textit{c}lustering), a novel deep unsupervised framework for influential node identification. We first reformalize influential node identification as a label-free deep clustering problem, then develop a contrastive learning mechanism that leverages regular equivalence-based similarity, which captures structural similarities between nodes beyond local neighborhoods, to generate positive and negative samples. This mechanism is integrated into a graph convolutional network to learn node embeddings that are used to differentiate influential from non-influential nodes. ReCC is pre-trained using network reconstruction loss and fine-tuned with a combined contrastive and clustering loss, with both phases being independent of labeled data. Additionally, ReCC enhances node representations by combining structural metrics with regular equivalence-based similarities. Extensive experiments demonstrate that ReCC outperforms state-of-the-art approaches across several benchmarks.",
        "arxiv_id": "2509.02609"
    }
}