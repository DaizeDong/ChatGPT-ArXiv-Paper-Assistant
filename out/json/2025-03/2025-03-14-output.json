{
    "2501.07139": {
        "SCORE": 17,
        "ARXIVID": "2501.07139",
        "COMMENT": "FlexQuant proposes an elastic quantization framework for LLMs on edge devices, which aligns with foundational research in model compression and efficiency.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Yuji Chai",
            "Mujin Kwen",
            "David Brooks",
            "Gu-Yeon Wei"
        ],
        "title": "FlexQuant: Elastic Quantization Framework for Locally Hosted LLM on Edge Devices",
        "abstract": "Deploying LLMs on edge devices presents serious technical challenges. Memory elasticity is crucial for edge devices with unified memory, where memory is shared and fluctuates dynamically. Existing solutions suffer from either poor transition granularity or high storage costs. We propose FlexQuant, a novel elasticity framework that generates an ensemble of quantized models, providing an elastic hosting solution with 15x granularity improvement and 10x storage reduction compared to SoTA methods. FlexQuant works with most quantization methods and creates a family of trade-off options under various storage limits through our pruning method. It brings great performance and flexibility to the edge deployment of LLMs.",
        "arxiv_id": "2501.07139"
    },
    "2501.07446": {
        "SCORE": 17,
        "ARXIVID": "2501.07446",
        "COMMENT": "The paper explores entropy-regularized optimal transport and its application to barycenter computation, which aligns with foundational research in representation learning and efficiency. The focus on theoretical insights and robustness is highly relevant.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Brendan Mallery",
            "James M. Murphy",
            "Shuchin Aeron"
        ],
        "title": "Synthesis and Analysis of Data as Probability Measures with Entropy-Regularized Optimal Transport",
        "abstract": "We consider synthesis and analysis of probability measures using the entropy-regularized Wasserstein-2 cost and its unbiased version, the Sinkhorn divergence. The synthesis problem consists of computing the barycenter, with respect to these costs, of $m$ reference measures given a set of coefficients belonging to the $m$-dimensional simplex. The analysis problem consists of finding the coefficients for the closest barycenter in the Wasserstein-2 distance to a given measure $\\mu$. Under the weakest assumptions on the measures thus far in the literature, we compute the derivative of the entropy-regularized Wasserstein-2 cost. We leverage this to establish a characterization of regularized barycenters as solutions to a fixed-point equation for the average of the entropic maps from the barycenter to the reference measures. This characterization yields a finite-dimensional, convex, quadratic program for solving the analysis problem when $\\mu$ is a barycenter. It is shown that these coordinates, as well as the value of the barycenter functional, can be estimated from samples with dimension-independent rates of convergence, a hallmark of entropy-regularized optimal transport, and we verify these rates experimentally. We also establish that barycentric coordinates are stable with respect to perturbations in the Wasserstein-2 metric, suggesting a robustness of these coefficients to corruptions. We employ the barycentric coefficients as features for classification of corrupted point cloud data, and show that compared to neural network baselines, our approach is more efficient in small training data regimes.",
        "arxiv_id": "2501.07446"
    },
    "2501.07523": {
        "SCORE": 17,
        "ARXIVID": "2501.07523",
        "COMMENT": "The paper introduces a framework for position-invariant RAG using key-value cache fusion, which aligns with model compression and efficiency breakthroughs, particularly in LLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Philhoon Oh",
            "Jinwoo Shin",
            "James Thorne"
        ],
        "title": "Parallel Key-Value Cache Fusion for Position Invariant RAG",
        "abstract": "Recent advancements in Large Language Models (LLMs) underscore the necessity of Retrieval Augmented Generation (RAG) to leverage external information. However, LLMs are sensitive to the position of relevant information within contexts and tend to generate incorrect responses when such information is placed in the middle, known as `Lost in the Middle' phenomenon. In this paper, we introduce a framework that generates consistent outputs for decoder-only models, irrespective of the input context order. Experimental results for three open domain question answering tasks demonstrate position invariance, where the model is not sensitive to input context order, and superior robustness to irrelevent passages compared to prevailing approaches for RAG pipelines.",
        "arxiv_id": "2501.07523"
    },
    "2501.07400": {
        "SCORE": 17,
        "ARXIVID": "2501.07400",
        "COMMENT": "This paper derives gradient flow equations and explores training dynamics in deep learning, which directly aligns with the representation learning criterion. The theoretical insights into gradient descent and data truncation are highly relevant.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Thomas Chen"
        ],
        "title": "Derivation of effective gradient flow equations and dynamical truncation of training data in Deep Learning",
        "abstract": "We derive explicit equations governing the cumulative biases and weights in Deep Learning with ReLU activation function, based on gradient descent for the Euclidean cost in the input layer, and under the assumption that the weights are, in a precise sense, adapted to the coordinate system distinguished by the activations. We show that gradient descent corresponds to a dynamical process in the input layer, whereby clusters of data are progressively reduced in complexity (\"truncated\") at an exponential rate that increases with the number of data points that have already been truncated. We provide a detailed discussion of several types of solutions to the gradient flow equations. A main motivation for this work is to shed light on the interpretability question in supervised learning.",
        "arxiv_id": "2501.07400"
    },
    "2501.07021": {
        "SCORE": 17,
        "ARXIVID": "2501.07021",
        "COMMENT": "The paper introduces Neural Probabilistic Circuits, which align with the model architecture criterion by proposing a novel interpretable architecture combining probabilistic circuits and neural networks. The focus on logical reasoning adds theoretical depth.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Weixin Chen",
            "Simon Yu",
            "Huajie Shao",
            "Lui Sha",
            "Han Zhao"
        ],
        "title": "Neural Probabilistic Circuits: Enabling Compositional and Interpretable Predictions through Logical Reasoning",
        "abstract": "End-to-end deep neural networks have achieved remarkable success across various domains but are often criticized for their lack of interpretability. While post hoc explanation methods attempt to address this issue, they often fail to accurately represent these black-box models, resulting in misleading or incomplete explanations. To overcome these challenges, we propose an inherently transparent model architecture called Neural Probabilistic Circuits (NPCs), which enable compositional and interpretable predictions through logical reasoning. In particular, an NPC consists of two modules: an attribute recognition model, which predicts probabilities for various attributes, and a task predictor built on a probabilistic circuit, which enables logical reasoning over recognized attributes to make class predictions. To train NPCs, we introduce a three-stage training algorithm comprising attribute recognition, circuit construction, and joint optimization. Moreover, we theoretically demonstrate that an NPC's error is upper-bounded by a linear combination of the errors from its modules. To further demonstrate the interpretability of NPC, we provide both the most probable explanations and the counterfactual explanations. Empirical results on four benchmark datasets show that NPCs strike a balance between interpretability and performance, achieving results competitive even with those of end-to-end black-box models while providing enhanced interpretability.",
        "arxiv_id": "2501.07021"
    },
    "2501.07237": {
        "SCORE": 17,
        "ARXIVID": "2501.07237",
        "COMMENT": "The paper introduces Gradient Wavelet Transform (GWT) to reduce memory requirements during LLM training, which is relevant to model compression and efficiency breakthroughs.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Ziqing Wen",
            "Ping Luo",
            "Jiahuan Wang",
            "Xiaoge Deng",
            "Jinping Zou",
            "Kun Yuan",
            "Tao Sun",
            "Dongsheng Li"
        ],
        "title": "Breaking Memory Limits: Gradient Wavelet Transform Enhances LLMs Training",
        "abstract": "Large language models (LLMs) have shown impressive performance across a range of natural language processing tasks. However, their vast number of parameters introduces significant memory challenges during training, particularly when using memory-intensive optimizers like Adam. Existing memory-efficient algorithms often rely on techniques such as singular value decomposition projection or weight freezing. While these approaches help alleviate memory constraints, they generally produce suboptimal results compared to full-rank updates. In this paper, we investigate the memory-efficient method beyond low-rank training, proposing a novel solution called Gradient Wavelet Transform (GWT), which applies wavelet transforms to gradients in order to significantly reduce the memory requirements for maintaining optimizer states. We demonstrate that GWT can be seamlessly integrated with memory-intensive optimizers, enabling efficient training without sacrificing performance. Through extensive experiments on both pre-training and fine-tuning tasks, we show that GWT achieves state-of-the-art performance compared with advanced memory-efficient optimizers and full-rank approaches in terms of both memory usage and training performance.",
        "arxiv_id": "2501.07237"
    },
    "2501.07155": {
        "SCORE": 16,
        "ARXIVID": "2501.07155",
        "COMMENT": "AlphaNet proposes a novel equivariant model for atomistic simulations, which aligns with foundational research in AI for Science by introducing architectural innovations for molecular modeling.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Bangchen Yin",
            "Jiaao Wang",
            "Weitao Du",
            "Pengbo Wang",
            "Penghua Ying",
            "Haojun Jia",
            "Zisheng Zhang",
            "Yuanqi Du",
            "Carla P. Gomes",
            "Graeme Henkelman",
            "Chenru Duan",
            "Hai Xiao"
        ],
        "title": "AlphaNet: Scaling Up Local Frame-based Atomistic Interatomic Potential",
        "abstract": "Molecular dynamics simulations demand unprecedented accuracy and scalability to tackle grand challenges in energy materials, catalytic processes, and biomolecular design. To bridge this gap, we present AlphaNet, a local frame-based equivariant model that simultaneously advances computational efficiency and predictive precision for atomistic systems. By constructing equivariant local frames with learnable geometric transitions, AlphaNet encodes atomic environments with enhanced representational capacity, achieving state of the art accuracy in energy and force predictions. Extensive benchmarks spanning defected graphene, formate decomposition, inorganic bulks, and large-scale datasets (OC2M and Matbench Discovery) demonstrate its superior performance over existing neural network interatomic potentials while ensuring scalability across diverse system sizes. The synergy of accuracy, efficiency, and transferability positions AlphaNet as a transformative tool for simulating multiscale phenomena, from catalyst dynamics to energy storage interfaces, with direct implications for accelerating the discovery of functional materials and complex molecular systems.",
        "arxiv_id": "2501.07155"
    },
    "2501.07754": {
        "SCORE": 16,
        "ARXIVID": "2501.07754",
        "COMMENT": "The introduction of the BOLT loss for achieving Bayes optimal classification accuracy is a novel contribution to representation learning, with potential implications for training dynamics and generalization.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Mohammadreza Tavasoli Naeini",
            "Ali Bereyhi",
            "Morteza Noshad",
            "Ben Liang",
            "Alfred O. Hero III"
        ],
        "title": "Universal Training of Neural Networks to Achieve Bayes Optimal Classification Accuracy",
        "abstract": "This work invokes the notion of $f$-divergence to introduce a novel upper bound on the Bayes error rate of a general classification task. We show that the proposed bound can be computed by sampling from the output of a parameterized model. Using this practical interpretation, we introduce the Bayes optimal learning threshold (BOLT) loss whose minimization enforces a classification model to achieve the Bayes error rate. We validate the proposed loss for image and text classification tasks, considering MNIST, Fashion-MNIST, CIFAR-10, and IMDb datasets. Numerical experiments demonstrate that models trained with BOLT achieve performance on par with or exceeding that of cross-entropy, particularly on challenging datasets. This highlights the potential of BOLT in improving generalization.",
        "arxiv_id": "2501.07754"
    },
    "2501.07681": {
        "SCORE": 16,
        "ARXIVID": "2501.07681",
        "COMMENT": "The paper links dataset distillation to optimal quantization and introduces a novel extension to an existing method, which aligns with representation learning and foundational generative paradigms.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Hong Ye Tan",
            "Emma Slade"
        ],
        "title": "Dataset Distillation as Pushforward Optimal Quantization",
        "abstract": "Dataset distillation aims to find a synthetic training set such that training on the synthetic data achieves similar performance to training on real data, with orders of magnitude less computational requirements. Existing methods can be broadly categorized as either bi-level optimization problems that have neural network training heuristics as the lower level problem, or disentangled methods that bypass the bi-level optimization by matching distributions of data. The latter method has the major advantages of speed and scalability in terms of size of both training and distilled datasets. We demonstrate that when equipped with an encoder-decoder structure, the empirically successful disentangled methods can be reformulated as an optimal quantization problem, where a finite set of points is found to approximate the underlying probability measure by minimizing the expected projection distance. In particular, we link existing disentangled dataset distillation methods to the classical optimal quantization and Wasserstein barycenter problems, demonstrating consistency of distilled datasets for diffusion-based generative priors. We propose a simple extension of the state-of-the-art data distillation method D4M, achieving better performance on the ImageNet-1K dataset with trivial additional computation, and state-of-the-art performance in higher image-per-class settings.",
        "arxiv_id": "2501.07681"
    },
    "2501.07045": {
        "SCORE": 15,
        "ARXIVID": "2501.07045",
        "COMMENT": "The paper introduces an angle-compensated contrastive regularizer for deep regression, which aligns with representation learning by addressing label relationships in feature space. The method is novel and could have broader implications for contrastive learning in regression tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Botao Zhao",
            "Xiaoyang Qu",
            "Zuheng Kang",
            "Junqing Peng",
            "Jing Xiao",
            "Jianzong Wang"
        ],
        "title": "ACCon: Angle-Compensated Contrastive Regularizer for Deep Regression",
        "abstract": "In deep regression, capturing the relationship among continuous labels in feature space is a fundamental challenge that has attracted increasing interest. Addressing this issue can prevent models from converging to suboptimal solutions across various regression tasks, leading to improved performance, especially for imbalanced regression and under limited sample sizes. However, existing approaches often rely on order-aware representation learning or distance-based weighting. In this paper, we hypothesize a linear negative correlation between label distances and representation similarities in regression tasks. To implement this, we propose an angle-compensated contrastive regularizer for deep regression, which adjusts the cosine distance between anchor and negative samples within the contrastive learning framework. Our method offers a plug-and-play compatible solution that extends most existing contrastive learning methods for regression tasks. Extensive experiments and theoretical analysis demonstrate that our proposed angle-compensated contrastive regularizer not only achieves competitive regression performance but also excels in data efficiency and effectiveness on imbalanced datasets.",
        "arxiv_id": "2501.07045"
    },
    "2501.07161": {
        "SCORE": 15,
        "ARXIVID": "2501.07161",
        "COMMENT": "The paper proposes a compiler-based mixed-precision quantization method, which aligns with the model compression criterion. The use of local metrics and operator fusion adds methodological insights, making it relevant to foundational efficiency research.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Jeongseok Kim",
            "Jemin Lee",
            "Yongin Kwon",
            "Daeyoung Kim"
        ],
        "title": "QuantuneV2: Compiler-Based Local Metric-Driven Mixed Precision Quantization for Practical Embedded AI Applications",
        "abstract": "Mixed-precision quantization methods have been proposed to reduce model size while minimizing accuracy degradation. However, existing studies require retraining and do not consider the computational overhead and intermediate representations (IR) generated during the compilation process, limiting their application at the compiler level. This computational overhead refers to the runtime latency caused by frequent quantization and dequantization operations during inference. Performing these operations at the individual operator level causes significant runtime delays. To address these issues, we propose QuantuneV2, a compiler-based mixed-precision quantization method designed for practical embedded AI applications. QuantuneV2 performs inference only twice, once before quantization and once after quantization, and operates with a computational complexity of O(n) that increases linearly with the number of model parameters. We also made the sensitivity analysis more stable by using local metrics like weights, activation values, the Signal to Quantization Noise Ratio, and the Mean Squared Error. We also cut down on computational overhead by choosing the best IR and using operator fusion. Experimental results show that QuantuneV2 achieved up to a 10.28 percent improvement in accuracy and a 12.52 percent increase in speed compared to existing methods across five models: ResNet18v1, ResNet50v1, SqueezeNetv1, VGGNet, and MobileNetv2. This demonstrates that QuantuneV2 enhances model performance while maintaining computational efficiency, making it suitable for deployment in embedded AI environments.",
        "arxiv_id": "2501.07161"
    },
    "2501.07032": {
        "SCORE": 15,
        "ARXIVID": "2501.07032",
        "COMMENT": "The paper introduces PRKANs, a parameter-reduced version of Kolmogorov-Arnold Networks, which aligns with the model architecture criterion. The focus on reducing parameter count and exploring Gaussian Radial Basis Functions adds novelty.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Hoang-Thang Ta",
            "Duy-Quy Thai",
            "Anh Tran",
            "Grigori Sidorov",
            "Alexander Gelbukh"
        ],
        "title": "PRKAN: Parameter-Reduced Kolmogorov-Arnold Networks",
        "abstract": "Kolmogorov-Arnold Networks (KANs) represent an innovation in neural network architectures, offering a compelling alternative to Multi-Layer Perceptrons (MLPs) in models such as Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Transformers. By advancing network design, KANs drive groundbreaking research and enable transformative applications across various scientific domains involving neural networks. However, existing KANs often require significantly more parameters in their network layers than MLPs. To address this limitation, this paper introduces PRKANs (Parameter-Reduced Kolmogorov-Arnold Networks), which employ several methods to reduce the parameter count in KAN layers, making them comparable to MLP layers. Experimental results on the MNIST and Fashion-MNIST datasets demonstrate that PRKANs outperform several existing KANs, and their variant with attention mechanisms rivals the performance of MLPs, albeit with slightly longer training times. Furthermore, the study highlights the advantages of Gaussian Radial Basis Functions (GRBFs) and layer normalization in KAN designs. The repository for this work is available at: https://github.com/hoangthangta/All-KAN.",
        "arxiv_id": "2501.07032"
    },
    "2501.07306": {
        "SCORE": 15,
        "ARXIVID": "2501.07306",
        "COMMENT": "The VBMM algorithm introduces a novel adaptive Bregman descent method, which is a foundational contribution to optimization techniques and has potential applications in representation learning.",
        "RELEVANCE": 7,
        "NOVELTY": 8,
        "authors": [
            "S\u00e9gol\u00e8ne Martin",
            "Jean-Christophe Pesquet",
            "Gabriele Steidl",
            "Ismail Ben Ayed"
        ],
        "title": "Variable Bregman Majorization-Minimization Algorithm and its Application to Dirichlet Maximum Likelihood Estimation",
        "abstract": "We propose a novel Bregman descent algorithm for minimizing a convex function that is expressed as the sum of a differentiable part (defined over an open set) and a possibly nonsmooth term. The approach, referred to as the Variable Bregman Majorization-Minimization (VBMM) algorithm, extends the Bregman Proximal Gradient method by allowing the Bregman function used in the divergence to adaptively vary at each iteration, provided it satisfies a majorizing condition on the objective function. This adaptive framework enables the algorithm to approximate the objective more precisely at each iteration, thereby allowing for accelerated convergence compared to the traditional Bregman Proximal Gradient descent. We establish the convergence of the VBMM algorithm to a minimizer under mild assumptions on the family of metrics used. Furthermore, we introduce a novel application of both the Bregman Proximal Gradient method and the VBMM algorithm to the estimation of the multidimensional parameters of a Dirichlet distribution through the maximization of its log-likelihood. Numerical experiments confirm that the VBMM algorithm outperforms existing approaches in terms of convergence speed.",
        "arxiv_id": "2501.07306"
    },
    "2501.07124": {
        "SCORE": 13,
        "ARXIVID": "2501.07124",
        "COMMENT": "The paper details the training of a 65B open-source LLM and provides transparency into large-scale model training. While it is significant for the community, it focuses on implementation details rather than foundational breakthroughs.",
        "RELEVANCE": 7,
        "NOVELTY": 6,
        "authors": [
            "Zhengzhong Liu",
            "Bowen Tan",
            "Hongyi Wang",
            "Willie Neiswanger",
            "Tianhua Tao",
            "Haonan Li",
            "Fajri Koto",
            "Yuqi Wang",
            "Suqi Sun",
            "Omkar Pangarkar",
            "Richard Fan",
            "Yi Gu",
            "Victor Miller",
            "Liqun Ma",
            "Liping Tang",
            "Nikhil Ranjan",
            "Yonghao Zhuang",
            "Guowei He",
            "Renxi Wang",
            "Mingkai Deng",
            "Robin Algayres",
            "Yuanzhi Li",
            "Zhiqiang Shen",
            "Preslav Nakov",
            "Eric Xing"
        ],
        "title": "LLM360 K2: Building a 65B 360-Open-Source Large Language Model from Scratch",
        "abstract": "We detail the training of the LLM360 K2-65B model, scaling up our 360-degree OPEN SOURCE approach to the largest and most powerful models under project LLM360. While open-source LLMs continue to advance, the answer to \"How are the largest LLMs trained?\" remains unclear within the community. The implementation details for such high-capacity models are often protected due to business considerations associated with their high cost. This lack of transparency prevents LLM researchers from leveraging valuable insights from prior experience, e.g., \"What are the best practices for addressing loss spikes?\" The LLM360 K2 project addresses this gap by providing full transparency and access to resources accumulated during the training of LLMs at the largest scale. This report highlights key elements of the K2 project, including our first model, K2 DIAMOND, a 65 billion-parameter LLM that surpasses LLaMA-65B and rivals LLaMA2-70B, while requiring fewer FLOPs and tokens. We detail the implementation steps and present a longitudinal analysis of K2 DIAMOND's capabilities throughout its training process. We also outline ongoing projects such as TXT360, setting the stage for future models in the series. By offering previously unavailable resources, the K2 project also resonates with the 360-degree OPEN SOURCE principles of transparency, reproducibility, and accessibility, which we believe are vital in the era of resource-intensive AI research.",
        "arxiv_id": "2501.07124"
    }
}