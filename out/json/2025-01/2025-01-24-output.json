{
    "2501.13734": {
        "authors": [
            "Maria-Florina Balcan",
            "Anh Tuan Nguyen",
            "Dravyansh Sharma"
        ],
        "title": "Sample complexity of data-driven tuning of model hyperparameters in neural networks with structured parameter-dependent dual function",
        "abstract": "Modern machine learning algorithms, especially deep learning based techniques, typically involve careful hyperparameter tuning to achieve the best performance. Despite the surge of intense interest in practical techniques like Bayesian optimization and random search based approaches to automating this laborious and compute-intensive task, the fundamental learning theoretic complexity of tuning hyperparameters for deep neural networks is poorly understood. Inspired by this glaring gap, we initiate the formal study of hyperparameter tuning complexity in deep learning through a recently introduced data driven setting. We assume that we have a series of deep learning tasks, and we have to tune hyperparameters to do well on average over the distribution of tasks. A major difficulty is that the utility function as a function of the hyperparameter is very volatile and furthermore, it is given implicitly by an optimization problem over the model parameters. This is unlike previous work in data driven design, where one can typically explicitly model the algorithmic behavior as a function of the hyperparameters. To tackle this challenge, we introduce a new technique to characterize the discontinuities and oscillations of the utility function on any fixed problem instance as we vary the hyperparameter, our analysis relies on subtle concepts including tools from differential/algebraic geometry and constrained optimization. This can be used to show that the learning theoretic complexity of the corresponding family of utility functions is bounded. We instantiate our results and provide sample complexity bounds for concrete applications tuning a hyperparameter that interpolates neural activation functions and setting the kernel parameter in graph neural networks.",
        "arxiv_id": "2501.13734",
        "ARXIVID": "2501.13734",
        "COMMENT": "This paper addresses hyperparameter tuning complexity in deep neural networks and introduces new theoretical insights using tools like differential geometry. It aligns closely with foundational research in representation learning and theoretical aspects of neural network training.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2501.13126": {
        "authors": [
            "Xuemiao Zhang",
            "Liangyu Xu",
            "Feiyu Duan",
            "Yongwei Zhou",
            "Sirui Wang",
            "Jingang Wang",
            "Xunliang Cai"
        ],
        "title": "Preference Curriculum: LLMs Should Always Be Pretrained on Their Preferred Data",
        "abstract": "Current large language models (LLMs) generally utilize a consistent data distribution throughout the entire pretraining process. However, as the model's ability improves, it intuitively should be pretrained with differentiated data. To achieve it, we propose the Perplexity Difference based Preference Curriculum learning (PDPC) framework, which always perceives and uses the data preferred by LLMs to train and boost them. Firstly, we introduce the PD metric to measure the difference in how well strong and weak models fit the samples. Samples with high PD are more challenging for weak models to learn and are more suitable to be arranged in the later stage of pretraining. Secondly, we propose the PD preference function to approximate the model and predict the data preference of the LLM at any time, so as to complete the arrangement of the entire data offline and ensure continuous training without interruption. Experimental results on 1.3B and 3B models demonstrate that our PDPC significantly surpasses baselines. Notably, the 3B model achieved more substantial gains, with an increased average accuracy of over 4.1% across various benchmarks.",
        "arxiv_id": "2501.13126",
        "ARXIVID": "2501.13126",
        "COMMENT": "The paper introduces a curriculum learning strategy targeting LLM pretraining ('Preference Curriculum'). It aligns best with the 'Large Language Models' criterion, offering a novel training approach with potential foundational implications.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2501.13491": {
        "authors": [
            "Munachiso Nwadike",
            "Zangir Iklassov",
            "Toluwani Aremu",
            "Tatsuya Hiraoka",
            "Velibor Bojkovic",
            "Benjamin Heinzerling",
            "Hilal Alqaubeh",
            "Martin Tak\\'a\\v{c}",
            "Kentaro Inui"
        ],
        "title": "RECALL: Library-Like Behavior In Language Models is Enhanced by Self-Referencing Causal Cycles",
        "abstract": "We introduce the concept of the self-referencing causal cycle (abbreviated RECALL) - a mechanism that enables large language models (LLMs) to bypass the limitations of unidirectional causality, which underlies a phenomenon known as the reversal curse. When an LLM is prompted with sequential data, it often fails to recall preceding context. For example, when we ask an LLM to recall the line preceding \"O say does that star-spangled banner yet wave\" in the U.S. National Anthem, it often fails to correctly return \"Gave proof through the night that our flag was still there\" - this is due to the reversal curse. It occurs because language models such as ChatGPT and Llama generate text based on preceding tokens, requiring facts to be learned and reproduced in a consistent token order. While the reversal curse is often viewed as a limitation, we offer evidence of an alternative view: it is not always an obstacle in practice. We find that RECALL is driven by what we designate as cycle tokens - sequences that connect different parts of the training data, enabling recall of preceding tokens from succeeding ones. Through rigorous probabilistic formalization and controlled experiments, we demonstrate how the cycles they induce influence a model's ability to reproduce information. To facilitate reproducibility, we provide our code and experimental details at https://anonymous.4open.science/r/remember-B0B8/.",
        "arxiv_id": "2501.13491",
        "ARXIVID": "2501.13491",
        "COMMENT": "This paper proposes the concept of self-referencing causal cycles (RECALL) to tackle the reversal curse in LLMs. It aligns with the 'Large Language Models (LLMs)' criterion as it contributes theoretical insights into behavior and mechanisms of LLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2501.13428": {
        "authors": [
            "Bo Gao",
            "Michael W. Spratling"
        ],
        "title": "Softplus Attention with Re-weighting Boosts Length Extrapolation in Large Language Models",
        "abstract": "Large language models have achieved remarkable success in recent years, primarily due to the implementation of self-attention mechanisms. However, traditional Softmax attention suffers from numerical instability and reduced performance as the length of inference tokens increases. This paper addresses these issues by decomposing the Softmax operation into a non-linear transformation and the $l_1$-norm. We identify the latter as essential for maintaining model performance. By replacing the non-linear transformation with the Softplus activation function and introducing a dynamic length scale factor for different token lengths based on invariance entropy, we create a novel attention mechanism with performance better than conventional Softmax attention across various inference lengths. To further improve the length extrapolation ability of the proposed attention mechanism, we introduce a re-weighting mechanism that amplifies significant attention weights while diminishing weaker ones, enabling the model to concentrate more effectively on relevant tokens. When combined with our proposed attention mechanism, this approach demonstrates significant promise in managing longer sequences, maintaining nearly constant validation loss even at 16$\\times$ the training token length while ensuring numerical stability. Our code is available at: https://github.com/iminfine/freeatten.",
        "arxiv_id": "2501.13428",
        "ARXIVID": "2501.13428",
        "COMMENT": "This paper introduces a new attention mechanism based on Softplus activation and re-weighting for improving length extrapolation in large language models. It provides architectural innovation in transformers, specifically addressing scalability and numerical stability.",
        "RELEVANCE": 9,
        "NOVELTY": 7
    },
    "2501.13312": {
        "authors": [
            "Yiming Yang",
            "Xiaoyuan Cheng",
            "Daniel Giles",
            "Sibo Cheng",
            "Yi He",
            "Xiao Xue",
            "Boli Chen",
            "Yukun Hu"
        ],
        "title": "Tensor-Var: Variational Data Assimilation in Tensor Product Feature Space",
        "abstract": "Variational data assimilation estimates the dynamical system states by minimizing a cost function that fits the numerical models with observational data. The widely used method, four-dimensional variational assimilation (4D-Var), has two primary challenges: (1) computationally demanding for complex nonlinear systems and (2) relying on state-observation mappings, which are often not perfectly known. Deep learning (DL) has been used as a more expressive class of efficient model approximators to address these challenges. However, integrating such models into 4D-Var remains challenging due to their inherent nonlinearities and the lack of theoretical guarantees for consistency in assimilation results. In this paper, we propose \\textit{Tensor-Var} to address these challenges using kernel Conditional Mean Embedding (CME). Tensor-Var improves optimization efficiency by characterizing system dynamics and state-observation mappings as linear operators, leading to a convex cost function in the feature space. Furthermore, our method provides a new perspective to incorporate CME into 4D-Var, offering theoretical guarantees of consistent assimilation results between the original and feature spaces. To improve scalability, we propose a method to learn deep features (DFs) using neural networks within the Tensor-Var framework. Experiments on chaotic systems and global weather prediction with real-time observations show that Tensor-Var outperforms conventional and DL hybrid 4D-Var baselines in accuracy while achieving efficiency comparable to the static 3D-Var method.",
        "arxiv_id": "2501.13312",
        "ARXIVID": "2501.13312",
        "COMMENT": "The paper on Tensor-Var offers a novel use of kernel Conditional Mean Embedding (CME) and tensor feature space for data assimilation. It relates to representation learning through its focus on theoretical embedding and optimization in feature spaces, which could provide insights into training dynamics.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2501.13198": {
        "authors": [
            "Yichen Wu",
            "Hongming Piao",
            "Long-Kai Huang",
            "Renzhen Wang",
            "Wanhua Li",
            "Hanspeter Pfister",
            "Deyu Meng",
            "Kede Ma",
            "Ying Wei"
        ],
        "title": "S-LoRA: Scalable Low-Rank Adaptation for Class Incremental Learning",
        "abstract": "Continual Learning (CL) with foundation models has recently emerged as a promising approach to harnessing the power of pre-trained models for sequential tasks. Existing prompt-based methods generally use a gating mechanism to select relevant prompts aligned with the test query for further processing. However, the success of these methods largely depends on the precision of the gating mechanism, which becomes less scalable with additional computational overhead as tasks increases. To overcome these issues, we propose a Scalable Low-Rank Adaptation (S-LoRA) method for CL (in particular class incremental learning), which incrementally decouples the learning of the direction and magnitude of LoRA parameters. S-LoRA supports efficient inference by employing the last-stage trained model for direct testing without a gating process. Our theoretical and empirical analysis demonstrates that S-LoRA tends to follow a low-loss trajectory that converges to an overlapped low-loss region, resulting in an excellent stability-plasticity trade-off in CL. Furthermore, based on our findings, we develop variants of S-LoRA with further improved scalability. Extensive experiments across multiple CL benchmarks and various foundation models consistently validate the effectiveness of S-LoRA.",
        "arxiv_id": "2501.13198",
        "ARXIVID": "2501.13198",
        "COMMENT": "This paper proposes S-LoRA, a method involving low-rank adaptations for Class Incremental Learning. The focus on low-rank parameter adaptation links to the 'Model Compression' criterion, with moderate novelty but limited foundational breakthroughs.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2501.13456": {
        "authors": [
            "Taoran Fang",
            "Tianhong Gao",
            "Chunping Wang",
            "Yihao Shang",
            "Wei Chow",
            "Lei Chen",
            "Yang Yang"
        ],
        "title": "KAA: Kolmogorov-Arnold Attention for Enhancing Attentive Graph Neural Networks",
        "abstract": "Graph neural networks (GNNs) with attention mechanisms, often referred to as attentive GNNs, have emerged as a prominent paradigm in advanced GNN models in recent years. However, our understanding of the critical process of scoring neighbor nodes remains limited, leading to the underperformance of many existing attentive GNNs. In this paper, we unify the scoring functions of current attentive GNNs and propose Kolmogorov-Arnold Attention (KAA), which integrates the Kolmogorov-Arnold Network (KAN) architecture into the scoring process. KAA enhances the performance of scoring functions across the board and can be applied to nearly all existing attentive GNNs. To compare the expressive power of KAA with other scoring functions, we introduce Maximum Ranking Distance (MRD) to quantitatively estimate their upper bounds in ranking errors for node importance. Our analysis reveals that, under limited parameters and constraints on width and depth, both linear transformation-based and MLP-based scoring functions exhibit finite expressive power. In contrast, our proposed KAA, even with a single-layer KAN parameterized by zero-order B-spline functions, demonstrates nearly infinite expressive power. Extensive experiments on both node-level and graph-level tasks using various backbone models show that KAA-enhanced scoring functions consistently outperform their original counterparts, achieving performance improvements of over 20% in some cases.",
        "arxiv_id": "2501.13456",
        "ARXIVID": "2501.13456",
        "COMMENT": "The proposed Kolmogorov-Arnold Attention introduces a theoretically grounded improvement to attentive GNNs, offering meaningful insights into scoring functions and architecture-level innovation.",
        "RELEVANCE": 7,
        "NOVELTY": 8
    },
    "2501.13390": {
        "authors": [
            "Thang Duong",
            "Zhi Wang",
            "Chicheng Zhang"
        ],
        "title": "Beyond Task Diversity: Provable Representation Transfer for Sequential Multi-Task Linear Bandits",
        "abstract": "We study lifelong learning in linear bandits, where a learner interacts with a sequence of linear bandit tasks whose parameters lie in an $m$-dimensional subspace of $\\mathbb{R}^d$, thereby sharing a low-rank representation. Current literature typically assumes that the tasks are diverse, i.e., their parameters uniformly span the $m$-dimensional subspace. This assumption allows the low-rank representation to be learned before all tasks are revealed, which can be unrealistic in real-world applications. In this work, we present the first nontrivial result for sequential multi-task linear bandits without the task diversity assumption. We develop an algorithm that efficiently learns and transfers low-rank representations. When facing $N$ tasks, each played over $\\tau$ rounds, our algorithm achieves a regret guarantee of $\\tilde{O}\\big (Nm \\sqrt{\\tau} + N^{\\frac{2}{3}} \\tau^{\\frac{2}{3}} d m^{\\frac13} + Nd^2 + \\tau m d \\big)$ under the ellipsoid action set assumption. This result can significantly improve upon the baseline of $\\tilde{O} \\left (Nd \\sqrt{\\tau}\\right)$ that does not leverage the low-rank structure when the number of tasks $N$ is sufficiently large and $m \\ll d$. We also demonstrate empirically on synthetic data that our algorithm outperforms baseline algorithms, which rely on the task diversity assumption.",
        "arxiv_id": "2501.13390",
        "ARXIVID": "2501.13390",
        "COMMENT": "The paper focuses on low-rank representation learning in sequential multi-task settings, which aligns with the 'representation learning' and 'low-rank approaches' criteria.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2501.13598": {
        "authors": [
            "Younes Yousef",
            "Lukas Galke",
            "Ansgar Scherp"
        ],
        "title": "A Transformer-based Autoregressive Decoder Architecture for Hierarchical Text Classification",
        "abstract": "Recent approaches in hierarchical text classification (HTC) rely on the capabilities of a pre-trained transformer model and exploit the label semantics and a graph encoder for the label hierarchy. In this paper, we introduce an effective hierarchical text classifier RADAr (Transformer-based Autoregressive Decoder Architecture) that is based only on an off-the-shelf RoBERTa transformer to process the input and a custom autoregressive decoder with two decoder layers for generating the classification output. Thus, unlike existing approaches for HTC, the encoder of RADAr has no explicit encoding of the label hierarchy and the decoder solely relies on the label sequences of the samples observed during training. We demonstrate on three benchmark datasets that RADAr achieves results competitive to the state of the art with less training and inference time. Our model consistently performs better when organizing the label sequences from children to parents versus the inverse, as done in existing HTC approaches. Our experiments show that neither the label semantics nor an explicit graph encoder for the hierarchy is needed. This has strong practical implications for HTC as the architecture has fewer requirements and provides a speed-up by a factor of 2 at inference time. Moreover, training a separate decoder from scratch in conjunction with fine-tuning the encoder allows future researchers and practitioners to exchange the encoder part as new models arise. The source code is available at https://github.com/yousef-younes/RADAr.",
        "arxiv_id": "2501.13598",
        "ARXIVID": "2501.13598",
        "COMMENT": "The RADAr hierarchical text classification framework introduces a simplified transformer-based autoregressive decoder architecture. This aligns strongly with interests in model architectures by proposing an effective simplification with practical implications.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    }
}