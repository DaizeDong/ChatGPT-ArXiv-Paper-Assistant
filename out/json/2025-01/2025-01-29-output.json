{
    "2501.16396": {
        "SCORE": 18,
        "ARXIVID": "2501.16396",
        "COMMENT": "The paper introduces TopoLoss, a loss function promoting topographic organization in models, closely aligning with the criteria for new methodologies in representation learning. Additionally, the integration into leading architectures like ResNet and GPT-Neo addresses architectural analysis. It also offers insights into neural encoding and efficiency, which are essential for foundational research.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Mayukh Deb",
            "Mainak Deb",
            "N. Apurva Ratan Murty"
        ],
        "title": "TopoNets: High Performing Vision and Language Models with Brain-Like Topography",
        "abstract": "Neurons in the brain are organized such that nearby cells tend to share similar functions. AI models lack this organization, and past efforts to introduce topography have often led to trade-offs between topography and task performance. In this work, we present TopoLoss, a new loss function that promotes spatially organized topographic representations in AI models without significantly sacrificing task performance. TopoLoss is highly adaptable and can be seamlessly integrated into the training of leading model architectures. We validate our method on both vision (ResNet-18, ResNet-50, ViT) and language models (GPT-Neo-125M, NanoGPT), collectively TopoNets. TopoNets are the highest-performing supervised topographic models to date, exhibiting brain-like properties such as localized feature processing, lower dimensionality, and increased efficiency. TopoNets also predict responses in the brain and replicate the key topographic signatures observed in the brain's visual and language cortices. Together, this work establishes a robust and generalizable framework for integrating topography into leading model architectures, advancing the development of high-performing models that more closely emulate the computational strategies of the human brain.",
        "arxiv_id": "2501.16396"
    },
    "2501.16783": {
        "SCORE": 18,
        "ARXIVID": "2501.16783",
        "COMMENT": "This paper provides a theoretical approach to understanding biases in large language models via a stochastic dynamical framework, offering insights into LLM behavior which aligns with foundational research on LLM interpretability and dynamics.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Jack David Carson"
        ],
        "title": "A Stochastic Dynamical Theory of LLM Self-Adversariality: Modeling Severity Drift as a Critical Process",
        "abstract": "This paper introduces a continuous-time stochastic dynamical framework for understanding how large language models (LLMs) may self-amplify latent biases or toxicity through their own chain-of-thought reasoning. The model posits an instantaneous \"severity\" variable $x(t) \\in [0,1]$ evolving under a stochastic differential equation (SDE) with a drift term $\\mu(x)$ and diffusion $\\sigma(x)$. Crucially, such a process can be consistently analyzed via the Fokker--Planck approach if each incremental step behaves nearly Markovian in severity space. The analysis investigates critical phenomena, showing that certain parameter regimes create phase transitions from subcritical (self-correcting) to supercritical (runaway severity). The paper derives stationary distributions, first-passage times to harmful thresholds, and scaling laws near critical points. Finally, it highlights implications for agents and extended LLM reasoning models: in principle, these equations might serve as a basis for formal verification of whether a model remains stable or propagates bias over repeated inferences.",
        "arxiv_id": "2501.16783"
    },
    "2501.16650": {
        "SCORE": 17,
        "ARXIVID": "2501.16650",
        "COMMENT": "The paper offers a theoretically grounded method for analyzing weight similarity in Large Language Models (LLMs) via a novel index, which aligns with the criteria for foundational insights in LLMs and architectural analysis. The focus on clusters and functional specialization supports deeper interpretability and efficiency insights.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Zeping Min",
            "Xinshang Wang"
        ],
        "title": "DOCS: Quantifying Weight Similarity for Deeper Insights into Large Language Models",
        "abstract": "We introduce a novel index, the Distribution of Cosine Similarity (DOCS), for quantitatively assessing the similarity between weight matrices in Large Language Models (LLMs), aiming to facilitate the analysis of their complex architectures. Leveraging DOCS, our analysis uncovers intriguing patterns in the latest open-source LLMs: adjacent layers frequently exhibit high weight similarity and tend to form clusters, suggesting depth-wise functional specialization. Additionally, we prove that DOCS is theoretically effective in quantifying similarity for orthogonal matrices, a crucial aspect given the prevalence of orthogonal initializations in LLMs. This research contributes to a deeper understanding of LLM architecture and behavior, offering tools with potential implications for developing more efficient and interpretable models.",
        "arxiv_id": "2501.16650"
    },
    "2501.16753": {
        "SCORE": 17,
        "ARXIVID": "2501.16753",
        "COMMENT": "The paper addresses semantic dilution in transformer-based models for next-frame prediction, aligning with architecture insights and improvements, and introduces a semantic concentration mechanism, aligning with model architecture innovation.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Hy Nguyen",
            "Srikanth Thudumu",
            "Hung Du",
            "Rajesh Vasa",
            "Kon Mouzakis"
        ],
        "title": "Overcoming Semantic Dilution in Transformer-Based Next Frame Prediction",
        "abstract": "Next-frame prediction in videos is crucial for applications such as autonomous driving, object tracking, and motion prediction. The primary challenge in next-frame prediction lies in effectively capturing and processing both spatial and temporal information from previous video sequences. The transformer architecture, known for its prowess in handling sequence data, has made remarkable progress in this domain. However, transformer-based next-frame prediction models face notable issues: (a) The multi-head self-attention (MHSA) mechanism requires the input embedding to be split into $N$ chunks, where $N$ is the number of heads. Each segment captures only a fraction of the original embeddings information, which distorts the representation of the embedding in the latent space, resulting in a semantic dilution problem; (b) These models predict the embeddings of the next frames rather than the frames themselves, but the loss function based on the errors of the reconstructed frames, not the predicted embeddings -- this creates a discrepancy between the training objective and the model output. We propose a Semantic Concentration Multi-Head Self-Attention (SCMHSA) architecture, which effectively mitigates semantic dilution in transformer-based next-frame prediction. Additionally, we introduce a loss function that optimizes SCMHSA in the latent space, aligning the training objective more closely with the model output. Our method demonstrates superior performance compared to the original transformer-based predictors.",
        "arxiv_id": "2501.16753"
    },
    "2501.16975": {
        "SCORE": 17,
        "ARXIVID": "2501.16975",
        "COMMENT": "The paper investigates tokenization in LLMs and introduces a framework for scaling vocabularies, directly addressing foundational aspects of language model architecture and training efficiency by highlighting a new scaling law around tokenization.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Hongzhi Huang",
            "Defa Zhu",
            "Banggu Wu",
            "Yutao Zeng",
            "Ya Wang",
            "Qiyang Min",
            "Xun Zhou"
        ],
        "title": "Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling",
        "abstract": "Tokenization is a fundamental component of large language models (LLMs), yet its influence on model scaling and performance is not fully explored. In this paper, we introduce Over-Tokenized Transformers, a novel framework that decouples input and output vocabularies to improve language modeling performance. Specifically, our approach scales up input vocabularies to leverage multi-gram tokens. Through extensive experiments, we uncover a log-linear relationship between input vocabulary size and training loss, demonstrating that larger input vocabularies consistently enhance model performance, regardless of model size. Using a large input vocabulary, we achieve performance comparable to double-sized baselines with no additional cost. Our findings highlight the importance of tokenization in scaling laws and provide practical insight for tokenizer design, paving the way for more efficient and powerful LLMs.",
        "arxiv_id": "2501.16975"
    },
    "2501.16615": {
        "SCORE": 17,
        "ARXIVID": "2501.16615",
        "COMMENT": "The paper focuses on the inherent variability in features learned by sparse autoencoders, touching upon representation learning and sparsity, offering insights into how such models encode information.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Gon\\c{c}alo Paulo",
            "Nora Belrose"
        ],
        "title": "Sparse Autoencoders Trained on the Same Data Learn Different Features",
        "abstract": "Sparse autoencoders (SAEs) are a useful tool for uncovering human-interpretable features in the activations of large language models (LLMs). While some expect SAEs to find the true underlying features used by a model, our research shows that SAEs trained on the same model and data, differing only in the random seed used to initialize their weights, identify different sets of features. For example, in an SAE with 131K latents trained on a feedforward network in Llama 3 8B, only 30% of the features were shared across different seeds. We observed this phenomenon across multiple layers of three different LLMs, two datasets, and several SAE architectures. While ReLU SAEs trained with the L1 sparsity loss showed greater stability across seeds, SAEs using the state-of-the-art TopK activation function were more seed-dependent, even when controlling for the level of sparsity. Our results suggest that the set of features uncovered by an SAE should be viewed as a pragmatically useful decomposition of activation space, rather than an exhaustive and universal list of features \"truly used\" by the model.",
        "arxiv_id": "2501.16615"
    },
    "2501.16337": {
        "SCORE": 16,
        "ARXIVID": "2501.16337",
        "COMMENT": "The paper explores activation sparsity in recurrent LLMs to enhance energy efficiency, which aligns well with the model compression criterion, specifically focusing on sparsity for neuromorphic hardware efficiency.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Ivan Knunyants",
            "Maryam Tavakol",
            "Manolis Sifalakis",
            "Yingfu Xu",
            "Amirreza Yousefzadeh",
            "Guangzhi Tang"
        ],
        "title": "Explore Activation Sparsity in Recurrent LLMs for Energy-Efficient Neuromorphic Computing",
        "abstract": "The recent rise of Large Language Models (LLMs) has revolutionized the deep learning field. However, the desire to deploy LLMs on edge devices introduces energy efficiency and latency challenges. Recurrent LLM (R-LLM) architectures have proven effective in mitigating the quadratic complexity of self-attention, making them a potential paradigm for computing on-edge neuromorphic processors. In this work, we propose a low-cost, training-free algorithm to sparsify R-LLMs' activations to enhance energy efficiency on neuromorphic hardware. Our approach capitalizes on the inherent structure of these models, rendering them well-suited for energy-constrained environments. Although primarily designed for R-LLMs, this method can be generalized to other LLM architectures, such as transformers, as demonstrated on the OPT model, achieving comparable sparsity and efficiency improvements. Empirical studies illustrate that our method significantly reduces computational demands while maintaining competitive accuracy across multiple zero-shot learning benchmarks. Additionally, hardware simulations with the SENECA neuromorphic processor underscore notable energy savings and latency improvements. These results pave the way for low-power, real-time neuromorphic deployment of LLMs and demonstrate the feasibility of training-free on-chip adaptation using activation sparsity.",
        "arxiv_id": "2501.16337"
    },
    "2501.16986": {
        "SCORE": 15,
        "ARXIVID": "2501.16986",
        "COMMENT": "The paper introduces a novel conditional generative quantum eigensolver using an encoder-decoder Transformer, aligning with architecture innovation. It doesn't directly address foundational AI models but presents an interesting fusion of quantum and classical methods.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Shunya Minami",
            "Kouhei Nakaji",
            "Yohichi Suzuki",
            "Al\\'an Aspuru-Guzik",
            "Tadashi Kadowaki"
        ],
        "title": "Generative quantum combinatorial optimization by means of a novel conditional generative quantum eigensolver",
        "abstract": "Quantum computing is entering a transformative phase with the emergence of logical quantum processors, which hold the potential to tackle complex problems beyond classical capabilities. While significant progress has been made, applying quantum algorithms to real-world problems remains challenging. Hybrid quantum-classical techniques have been explored to bridge this gap, but they often face limitations in expressiveness, trainability, or scalability. In this work, we introduce conditional Generative Quantum Eigensolver (conditional-GQE), a context-aware quantum circuit generator powered by an encoder-decoder Transformer. Focusing on combinatorial optimization, we train our generator for solving problems with up to 10 qubits, exhibiting nearly perfect performance on new problems. By leveraging the high expressiveness and flexibility of classical generative models, along with an efficient preference-based training scheme, conditional-GQE provides a generalizable and scalable framework for quantum circuit generation. Our approach advances hybrid quantum-classical computing and contributes to accelerate the transition toward fault-tolerant quantum computing.",
        "arxiv_id": "2501.16986"
    },
    "2501.17088": {
        "SCORE": 14,
        "ARXIVID": "2501.17088",
        "COMMENT": "The work focuses on compressing Selective Structured State Space Models by identifying redundancies, which aligns with model compression through novel efficiency improvements. However, it lacks broader theoretical contributions or completely novel techniques, focusing primarily on application-specific pruning of an alternative architecture.",
        "RELEVANCE": 7,
        "NOVELTY": 7,
        "authors": [
            "J. Pablo Mu\\~noz",
            "Jinjie Yuan",
            "Nilesh Jain"
        ],
        "title": "Mamba-Shedder: Post-Transformer Compression for Efficient Selective Structured State Space Models",
        "abstract": "Large pre-trained models have achieved outstanding results in sequence modeling. The Transformer block and its attention mechanism have been the main drivers of the success of these models. Recently, alternative architectures, such as Selective Structured State Space Models (SSMs), have been proposed to address the inefficiencies of Transformers. This paper explores the compression of SSM-based models, particularly Mamba and its hybrids. We study the sensitivity of these models to the removal of selected components at different granularities to reduce the model size and computational overhead, thus improving their efficiency while maintaining accuracy. The proposed solutions, collectively referred to as Mamba-Shedder, achieve a speedup of up to 1.4x during inference, demonstrating that model efficiency can be improved by eliminating several redundancies with minimal impact on the overall model performance. The code is available at https://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning.",
        "arxiv_id": "2501.17088"
    },
    "2501.16817": {
        "SCORE": 13,
        "ARXIVID": "2501.16817",
        "COMMENT": "The paper uses independent component analysis in a novel neural architecture for energy disaggregation, focusing on representation learning by extracting features, making it relevant to core topics.",
        "RELEVANCE": 7,
        "NOVELTY": 6,
        "authors": [
            "Sahar Moghimian Hoosh",
            "Ilia Kamyshev",
            "Henni Ouerdane"
        ],
        "title": "Enhancing Non-Intrusive Load Monitoring with Features Extracted by Independent Component Analysis",
        "abstract": "In this paper, a novel neural network architecture is proposed to address the challenges in energy disaggregation algorithms. These challenges include the limited availability of data and the complexity of disaggregating a large number of appliances operating simultaneously. The proposed model utilizes independent component analysis as the backbone of the neural network and is evaluated using the F1-score for varying numbers of appliances working concurrently. Our results demonstrate that the model is less prone to overfitting, exhibits low complexity, and effectively decomposes signals with many individual components. Furthermore, we show that the proposed model outperforms existing algorithms when applied to real-world data.",
        "arxiv_id": "2501.16817"
    }
}