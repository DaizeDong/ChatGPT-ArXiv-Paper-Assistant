## Relevant Topics

1. Representation Learning
   - Relevant: Feature learning, sparse/contrastive learning, dictionary learning, or theoretical insights into how deep networks encode information.
   - Irrelevant: Application-only work using standard representation learning without innovative insights.

2. Model Architecture
   - Relevant: Mixture-of-Experts (MoE), Transformers, Conditional/Dynamic Networks, Autoencoders, and other foundational structures.
   - Irrelevant: Simply applying existing architectures to new tasks without structural/theoretical innovation.

3. Model Compression
   - Relevant: Sparsity, pruning, quantization, low-rank, KV cache, or theoretical/algorithmic innovations for efficiency, etc.
   - Irrelevant: Simply applying existing compression to new tasks.

4. Large Language Models (LLMs)
   - Relevant: Strong theoretical insights on LLM behavior, architecture/training breakthroughs (e.g., MoE).
   - Irrelevant: Domain-specific usage or small tweaks (e.g., instruction tuning), lack of theoretical advancement (e.g., benchmarks/datasets, inference tricks like RAG).

5. AI for Science
   - Relevant: Foundational research in molecule/protein modeling (e.g., new training paradigms, advanced generative methods, or theoretical perspectives), or major architecture-level innovation.
   - Irrelevant: Conventional, domain-limited applications lacking insights on the foundational side.

6. Emerging Trends
   - Relevant: Cutting-edge theoretical work challenging assumptions, or broad new paradigms/concepts in AI research.
   - Irrelevant: Trend-following or incremental extensions on existing methods.