{
    "2508.21353": {
        "SCORE": 17,
        "ARXIVID": "2508.21353",
        "COMMENT": "The paper introduces Adaptive Heavy Tailed Stochastic Gradient Descent (AHTSGD), which is relevant to representation learning as it provides insights into training dynamics and optimization in neural networks.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Bodu Gong",
            "Gustavo Enrique Batista",
            "Pierre Lafaye de Micheaux"
        ],
        "title": "Adaptive Heavy-Tailed Stochastic Gradient Descent",
        "abstract": "In the era of large-scale neural network models, optimization algorithms often struggle with generalization due to an overreliance on training loss. One key insight widely accepted in the machine learning community is the idea that wide basins (regions around a local minimum where the loss increases gradually) promote better generalization by offering greater stability to small changes in input data or model parameters. In contrast, sharp minima are typically more sensitive and less stable. Motivated by two key empirical observations - the inherent heavy-tailed distribution of gradient noise in stochastic gradient descent and the Edge of Stability phenomenon during neural network training, in which curvature grows before settling at a plateau, we introduce Adaptive Heavy Tailed Stochastic Gradient Descent (AHTSGD). The algorithm injects heavier-tailed noise into the optimizer during the early stages of training to enhance exploration and gradually transitions to lighter-tailed noise as sharpness stabilizes. By dynamically adapting to the sharpness of the loss landscape throughout training, AHTSGD promotes accelerated convergence to wide basins. AHTSGD is the first algorithm to adjust the nature of injected noise into an optimizer based on the Edge of Stability phenomenon. AHTSGD consistently outperforms SGD and other noise-based methods on benchmarks like MNIST and CIFAR-10, with marked gains on noisy datasets such as SVHN. It ultimately accelerates early training from poor initializations and improves generalization across clean and noisy settings, remaining robust to learning rate choices.",
        "arxiv_id": "2508.21353"
    },
    "2508.21324": {
        "SCORE": 17,
        "ARXIVID": "2508.21324",
        "COMMENT": "The paper discusses Distribution-Aware Feature Selection for Sparse Autoencoders (SAEs), which is relevant to representation learning and model compression as it involves sparse methods and feature selection.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Narmeen Oozeer",
            "Nirmalendu Prakash",
            "Michael Lan",
            "Alice Rigg",
            "Amirali Abdullah"
        ],
        "title": "Distribution-Aware Feature Selection for SAEs",
        "abstract": "Sparse autoencoders (SAEs) decompose neural activations into interpretable features. A widely adopted variant, the TopK SAE, reconstructs each token from its K most active latents. However, this approach is inefficient, as some tokens carry more information than others. BatchTopK addresses this limitation by selecting top activations across a batch of tokens. This improves average reconstruction but risks an \"activation lottery,\" where rare high-magnitude features crowd out more informative but lower-magnitude ones. To address this issue, we introduce Sampled-SAE: we score the columns (representing features) of the batch activation matrix (via $L_2$ norm or entropy), forming a candidate pool of size $Kl$, and then apply Top-$K$ to select tokens across the batch from the restricted pool of features. Varying $l$ traces a spectrum between batch-level and token-specific selection. At $l=1$, tokens draw only from $K$ globally influential features, while larger $l$ expands the pool toward standard BatchTopK and more token-specific features across the batch. Small $l$ thus enforces global consistency; large $l$ favors fine-grained reconstruction. On Pythia-160M, no single value optimizes $l$ across all metrics: the best choice depends on the trade-off between shared structure, reconstruction fidelity, and downstream performance. Sampled-SAE thus reframes BatchTopK as a tunable, distribution-aware family.",
        "arxiv_id": "2508.21324"
    },
    "2508.21466": {
        "SCORE": 17,
        "ARXIVID": "2508.21466",
        "COMMENT": "The paper extends the concept of Normalized Maximum Likelihood to Riemannian manifolds, which is a significant theoretical contribution to model selection and representation learning in non-Euclidean spaces.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Kota Fukuzawa",
            "Atsushi Suzuki",
            "Kenji Yamanishi"
        ],
        "title": "Normalized Maximum Likelihood Code-Length on Riemannian Manifold Data Spaces",
        "abstract": "In recent years, with the large-scale expansion of graph data, there has been an increased focus on Riemannian manifold data spaces other than Euclidean space. In particular, the development of hyperbolic spaces has been remarkable, and they have high expressive power for graph data with hierarchical structures. Normalized Maximum Likelihood (NML) is employed in regret minimization and model selection. However, existing formulations of NML have been developed primarily in Euclidean spaces and are inherently dependent on the choice of coordinate systems, making it non-trivial to extend NML to Riemannian manifolds. In this study, we define a new NML that reflects the geometric structure of Riemannian manifolds, called the Riemannian manifold NML (Rm-NML). This Rm-NML is invariant under coordinate transformations and coincides with the conventional NML under the natural parameterization in Euclidean space. We extend existing computational techniques for NML to the setting of Riemannian manifolds. Furthermore, we derive a method to simplify the computation of Rm-NML on Riemannian symmetric spaces, which encompass data spaces of growing interest such as hyperbolic spaces. To illustrate the practical application of our proposed method, we explicitly computed the Rm-NML for normal distributions on hyperbolic spaces.",
        "arxiv_id": "2508.21466"
    },
    "2508.21566": {
        "SCORE": 17,
        "ARXIVID": "2508.21566",
        "COMMENT": "The paper introduces a novel method for spiking neural networks (SNNs) focusing on nonlinear synaptic pruning and dendritic integration, which aligns with model compression and representation learning through sparsity and pruning.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Wuque Cai",
            "Hongze Sun",
            "Jiayi He",
            "Qianqian Liao",
            "Yunliang Zang",
            "Duo Chen",
            "Dezhong Yao",
            "Daqing Guo"
        ],
        "title": "NSPDI-SNN: An efficient lightweight SNN based on nonlinear synaptic pruning and dendritic integration",
        "abstract": "Spiking neural networks (SNNs) are artificial neural networks based on simulated biological neurons and have attracted much attention in recent artificial intelligence technology studies. The dendrites in biological neurons have efficient information processing ability and computational power; however, the neurons of SNNs rarely match the complex structure of the dendrites. Inspired by the nonlinear structure and highly sparse properties of neuronal dendrites, in this study, we propose an efficient, lightweight SNN method with nonlinear pruning and dendritic integration (NSPDI-SNN). In this method, we introduce nonlinear dendritic integration (NDI) to improve the representation of the spatiotemporal information of neurons. We implement heterogeneous state transition ratios of dendritic spines and construct a new and flexible nonlinear synaptic pruning (NSP) method to achieve the high sparsity of SNN. We conducted systematic experiments on three benchmark datasets (DVS128 Gesture, CIFAR10-DVS, and CIFAR10) and extended the evaluation to two complex tasks (speech recognition and reinforcement learning-based maze navigation task). Across all tasks, NSPDI-SNN consistently achieved high sparsity with minimal performance degradation. In particular, our method achieved the best experimental results on all three event stream datasets. Further analysis showed that NSPDI significantly improved the efficiency of synaptic information transfer as sparsity increased. In conclusion, our results indicate that the complex structure and nonlinear computation of neuronal dendrites provide a promising approach for developing efficient SNN methods.",
        "arxiv_id": "2508.21566"
    },
    "2508.21258": {
        "SCORE": 17,
        "ARXIVID": "2508.21258",
        "COMMENT": "The paper introduces Relevance Patching (RelP), a method improving mechanistic interpretability in neural networks, which is relevant to representation learning and theoretical insights into model behavior.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Farnoush Rezaei Jafari",
            "Oliver Eberle",
            "Ashkan Khakzar",
            "Neel Nanda"
        ],
        "title": "RelP: Faithful and Efficient Circuit Discovery via Relevance Patching",
        "abstract": "Activation patching is a standard method in mechanistic interpretability for localizing the components of a model responsible for specific behaviors, but it is computationally expensive to apply at scale. Attribution patching offers a faster, gradient-based approximation, yet suffers from noise and reduced reliability in deep, highly non-linear networks. In this work, we introduce Relevance Patching (RelP), which replaces the local gradients in attribution patching with propagation coefficients derived from Layer-wise Relevance Propagation (LRP). LRP propagates the network's output backward through the layers, redistributing relevance to lower-level components according to local propagation rules that ensure properties such as relevance conservation or improved signal-to-noise ratio. Like attribution patching, RelP requires only two forward passes and one backward pass, maintaining computational efficiency while improving faithfulness. We validate RelP across a range of models and tasks, showing that it more accurately approximates activation patching than standard attribution patching, particularly when analyzing residual stream and MLP outputs in the Indirect Object Identification (IOI) task. For instance, for MLP outputs in GPT-2 Large, attribution patching achieves a Pearson correlation of 0.006, whereas RelP reaches 0.956, highlighting the improvement offered by RelP. Additionally, we compare the faithfulness of sparse feature circuits identified by RelP and Integrated Gradients (IG), showing that RelP achieves comparable faithfulness without the extra computational cost associated with IG.",
        "arxiv_id": "2508.21258"
    },
    "2508.21769": {
        "SCORE": 17,
        "ARXIVID": "2508.21769",
        "COMMENT": "The paper focuses on enhancing domain awareness in foundational models like CLIP, which aligns with representation learning and foundational model insights. It introduces a novel approach to disentangle classification from domain-aware representations, which is a significant theoretical contribution.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Ha Min Son",
            "Zhe Zhao",
            "Shahbaz Rezaei",
            "Xin Liu"
        ],
        "title": "Domain Generalization in-the-Wild: Disentangling Classification from Domain-Aware Representations",
        "abstract": "Evaluating domain generalization (DG) for foundational models like CLIP is challenging, as web-scale pretraining data potentially covers many existing benchmarks. Consequently, current DG evaluation may neither be sufficiently challenging nor adequately test genuinely unseen data scenarios. To better assess the performance of CLIP on DG in-the-wild, a scenario where CLIP encounters challenging unseen data, we consider two approaches: (1) evaluating on 33 diverse datasets with quantified out-of-distribution (OOD) scores after fine-tuning CLIP on ImageNet, and (2) using unlearning to make CLIP `forget' some domains as an approximation. We observe that CLIP's performance deteriorates significantly on more OOD datasets. To address this, we present CLIP-DCA (Disentangling Classification from enhanced domain Aware representations). Our approach is motivated by the observation that while standard domain invariance losses aim to make representations domain-invariant, this can be harmful to foundation models by forcing the discarding of domain-aware representations beneficial for generalization. We instead hypothesize that enhancing domain awareness is a prerequisite for effective domain-invariant classification in foundation models. CLIP-DCA identifies and enhances domain awareness within CLIP's encoders using a separate domain head and synthetically generated diverse domain data. Simultaneously, it encourages domain-invariant classification through disentanglement from the domain features. CLIP-DCA shows significant improvements within this challenging evaluation compared to existing methods, particularly on datasets that are more OOD.",
        "arxiv_id": "2508.21769"
    },
    "2508.21793": {
        "SCORE": 16,
        "ARXIVID": "2508.21793",
        "COMMENT": "The paper presents MoE-Health, a Mixture of Experts framework for multimodal healthcare prediction, which is relevant to model architecture as it involves MoE and dynamic networks.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Xiaoyang Wang",
            "Christopher C. Yang"
        ],
        "title": "MoE-Health: A Mixture of Experts Framework for Robust Multimodal Healthcare Prediction",
        "abstract": "Healthcare systems generate diverse multimodal data, including Electronic Health Records (EHR), clinical notes, and medical images. Effectively leveraging this data for clinical prediction is challenging, particularly as real-world samples often present with varied or incomplete modalities. Existing approaches typically require complete modality data or rely on manual selection strategies, limiting their applicability in real-world clinical settings where data availability varies across patients and institutions. To address these limitations, we propose MoE-Health, a novel Mixture of Experts framework designed for robust multimodal fusion in healthcare prediction. MoE-Health architecture is specifically developed to handle samples with differing modalities and improve performance on critical clinical tasks. By leveraging specialized expert networks and a dynamic gating mechanism, our approach dynamically selects and combines relevant experts based on available data modalities, enabling flexible adaptation to varying data availability scenarios. We evaluate MoE-Health on the MIMIC-IV dataset across three critical clinical prediction tasks: in-hospital mortality prediction, long length of stay, and hospital readmission prediction. Experimental results demonstrate that MoE-Health achieves superior performance compared to existing multimodal fusion methods while maintaining robustness across different modality availability patterns. The framework effectively integrates multimodal information, offering improved predictive performance and robustness in handling heterogeneous and incomplete healthcare data, making it particularly suitable for deployment in diverse healthcare environments with heterogeneous data availability.",
        "arxiv_id": "2508.21793"
    },
    "2508.21255": {
        "SCORE": 16,
        "ARXIVID": "2508.21255",
        "COMMENT": "The paper introduces a new generative modeling framework using random weighted support points, which offers a compact representation of data. This aligns with representation learning and introduces a novel method that is interpretable and efficient, providing a theoretical contribution to generative modeling.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Peiqi Zhao",
            "Carlos E. Rodr\\'iguez",
            "Rams\\'es H. Mena",
            "Stephen G. Walker"
        ],
        "title": "Weighted Support Points from Random Measures: An Interpretable Alternative for Generative Modeling",
        "abstract": "Support points summarize a large dataset through a smaller set of representative points that can be used for data operations, such as Monte Carlo integration, without requiring access to the full dataset. In this sense, support points offer a compact yet informative representation of the original data. We build on this idea to introduce a generative modeling framework based on random weighted support points, where the randomness arises from a weighting scheme inspired by the Dirichlet process and the Bayesian bootstrap. The proposed method generates diverse and interpretable sample sets from a fixed dataset, without relying on probabilistic modeling assumptions or neural network architectures. We present the theoretical formulation of the method and develop an efficient optimization algorithm based on the Convex--Concave Procedure (CCP). Empirical results on the MNIST and CelebA-HQ datasets show that our approach produces high-quality and diverse outputs at a fraction of the computational cost of black-box alternatives such as Generative Adversarial Networks (GANs) or Denoising Diffusion Probabilistic Models (DDPMs). These results suggest that random weighted support points offer a principled, scalable, and interpretable alternative for generative modeling. A key feature is their ability to produce genuinely interpolative samples that preserve underlying data structure.",
        "arxiv_id": "2508.21255"
    },
    "2508.21148": {
        "SCORE": 15,
        "ARXIVID": "2508.21148",
        "COMMENT": "The survey on Scientific Large Language Models (Sci-LLMs) provides a comprehensive overview of foundational models in scientific research, focusing on data-centric challenges and evaluation protocols, which aligns with the interest in foundational research in LLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Ming Hu",
            "Chenglong Ma",
            "Wei Li",
            "Wanghan Xu",
            "Jiamin Wu",
            "Jucheng Hu",
            "Tianbin Li",
            "Guohang Zhuang",
            "Jiaqi Liu",
            "Yingzhou Lu",
            "Ying Chen",
            "Chaoyang Zhang",
            "Cheng Tan",
            "Jie Ying",
            "Guocheng Wu",
            "Shujian Gao",
            "Pengcheng Chen",
            "Jiashi Lin",
            "Haitao Wu",
            "Lulu Chen",
            "Fengxiang Wang",
            "Yuanyuan Zhang",
            "Xiangyu Zhao",
            "Feilong Tang",
            "Encheng Su",
            "Junzhi Ning",
            "Xinyao Liu",
            "Ye Du",
            "Changkai Ji",
            "Cheng Tang",
            "Huihui Xu",
            "Ziyang Chen",
            "Ziyan Huang",
            "Jiyao Liu",
            "Pengfei Jiang",
            "Yizhou Wang",
            "Chen Tang",
            "Jianyu Wu",
            "Yuchen Ren",
            "Siyuan Yan",
            "Zhonghua Wang",
            "Zhongxing Xu",
            "Shiyan Su",
            "Shangquan Sun",
            "Runkai Zhao",
            "Zhisheng Zhang",
            "Yu Liu",
            "Fudi Wang",
            "Yuanfeng Ji",
            "Yanzhou Su",
            "Hongming Shan",
            "Chunmei Feng",
            "Jiahao Xu",
            "Jiangtao Yan",
            "Wenhao Tang",
            "Diping Song",
            "Lihao Liu",
            "Yanyan Huang",
            "Lequan Yu",
            "Bin Fu",
            "Shujun Wang",
            "Xiaomeng Li",
            "Xiaowei Hu",
            "Yun Gu",
            "Ben Fei",
            "Zhongying Deng",
            "Benyou Wang",
            "Yuewen Cao",
            "Minjie Shen",
            "Haodong Duan",
            "Jie Xu",
            "Yirong Chen",
            "Fang Yan",
            "Hongxia Hao",
            "Jielan Li",
            "Jiajun Du",
            "Yanbo Wang",
            "Imran Razzak",
            "Chi Zhang",
            "Lijun Wu",
            "Conghui He",
            "Zhaohui Lu",
            "Jinhai Huang",
            "Yihao Liu",
            "Fenghua Ling",
            "Yuqiang Li",
            "Aoran Wang",
            "Qihao Zheng",
            "Nanqing Dong",
            "Tianfan Fu",
            "Dongzhan Zhou",
            "Yan Lu",
            "Wenlong Zhang",
            "Jin Ye",
            "Jianfei Cai",
            "Wanli Ouyang",
            "Yu Qiao",
            "Zongyuan Ge",
            "Shixiang Tang",
            "Junjun He",
            "Chunfeng Song",
            "Lei Bai",
            "Bowen Zhou"
        ],
        "title": "A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers",
        "abstract": "Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is represented, integrated, and applied in scientific research, yet their progress is shaped by the complex nature of scientific data. This survey presents a comprehensive, data-centric synthesis that reframes the development of Sci-LLMs as a co-evolution between models and their underlying data substrate. We formulate a unified taxonomy of scientific data and a hierarchical model of scientific knowledge, emphasizing the multimodal, cross-scale, and domain-specific challenges that differentiate scientific corpora from general natural language processing datasets. We systematically review recent Sci-LLMs, from general-purpose foundations to specialized models across diverse scientific disciplines, alongside an extensive analysis of over 270 pre-/post-training datasets, showing why Sci-LLMs pose distinct demands -- heterogeneous, multi-scale, uncertainty-laden corpora that require representations preserving domain invariance and enabling cross-modal reasoning. On evaluation, we examine over 190 benchmark datasets and trace a shift from static exams toward process- and discovery-oriented assessments with advanced evaluation protocols. These data-centric analyses highlight persistent issues in scientific data development and discuss emerging solutions involving semi-automated annotation pipelines and expert validation. Finally, we outline a paradigm shift toward closed-loop systems where autonomous agents based on Sci-LLMs actively experiment, validate, and contribute to a living, evolving knowledge base. Collectively, this work provides a roadmap for building trustworthy, continually evolving artificial intelligence (AI) systems that function as a true partner in accelerating scientific discovery.",
        "arxiv_id": "2508.21148"
    },
    "2508.21421": {
        "SCORE": 15,
        "ARXIVID": "2508.21421",
        "COMMENT": "The paper proposes a novel method for model merging that accounts for inter-layer dependencies, which is relevant to model architecture and efficiency. It introduces a new approach to mitigate internal covariate shift, showing substantial insights.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Pietro Buzzega",
            "Riccardo Salami",
            "Angelo Porrello",
            "Simone Calderara"
        ],
        "title": "Rethinking Layer-wise Model Merging through Chain of Merges",
        "abstract": "Fine-tuning pretrained models has become a standard pathway to achieve state-of-the-art performance across a wide range of domains, leading to a proliferation of task-specific model variants. As the number of such specialized modules in-creases, merging them into a unified model without retraining has become a critical challenge. Existing merging techniques often rely on interference heuristics,importance weighting, or activation matching while treating each layer independently, thereby failing to account for the inter-layer dependencies inherent in deep networks. This simplification leads to distributional mismatches, especially inactivation-based methods, when changes in early layers are not properly reflected in downstream ones. We identify these mismatches as a form of internal covariate shift, comparable to the phenomenon encountered in the initial phases of neural networks training. To address it, we propose Chain of Merges (CoM), a layer-wise merging procedure that updates activation statistics in an auto-regressive fashion, explicitly accounting for cross-layer interactions. CoM produces a coherent merged model through a series of conditionally optimal updates, effectively mitigating degradation caused by covariate shift. Experiments on standard bench-marks demonstrate that CoM achieves state-of-the-art performance.",
        "arxiv_id": "2508.21421"
    },
    "2508.21172": {
        "SCORE": 15,
        "ARXIVID": "2508.21172",
        "COMMENT": "The paper presents Deep Residual Echo State Networks, exploring residual orthogonal connections in untrained RNNs, which is relevant to model architecture innovations.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Matteo Pinna",
            "Andrea Ceni",
            "Claudio Gallicchio"
        ],
        "title": "Deep Residual Echo State Networks: exploring residual orthogonal connections in untrained Recurrent Neural Networks",
        "abstract": "Echo State Networks (ESNs) are a particular type of untrained Recurrent Neural Networks (RNNs) within the Reservoir Computing (RC) framework, popular for their fast and efficient learning. However, traditional ESNs often struggle with long-term information processing. In this paper, we introduce a novel class of deep untrained RNNs based on temporal residual connections, called Deep Residual Echo State Networks (DeepResESNs). We show that leveraging a hierarchy of untrained residual recurrent layers significantly boosts memory capacity and long-term temporal modeling. For the temporal residual connections, we consider different orthogonal configurations, including randomly generated and fixed-structure configurations, and we study their effect on network dynamics. A thorough mathematical analysis outlines necessary and sufficient conditions to ensure stable dynamics within DeepResESN. Our experiments on a variety of time series tasks showcase the advantages of the proposed approach over traditional shallow and deep RC.",
        "arxiv_id": "2508.21172"
    }
}