{
    "2501.19281": {
        "SCORE": 18,
        "ARXIVID": "2501.19281",
        "COMMENT": "The paper explores deep learning through a statistical physics lens, offering theoretical insights into generalization, finite-width behaviors, and feature learning. It aligns strongly with foundational topics such as representation learning and training dynamics, with potentially transformative insights into neural architectures.",
        "RELEVANCE": 9,
        "NOVELTY": 9,
        "authors": [
            "Sebastiano Ariosto"
        ],
        "title": "Statistical Physics of Deep Neural Networks: Generalization Capability, Beyond the Infinite Width, and Feature Learning",
        "abstract": "Deep Neural Networks (DNNs) excel at many tasks, often rivaling or surpassing human performance. Yet their internal processes remain elusive, frequently described as \"black boxes.\" While performance can be refined experimentally, achieving a fundamental grasp of their inner workings is still a challenge.   Statistical Mechanics has long tackled computational problems, and this thesis applies physics-based insights to understand DNNs via three complementary approaches.   First, by averaging over data, we derive an asymptotic bound on generalization that depends solely on the size of the last layer, rather than on the total number of parameters -- revealing how deep architectures process information differently across layers.   Second, adopting a data-dependent viewpoint, we explore a finite-width thermodynamic limit beyond the infinite-width regime. This leads to: (i) a closed-form expression for the generalization error in a finite-width one-hidden-layer network (regression task); (ii) an approximate partition function for deeper architectures; and (iii) a link between deep networks in this thermodynamic limit and Student's t-processes.   Finally, from a task-explicit perspective, we present a preliminary analysis of how DNNs interact with a controlled dataset, investigating whether they truly internalize its structure -- collapsing to the teacher -- or merely memorize it. By understanding when a network must learn data structure rather than just memorize, it sheds light on fostering meaningful internal representations.   In essence, this thesis leverages the synergy between Statistical Physics and Machine Learning to illuminate the inner behavior of DNNs.",
        "arxiv_id": "2501.19281"
    },
    "2501.18666": {
        "SCORE": 17,
        "ARXIVID": "2501.18666",
        "COMMENT": "The paper provides insights into how deep networks encode information, focusing on the development of internal structures in transformers during training. It aligns with the Representation Learning criteria and provides theoretical insights relevant to LLMs as it studies mechanisms resembling those seen in GPT-2.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Einar Urdshals",
            "Jasmina Urdshals"
        ],
        "title": "Structure Development in List-Sorting Transformers",
        "abstract": "We study how a one-layer attention-only transformer develops relevant structures while learning to sort lists of numbers. At the end of training, the model organizes its attention heads in two main modes that we refer to as vocabulary-splitting and copy-suppression. Both represent simpler modes than having multiple heads handle overlapping ranges of numbers. Interestingly, vocabulary-splitting is present regardless of whether we use weight decay, a common regularization technique thought to drive simplification, supporting the thesis that neural networks naturally prefer simpler solutions. We relate copy-suppression to a mechanism in GPT-2 and investigate its functional role in our model. Guided by insights from a developmental analysis of the model, we identify features in the training data that drive the model's final acquired solution. This provides a concrete example of how the training data shape the internal organization of transformers, paving the way for future studies that could help us better understand how LLMs develop their internal structures.",
        "arxiv_id": "2501.18666"
    },
    "2501.19399": {
        "SCORE": 17,
        "ARXIVID": "2501.19399",
        "COMMENT": "This paper introduces Scalable-Softmax (SSMax), a modification to the attention mechanism in Transformers, addressing issues with attention flattening in long contexts. It is highly relevant under Model Architecture as it proposes a significant improvement to attention mechanisms in foundational Transformer models.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Ken M. Nakanishi"
        ],
        "title": "Scalable-Softmax Is Superior for Attention",
        "abstract": "The maximum element of the vector output by the Softmax function approaches zero as the input vector size increases. Transformer-based language models rely on Softmax to compute attention scores, causing the attention distribution to flatten as the context size grows. This reduces the model's ability to prioritize key information effectively and potentially limits its length generalization. To address this problem, we propose Scalable-Softmax (SSMax), which replaces Softmax in scenarios where the input vector size varies. SSMax can be seamlessly integrated into existing Transformer-based architectures. Experimental results in language modeling show that models using SSMax not only achieve faster loss reduction during pretraining but also significantly improve performance in long contexts and key information retrieval. Furthermore, an analysis of attention scores reveals that SSMax enables the model to focus attention on key information even in long contexts. Additionally, although models that use SSMax from the beginning of pretraining achieve better length generalization, those that have already started pretraining can still gain some of this ability by replacing Softmax in the attention layers with SSMax, either during or after pretraining.",
        "arxiv_id": "2501.19399"
    },
    "2501.19183": {
        "SCORE": 17,
        "ARXIVID": "2501.19183",
        "COMMENT": "The paper argues for democratizing curvature matrix computations via linear operators, which is pertinent to understanding loss landscapes and second-order optimization, aligning with foundational research in deep network behavior and sparsity-related methods.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Felix Dangel",
            "Runa Eschenhagen",
            "Weronika Ormaniec",
            "Andres Fernandez",
            "Lukas Tatzel",
            "Agustinus Kristiadi"
        ],
        "title": "Position: Curvature Matrices Should Be Democratized via Linear Operators",
        "abstract": "Structured large matrices are prevalent in machine learning. A particularly important class is curvature matrices like the Hessian, which are central to understanding the loss landscape of neural nets (NNs), and enable second-order optimization, uncertainty quantification, model pruning, data attribution, and more. However, curvature computations can be challenging due to the complexity of automatic differentiation, and the variety and structural assumptions of curvature proxies, like sparsity and Kronecker factorization. In this position paper, we argue that linear operators -- an interface for performing matrix-vector products -- provide a general, scalable, and user-friendly abstraction to handle curvature matrices. To support this position, we developed $\\textit{curvlinops}$, a library that provides curvature matrices through a unified linear operator interface. We demonstrate with $\\textit{curvlinops}$ how this interface can hide complexity, simplify applications, be extensible and interoperable with other libraries, and scale to large NNs.",
        "arxiv_id": "2501.19183"
    },
    "2501.18863": {
        "SCORE": 16,
        "ARXIVID": "2501.18863",
        "COMMENT": "The paper introduces a theoretical analysis of the probability flow ODE sampler in diffusion generative models, focusing on its ability to exploit intrinsic low-dimensional structures. This aligns well with cutting-edge theoretical work and insights into representation learning.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Jiaqi Tang",
            "Yuling Yan"
        ],
        "title": "Adaptivity and Convergence of Probability Flow ODEs in Diffusion Generative Models",
        "abstract": "Score-based generative models, which transform noise into data by learning to reverse a diffusion process, have become a cornerstone of modern generative AI. This paper contributes to establishing theoretical guarantees for the probability flow ODE, a widely used diffusion-based sampler known for its practical efficiency. While a number of prior works address its general convergence theory, it remains unclear whether the probability flow ODE sampler can adapt to the low-dimensional structures commonly present in natural image data. We demonstrate that, with accurate score function estimation, the probability flow ODE sampler achieves a convergence rate of $O(k/T)$ in total variation distance (ignoring logarithmic factors), where $k$ is the intrinsic dimension of the target distribution and $T$ is the number of iterations. This dimension-free convergence rate improves upon existing results that scale with the typically much larger ambient dimension, highlighting the ability of the probability flow ODE sampler to exploit intrinsic low-dimensional structures in the target distribution for faster sampling.",
        "arxiv_id": "2501.18863"
    },
    "2501.19149": {
        "SCORE": 15,
        "ARXIVID": "2501.19149",
        "COMMENT": "The study examines the inductive bias of deep ResNets, specifically focusing on bottleneck ranks and the interplay of nuclear norm and rank minimization. This foundational research aligns with the Model Architecture criterion, offering insights into training dynamics.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Enric Boix-Adsera"
        ],
        "title": "On the inductive bias of infinite-depth ResNets and the bottleneck rank",
        "abstract": "We compute the minimum-norm weights of a deep linear ResNet, and find that the inductive bias of this architecture lies between minimizing nuclear norm and rank. This implies that, with appropriate hyperparameters, deep nonlinear ResNets have an inductive bias towards minimizing bottleneck rank.",
        "arxiv_id": "2501.19149"
    },
    "2501.18973": {
        "SCORE": 15,
        "ARXIVID": "2501.18973",
        "COMMENT": "The paper introduces GPO-VAE, a VAE variant incorporating explainability through GRN-aligned parameter optimization, addressing foundational challenges in representation learning by enhancing biological interpretability. Although domain-specific (biology), the emphasis on latent space structure and optimization aligns with representation learning criteria.",
        "RELEVANCE": 7,
        "NOVELTY": 8,
        "authors": [
            "Seungheun Baek",
            "Soyon Park",
            "Yan Ting Chok",
            "Mogan Gim",
            "Jaewoo Kang"
        ],
        "title": "GPO-VAE: Modeling Explainable Gene Perturbation Responses utilizing GRN-Aligned Parameter Optimization",
        "abstract": "Motivation: Predicting cellular responses to genetic perturbations is essential for understanding biological systems and developing targeted therapeutic strategies. While variational autoencoders (VAEs) have shown promise in modeling perturbation responses, their limited explainability poses a significant challenge, as the learned features often lack clear biological meaning. Nevertheless, model explainability is one of the most important aspects in the realm of biological AI. One of the most effective ways to achieve explainability is incorporating the concept of gene regulatory networks (GRNs) in designing deep learning models such as VAEs. GRNs elicit the underlying causal relationships between genes and are capable of explaining the transcriptional responses caused by genetic perturbation treatments. Results: We propose GPO-VAE, an explainable VAE enhanced by GRN-aligned Parameter Optimization that explicitly models gene regulatory networks in the latent space. Our key approach is to optimize the learnable parameters related to latent perturbation effects towards GRN-aligned explainability. Experimental results on perturbation prediction show our model achieves state-of-the-art performance in predicting transcriptional responses across multiple benchmark datasets. Furthermore, additional results on evaluating the GRN inference task reveal our model's ability to generate meaningful GRNs compared to other methods. According to qualitative analysis, GPO-VAE posseses the ability to construct biologically explainable GRNs that align with experimentally validated regulatory pathways. GPO-VAE is available at https://github.com/dmis-lab/GPO-VAE",
        "arxiv_id": "2501.18973"
    },
    "2501.18812": {
        "SCORE": 15,
        "ARXIVID": "2501.18812",
        "COMMENT": "The paper explores estimating the probability of sampling neural network behaviors in parameter space and ties this to minimum description length principles and inductive biases. This connects to insights about training dynamics and generalization in representation learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Adam Scherlis",
            "Nora Belrose"
        ],
        "title": "Estimating the Probability of Sampling a Trained Neural Network at Random",
        "abstract": "We present an algorithm for estimating the probability mass, under a Gaussian or uniform prior, of a region in neural network parameter space corresponding to a particular behavior, such as achieving test loss below some threshold. When the prior is uniform, this problem is equivalent to measuring the volume of a region. We show empirically and theoretically that existing algorithms for estimating volumes in parameter space underestimate the true volume by millions of orders of magnitude. We find that this error can be dramatically reduced, but not entirely eliminated, with an importance sampling method using gradient information that is already provided by popular optimizers. The negative logarithm of this probability can be interpreted as a measure of a network's information content, in accordance with minimum description length (MDL) principles and rate-distortion theory. As expected, this quantity increases during language model training. We also find that badly-generalizing behavioral regions are smaller, and therefore less likely to be sampled at random, demonstrating an inductive bias towards well-generalizing functions.",
        "arxiv_id": "2501.18812"
    },
    "2501.19105": {
        "SCORE": 14,
        "ARXIVID": "2501.19105",
        "COMMENT": "This paper introduces a generalized theoretical framework for performance gain in weak-to-strong generalization beyond squared loss, connecting to insights about model training dynamics and representation learning. While not a direct innovation in foundational methods, it extends theoretical understanding in relevant ways.",
        "RELEVANCE": 7,
        "NOVELTY": 7,
        "authors": [
            "Abhijeet Mulgund",
            "Chirag Pabbaraju"
        ],
        "title": "Relating Misfit to Gain in Weak-to-Strong Generalization Beyond the Squared Loss",
        "abstract": "The paradigm of weak-to-strong generalization constitutes the training of a strong AI model on data labeled by a weak AI model, with the goal that the strong model nevertheless outperforms its weak supervisor on the target task of interest. For the setting of real-valued regression with the squared loss, recent work quantitatively characterizes the gain in performance of the strong model over the weak model in terms of the misfit between the strong and weak model. We generalize such a characterization to learning tasks whose loss functions correspond to arbitrary Bregman divergences when the strong class is convex. This extends the misfit-based characterization of performance gain in weak-to-strong generalization to classification tasks, as the cross-entropy loss can be expressed in terms of a Bregman divergence. In most practical scenarios, however, the strong model class may not be convex. We therefore weaken this assumption and study weak-to-strong generalization for convex combinations of $k$ strong models in the strong class, in the concrete setting of classification. This allows us to obtain a similar misfit-based characterization of performance gain, upto an additional error term that vanishes as $k$ gets large. Our theoretical findings are supported by thorough experiments on synthetic as well as real-world datasets.",
        "arxiv_id": "2501.19105"
    }
}