{
    "2501.10054": {
        "authors": [
            "Gansen Hu",
            "Zhaoguo Wang",
            "Jinglin Wei",
            "Wei Huang",
            "Haibo Chen"
        ],
        "title": "Accelerating Large Language Models through Partially Linear Feed-Forward Network",
        "abstract": "Large language models (LLMs) demonstrate remarkable capabilities but face deployment challenges due to their massive parameter counts. While existing compression techniques like pruning can reduce model size, it leads to significant accuracy degradation under high compression ratios. We present a novel perspective inspired by constant folding in compiler optimization. Our approach enables parameter reduction by treating activation functions in LLMs as linear functions.   However, recent LLMs use complex non-linear activations like GELU that prevent direct application of this technique. We propose TARDIS, which enables optimization of LLMs with non-linear activations by partially approximating them with linear functions in frequently occurring input ranges. For outlier inputs, TARDIS employs an online predictor to dynamically fall back to original computations.   Our experiments demonstrate that TARDIS achieves 80% parameter reduction in feed-forward networks, while significantly outperforming state-of-the-art pruning methods Wanda and RIA with up to 65% higher accuracy. In practical deployments for a 7B model, TARDIS achieves 1.6x end-to-end inference speedup when integrated with the vLLM serving system, and 1.4x speedup with the widely adopted HuggingFace implementation, while incurring only a 10.9% accuracy trade-off.",
        "arxiv_id": "2501.10054",
        "ARXIVID": "2501.10054",
        "COMMENT": "The paper proposes TARDIS, a novel method for compressing feed-forward networks in LLMs by leveraging partial linear approximations, which ties closely to the model compression topic with innovative insights into efficiency improvements.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2501.09767": {
        "authors": [
            "Tuowei Wang",
            "Xingyu Chen",
            "Kun Li",
            "Ting Cao",
            "Ju Ren",
            "Yaoxue Zhang"
        ],
        "title": "LeMo: Enabling LEss Token Involvement for MOre Context Fine-tuning",
        "abstract": "The escalating demand for long-context applications has intensified the necessity of extending the LLM context windows. Despite recent fine-tuning approaches successfully expanding context lengths, their high memory footprints, especially for activations, present a critical practical limitation. Current parameter-efficient fine-tuning methods prioritize reducing parameter update overhead over addressing activation memory constraints. Similarly, existing sparsity mechanisms improve computational efficiency but overlook activation memory optimization due to the phenomenon of Shadowy Activation.   In this paper, we propose LeMo, the first LLM fine-tuning system that explores and exploits a new token-level sparsity mechanism inherent in long-context scenarios, termed Contextual Token Sparsity. LeMo minimizes redundant token involvement by assessing the informativeness of token embeddings while preserving model accuracy. Specifically, LeMo introduces three key techniques: (1) Token Elimination, dynamically identifying and excluding redundant tokens across varying inputs and layers. (2) Pattern Prediction, utilizing well-trained predictors to approximate token sparsity patterns with minimal overhead. (3) Kernel Optimization, employing permutation-free and segment-based strategies to boost system performance. We implement LeMo as an end-to-end fine-tuning system compatible with various LLM architectures and other optimization techniques. Comprehensive evaluations demonstrate that LeMo reduces memory consumption by up to 1.93x and achieves up to 1.36x speedups, outperforming state-of-the-art fine-tuning systems.",
        "arxiv_id": "2501.09767",
        "ARXIVID": "2501.09767",
        "COMMENT": "Proposes a fine-tuning system for LLMs addressing activation memory constraints using token-level sparsity. Relevant to the compression and efficiency domain of LLMs, and includes novel memory-related optimization techniques.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2501.09949": {
        "authors": [
            "J. Pablo Mu\\~noz",
            "Jinjie Yuan",
            "Nilesh Jain"
        ],
        "title": "MultiPruner: Balanced Structure Removal in Foundation Models",
        "abstract": "Recently, state-of-the-art approaches for pruning large pre-trained models (LPMs) have demonstrated that the training-free removal of non-critical residual blocks in Transformers is viable for reducing model size, achieving results that outperform previous training-free pruning approaches. Motivated by these findings, we extend BlockPruner (Zhong et al., 2024) and propose MultiPruner, a pruning approach that surpasses recent training-free pruning methods by adopting a multidimensional, iterative, fine-grained pruning strategy. In MultiPruner, multidimensional pruning reinstates the structural balance in block-pruned models by sequentially compressing along three dimensions: i) residual blocks, ii) channels of multilayer perceptrons (MLP), and iii) attention heads. This solution enhances zero-shot accuracy on downstream tasks compared to other techniques while improving model compression ratios, producing compressed models with fewer computing and memory requirements. Extensive experiments demonstrate the advantages of the proposed method across various large pre-trained models. The code and pruning configurations are available at https://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning.",
        "arxiv_id": "2501.09949",
        "ARXIVID": "2501.09949",
        "COMMENT": "This paper introduces MultiPruner, which enhances model compression strategies by adopting a multi-dimensional, balanced pruning approach. It directly targets model compression with structural and algorithmic innovation, aligning well with the core topics.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2501.09997": {
        "authors": [
            "Qiang Liu",
            "Xinlong Chen",
            "Yue Ding",
            "Shizhen Xu",
            "Shu Wu",
            "Liang Wang"
        ],
        "title": "Attention-guided Self-reflection for Zero-shot Hallucination Detection in Large Language Models",
        "abstract": "Hallucination has emerged as a significant barrier to the effective application of Large Language Models (LLMs). In this work, we introduce a novel Attention-Guided SElf-Reflection (AGSER) approach for zero-shot hallucination detection in LLMs. The AGSER method utilizes attention contributions to categorize the input query into attentive and non-attentive queries. Each query is then processed separately through the LLMs, allowing us to compute consistency scores between the generated responses and the original answer. The difference between the two consistency scores serves as a hallucination estimator. In addition to its efficacy in detecting hallucinations, AGSER notably reduces computational complexity, requiring only three passes through the LLM and utilizing two sets of tokens. We have conducted extensive experiments with four widely-used LLMs across three different hallucination benchmarks, demonstrating that our approach significantly outperforms existing methods in zero-shot hallucination detection.",
        "arxiv_id": "2501.09997",
        "ARXIVID": "2501.09997",
        "COMMENT": "The paper proposes a novel attention-guided self-reflection (AGSER) method for zero-shot hallucination detection in LLMs. It aligns with foundational insights into LLM behavior and efficiency, fitting well into topics like sparsity and innovative architectural features for error mitigation.",
        "RELEVANCE": 9,
        "NOVELTY": 8
    },
    "2501.09954": {
        "authors": [
            "Jamin Seo",
            "Akshat Ramachandran",
            "Yu-Chuan Chuang",
            "Anirudh Itagi",
            "Tushar Krishna"
        ],
        "title": "AIRCHITECT v2: Learning the Hardware Accelerator Design Space through Unified Representations",
        "abstract": "Design space exploration (DSE) plays a crucial role in enabling custom hardware architectures, particularly for emerging applications like AI, where optimized and specialized designs are essential. With the growing complexity of deep neural networks (DNNs) and the introduction of advanced foundational models (FMs), the design space for DNN accelerators is expanding at an exponential rate. Additionally, this space is highly non-uniform and non-convex, making it increasingly difficult to navigate and optimize. Traditional DSE techniques rely on search-based methods, which involve iterative sampling of the design space to find the optimal solution. However, this process is both time-consuming and often fails to converge to the global optima for such design spaces. Recently, AIrchitect v1, the first attempt to address the limitations of search-based techniques, transformed DSE into a constant-time classification problem using recommendation networks. In this work, we propose AIrchitect v2, a more accurate and generalizable learning-based DSE technique applicable to large-scale design spaces that overcomes the shortcomings of earlier approaches. Specifically, we devise an encoder-decoder transformer model that (a) encodes the complex design space into a uniform intermediate representation using contrastive learning and (b) leverages a novel unified representation blending the advantages of classification and regression to effectively explore the large DSE space without sacrificing accuracy. Experimental results evaluated on 10^5 real DNN workloads demonstrate that, on average, AIrchitect v2 outperforms existing techniques by 15% in identifying optimal design points. Furthermore, to demonstrate the generalizability of our method, we evaluate performance on unseen model workloads (LLMs) and attain a 1.7x improvement in inference latency on the identified hardware architecture.",
        "arxiv_id": "2501.09954",
        "ARXIVID": "2501.09954",
        "COMMENT": "AIrchitect v2 proposes a transformer-based approach for learning hardware design spaces, addressing scalability and efficiency in DNN accelerator optimization. This is relevant to model efficiency and emerging trends in foundational AI for hardware applications.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2501.09751": {
        "authors": [
            "Zekun Xi",
            "Wenbiao Yin",
            "Jizhan Fang",
            "Jialong Wu",
            "Runnan Fang",
            "Ningyu Zhang",
            "Jiang Yong",
            "Pengjun Xie",
            "Fei Huang",
            "Huajun Chen"
        ],
        "title": "OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking",
        "abstract": "Machine writing with large language models often relies on retrieval-augmented generation. However, these approaches remain confined within the boundaries of the model's predefined scope, limiting the generation of content with rich information. Specifically, vanilla-retrieved information tends to lack depth, utility, and suffers from redundancy, which negatively impacts the quality of generated articles, leading to shallow, repetitive, and unoriginal outputs. To address these issues, we propose OmniThink, a machine writing framework that emulates the human-like process of iterative expansion and reflection. The core idea behind OmniThink is to simulate the cognitive behavior of learners as they progressively deepen their knowledge of the topics. Experimental results demonstrate that OmniThink improves the knowledge density of generated articles without compromising metrics such as coherence and depth. Human evaluations and expert feedback further highlight the potential of OmniThink to address real-world challenges in the generation of long-form articles.",
        "arxiv_id": "2501.09751",
        "ARXIVID": "2501.09751",
        "COMMENT": "OmniThink proposes a framework for iterative knowledge expansion in LLMs, emulating human-like cognitive processes for long-form content generation. This aligns with LLM theoretical topics and introduces novel insights into enhancing knowledge density in outputs.",
        "RELEVANCE": 8,
        "NOVELTY": 8
    },
    "2501.09891": {
        "authors": [
            "Kuang-Huei Lee",
            "Ian Fischer",
            "Yueh-Hua Wu",
            "Dave Marwood",
            "Shumeet Baluja",
            "Dale Schuurmans",
            "Xinyun Chen"
        ],
        "title": "Evolving Deeper LLM Thinking",
        "abstract": "We explore an evolutionary search strategy for scaling inference time compute in Large Language Models. The proposed approach, Mind Evolution, uses a language model to generate, recombine and refine candidate responses. The proposed approach avoids the need to formalize the underlying inference problem whenever a solution evaluator is available. Controlling for inference cost, we find that Mind Evolution significantly outperforms other inference strategies such as Best-of-N and Sequential Revision in natural language planning tasks. In the TravelPlanner and Natural Plan benchmarks, Mind Evolution solves more than 98% of the problem instances using Gemini 1.5 Pro without the use of a formal solver.",
        "arxiv_id": "2501.09891",
        "ARXIVID": "2501.09891",
        "COMMENT": "The paper introduces 'Mind Evolution' for scaling inference time compute in LLMs. The evolutionary search strategy and problem-solving insights show considerable relevance to scaling and inference cost strategies in LLMs, though foundational breakthroughs are limited.",
        "RELEVANCE": 8,
        "NOVELTY": 7
    },
    "2501.09804": {
        "authors": [
            "Maxwell J. Yin",
            "Dingyi Jiang",
            "Yongbing Chen",
            "Boyu Wang",
            "Charles Ling"
        ],
        "title": "Enhancing Generalization in Chain of Thought Reasoning for Smaller Models",
        "abstract": "Chain-of-Thought (CoT) reasoning in smaller language models is a challenging natural language process problem yet highly desirable in many real-life applications. Existing CoT knowledge distillation methods often suffer from overly conservative memorization in smaller LLMs, leading to low generalization confidence. As fully preserving the CoT ability of teacher model is impossible, we hypothesize that adversarial CoT fine-tuning is crucial for developing smaller LLM with robust CoT generalization. To this end, we propose \\textit{PRompt-Assisted Domain-Adversarial fine-tuning} (PRADA), a principled fine-tuning framework that integrates diverse CoT domains. Specifically, PRADA pioneers two CoT improvements in smaller LLM: (1) Recovering the domain-invariant feature insight which typically lost during distillation with domain adversarial fine-tuning; (2) Enhancing the domain adaptability of CoT prompt engineering by employing domain-adversarial approaches. We theoretically demonstrate the effectiveness of our approach and empirically show that it significantly outperforms the state of the arts in a wide range of tasks. Moreover, our empirical findings reveal that the smaller LLM, when leveraging PRADA, aligns closely with domain knowledge, thereby improving the explainability of our approach.",
        "arxiv_id": "2501.09804",
        "ARXIVID": "2501.09804",
        "COMMENT": "Proposes PRADA, which focuses on enhancing chain-of-thought reasoning in smaller LLMs via adversarial finetuning. This has relevance in representation learning and LLM efficiency, though it is not a paradigm shift and mainly extends existing techniques.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2501.09766": {
        "authors": [
            "Yirong Zeng",
            "Xiao Ding",
            "Yuxian Wang",
            "Weiwen Liu",
            "Wu Ning",
            "Yutai Hou",
            "Xu Huang",
            "Bing Qin",
            "Ting Liu"
        ],
        "title": "Boosting Tool Use of Large Language Models via Iterative Reinforced Fine-Tuning",
        "abstract": "Augmenting large language models (LLMs) with external tools is a promising approach to enhance their capabilities. Effectively leveraging this potential for complex tasks hinges crucially on improving their ability to use tools. Synthesizing tool use data by simulating the real world is an effective approach. Nevertheless, our investigation reveals that training gains significantly decay as the scale of these data increases. The primary factor is the model's poor performance (a.k.a deficiency) in complex scenarios, which hinders learning from data using SFT. Driven by this objective, we propose an iterative reinforced fine-tuning strategy to continually guide the model to alleviate it. Specifically, we first identify deficiency-related data based on feedback from the policy model, then perform a Monte Carlo Tree Search to collect fine-grained preference pairs to pinpoint deficiencies. Subsequently, we update the policy model using preference optimization to align with ground truth and misalign with deficiencies. This process can be iterated. Moreover, before the iteration, we propose an easy-to-hard warm-up SFT strategy to facilitate learning from challenging data. The experiments demonstrate our models go beyond the same parametric models, outperforming many larger open-source and closed-source models. Additionally, it has achieved notable training gains in complex tool use scenarios.",
        "arxiv_id": "2501.09766",
        "ARXIVID": "2501.09766",
        "COMMENT": "The paper discusses iterative reinforced fine-tuning to address deficiencies in complex tool-use scenarios for LLMs. It aligns somewhat with the Large Language Models (LLMs) topic, focusing on training advancements like iterative fine-tuning but lacks foundational or architectural breakthroughs.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    },
    "2501.10187": {
        "authors": [
            "Burcu Canakci",
            "Junyi Liu",
            "Xingbo Wu",
            "Nathana\\\"el Cheriere",
            "Paolo Costa",
            "Sergey Legtchenko",
            "Dushyanth Narayanan",
            "Ant Rowstron"
        ],
        "title": "Good things come in small packages: Should we adopt Lite-GPUs in AI infrastructure?",
        "abstract": "To match the blooming demand of generative AI workloads, GPU designers have so far been trying to pack more and more compute and memory into single complex and expensive packages. However, there is growing uncertainty about the scalability of individual GPUs and thus AI clusters, as state-of-the-art GPUs are already displaying packaging, yield, and cooling limitations. We propose to rethink the design and scaling of AI clusters through efficiently-connected large clusters of Lite-GPUs, GPUs with single, small dies and a fraction of the capabilities of larger GPUs. We think recent advances in co-packaged optics can be key in overcoming the communication challenges of distributing AI workloads onto more Lite-GPUs. In this paper, we present the key benefits of Lite-GPUs on manufacturing cost, blast radius, yield, and power efficiency; and discuss systems opportunities and challenges around resource, workload, memory, and network management.",
        "arxiv_id": "2501.10187",
        "ARXIVID": "2501.10187",
        "COMMENT": "The paper proposes the use of Lite-GPUs to address scalability and efficiency in AI clusters. This potentially links to model compression and scaling themes, which are relevant topics. However, the focus is more on hardware-level innovations rather than core algorithmic or architectural insights.",
        "RELEVANCE": 7,
        "NOVELTY": 7
    }
}