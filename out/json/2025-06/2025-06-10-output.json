{
    "2506.07751": {
        "COMMENT": "Author match",
        "SCORE": 20.0,
        "authors": [
            "Silin Gao",
            "Antoine Bosselut",
            "Samy Bengio",
            "Emmanuel Abbe"
        ],
        "title": "Augmenting LLMs' Reasoning by Reinforcing Abstract Thinking",
        "abstract": "Recent studies have shown that large language models (LLMs), especially smaller ones, often lack robustness in their reasoning. I.e., they tend to experience performance drops when faced with distribution shifts, such as changes to numerical or nominal variables, or insertions of distracting clauses. A possible strategy to address this involves generating synthetic data to further \"instantiate\" reasoning problems on potential variations. In contrast, our approach focuses on \"abstracting\" reasoning problems. This not only helps counteract distribution shifts but also facilitates the connection to symbolic tools for deriving solutions. We find that this abstraction process is better acquired through reinforcement learning (RL) than just supervised fine-tuning, which often fails to produce faithful abstractions. Our method, AbstraL -- which promotes abstract reasoning in LLMs using RL on granular abstraction data -- significantly mitigates performance degradation on recent GSM perturbation benchmarks.",
        "arxiv_id": "2506.07751"
    },
    "2506.06941": {
        "COMMENT": "Author match",
        "SCORE": 20.0,
        "authors": [
            "Parshin Shojaee",
            "Iman Mirzadeh",
            "Keivan Alizadeh",
            "Maxwell Horton",
            "Samy Bengio",
            "Mehrdad Farajtabar"
        ],
        "title": "The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity",
        "abstract": "Recent generations of language models have introduced Large Reasoning Models (LRMs) that generate detailed thinking processes before providing answers. While these models demonstrate improved performance on reasoning benchmarks, their fundamental capabilities, scaling properties, and limitations remain insufficiently understood. Current evaluations primarily focus on established math and coding benchmarks, emphasizing final answer accuracy. However, this evaluation paradigm often suffers from contamination and does not provide insights into the reasoning traces. In this work, we systematically investigate these gaps with the help of controllable puzzle environments that allow precise manipulation of complexity while maintaining consistent logical structures. This setup enables the analysis of not only final answers but also the internal reasoning traces, offering insights into how LRMs think. Through extensive experiments, we show that LRMs face a complete accuracy collapse beyond certain complexities. Moreover, they exhibit a counterintuitive scaling limit: their reasoning effort increases with problem complexity up to a point, then declines despite having remaining token budget. By comparing LRMs with their standard LLM counterparts under same inference compute, we identify three performance regimes: (1) low-complexity tasks where standard models outperform LRMs, (2) medium-complexity tasks where LRMs demonstrates advantage, and (3) high-complexity tasks where both models face complete collapse. We found that LRMs have limitations in exact computation: they fail to use explicit algorithms and reason inconsistently across scales. We also investigate the reasoning traces in more depth, studying the patterns of explored solutions and analyzing the models' computational behavior, shedding light on their strengths, limitations, and raising questions about their reasoning capabilities.",
        "arxiv_id": "2506.06941"
    },
    "2506.06489": {
        "SCORE": 19,
        "ARXIVID": "2506.06489",
        "COMMENT": "The paper introduces Alternating Gradient Flows, a framework for understanding feature learning dynamics in neural networks, which is highly relevant to representation learning.",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "Daniel Kunin",
            "Giovanni Luca Marchetti",
            "Feng Chen",
            "Dhruva Karkada",
            "James B. Simon",
            "Michael R. DeWeese",
            "Surya Ganguli",
            "Nina Miolane"
        ],
        "title": "Alternating Gradient Flows: A Theory of Feature Learning in Two-layer Neural Networks",
        "abstract": "What features neural networks learn, and how, remains an open question. In this paper, we introduce Alternating Gradient Flows (AGF), an algorithmic framework that describes the dynamics of feature learning in two-layer networks trained from small initialization. Prior works have shown that gradient flow in this regime exhibits a staircase-like loss curve, alternating between plateaus where neurons slowly align to useful directions and sharp drops where neurons rapidly grow in norm. AGF approximates this behavior as an alternating two-step process: maximizing a utility function over dormant neurons and minimizing a cost function over active ones. AGF begins with all neurons dormant. At each round, a dormant neuron activates, triggering the acquisition of a feature and a drop in the loss. AGF quantifies the order, timing, and magnitude of these drops, matching experiments across architectures. We show that AGF unifies and extends existing saddle-to-saddle analyses in fully connected linear networks and attention-only linear transformers, where the learned features are singular modes and principal components, respectively. In diagonal linear networks, we prove AGF converges to gradient flow in the limit of vanishing initialization. Applying AGF to quadratic networks trained to perform modular addition, we give the first complete characterization of the training dynamics, revealing that networks learn Fourier features in decreasing order of coefficient magnitude. Altogether, AGF offers a promising step towards understanding feature learning in neural networks.",
        "arxiv_id": "2506.06489"
    },
    "2506.07932": {
        "SCORE": 17,
        "ARXIVID": "2506.07932",
        "COMMENT": "The paper presents a novel framework for 3D data compression using pre-trained generative models, aligning with model compression and efficiency.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Rishit Dagli",
            "Yushi Guan",
            "Sankeerth Durvasula",
            "Mohammadreza Mofayezi",
            "Nandita Vijaykumar"
        ],
        "title": "Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural Compressor",
        "abstract": "We propose Squeeze3D, a novel framework that leverages implicit prior knowledge learnt by existing pre-trained 3D generative models to compress 3D data at extremely high compression ratios. Our approach bridges the latent spaces between a pre-trained encoder and a pre-trained generation model through trainable mapping networks. Any 3D model represented as a mesh, point cloud, or a radiance field is first encoded by the pre-trained encoder and then transformed (i.e. compressed) into a highly compact latent code. This latent code can effectively be used as an extremely compressed representation of the mesh or point cloud. A mapping network transforms the compressed latent code into the latent space of a powerful generative model, which is then conditioned to recreate the original 3D model (i.e. decompression). Squeeze3D is trained entirely on generated synthetic data and does not require any 3D datasets. The Squeeze3D architecture can be flexibly used with existing pre-trained 3D encoders and existing generative models. It can flexibly support different formats, including meshes, point clouds, and radiance fields. Our experiments demonstrate that Squeeze3D achieves compression ratios of up to 2187x for textured meshes, 55x for point clouds, and 619x for radiance fields while maintaining visual quality comparable to many existing methods. Squeeze3D only incurs a small compression and decompression latency since it does not involve training object-specific networks to compress an object.",
        "arxiv_id": "2506.07932"
    },
    "2506.07900": {
        "SCORE": 17,
        "ARXIVID": "2506.07900",
        "COMMENT": "The paper introduces an efficient LLM for end devices, focusing on model architecture and efficiency improvements.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "MiniCPM Team",
            "Chaojun Xiao",
            "Yuxuan Li",
            "Xu Han",
            "Yuzhuo Bai",
            "Jie Cai",
            "Haotian Chen",
            "Wentong Chen",
            "Xin Cong",
            "Ganqu Cui",
            "Ning Ding",
            "Shengdan Fan",
            "Yewei Fang",
            "Zixuan Fu",
            "Wenyu Guan",
            "Yitong Guan",
            "Junshao Guo",
            "Yufeng Han",
            "Bingxiang He",
            "Yuxiang Huang",
            "Cunliang Kong",
            "Qiuzuo Li",
            "Siyuan Li",
            "Wenhao Li",
            "Yanghao Li",
            "Yishan Li",
            "Zhen Li",
            "Dan Liu",
            "Biyuan Lin",
            "Yankai Lin",
            "Xiang Long",
            "Quanyu Lu",
            "Yaxi Lu",
            "Peiyan Luo",
            "Hongya Lyu",
            "Litu Ou",
            "Yinxu Pan",
            "Zekai Qu",
            "Qundong Shi",
            "Zijun Song",
            "Jiayuan Su",
            "Zhou Su",
            "Ao Sun",
            "Xianghui Sun",
            "Peijun Tang",
            "Fangzheng Wang",
            "Feng Wang",
            "Shuo Wang",
            "Yudong Wang",
            "Yesai Wu",
            "Zhenyu Xiao",
            "Jie Xie",
            "Zihao Xie",
            "Yukun Yan",
            "Jiarui Yuan",
            "Kaihuo Zhang",
            "Lei Zhang",
            "Linyue Zhang",
            "Xueren Zhang",
            "Yudi Zhang",
            "Hengyu Zhao",
            "Weilin Zhao",
            "Weilun Zhao",
            "Yuanqian Zhao",
            "Zhi Zheng",
            "Ge Zhou",
            "Jie Zhou",
            "Wei Zhou",
            "Zihan Zhou",
            "Zixuan Zhou",
            "Zhiyuan Liu",
            "Guoyang Zeng",
            "Chao Jia",
            "Dahai Li",
            "Maosong Sun"
        ],
        "title": "MiniCPM4: Ultra-Efficient LLMs on End Devices",
        "abstract": "This paper introduces MiniCPM4, a highly efficient large language model (LLM) designed explicitly for end-side devices. We achieve this efficiency through systematic innovation in four key dimensions: model architecture, training data, training algorithms, and inference systems. Specifically, in terms of model architecture, we propose InfLLM v2, a trainable sparse attention mechanism that accelerates both prefilling and decoding phases for long-context processing. Regarding training data, we propose UltraClean, an efficient and accurate pre-training data filtering and generation strategy, and UltraChat v2, a comprehensive supervised fine-tuning dataset. These datasets enable satisfactory model performance to be achieved using just 8 trillion training tokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient pre-training strategy search, and improve existing post-training methods by introducing chunk-wise rollout for load-balanced reinforcement learning and data-efficient tenary LLM, BitCPM. Regarding inference systems, we propose CPM.cu that integrates sparse attention, model quantization, and speculative sampling to achieve efficient prefilling and decoding. To meet diverse on-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B parameters, respectively. Sufficient evaluation results show that MiniCPM4 outperforms open-source models of similar size across multiple benchmarks, highlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B demonstrates significant speed improvements over Qwen3-8B when processing long sequences. Through further adaptation, MiniCPM4 successfully powers diverse applications, including trustworthy survey generation and tool use with model context protocol, clearly showcasing its broad usability.",
        "arxiv_id": "2506.07900"
    },
    "2506.07975": {
        "SCORE": 17,
        "ARXIVID": "2506.07975",
        "COMMENT": "The paper introduces a novel Lyapunov Spectrum-based metric for efficient pruning in recurrent neural networks, aligning with the model compression criterion.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Caleb Zheng",
            "Eli Shlizerman"
        ],
        "title": "Hyperpruning: Efficient Search through Pruned Variants of Recurrent Neural Networks Leveraging Lyapunov Spectrum",
        "abstract": "A variety of pruning methods have been introduced for over-parameterized Recurrent Neural Networks to improve efficiency in terms of power consumption and storage utilization. These advances motivate a new paradigm, termed `hyperpruning', which seeks to identify the most suitable pruning strategy for a given network architecture and application. Unlike conventional hyperparameter search, where the optimal configuration's accuracy remains uncertain, in the context of network pruning, the accuracy of the dense model sets the target for the accuracy of the pruned one. The goal, therefore, is to discover pruned variants that match or even surpass this established accuracy. However, exhaustive search over pruning configurations is computationally expensive and lacks early performance guarantees. To address this challenge, we propose a novel Lyapunov Spectrum (LS)-based distance metric that enables early comparison between pruned and dense networks, allowing accurate prediction of post-training performance. By integrating this LS-based distance with standard hyperparameter optimization algorithms, we introduce an efficient hyperpruning framework, termed LS-based Hyperpruning (LSH). LSH reduces search time by an order of magnitude compared to conventional approaches relying on full training. Experiments on stacked LSTM and RHN architectures using the Penn Treebank dataset, and on AWD-LSTM-MoS using WikiText-2, demonstrate that under fixed training budgets and target pruning ratios, LSH consistently identifies superior pruned models. Remarkably, these pruned variants not only outperform those selected by loss-based baseline but also exceed the performance of their dense counterpart.",
        "arxiv_id": "2506.07975"
    },
    "2506.07413": {
        "SCORE": 17,
        "ARXIVID": "2506.07413",
        "COMMENT": "The paper introduces a novel approach to supervised contrastive learning, which is relevant to representation learning by addressing limitations in embedding distribution and generalization.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Ziwen Wang",
            "Jiajun Fan",
            "Thao Nguyen",
            "Heng Ji",
            "Ge Liu"
        ],
        "title": "Variational Supervised Contrastive Learning",
        "abstract": "Contrastive learning has proven to be highly efficient and adaptable in shaping representation spaces across diverse modalities by pulling similar samples together and pushing dissimilar ones apart. However, two key limitations persist: (1) Without explicit regulation of the embedding distribution, semantically related instances can inadvertently be pushed apart unless complementary signals guide pair selection, and (2) excessive reliance on large in-batch negatives and tailored augmentations hinders generalization. To address these limitations, we propose Variational Supervised Contrastive Learning (VarCon), which reformulates supervised contrastive learning as variational inference over latent class variables and maximizes a posterior-weighted evidence lower bound (ELBO) that replaces exhaustive pair-wise comparisons for efficient class-aware matching and grants fine-grained control over intra-class dispersion in the embedding space. Trained exclusively on image data, our experiments on CIFAR-10, CIFAR-100, ImageNet-100, and ImageNet-1K show that VarCon (1) achieves state-of-the-art performance for contrastive learning frameworks, reaching 79.36% Top-1 accuracy on ImageNet-1K and 78.29% on CIFAR-100 with a ResNet-50 encoder while converging in just 200 epochs; (2) yields substantially clearer decision boundaries and semantic organization in the embedding space, as evidenced by KNN classification, hierarchical clustering results, and transfer-learning assessments; and (3) demonstrates superior performance in few-shot learning than supervised baseline and superior robustness across various augmentation strategies.",
        "arxiv_id": "2506.07413"
    },
    "2506.06406": {
        "SCORE": 17,
        "ARXIVID": "2506.06406",
        "COMMENT": "The paper introduces SMAR, a routing strategy for MoE-based multimodal models, which is relevant to model architecture, specifically MoE.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Guoyang Xia",
            "Yifeng Ding",
            "Fengfa Li",
            "Lei Ren",
            "Chen Wei",
            "Fangxiang Feng",
            "Xiaojie Wang"
        ],
        "title": "SMAR: Soft Modality-Aware Routing Strategy for MoE-based Multimodal Large Language Models Preserving Language Capabilities",
        "abstract": "Mixture of Experts (MoE) architectures have become a key approach for scaling large language models, with growing interest in extending them to multimodal tasks. Existing methods to build multimodal MoE models either incur high training costs or suffer from degraded language capabilities when adapting pretrained models. To address this, we propose Soft ModalityAware Routing (SMAR), a novel regularization technique that uses Kullback Leibler divergence to control routing probability distributions across modalities, encouraging expert specialization without modifying model architecture or heavily relying on textual data. Experiments on visual instruction tuning show that SMAR preserves language ability at 86.6% retention with only 2.5% pure text, outperforming baselines while maintaining strong multimodal performance. Our approach offers a practical and efficient solution to balance modality differentiation and language capabilities in multimodal MoE models.",
        "arxiv_id": "2506.06406"
    },
    "2506.07806": {
        "SCORE": 17,
        "ARXIVID": "2506.07806",
        "COMMENT": "The paper presents a novel approach to object representation learning, addressing spatial ambiguities with theoretical guarantees.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Avinash Kori",
            "Francesca Toni",
            "Ben Glocker"
        ],
        "title": "Identifiable Object Representations under Spatial Ambiguities",
        "abstract": "Modular object-centric representations are essential for *human-like reasoning* but are challenging to obtain under spatial ambiguities, *e.g. due to occlusions and view ambiguities*. However, addressing challenges presents both theoretical and practical difficulties. We introduce a novel multi-view probabilistic approach that aggregates view-specific slots to capture *invariant content* information while simultaneously learning disentangled global *viewpoint-level* information. Unlike prior single-view methods, our approach resolves spatial ambiguities, provides theoretical guarantees for identifiability, and requires *no viewpoint annotations*. Extensive experiments on standard benchmarks and novel complex datasets validate our method's robustness and scalability.",
        "arxiv_id": "2506.07806"
    },
    "2506.07919": {
        "SCORE": 17,
        "ARXIVID": "2506.07919",
        "COMMENT": "The paper investigates the role of nonlinearity in memory for recurrent networks, providing insights into representation learning.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Manuel Brenner",
            "Georgia Koppe"
        ],
        "title": "Uncovering the Functional Roles of Nonlinearity in Memory",
        "abstract": "Memory and long-range temporal processing are core requirements for sequence modeling tasks across natural language processing, time-series forecasting, speech recognition, and control. While nonlinear recurrence has long been viewed as essential for enabling such mechanisms, recent work suggests that linear dynamics may often suffice. In this study, we go beyond performance comparisons to systematically dissect the functional role of nonlinearity in recurrent networks--identifying both when it is computationally necessary, and what mechanisms it enables. We use Almost Linear Recurrent Neural Networks (AL-RNNs), which allow fine-grained control over nonlinearity, as both a flexible modeling tool and a probe into the internal mechanisms of memory. Across a range of classic sequence modeling tasks and a real-world stimulus selection task, we find that minimal nonlinearity is not only sufficient but often optimal, yielding models that are simpler, more robust, and more interpretable than their fully nonlinear or linear counterparts. Our results provide a principled framework for selectively introducing nonlinearity, bridging dynamical systems theory with the functional demands of long-range memory and structured computation in recurrent neural networks, with implications for both artificial and biological neural systems.",
        "arxiv_id": "2506.07919"
    },
    "2506.07334": {
        "SCORE": 17,
        "ARXIVID": "2506.07334",
        "COMMENT": "Graph-KV introduces structural biases into LLMs, which is relevant to model architecture and efficiency improvements.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Haoyu Wang",
            "Peihao Wang",
            "Mufei Li",
            "Shikun Liu",
            "Siqi Miao",
            "Zhangyang Wang",
            "Pan Li"
        ],
        "title": "Graph-KV: Breaking Sequence via Injecting Structural Biases into Large Language Models",
        "abstract": "Modern large language models (LLMs) are inherently auto-regressive, requiring input to be serialized into flat sequences regardless of their structural dependencies. This serialization hinders the model's ability to leverage structural inductive biases, especially in tasks such as retrieval-augmented generation (RAG) and reasoning on data with native graph structures, where inter-segment dependencies are crucial. We introduce Graph-KV with the potential to overcome this limitation. Graph-KV leverages the KV-cache of text segments as condensed representations and governs their interaction through structural inductive biases. In this framework, 'target' segments selectively attend only to the KV-caches of their designated 'source' segments, rather than all preceding segments in a serialized sequence. This approach induces a graph-structured block mask, sparsifying attention and enabling a message-passing-like step within the LLM. Furthermore, strategically allocated positional encodings for source and target segments reduce positional bias and context window consumption. We evaluate Graph-KV across three scenarios: (1) seven RAG benchmarks spanning direct inference, multi-hop reasoning, and long-document understanding; (2) Arxiv-QA, a novel academic paper QA task with full-text scientific papers structured as citation ego-graphs; and (3) paper topic classification within a citation network. By effectively reducing positional bias and harnessing structural inductive biases, Graph-KV substantially outperforms baselines, including standard costly sequential encoding, across various settings. Code and the Graph-KV data are publicly available.",
        "arxiv_id": "2506.07334"
    },
    "2506.06444": {
        "SCORE": 17,
        "ARXIVID": "2506.06444",
        "COMMENT": "The paper introduces a new inference scaling paradigm for LLM safety assurance, which is relevant to foundational research in LLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Ruizhong Qiu",
            "Gaotang Li",
            "Tianxin Wei",
            "Jingrui He",
            "Hanghang Tong"
        ],
        "title": "Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety Assurance",
        "abstract": "Existing safety assurance research has primarily focused on training-phase alignment to instill safe behaviors into LLMs. However, recent studies have exposed these methods' susceptibility to diverse jailbreak attacks. Concurrently, inference scaling has significantly advanced LLM reasoning capabilities but remains unexplored in the context of safety assurance. Addressing this gap, our work pioneers inference scaling for robust and effective LLM safety against emerging threats. We reveal that conventional inference scaling techniques, despite their success in reasoning tasks, perform poorly in safety contexts, even falling short of basic approaches like Best-of-N Sampling. We attribute this inefficiency to a newly identified challenge, the exploration--efficiency dilemma, arising from the high computational overhead associated with frequent process reward model (PRM) evaluations. To overcome this dilemma, we propose SAFFRON, a novel inference scaling paradigm tailored explicitly for safety assurance. Central to our approach is the introduction of a multifurcation reward model (MRM) that significantly reduces the required number of reward model evaluations. To operationalize this paradigm, we further propose: (i) a partial supervision training objective for MRM, (ii) a conservative exploration constraint to prevent out-of-distribution explorations, and (iii) a Trie-based key--value caching strategy that facilitates cache sharing across sequences during tree search. Extensive experiments validate the effectiveness of our method. Additionally, we publicly release our trained multifurcation reward model (Saffron-1) and the accompanying token-level safety reward dataset (Safety4M) to accelerate future research in LLM safety. Our code, model, and data are publicly available at https://github.com/q-rz/saffron , and our project homepage is at https://q-rz.github.io/p/saffron .",
        "arxiv_id": "2506.06444"
    },
    "2506.07661": {
        "SCORE": 17,
        "ARXIVID": "2506.07661",
        "COMMENT": "The paper provides a theoretical explanation for why over-parameterized models generalize well, aligning with emerging trends in understanding model behavior.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Meir Feder",
            "Ruediger Urbanke",
            "Yaniv Fogel"
        ],
        "title": "The Universality Lens: Why Even Highly Over-Parametrized Models Learn Well",
        "abstract": "A fundamental question in modern machine learning is why large, over-parameterized models, such as deep neural networks and transformers, tend to generalize well, even when their number of parameters far exceeds the number of training samples.   We investigate this phenomenon through the lens of information theory, grounded in universal learning theory. Specifically, we study a Bayesian mixture learner with log-loss and (almost) uniform prior over an expansive hypothesis class.   Our key result shows that the learner's regret is not determined by the overall size of the hypothesis class, but rather by the cumulative probability of all models that are close, in Kullback-Leibler divergence distance, to the true data-generating process. We refer to this cumulative probability as the weight of the hypothesis.   This leads to a natural notion of model simplicity: simple models are those with large weight and thus require fewer samples to generalize, while complex models have small weight and need more data. This perspective provides a rigorous and intuitive explanation for why over-parameterized models often avoid overfitting: the presence of simple hypotheses allows the posterior to concentrate on them when supported by the data.   We further bridge theory and practice by recalling that stochastic gradient descent with Langevin dynamics samples from the correct posterior distribution, enabling our theoretical learner to be approximated using standard machine learning methods combined with ensemble learning.   Our analysis yields non-uniform regret bounds and aligns with key practical concepts such as flat minima and model distillation. The results apply broadly across online, batch, and supervised learning settings, offering a unified and principled understanding of the generalization behavior of modern AI systems.",
        "arxiv_id": "2506.07661"
    },
    "2506.07691": {
        "SCORE": 17,
        "ARXIVID": "2506.07691",
        "COMMENT": "The paper focuses on sparse autoencoders for mechanistic interpretability in LLMs, aligning with representation learning and model architecture criteria.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Jiaming Li",
            "Haoran Ye",
            "Yukun Chen",
            "Xinyue Li",
            "Lei Zhang",
            "Hamid Alinejad-Rokny",
            "Jimmy Chih-Hsien Peng",
            "Min Yang"
        ],
        "title": "Training Superior Sparse Autoencoders for Instruct Models",
        "abstract": "As large language models (LLMs) grow in scale and capability, understanding their internal mechanisms becomes increasingly critical. Sparse autoencoders (SAEs) have emerged as a key tool in mechanistic interpretability, enabling the extraction of human-interpretable features from LLMs. However, existing SAE training methods are primarily designed for base models, resulting in reduced reconstruction quality and interpretability when applied to instruct models. To bridge this gap, we propose $\\underline{\\textbf{F}}$inetuning-$\\underline{\\textbf{a}}$ligned $\\underline{\\textbf{S}}$equential $\\underline{\\textbf{T}}$raining ($\\textit{FAST}$), a novel training method specifically tailored for instruct models. $\\textit{FAST}$ aligns the training process with the data distribution and activation patterns characteristic of instruct models, resulting in substantial improvements in both reconstruction and feature interpretability. On Qwen2.5-7B-Instruct, $\\textit{FAST}$ achieves a mean squared error of 0.6468 in token reconstruction, significantly outperforming baseline methods with errors of 5.1985 and 1.5096. In feature interpretability, $\\textit{FAST}$ yields a higher proportion of high-quality features, for Llama3.2-3B-Instruct, $21.1\\%$ scored in the top range, compared to $7.0\\%$ and $10.2\\%$ for $\\textit{BT(P)}$ and $\\textit{BT(F)}$. Surprisingly, we discover that intervening on the activations of special tokens via the SAEs leads to improvements in output quality, suggesting new opportunities for fine-grained control of model behavior. Code, data, and 240 trained SAEs are available at https://github.com/Geaming2002/FAST.",
        "arxiv_id": "2506.07691"
    },
    "2506.06644": {
        "SCORE": 17,
        "ARXIVID": "2506.06644",
        "COMMENT": "The paper introduces a novel transformer architecture with activation sparsity, aligning with model architecture and model compression criteria.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Chong You",
            "Kan Wu",
            "Zhipeng Jia",
            "Lin Chen",
            "Srinadh Bhojanapalli",
            "Jiaxian Guo",
            "Utku Evci",
            "Jan Wassenberg",
            "Praneeth Netrapalli",
            "Jeremiah J. Willcock",
            "Suvinay Subramanian",
            "Felix Chern",
            "Alek Andreev",
            "Shreya Pathak",
            "Felix Yu",
            "Prateek Jain",
            "David E. Culler",
            "Henry M. Levy",
            "Sanjiv Kumar"
        ],
        "title": "Spark Transformer: Reactivating Sparsity in FFN and Attention",
        "abstract": "The discovery of the lazy neuron phenomenon in trained Transformers, where the vast majority of neurons in their feed-forward networks (FFN) are inactive for each token, has spurred tremendous interests in activation sparsity for enhancing large model efficiency. While notable progress has been made in translating such sparsity to wall-time benefits, modern Transformers have moved away from the ReLU activation function crucial to this phenomenon. Existing efforts on re-introducing activation sparsity often degrade model quality, increase parameter count, complicate or slow down training. Sparse attention, the application of sparse activation to the attention mechanism, often faces similar challenges.   This paper introduces the Spark Transformer, a novel architecture that achieves a high level of activation sparsity in both FFN and the attention mechanism while maintaining model quality, parameter count, and standard training procedures. Our method realizes sparsity via top-k masking for explicit control over sparsity level. Crucially, we introduce statistical top-k, a hardware-accelerator-friendly, linear-time approximate algorithm that avoids costly sorting and mitigates significant training slowdown from standard top-$k$ operators. Furthermore, Spark Transformer reallocates existing FFN parameters and attention key embeddings to form a low-cost predictor for identifying activated entries. This design not only mitigates quality loss from enforced sparsity, but also enhances wall-time benefit. Pretrained with the Gemma-2 recipe, Spark Transformer demonstrates competitive performance on standard benchmarks while exhibiting significant sparsity: only 8% of FFN neurons are activated, and each token attends to a maximum of 256 tokens. This sparsity translates to a 2.5x reduction in FLOPs, leading to decoding wall-time speedups of up to 1.79x on CPU and 1.40x on GPU.",
        "arxiv_id": "2506.06644"
    },
    "2506.07956": {
        "SCORE": 16,
        "ARXIVID": "2506.07956",
        "COMMENT": "The paper addresses canonicality in token-level language models, which is relevant to foundational research in LLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Tim Vieira",
            "Tianyu Liu",
            "Clemente Pasti",
            "Yahya Emara",
            "Brian DuSell",
            "Benjamin LeBrun",
            "Mario Giulianelli",
            "Juan Luis Gastaldi",
            "Timothy J. O'Donnell",
            "Ryan Cotterell"
        ],
        "title": "Language Models over Canonical Byte-Pair Encodings",
        "abstract": "Modern language models represent probability distributions over character strings as distributions over (shorter) token strings derived via a deterministic tokenizer, such as byte-pair encoding. While this approach is highly effective at scaling up language models to large corpora, its current incarnations have a concerning property: the model assigns nonzero probability mass to an exponential number of $\\it{noncanonical}$ token encodings of each character string -- these are token strings that decode to valid character strings but are impossible under the deterministic tokenizer (i.e., they will never be seen in any training corpus, no matter how large). This misallocation is both erroneous, as noncanonical strings never appear in training data, and wasteful, diverting probability mass away from plausible outputs. These are avoidable mistakes! In this work, we propose methods to enforce canonicality in token-level language models, ensuring that only canonical token strings are assigned positive probability. We present two approaches: (1) canonicality by conditioning, leveraging test-time inference strategies without additional training, and (2) canonicality by construction, a model parameterization that guarantees canonical outputs but requires training. We demonstrate that fixing canonicality mistakes improves the likelihood of held-out data for several models and corpora.",
        "arxiv_id": "2506.07956"
    },
    "2506.07816": {
        "SCORE": 16,
        "ARXIVID": "2506.07816",
        "COMMENT": "The paper discusses a theoretical approach to improve sampling methods, which is relevant to foundational research in model efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Yingli Wang",
            "Changwei Tu",
            "Xiaoyu Wang",
            "Lingjiong Zhu"
        ],
        "title": "Accelerating Constrained Sampling: A Large Deviations Approach",
        "abstract": "The problem of sampling a target probability distribution on a constrained domain arises in many applications including machine learning. For constrained sampling, various Langevin algorithms such as projected Langevin Monte Carlo (PLMC) based on the discretization of reflected Langevin dynamics (RLD) and more generally skew-reflected non-reversible Langevin Monte Carlo (SRNLMC) based on the discretization of skew-reflected non-reversible Langevin dynamics (SRNLD) have been proposed and studied in the literature. This work focuses on the long-time behavior of SRNLD, where a skew-symmetric matrix is added to RLD. Although the non-asymptotic convergence analysis for SRNLD (and SRNLMC) and the acceleration compared to RLD (and PMLC) have been studied in the literature, it is not clear how one should design the skew-symmetric matrix in the dynamics to achieve good performance in practice. We establish a large deviation principle (LDP) for the empirical measure of SRNLD when the skew-symmetric matrix is chosen such that its product with the inward unit normal vector field on the boundary is zero. By explicitly characterizing the rate functions, we show that SRNLD can accelerate the convergence to the target distribution compared to RLD with this choice of the skew-symmetric matrix. Numerical experiments for SRNLMC based on the proposed skew-symmetric matrix show superior performance which validate the theoretical findings from the large deviations theory.",
        "arxiv_id": "2506.07816"
    },
    "2506.07106": {
        "SCORE": 16,
        "ARXIVID": "2506.07106",
        "COMMENT": "The paper introduces a multi-agent framework for reasoning in LLMs, which could provide insights into LLM behavior and interpretability.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Samir Abdaljalil",
            "Hasan Kurban",
            "Khalid Qaraqe",
            "Erchin Serpedin"
        ],
        "title": "Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models",
        "abstract": "Large language models (LLMs) have shown strong performance across natural language reasoning tasks, yet their reasoning processes remain brittle and difficult to interpret. Prompting techniques like Chain-of-Thought (CoT) enhance reliability by eliciting intermediate reasoning steps or aggregating multiple outputs. However, they lack mechanisms for enforcing logical structure and assessing internal coherence. We introduce Theorem-of-Thought (ToTh), a novel framework that models reasoning as collaboration among three parallel agents, each simulating a distinct mode of inference: abductive, deductive, and inductive. Each agent produces a reasoning trace, which is structured into a formal reasoning graph. To evaluate consistency, we apply Bayesian belief propagation guided by natural language inference (NLI), assigning confidence scores to each step. The most coherent graph is selected to derive the final answer. Experiments on symbolic (WebOfLies) and numerical (MultiArith) reasoning benchmarks show that ToTh consistently outperforms CoT, Self-Consistency, and CoT-Decoding across multiple LLMs, while producing interpretable and logically grounded reasoning chains. Our findings suggest a promising direction for building more robust and cognitively inspired LLM reasoning. The implementation is available at https://github.com/KurbanIntelligenceLab/theorem-of-thought.",
        "arxiv_id": "2506.07106"
    },
    "2506.07047": {
        "SCORE": 16,
        "ARXIVID": "2506.07047",
        "COMMENT": "The paper presents a novel approach to formal theorem proving from natural languages, which could be relevant to foundational research in LLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Yu Xuejun",
            "Jianyuan Zhong",
            "Zijin Feng",
            "Pengyi Zhai",
            "Roozbeh Yousefzadeh",
            "Wei Chong Ng",
            "Haoxiong Liu",
            "Ziyi Shou",
            "Jing Xiong",
            "Yudong Zhou",
            "Claudia Beth Ong",
            "Austen Jeremy Sugiarto",
            "Yaoxi Zhang",
            "Wai Ming Tai",
            "Huan Cao",
            "Dongcai Lu",
            "Jiacheng Sun",
            "Qiang Xu",
            "Shen Xin",
            "Zhenguo Li"
        ],
        "title": "Mathesis: Towards Formal Theorem Proving from Natural Languages",
        "abstract": "Recent advances in large language models show strong promise for formal reasoning. However, most LLM-based theorem provers have long been constrained by the need for expert-written formal statements as inputs, limiting their applicability to real-world problems expressed in natural language. We tackle this gap with Mathesis, the first end-to-end theorem proving pipeline processing informal problem statements. It contributes Mathesis-Autoformalizer, the first autoformalizer using reinforcement learning to enhance the formalization ability of natural language problems, aided by our novel LeanScorer framework for nuanced formalization quality assessment. It also proposes a Mathesis-Prover, which generates formal proofs from the formalized statements. To evaluate the real-world applicability of end-to-end formal theorem proving, we introduce Gaokao-Formal, a benchmark of 488 complex problems from China's national college entrance exam. Our approach is carefully designed, with a thorough study of each component. Experiments demonstrate Mathesis's effectiveness, with the autoformalizer outperforming the best baseline by 22% in pass-rate on Gaokao-Formal. The full system surpasses other model combinations, achieving 64% accuracy on MiniF2F with pass@32 and a state-of-the-art 18% on Gaokao-Formal.",
        "arxiv_id": "2506.07047"
    },
    "2506.07884": {
        "SCORE": 16,
        "ARXIVID": "2506.07884",
        "COMMENT": "The paper constructs Schauder bases using ReLU and other functions, which is relevant to representation learning and foundational research.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Anand Ganesh",
            "Babhrubahan Bose",
            "Anand Rajagopalan"
        ],
        "title": "Schauder Bases for $C[0, 1]$ Using ReLU, Softplus and Two Sigmoidal Functions",
        "abstract": "We construct four Schauder bases for the space $C[0,1]$, one using ReLU functions, another using Softplus functions, and two more using sigmoidal versions of the ReLU and Softplus functions. This establishes the existence of a basis using these functions for the first time, and improves on the universal approximation property associated with them.",
        "arxiv_id": "2506.07884"
    },
    "2506.06843": {
        "SCORE": 16,
        "ARXIVID": "2506.06843",
        "COMMENT": "The paper introduces a novel multi-agent framework for LLMs based on Cognitive Load Theory, which aligns with foundational research in LLM behavior and architecture.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "HaoYang Shang",
            "Xuan Liu",
            "Zi Liang",
            "Jie Zhang",
            "Haibo Hu",
            "Song Guo"
        ],
        "title": "United Minds or Isolated Agents? Exploring Coordination of LLMs under Cognitive Load Theory",
        "abstract": "Large Language Models (LLMs) exhibit a notable performance ceiling on complex, multi-faceted tasks, as they often fail to integrate diverse information or adhere to multiple constraints. We posit that such limitation arises when the demands of a task exceed the LLM's effective cognitive load capacity. This interpretation draws a strong analogy to Cognitive Load Theory (CLT) in cognitive science, which explains similar performance boundaries in the human mind, and is further supported by emerging evidence that reveals LLMs have bounded working memory characteristics. Building upon this CLT-grounded understanding, we introduce CoThinker, a novel LLM-based multi-agent framework designed to mitigate cognitive overload and enhance collaborative problem-solving abilities. CoThinker operationalizes CLT principles by distributing intrinsic cognitive load through agent specialization and managing transactional load via structured communication and a collective working memory. We empirically validate CoThinker on complex problem-solving tasks and fabricated high cognitive load scenarios, demonstrating improvements over existing multi-agent baselines in solution quality and efficiency. Our analysis reveals characteristic interaction patterns, providing insights into the emergence of collective cognition and effective load management, thus offering a principled approach to overcoming LLM performance ceilings.",
        "arxiv_id": "2506.06843"
    },
    "2506.07899": {
        "SCORE": 16,
        "ARXIVID": "2506.07899",
        "COMMENT": "The paper introduces a novel framework for lifelong model editing in LLMs, which aligns with foundational research in LLM behavior and architecture.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Ke Wang",
            "Yiming Qin",
            "Nikolaos Dimitriadis",
            "Alessandro Favero",
            "Pascal Frossard"
        ],
        "title": "MEMOIR: Lifelong Model Editing with Minimal Overwrite and Informed Retention for LLMs",
        "abstract": "Language models deployed in real-world systems often require post-hoc updates to incorporate new or corrected knowledge. However, editing such models efficiently and reliably - without retraining or forgetting previous information - remains a major challenge. Existing methods for lifelong model editing either compromise generalization, interfere with past edits, or fail to scale to long editing sequences. We propose MEMOIR, a novel scalable framework that injects knowledge through a residual memory, i.e., a dedicated parameter module, while preserving the core capabilities of the pre-trained model. By sparsifying input activations through sample-dependent masks, MEMOIR confines each edit to a distinct subset of the memory parameters, minimizing interference among edits. At inference, it identifies relevant edits by comparing the sparse activation patterns of new queries to those stored during editing. This enables generalization to rephrased queries by activating only the relevant knowledge while suppressing unnecessary memory activation for unrelated prompts. Experiments on question answering, hallucination correction, and out-of-distribution generalization benchmarks across LLaMA-3 and Mistral demonstrate that MEMOIR achieves state-of-the-art performance across reliability, generalization, and locality metrics, scaling to thousands of sequential edits with minimal forgetting.",
        "arxiv_id": "2506.07899"
    },
    "2506.06985": {
        "SCORE": 15,
        "ARXIVID": "2506.06985",
        "COMMENT": "The paper introduces a novel method for certified machine unlearning, which is a foundational research area related to model efficiency and privacy.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Anastasia Koloskova",
            "Youssef Allouah",
            "Animesh Jha",
            "Rachid Guerraoui",
            "Sanmi Koyejo"
        ],
        "title": "Certified Unlearning for Neural Networks",
        "abstract": "We address the problem of machine unlearning, where the goal is to remove the influence of specific training data from a model upon request, motivated by privacy concerns and regulatory requirements such as the \"right to be forgotten.\" Unfortunately, existing methods rely on restrictive assumptions or lack formal guarantees. To this end, we propose a novel method for certified machine unlearning, leveraging the connection between unlearning and privacy amplification by stochastic post-processing. Our method uses noisy fine-tuning on the retain data, i.e., data that does not need to be removed, to ensure provable unlearning guarantees. This approach requires no assumptions about the underlying loss function, making it broadly applicable across diverse settings. We analyze the theoretical trade-offs in efficiency and accuracy and demonstrate empirically that our method not only achieves formal unlearning guarantees but also performs effectively in practice, outperforming existing baselines. Our code is available at https://github.com/stair-lab/certified-unlearningneural-networks-icml-2025",
        "arxiv_id": "2506.06985"
    },
    "2506.07484": {
        "SCORE": 15,
        "ARXIVID": "2506.07484",
        "COMMENT": "The paper proposes a mixture model for context optimization, which is related to model architecture innovations.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Dasol Hong",
            "Wooju Lee",
            "Hyun Myung"
        ],
        "title": "CoCoA-Mix: Confusion-and-Confidence-Aware Mixture Model for Context Optimization",
        "abstract": "Prompt tuning, which adapts vision-language models by freezing model parameters and optimizing only the prompt, has proven effective for task-specific adaptations. The core challenge in prompt tuning is improving specialization for a specific task and generalization for unseen domains. However, frozen encoders often produce misaligned features, leading to confusion between classes and limiting specialization. To overcome this issue, we propose a confusion-aware loss (CoA-loss) that improves specialization by refining the decision boundaries between confusing classes. Additionally, we mathematically demonstrate that a mixture model can enhance generalization without compromising specialization. This is achieved using confidence-aware weights (CoA-weights), which adjust the weights of each prediction in the mixture model based on its confidence within the class domains. Extensive experiments show that CoCoA-Mix, a mixture model with CoA-loss and CoA-weights, outperforms state-of-the-art methods by enhancing specialization and generalization. Our code is publicly available at https://github.com/url-kaist/CoCoA-Mix.",
        "arxiv_id": "2506.07484"
    },
    "2506.07687": {
        "SCORE": 15,
        "ARXIVID": "2506.07687",
        "COMMENT": "The paper proposes a new gradient estimator for models with latent Gaussian variables, which is related to representation learning and training dynamics.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Kevin Lam",
            "Thang Bui",
            "George Deligiannidis",
            "Yee Whye Teh"
        ],
        "title": "Rao-Blackwellised Reparameterisation Gradients",
        "abstract": "Latent Gaussian variables have been popularised in probabilistic machine learning. In turn, gradient estimators are the machinery that facilitates gradient-based optimisation for models with latent Gaussian variables. The reparameterisation trick is often used as the default estimator as it is simple to implement and yields low-variance gradients for variational inference. In this work, we propose the R2-G2 estimator as the Rao-Blackwellisation of the reparameterisation gradient estimator. Interestingly, we show that the local reparameterisation gradient estimator for Bayesian MLPs is an instance of the R2-G2 estimator and Rao-Blackwellisation. This lets us extend benefits of Rao-Blackwellised gradients to a suite of probabilistic models. We show that initial training with R2-G2 consistently yields better performance in models with multiple applications of the reparameterisation trick.",
        "arxiv_id": "2506.07687"
    },
    "2506.06276": {
        "SCORE": 15,
        "ARXIVID": "2506.06276",
        "COMMENT": "The paper presents STARFlow, a scalable generative model using normalizing flows and Transformers, which aligns with the model architecture criterion.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Jiatao Gu",
            "Tianrong Chen",
            "David Berthelot",
            "Huangjie Zheng",
            "Yuyang Wang",
            "Ruixiang Zhang",
            "Laurent Dinh",
            "Miguel Angel Bautista",
            "Josh Susskind",
            "Shuangfei Zhai"
        ],
        "title": "STARFlow: Scaling Latent Normalizing Flows for High-resolution Image Synthesis",
        "abstract": "We present STARFlow, a scalable generative model based on normalizing flows that achieves strong performance in high-resolution image synthesis. The core of STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the expressive power of normalizing flows with the structured modeling capabilities of Autoregressive Transformers. We first establish the theoretical universality of TARFlow for modeling continuous distributions. Building on this foundation, we introduce several key architectural and algorithmic innovations to significantly enhance scalability: (1) a deep-shallow design, wherein a deep Transformer block captures most of the model representational capacity, complemented by a few shallow Transformer blocks that are computationally efficient yet substantially beneficial; (2) modeling in the latent space of pretrained autoencoders, which proves more effective than direct pixel-level modeling; and (3) a novel guidance algorithm that significantly boosts sample quality. Crucially, our model remains an end-to-end normalizing flow, enabling exact maximum likelihood training in continuous spaces without discretization. STARFlow achieves competitive performance in both class-conditional and text-conditional image generation tasks, approaching state-of-the-art diffusion models in sample quality. To our knowledge, this work is the first successful demonstration of normalizing flows operating effectively at this scale and resolution.",
        "arxiv_id": "2506.06276"
    },
    "2506.07011": {
        "SCORE": 15,
        "ARXIVID": "2506.07011",
        "COMMENT": "The paper advances the VAE framework for ICA, focusing on enhancing latent variable independence, aligning with representation learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Yuan-Hao Wei",
            "Yan-Jie Sun"
        ],
        "title": "Half-AVAE: Adversarial-Enhanced Factorized and Structured Encoder-Free VAE for Underdetermined Independent Component Analysis",
        "abstract": "This study advances the Variational Autoencoder (VAE) framework by addressing challenges in Independent Component Analysis (ICA) under both determined and underdetermined conditions, focusing on enhancing the independence and interpretability of latent variables. Traditional VAEs map observed data to latent variables and back via an encoder-decoder architecture, but struggle with underdetermined ICA where the number of latent variables exceeds observed signals. The proposed Half Adversarial VAE (Half-AVAE) builds on the encoder-free Half-VAE framework, eliminating explicit inverse mapping to tackle underdetermined scenarios. By integrating adversarial networks and External Enhancement (EE) terms, Half-AVAE promotes mutual independence among latent dimensions, achieving factorized and interpretable representations. Experiments with synthetic signals demonstrate that Half-AVAE outperforms baseline models, including GP-AVAE and Half-VAE, in recovering independent components under underdetermined conditions, as evidenced by lower root mean square errors. The study highlights the flexibility of VAEs in variational inference, showing that encoder omission, combined with adversarial training and structured priors, enables effective solutions for complex ICA tasks, advancing applications in disentanglement, causal inference, and generative modeling.",
        "arxiv_id": "2506.07011"
    },
    "2506.07406": {
        "SCORE": 15,
        "ARXIVID": "2506.07406",
        "COMMENT": "The paper introduces InverseScope for interpreting LLM activations, aligning with the large language models criterion.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Yifan Luo",
            "Zhennan Zhou",
            "Bin Dong"
        ],
        "title": "InverseScope: Scalable Activation Inversion for Interpreting Large Language Models",
        "abstract": "Understanding the internal representations of large language models (LLMs) is a central challenge in interpretability research. Existing feature interpretability methods often rely on strong assumptions about the structure of representations that may not hold in practice. In this work, we introduce InverseScope, an assumption-light and scalable framework for interpreting neural activations via input inversion. Given a target activation, we define a distribution over inputs that generate similar activations and analyze this distribution to infer the encoded features. To address the inefficiency of sampling in high-dimensional spaces, we propose a novel conditional generation architecture that significantly improves sample efficiency compared to previous methods. We further introduce a quantitative evaluation protocol that tests interpretability hypotheses using feature consistency rate computed over the sampled inputs. InverseScope scales inversion-based interpretability methods to larger models and practical tasks, enabling systematic and quantitative analysis of internal representations in real-world LLMs.",
        "arxiv_id": "2506.07406"
    },
    "2506.06485": {
        "SCORE": 15,
        "ARXIVID": "2506.06485",
        "COMMENT": "The paper provides insights into the behavior of large language models under knowledge conflict, which is relevant to understanding LLM behavior and interpretability.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Kaiser Sun",
            "Fan Bai",
            "Mark Dredze"
        ],
        "title": "What Is Seen Cannot Be Unseen: The Disruptive Effect of Knowledge Conflict on Large Language Models",
        "abstract": "Large language models frequently rely on both contextual input and parametric knowledge to perform tasks. However, these sources can come into conflict, especially when retrieved documents contradict the model's parametric knowledge. We propose a diagnostic framework to systematically evaluate LLM behavior under context-memory conflict, where the contextual information diverges from their parametric beliefs. We construct diagnostic data that elicit these conflicts and analyze model performance across multiple task types. Our findings reveal that (1) knowledge conflict has minimal impact on tasks that do not require knowledge utilization, (2) model performance is consistently higher when contextual and parametric knowledge are aligned, (3) models are unable to fully suppress their internal knowledge even when instructed, and (4) providing rationales that explain the conflict increases reliance on contexts. These insights raise concerns about the validity of model-based evaluation and underscore the need to account for knowledge conflict in the deployment of LLMs.",
        "arxiv_id": "2506.06485"
    },
    "2506.07998": {
        "SCORE": 15,
        "ARXIVID": "2506.07998",
        "COMMENT": "The paper examines generative modeling of neural network weights, which is relevant to representation learning and model architecture by exploring the synthesis of model weights.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Boya Zeng",
            "Yida Yin",
            "Zhiqiu Xu",
            "Zhuang Liu"
        ],
        "title": "Generative Modeling of Weights: Generalization or Memorization?",
        "abstract": "Generative models, with their success in image and video generation, have recently been explored for synthesizing effective neural network weights. These approaches take trained neural network checkpoints as training data, and aim to generate high-performing neural network weights during inference. In this work, we examine four representative methods on their ability to generate novel model weights, i.e., weights that are different from the checkpoints seen during training. Surprisingly, we find that these methods synthesize weights largely by memorization: they produce either replicas, or at best simple interpolations, of the training checkpoints. Current methods fail to outperform simple baselines, such as adding noise to the weights or taking a simple weight ensemble, in obtaining different and simultaneously high-performing models. We further show that this memorization cannot be effectively mitigated by modifying modeling factors commonly associated with memorization in image diffusion models, or applying data augmentations. Our findings provide a realistic assessment of what types of data current generative models can model, and highlight the need for more careful evaluation of generative models in new domains. Our code is available at https://github.com/boyazeng/weight_memorization.",
        "arxiv_id": "2506.07998"
    },
    "2506.06609": {
        "SCORE": 15,
        "ARXIVID": "2506.06609",
        "COMMENT": "The paper explores feature transfer across language models using model stitching, which is relevant to representation learning and model architecture.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Alan Chen",
            "Jack Merullo",
            "Alessandro Stolfo",
            "Ellie Pavlick"
        ],
        "title": "Transferring Features Across Language Models With Model Stitching",
        "abstract": "In this work, we demonstrate that affine mappings between residual streams of language models is a cheap way to effectively transfer represented features between models. We apply this technique to transfer the weights of Sparse Autoencoders (SAEs) between models of different sizes to compare their representations. We find that small and large models learn highly similar representation spaces, which motivates training expensive components like SAEs on a smaller model and transferring to a larger model at a FLOPs savings. For example, using a small-to-large transferred SAE as initialization can lead to 50% cheaper training runs when training SAEs on larger models. Next, we show that transferred probes and steering vectors can effectively recover ground truth performance. Finally, we dive deeper into feature-level transferability, finding that semantic and structural features transfer noticeably differently while specific classes of functional features have their roles faithfully mapped. Overall, our findings illustrate similarities and differences in the linear representation spaces of small and large models and demonstrate a method for improving the training efficiency of SAEs.",
        "arxiv_id": "2506.06609"
    },
    "2506.07364": {
        "SCORE": 15,
        "ARXIVID": "2506.07364",
        "COMMENT": "The paper proposes a method for unsupervised representation learning in multi-object images, which is relevant to representation learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Chengchao Shen",
            "Dawei Liu",
            "Jianxin Wang"
        ],
        "title": "Multiple Object Stitching for Unsupervised Representation Learning",
        "abstract": "Contrastive learning for single object centric images has achieved remarkable progress on unsupervised representation, but suffering inferior performance on the widespread images with multiple objects. In this paper, we propose a simple but effective method, Multiple Object Stitching (MOS), to refine the unsupervised representation for multi-object images. Specifically, we construct the multi-object images by stitching the single object centric ones, where the objects in the synthesized multi-object images are predetermined. Hence, compared to the existing contrastive methods, our method provides additional object correspondences between multi-object images without human annotations. In this manner, our method pays more attention to the representations of each object in multi-object image, thus providing more detailed representations for complicated downstream tasks, such as object detection and semantic segmentation. Experimental results on ImageNet, CIFAR and COCO datasets demonstrate that our proposed method achieves the leading unsupervised representation performance on both single object centric images and multi-object ones. The source code is available at https://github.com/visresearch/MultipleObjectStitching.",
        "arxiv_id": "2506.07364"
    },
    "2506.07735": {
        "SCORE": 15,
        "ARXIVID": "2506.07735",
        "COMMENT": "The paper explores neural architecture representation learning with a novel framework integrating language embedding and dynamic graph representation.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Haizhao Jing",
            "Haokui Zhang",
            "Zhenhao Shang",
            "Rong Xiao",
            "Peng Wang",
            "Yanning Zhang"
        ],
        "title": "Language Embedding Meets Dynamic Graph: A New Exploration for Neural Architecture Representation Learning",
        "abstract": "Neural Architecture Representation Learning aims to transform network models into feature representations for predicting network attributes, playing a crucial role in deploying and designing networks for real-world applications. Recently, inspired by the success of transformers, transformer-based models integrated with Graph Neural Networks (GNNs) have achieved significant progress in representation learning. However, current methods still have some limitations. First, existing methods overlook hardware attribute information, which conflicts with the current trend of diversified deep learning hardware and limits the practical applicability of models. Second, current encoding approaches rely on static adjacency matrices to represent topological structures, failing to capture the structural differences between computational nodes, which ultimately compromises encoding effectiveness. In this paper, we introduce LeDG-Former, an innovative framework that addresses these limitations through the synergistic integration of language-based semantic embedding and dynamic graph representation learning. Specifically, inspired by large language models (LLMs), we propose a language embedding framework where both neural architectures and hardware platform specifications are projected into a unified semantic space through tokenization and LLM processing, enabling zero-shot prediction across different hardware platforms for the first time. Then, we propose a dynamic graph-based transformer for modeling neural architectures, resulting in improved neural architecture modeling performance. On the NNLQP benchmark, LeDG-Former surpasses previous methods, establishing a new SOTA while demonstrating the first successful cross-hardware latency prediction capability. Furthermore, our framework achieves superior performance on the cell-structured NAS-Bench-101 and NAS-Bench-201 datasets.",
        "arxiv_id": "2506.07735"
    },
    "2506.07458": {
        "SCORE": 15,
        "ARXIVID": "2506.07458",
        "COMMENT": "The paper proposes a framework for characterizing LLM knowledge, which could provide theoretical insights into LLM behavior.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Yuxin Xiao",
            "Shan Chen",
            "Jack Gallifant",
            "Danielle Bitterman",
            "Thomas Hartvigsen",
            "Marzyeh Ghassemi"
        ],
        "title": "KScope: A Framework for Characterizing the Knowledge Status of Language Models",
        "abstract": "Characterizing a large language model's (LLM's) knowledge of a given question is challenging. As a result, prior work has primarily examined LLM behavior under knowledge conflicts, where the model's internal parametric memory contradicts information in the external context. However, this does not fully reflect how well the model knows the answer to the question. In this paper, we first introduce a taxonomy of five knowledge statuses based on the consistency and correctness of LLM knowledge modes. We then propose KScope, a hierarchical framework of statistical tests that progressively refines hypotheses about knowledge modes and characterizes LLM knowledge into one of these five statuses. We apply KScope to nine LLMs across four datasets and systematically establish: (1) Supporting context narrows knowledge gaps across models. (2) Context features related to difficulty, relevance, and familiarity drive successful knowledge updates. (3) LLMs exhibit similar feature preferences when partially correct or conflicted, but diverge sharply when consistently wrong. (4) Context summarization constrained by our feature analysis, together with enhanced credibility, further improves update effectiveness and generalizes across LLMs.",
        "arxiv_id": "2506.07458"
    },
    "2506.06656": {
        "SCORE": 15,
        "ARXIVID": "2506.06656",
        "COMMENT": "The paper introduces rescaled influence functions for data attribution, which is relevant to representation learning and training dynamics.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Ittai Rubinstein",
            "Samuel B. Hopkins"
        ],
        "title": "Rescaled Influence Functions: Accurate Data Attribution in High Dimension",
        "abstract": "How does the training data affect a model's behavior? This is the question we seek to answer with data attribution. The leading practical approaches to data attribution are based on influence functions (IF). IFs utilize a first-order Taylor approximation to efficiently predict the effect of removing a set of samples from the training set without retraining the model, and are used in a wide variety of machine learning applications. However, especially in the high-dimensional regime (# params $\\geq \\Omega($# samples$)$), they are often imprecise and tend to underestimate the effect of sample removals, even for simple models such as logistic regression. We present rescaled influence functions (RIF), a new tool for data attribution which can be used as a drop-in replacement for influence functions, with little computational overhead but significant improvement in accuracy. We compare IF and RIF on a range of real-world datasets, showing that RIFs offer significantly better predictions in practice, and present a theoretical analysis explaining this improvement. Finally, we present a simple class of data poisoning attacks that would fool IF-based detections but would be detected by RIF.",
        "arxiv_id": "2506.06656"
    },
    "2506.06412": {
        "SCORE": 15,
        "ARXIVID": "2506.06412",
        "COMMENT": "The paper introduces a framework for novel class discovery using implicit neural representation, which is relevant to representation learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Junming Wang",
            "Yi Shi"
        ],
        "title": "NeurNCD: Novel Class Discovery via Implicit Neural Representation",
        "abstract": "Discovering novel classes in open-world settings is crucial for real-world applications. Traditional explicit representations, such as object descriptors or 3D segmentation maps, are constrained by their discrete, hole-prone, and noisy nature, which hinders accurate novel class discovery. To address these challenges, we introduce NeurNCD, the first versatile and data-efficient framework for novel class discovery that employs the meticulously designed Embedding-NeRF model combined with KL divergence as a substitute for traditional explicit 3D segmentation maps to aggregate semantic embedding and entropy in visual embedding space. NeurNCD also integrates several key components, including feature query, feature modulation and clustering, facilitating efficient feature augmentation and information exchange between the pre-trained semantic segmentation network and implicit neural representations. As a result, our framework achieves superior segmentation performance in both open and closed-world settings without relying on densely labelled datasets for supervised training or human interaction to generate sparse label supervision. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art approaches on the NYUv2 and Replica datasets.",
        "arxiv_id": "2506.06412"
    },
    "2506.07060": {
        "SCORE": 15,
        "ARXIVID": "2506.07060",
        "COMMENT": "The paper discusses principles of natural intelligence that could inspire more efficient AI systems, aligning with emerging trends in AI efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Laura Cohen",
            "Xavier Hinaut",
            "Lilyana Petrova",
            "Alexandre Pitti",
            "Syd Reynal",
            "Ichiro Tsuda"
        ],
        "title": "Less is More: some Computational Principles based on Parcimony, and Limitations of Natural Intelligence",
        "abstract": "Natural intelligence (NI) consistently achieves more with less. Infants learn language, develop abstract concepts, and acquire sensorimotor skills from sparse data, all within tight neural and energy limits. In contrast, today's AI relies on virtually unlimited computational power, energy, and data to reach high performance. This paper argues that constraints in NI are paradoxically catalysts for efficiency, adaptability, and creativity. We first show how limited neural bandwidth promotes concise codes that still capture complex patterns. Spiking neurons, hierarchical structures, and symbolic-like representations emerge naturally from bandwidth constraints, enabling robust generalization. Next, we discuss chaotic itinerancy, illustrating how the brain transits among transient attractors to flexibly retrieve memories and manage uncertainty. We then highlight reservoir computing, where random projections facilitate rapid generalization from small datasets. Drawing on developmental perspectives, we emphasize how intrinsic motivation, along with responsive social environments, drives infant language learning and discovery of meaning. Such active, embodied processes are largely absent in current AI. Finally, we suggest that adopting 'less is more' principles -- energy constraints, parsimonious architectures, and real-world interaction -- can foster the emergence of more efficient, interpretable, and biologically grounded artificial systems.",
        "arxiv_id": "2506.07060"
    },
    "2506.07587": {
        "SCORE": 15,
        "ARXIVID": "2506.07587",
        "COMMENT": "The paper introduces a novel pruning approach for parameter-efficient fine-tuning of LLMs, relevant to model compression.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Tongzhou Yu",
            "Zhuhao Zhang",
            "Guanghui Zhu",
            "Shen Jiang",
            "Meikang Qiu",
            "Yihua Huang"
        ],
        "title": "PrunePEFT: Iterative Hybrid Pruning for Parameter-Efficient Fine-tuning of LLMs",
        "abstract": "Parameter Efficient Fine-Tuning (PEFT) methods have emerged as effective and promising approaches for fine-tuning pre-trained language models. Compared with Full parameter Fine-Tuning (FFT), PEFT achieved comparable task performance with a substantial reduction of trainable parameters, which largely saved the training and storage costs. However, using the PEFT method requires considering a vast design space, such as the type of PEFT modules and their insertion layers. Inadequate configurations can lead to sub-optimal results. Conventional solutions such as architectural search techniques, while effective, tend to introduce substantial additional overhead. In this paper, we propose a novel approach, PrunePEFT, which formulates the PEFT strategy search as a pruning problem and introduces a hybrid pruning strategy that capitalizes on the sensitivity of pruning methods to different PEFT modules. This method extends traditional pruning techniques by iteratively removing redundant or conflicting PEFT modules, thereby optimizing the fine-tuned configuration. By efficiently identifying the most relevant modules, our approach significantly reduces the computational burden typically associated with architectural search processes, making it a more scalable and efficient solution for fine-tuning large pre-trained models.",
        "arxiv_id": "2506.07587"
    },
    "2506.07621": {
        "SCORE": 15,
        "ARXIVID": "2506.07621",
        "COMMENT": "The paper proposes a low-rank adaptation method for LLMs, relevant to model compression and efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Harsh Bihany",
            "Shubham Patel",
            "Ashutosh Modi"
        ],
        "title": "LoRMA: Low-Rank Multiplicative Adaptation for LLMs",
        "abstract": "Large Language Models have shown remarkable capabilities in the NLP domain. Their effectiveness can mainly be attributed to their ability to adapt to an array of downstream tasks. However, generally, full fine-tuning is a computationally expensive job. To mitigate this, many techniques have been developed that prime efficiency, a prominent one being Low-Rank Adaptation (LoRA). However, LoRA and its variants employ re-parametrized additive updates. In this paper, we propose Low-Rank Multiplicative Adaptation (LoRMA), which shifts the paradigm of additive updates to a richer space of matrix multiplicative transformations. We tackle challenges such as computational complexity and rank bottleneck of matrix multiplication by effectively re-ordering operations and introducing rank inflation strategies. We conduct extensive experiments to demonstrate the effectiveness of our approach in terms of various evaluation metrics.",
        "arxiv_id": "2506.07621"
    },
    "2506.08010": {
        "SCORE": 15,
        "ARXIVID": "2506.08010",
        "COMMENT": "The paper proposes a training-free approach to improve Vision Transformers, relevant to model architecture.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Nick Jiang",
            "Amil Dravid",
            "Alexei Efros",
            "Yossi Gandelsman"
        ],
        "title": "Vision Transformers Don't Need Trained Registers",
        "abstract": "We investigate the mechanism underlying a previously identified phenomenon in Vision Transformers -- the emergence of high-norm tokens that lead to noisy attention maps. We observe that in multiple models (e.g., CLIP, DINOv2), a sparse set of neurons is responsible for concentrating high-norm activations on outlier tokens, leading to irregular attention patterns and degrading downstream visual processing. While the existing solution for removing these outliers involves retraining models from scratch with additional learned register tokens, we use our findings to create a training-free approach to mitigate these artifacts. By shifting the high-norm activations from our discovered register neurons into an additional untrained token, we can mimic the effect of register tokens on a model already trained without registers. We demonstrate that our method produces cleaner attention and feature maps, enhances performance over base models across multiple downstream visual tasks, and achieves results comparable to models explicitly trained with register tokens. We then extend test-time registers to off-the-shelf vision-language models to improve their interpretability. Our results suggest that test-time registers effectively take on the role of register tokens at test-time, offering a training-free solution for any pre-trained model released without them.",
        "arxiv_id": "2506.08010"
    },
    "2506.07500": {
        "SCORE": 15,
        "ARXIVID": "2506.07500",
        "COMMENT": "The paper addresses model compression by reducing the discretization gap in logic gate networks, aligning with the model compression criterion.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Shakir Yousefi",
            "Andreas Plesner",
            "Till Aczel",
            "Roger Wattenhofer"
        ],
        "title": "Mind the Gap: Removing the Discretization Gap in Differentiable Logic Gate Networks",
        "abstract": "Modern neural networks demonstrate state-of-the-art performance on numerous existing benchmarks; however, their high computational requirements and energy consumption prompt researchers to seek more efficient solutions for real-world deployment. Logic gate networks (LGNs) learns a large network of logic gates for efficient image classification. However, learning a network that can solve a simple problem like CIFAR-10 can take days to weeks to train. Even then, almost half of the network remains unused, causing a discretization gap. This discretization gap hinders real-world deployment of LGNs, as the performance drop between training and inference negatively impacts accuracy. We inject Gumbel noise with a straight-through estimator during training to significantly speed up training, improve neuron utilization, and decrease the discretization gap. We theoretically show that this results from implicit Hessian regularization, which improves the convergence properties of LGNs. We train networks $4.5 \\times$ faster in wall-clock time, reduce the discretization gap by $98\\%$, and reduce the number of unused gates by $100\\%$.",
        "arxiv_id": "2506.07500"
    },
    "2506.06832": {
        "SCORE": 15,
        "ARXIVID": "2506.06832",
        "COMMENT": "The paper discusses a novel framework for evaluating LLM capabilities, which aligns with the large language models criterion.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Cl\\'ement Hongler",
            "Andrew Emil"
        ],
        "title": "Cross-Entropy Games for Language Models: From Implicit Knowledge to General Capability Measures",
        "abstract": "Large Language Models (LLMs) define probability measures on text. By considering the implicit knowledge question of what it means for an LLM to know such a measure and what it entails algorithmically, we are naturally led to formulate a series of tasks that go beyond generative sampling, involving forms of summarization, counterfactual thinking, anomaly detection, originality search, reverse prompting, debating, creative solving, etc. These tasks can be formulated as games based on LLM measures, which we call Cross-Entropy (Xent) Games. Xent Games can be single-player or multi-player. They involve cross-entropy scores and cross-entropy constraints, and can be expressed as simple computational graphs and programs. We show the Xent Game space is large enough to contain a wealth of interesting examples, while being constructible from basic game-theoretic consistency axioms. We then discuss how the Xent Game space can be used to measure the abilities of LLMs. This leads to the construction of Xent Game measures: finite families of Xent Games that can be used as capability benchmarks, built from a given scope, by extracting a covering measure. To address the unbounded scope problem associated with the challenge of measuring general abilities, we propose to explore the space of Xent Games in a coherent fashion, using ideas inspired by evolutionary dynamics.",
        "arxiv_id": "2506.06832"
    },
    "2506.06682": {
        "SCORE": 15,
        "ARXIVID": "2506.06682",
        "COMMENT": "The paper introduces a novel framework for heterogeneous graph representation learning, aligning with representation learning criteria.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Di Lin",
            "Wanjing Ren",
            "Xuanbin Li",
            "Rui Zhang"
        ],
        "title": "Learning Robust Heterogeneous Graph Representations via Contrastive-Reconstruction under Sparse Semantics",
        "abstract": "In graph self-supervised learning, masked autoencoders (MAE) and contrastive learning (CL) are two prominent paradigms. MAE focuses on reconstructing masked elements, while CL maximizes similarity between augmented graph views. Recent studies highlight their complementarity: MAE excels at local feature capture, and CL at global information extraction. Hybrid frameworks for homogeneous graphs have been proposed, but face challenges in designing shared encoders to meet the semantic requirements of both tasks. In semantically sparse scenarios, CL struggles with view construction, and gradient imbalance between positive and negative samples persists. This paper introduces HetCRF, a novel dual-channel self-supervised learning framework for heterogeneous graphs. HetCRF uses a two-stage aggregation strategy to adapt embedding semantics, making it compatible with both MAE and CL. To address semantic sparsity, it enhances encoder output for view construction instead of relying on raw features, improving efficiency. Two positive sample augmentation strategies are also proposed to balance gradient contributions. Node classification experiments on four real-world heterogeneous graph datasets demonstrate that HetCRF outperforms state-of-the-art baselines. On datasets with missing node features, such as Aminer and Freebase, at a 40% label rate in node classification, HetCRF improves the Macro-F1 score by 2.75% and 2.2% respectively compared to the second-best baseline, validating its effectiveness and superiority.",
        "arxiv_id": "2506.06682"
    },
    "2506.07492": {
        "SCORE": 15,
        "ARXIVID": "2506.07492",
        "COMMENT": "The paper introduces a new framework for preference optimization in LLMs, aligning with the large language models criterion.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Xiangkun Hu",
            "Lemin Kong",
            "Tong He",
            "David Wipf"
        ],
        "title": "Explicit Preference Optimization: No Need for an Implicit Reward Model",
        "abstract": "The generated responses of large language models (LLMs) are often fine-tuned to human preferences through a process called reinforcement learning from human feedback (RLHF). As RLHF relies on a challenging training sequence, whereby a separate reward model is independently learned and then later applied to LLM policy updates, ongoing research effort has targeted more straightforward alternatives. In this regard, direct preference optimization (DPO) and its many offshoots circumvent the need for a separate reward training step. Instead, through the judicious use of a reparameterization trick that induces an \\textit{implicit} reward, DPO and related methods consolidate learning to the minimization of a single loss function. And yet despite demonstrable success in some real-world settings, we prove that DPO-based objectives are nonetheless subject to sub-optimal regularization and counter-intuitive interpolation behaviors, underappreciated artifacts of the reparameterizations upon which they are based. To this end, we introduce an \\textit{explicit} preference optimization framework termed EXPO that requires no analogous reparameterization to achieve an implicit reward. Quite differently, we merely posit intuitively-appealing regularization factors from scratch that transparently avoid the potential pitfalls of key DPO variants, provably satisfying regularization desiderata that prior methods do not. Empirical results serve to corroborate our analyses and showcase the efficacy of EXPO.",
        "arxiv_id": "2506.07492"
    },
    "2506.07624": {
        "SCORE": 15,
        "ARXIVID": "2506.07624",
        "COMMENT": "The paper revisits ChebNet, a foundational GNN architecture, and proposes improvements for long-range tasks, aligning with model architecture analysis.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Ali Hariri",
            "\\'Alvaro Arroyo",
            "Alessio Gravina",
            "Moshe Eliasof",
            "Carola-Bibiane Sch\\\"onlieb",
            "Davide Bacciu",
            "Kamyar Azizzadenesheli",
            "Xiaowen Dong",
            "Pierre Vandergheynst"
        ],
        "title": "Return of ChebNet: Understanding and Improving an Overlooked GNN on Long Range Tasks",
        "abstract": "ChebNet, one of the earliest spectral GNNs, has largely been overshadowed by Message Passing Neural Networks (MPNNs), which gained popularity for their simplicity and effectiveness in capturing local graph structure. Despite their success, MPNNs are limited in their ability to capture long-range dependencies between nodes. This has led researchers to adapt MPNNs through rewiring or make use of Graph Transformers, which compromises the computational efficiency that characterized early spatial message-passing architectures, and typically disregards the graph structure. Almost a decade after its original introduction, we revisit ChebNet to shed light on its ability to model distant node interactions. We find that out-of-box, ChebNet already shows competitive advantages relative to classical MPNNs and GTs on long-range benchmarks, while maintaining good scalability properties for high-order polynomials. However, we uncover that this polynomial expansion leads ChebNet to an unstable regime during training. To address this limitation, we cast ChebNet as a stable and non-dissipative dynamical system, which we coin Stable-ChebNet. Our Stable-ChebNet model allows for stable information propagation, and has controllable dynamics which do not require the use of eigendecompositions, positional encodings, or graph rewiring. Across several benchmarks, Stable-ChebNet achieves near state-of-the-art performance.",
        "arxiv_id": "2506.07624"
    },
    "2506.07549": {
        "SCORE": 15,
        "ARXIVID": "2506.07549",
        "COMMENT": "The paper proposes a novel method for improving memory efficiency in KANs, which aligns with foundational research in model compression and efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Zhangchi Zhao",
            "Jun Shu",
            "Deyu Meng",
            "Zongben Xu"
        ],
        "title": "Improving Memory Efficiency for Training KANs via Meta Learning",
        "abstract": "Inspired by the Kolmogorov-Arnold representation theorem, KANs offer a novel framework for function approximation by replacing traditional neural network weights with learnable univariate functions. This design demonstrates significant potential as an efficient and interpretable alternative to traditional MLPs. However, KANs are characterized by a substantially larger number of trainable parameters, leading to challenges in memory efficiency and higher training costs compared to MLPs. To address this limitation, we propose to generate weights for KANs via a smaller meta-learner, called MetaKANs. By training KANs and MetaKANs in an end-to-end differentiable manner, MetaKANs achieve comparable or even superior performance while significantly reducing the number of trainable parameters and maintaining promising interpretability. Extensive experiments on diverse benchmark tasks, including symbolic regression, partial differential equation solving, and image classification, demonstrate the effectiveness of MetaKANs in improving parameter efficiency and memory usage. The proposed method provides an alternative technique for training KANs, that allows for greater scalability and extensibility, and narrows the training cost gap with MLPs stated in the original paper of KANs. Our code is available at https://github.com/Murphyzc/MetaKAN.",
        "arxiv_id": "2506.07549"
    }
}