# Personalized Daily Arxiv Papers 02/18/2025

|           | Prompt   | Completion   | Total   |
|-----------|----------|--------------|---------|
| **Token** | 159927   | 12875        | 172802  |
| **Cost**  | $0.4     | $0.13        | $0.53   |

Total scanned papers: 681

Total relevant papers: 51

**Table of contents with paper titles:**

1. [Intuitive physics understanding emerges from self-supervised pretraining on natural videos](#user-content-link1)
**Authors:** Quentin Garrido, Nicolas Ballas, Mahmoud Assran, Adrien Bardes, Laurent Najman, Michael Rabbat, Emmanuel Dupoux, Yann LeCun

2. [In-Context Parametric Inference: Point or Distribution Estimators?](#user-content-link2)
**Authors:** Sarthak Mittal, Yoshua Bengio, Nikolay Malkin, Guillaume Lajoie

3. [The underlying structures of self-attention: symmetry, directionality, and emergent dynamics in Transformer training](#user-content-link3)
**Authors:** Matteo Saponati, Pascal Sager, Pau Vilimelis Aceituno, Thilo Stadelmann, Benjamin Grewe

4. [Low-Rank Thinning](#user-content-link4)
**Authors:** Annabelle Michael Carrell, Albert Gong, Abhishek Shetty, Raaz Dwivedi, Lester Mackey

5. [Exact Upper and Lower Bounds for the Output Distribution of Neural Networks with Random Inputs](#user-content-link5)
**Authors:** Andrey Kofnov, Daniel Kapla, Ezio Bartocci, Efstathia Bura

6. [Does Editing Provide Evidence for Localization?](#user-content-link6)
**Authors:** Zihao Wang, Victor Veitch

7. [LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws](#user-content-link7)
**Authors:** Prasanna Mayilvahanan, Thadd\"aus Wiedemer, Sayak Mallick, Matthias Bethge, Wieland Brendel

8. [A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia](#user-content-link8)
**Authors:** Giovanni Monea, Maxime Peyrard, Martin Josifoski, Vishrav Chaudhary, Jason Eisner, Emre K{\i}c{\i}man, Hamid Palangi, Barun Patra, Robert West

9. [Neuron Platonic Intrinsic Representation From Dynamics Using Contrastive Learning](#user-content-link9)
**Authors:** Wei Wu, Can Liao, Zizhen Deng, Zhengrui Guo, Jinzhuo Wang

10. [AdaSplash: Adaptive Sparse Flash Attention](#user-content-link10)
**Authors:** Nuno Gon\c{c}alves, Marcos Treviso, Andr\'e F. T. Martins

11. [Approximation of Permutation Invariant Polynomials by Transformers: Efficient Construction in Column-Size](#user-content-link11)
**Authors:** Naoki Takeshita, Masaaki Imaizumi

12. [From Layers to States: A State Space Model Perspective to Deep Neural Network Layer Dynamics](#user-content-link12)
**Authors:** Qinshuo Liu, Weiqin Zhao, Wei Huang, Yanwen Fang, Lequan Yu, Guodong Li

13. [Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention](#user-content-link13)
**Authors:** Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Y. X. Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, Chong Ruan, Ming Zhang, Wenfeng Liang, Wangding Zeng

14. [Cognitive Neural Architecture Search Reveals Hierarchical Entailment](#user-content-link14)
**Authors:** Lukas Kuhn, Sari Saba-Sadiya, Gemma Roig

15. [QuantSpec: Self-Speculative Decoding with Hierarchical Quantized KV Cache](#user-content-link15)
**Authors:** Rishabh Tiwari, Haocheng Xi, Aditya Tomar, Coleman Hooper, Sehoon Kim, Maxwell Horton, Mahyar Najibi, Michael W. Mahoney, Kurt Keutzer, Amir Gholami

16. [How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training](#user-content-link16)
**Authors:** Yixin Ou, Yunzhi Yao, Ningyu Zhang, Hui Jin, Jiacheng Sun, Shumin Deng, Zhenguo Li, Huajun Chen

17. [Atom of Thoughts for Markov LLM Test-Time Scaling](#user-content-link17)
**Authors:** Fengwei Teng, Zhaoyang Yu, Quan Shi, Jiayi Zhang, Chenglin Wu, Yuyu Luo

18. [Towards Efficient Pre-training: Exploring FP4 Precision in Large Language Models](#user-content-link18)
**Authors:** Jiecheng Zhou, Ding Tang, Rong Fu, Boni Hu, Haoran Xu, Yi Wang, Zhilin Pei, Zhongling Su, Liang Liu, Xingcheng Zhang, Weiming Zhang

19. [CoLA: Compute-Efficient Pre-Training of LLMs via Low-Rank Activation](#user-content-link19)
**Authors:** Ziyue Liu, Ruijie Zhang, Zhengyang Wang, Zi Yang, Paul Hovland, Bogdan Nicolae, Franck Cappello, Zheng Zhang

20. [Unlocking the Power of Function Vectors for Characterizing and Mitigating Catastrophic Forgetting in Continual Instruction Tuning](#user-content-link20)
**Authors:** Gangwei Jiang, Caigao Jiang, Zhaoyi Li, Siqiao Xue, Jun Zhou, Linqi Song, Defu Lian, Yin Wei

21. [An Efficient Row-Based Sparse Fine-Tuning](#user-content-link21)
**Authors:** Cen-Jhih Li, Aditya Bhaskara

22. [MoHAVE: Mixture of Hierarchical Audio-Visual Experts for Robust Speech Recognition](#user-content-link22)
**Authors:** Sungnyun Kim, Kangwook Jang, Sangmin Bae, Sungwoo Cho, Se-Young Yun

23. [LEAPS: A discrete neural sampler via locally equivariant networks](#user-content-link23)
**Authors:** Peter Holderrieth, Michael S. Albergo, Tommi Jaakkola

24. [Raising the Bar in Graph OOD Generalization: Invariant Learning Beyond Explicit Environment Modeling](#user-content-link24)
**Authors:** Xu Shen, Yixin Liu, Yili Wang, Rui Miao, Yiwei Dai, Shirui Pan, Xin Wang

25. [Why Domain Generalization Fail? A View of Necessity and Sufficiency](#user-content-link25)
**Authors:** Long-Tung Vuong, Vy Vo, Hien Dang, Van-Anh Nguyen, Thanh-Toan Do, Mehrtash Harandi, Trung Le, Dinh Phung

26. [PEA: Enhancing LLM Performance on Computational-Reasoning Tasks](#user-content-link26)
**Authors:** Zi Wang, Shiwei Weng, Mohannad Alhanahnah, Somesh Jha, Tom Reps

27. [Towards Data-Efficient Pretraining for Atomic Property Prediction](#user-content-link27)
**Authors:** Yasir Ghunaim, Hasan Abed Al Kader Hammoud, Bernard Ghanem

28. [Sharp-PINNs: staggered hard-constrained physics-informed neural networks for phase field modelling of corrosion](#user-content-link28)
**Authors:** Nanxi Chen, Chuanjie Cui, Rujin Ma, Airong Chen, Sifan Wang

29. [Learning the Exact Time Integration Algorithm for Initial Value Problems by Randomized Neural Networks](#user-content-link29)
**Authors:** Suchuan Dong, Naxian Ni

30. [Preconditioned Inexact Stochastic ADMM for Deep Model](#user-content-link30)
**Authors:** Shenglong Zhou, Ouya Wang, Ziyan Luo, Yongxu Zhu, Geoffrey Ye Li

31. [LIMR: Less is More for RL Scaling](#user-content-link31)
**Authors:** Xuefeng Li, Haoyang Zou, Pengfei Liu

32. [Adversarial Alignment for LLMs Requires Simpler, Reproducible, and More Measurable Objectives](#user-content-link32)
**Authors:** Leo Schwinn, Yan Scholten, Tom Wollschl\"ager, Sophie Xhonneux, Stephen Casper, Stephan G\"unnemann, Gauthier Gidel

33. [Dynamic Chain-of-Thought: Towards Adaptive Deep Reasoning](#user-content-link33)
**Authors:** Libo Wang

34. [Deep Subspace Learning for Surface Anomaly Classification Based on 3D Point Cloud Data](#user-content-link34)
**Authors:** Xuanming Cao, Chengyu Tao, Juan Du

35. [Dictionary-Learning-Based Data Pruning for System Identification](#user-content-link35)
**Authors:** Tingna Wang (Department of Bridge Engineering, Tongji University, Shanghai, China, Shanghai Qi Zhi Institute, Shanghai, China), Sikai Zhang (Baosight Software), Limin Sun (Department of Bridge Engineering, Tongji University, Shanghai, China, State Key Laboratory of Disaster Reduction in Civil Engineering, Tongji University, Shanghai, China)

36. [Towards Reasoning Ability of Small Language Models](#user-content-link36)
**Authors:** Gaurav Srivastava, Shuxiang Cao, Xuan Wang

37. [K-Edit: Language Model Editing with Contextual Knowledge Awareness](#user-content-link37)
**Authors:** Elan Markowitz, Anil Ramakrishna, Ninareh Mehrabi, Charith Peris, Rahul Gupta, Kai-Wei Chang, Aram Galstyan

38. [A Neural Network Training Method Based on Neuron Connection Coefficient Adjustments](#user-content-link38)
**Authors:** Kun Jiang

39. [The Rotary Position Embedding May Cause Dimension Inefficiency in Attention Heads for Long-Distance Retrieval](#user-content-link39)
**Authors:** Ting-Rui Chiang, Dani Yogatama

40. [Neural Networks Remember More: The Power of Parameter Isolation and Combination](#user-content-link40)
**Authors:** Biqing Zeng, Zehan Li, Aladdin Ayesh

41. [Why is prompting hard? Understanding prompts on binary sequence predictors](#user-content-link41)
**Authors:** Li Kevin Wenliang, Anian Ruoss, Jordi Grau-Moya, Marcus Hutter, Tim Genewein

42. [Minimal Ranks, Maximum Confidence: Parameter-efficient Uncertainty Quantification for LoRA](#user-content-link42)
**Authors:** Patryk Marsza{\l}ek, Klaudia Ba{\l}azy, Jacek Tabor, Tomasz Ku\'smierczyk

43. [InfiR : Crafting Effective Small Language Models and Multimodal Small Language Models in Reasoning](#user-content-link43)
**Authors:** Congkai Xie, Shuo Cai, Wenjun Wang, Pengxiang Li, Zhijie Sang, Kejing Yang, Yiming Zhang, Zhen Li, Guanghao Zhu, Zeyu Liu, Yang Yu, Yuhang Liu, Su Lu, Baoyi He, Qi Zhou, Xiaotian Han, Jianbo Yuan, Shengyu Zhang, Fei Wu, Hongxia Yang

44. [Navigating the Helpfulness-Truthfulness Trade-Off with Uncertainty-Aware Instruction Fine-Tuning](#user-content-link44)
**Authors:** Tianyi Wu, Jingwei Ni, Bryan Hooi, Jiaheng Zhang, Elliott Ash, See-Kiong Ng, Mrinmaya Sachan, Markus Leippold

45. [Revealing Bias Formation in Deep Neural Networks Through the Geometric Mechanisms of Human Visual Decoupling](#user-content-link45)
**Authors:** Yanbiao Ma, Bowei Liu, Wei Dai, Jiayi Chen, Shuo Li

46. [Analysis of Overparameterization in Continual Learning under a Linear Model](#user-content-link46)
**Authors:** Daniel Goldfarb, Paul Hand

47. [Statistical Query Hardness of Multiclass Linear Classification with Random Classification Noise](#user-content-link47)
**Authors:** Ilias Diakonikolas, Mingchen Ma, Lisheng Ren, Christos Tzamos

48. [Generative Adversarial Networks for High-Dimensional Item Factor Analysis: A Deep Adversarial Learning Algorithm](#user-content-link48)
**Authors:** Nanyu Luo, Feng Ji

49. [One Class Restricted Kernel Machines](#user-content-link49)
**Authors:** A. Quadir, M. Sajid, M. Tanveer

50. [Reduced Order Modeling with Shallow Recurrent Decoder Networks](#user-content-link50)
**Authors:** Matteo Tomasetto, Jan P. Williams, Francesco Braghin, Andrea Manzoni, J. Nathan Kutz

51. [GraphiT: Efficient Node Classification on Text-Attributed Graphs with Prompt Optimized LLMs](#user-content-link51)
**Authors:** Shima Khoshraftar, Niaz Abedini, Amir Hajian

---

## 1. [Intuitive physics understanding emerges from self-supervised pretraining on natural videos](https://arxiv.org/abs/2502.11831) <a id="link1"></a>

**ArXiv ID:** 2502.11831

**Authors:** Quentin Garrido, Nicolas Ballas, Mahmoud Assran, Adrien Bardes, Laurent Najman, Michael Rabbat, Emmanuel Dupoux, Yann LeCun

**Abstract:** We investigate the emergence of intuitive physics understanding in general-purpose deep neural network models trained to predict masked regions in natural videos. Leveraging the violation-of-expectation framework, we find that video prediction models trained to predict outcomes in a learned representation space demonstrate an understanding of various intuitive physics properties, such as object permanence and shape consistency. In contrast, video prediction in pixel space and multimodal large language models, which reason through text, achieve performance closer to chance. Our comparisons of these architectures reveal that jointly learning an abstract representation space while predicting missing parts of sensory input, akin to predictive coding, is sufficient to acquire an understanding of intuitive physics, and that even models trained on one week of unique video achieve above chance performance. This challenges the idea that core knowledge -- a set of innate systems to help understand the world -- needs to be hardwired to develop an understanding of intuitive physics.

**Comment:** Author match



---

## 2. [In-Context Parametric Inference: Point or Distribution Estimators?](https://arxiv.org/abs/2502.11617) <a id="link2"></a>

**ArXiv ID:** 2502.11617

**Authors:** Sarthak Mittal, Yoshua Bengio, Nikolay Malkin, Guillaume Lajoie

**Abstract:** Bayesian and frequentist inference are two fundamental paradigms in statistical estimation. Bayesian methods treat hypotheses as random variables, incorporating priors and updating beliefs via Bayes' theorem, whereas frequentist methods assume fixed but unknown hypotheses, relying on estimators like maximum likelihood. While extensive research has compared these approaches, the frequentist paradigm of obtaining point estimates has become predominant in deep learning, as Bayesian inference is challenging due to the computational complexity and the approximation gap of posterior estimation methods. However, a good understanding of trade-offs between the two approaches is lacking in the regime of amortized estimators, where in-context learners are trained to estimate either point values via maximum likelihood or maximum a posteriori estimation, or full posteriors using normalizing flows, score-based diffusion samplers, or diagonal Gaussian approximations, conditioned on observations. To help resolve this, we conduct a rigorous comparative analysis spanning diverse problem settings, from linear models to shallow neural networks, with a robust evaluation framework assessing both in-distribution and out-of-distribution generalization on tractable tasks. Our experiments indicate that amortized point estimators generally outperform posterior inference, though the latter remain competitive in some low-dimensional problems, and we further discuss why this might be the case.

**Comment:** Author match



---

## 3. [The underlying structures of self-attention: symmetry, directionality, and emergent dynamics in Transformer training](https://arxiv.org/abs/2502.10927) <a id="link3"></a>

**ArXiv ID:** 2502.10927

**Authors:** Matteo Saponati, Pascal Sager, Pau Vilimelis Aceituno, Thilo Stadelmann, Benjamin Grewe

**Abstract:** Self-attention is essential to Transformer architectures, yet how information is embedded in the self-attention matrices and how different objective functions impact this process remains unclear. We present a mathematical framework to analyze self-attention matrices by deriving the structures governing their weight updates. Using this framework, we demonstrate that bidirectional training induces symmetry in the weight matrices, while autoregressive training results in directionality and column dominance. Our theoretical findings are validated across multiple Transformer models - including ModernBERT, GPT, LLaMA3, and Mistral - and input modalities like text, vision, and audio. Finally, we apply these insights by showing that symmetric initialization improves the performance of encoder-only models on language tasks. This mathematical analysis offers a novel theoretical perspective on how information is embedded through self-attention, thereby improving the interpretability of Transformer models.

**Comment:** The paper offers a mathematical framework to analyze self-attention in Transformers, providing theoretical insights into training dynamics and architecture behavior.

**Relevance:** 10
**Novelty:** 9

---

## 4. [Low-Rank Thinning](https://arxiv.org/abs/2502.12063) <a id="link4"></a>

**ArXiv ID:** 2502.12063

**Authors:** Annabelle Michael Carrell, Albert Gong, Abhishek Shetty, Raaz Dwivedi, Lester Mackey

**Abstract:** The goal in thinning is to summarize a dataset using a small set of representative points. Remarkably, sub-Gaussian thinning algorithms like Kernel Halving and Compress can match the quality of uniform subsampling while substantially reducing the number of summary points. However, existing guarantees cover only a restricted range of distributions and kernel-based quality measures and suffer from pessimistic dimension dependence. To address these deficiencies, we introduce a new low-rank analysis of sub-Gaussian thinning that applies to any distribution and any kernel, guaranteeing high-quality compression whenever the kernel or data matrix is approximately low-rank. To demonstrate the broad applicability of the techniques, we design practical sub-Gaussian thinning approaches that improve upon the best known guarantees for approximating attention in transformers, accelerating stochastic gradient training through reordering, and distinguishing distributions in near-linear time.

**Comment:** The paper introduces a low-rank analysis for sub-Gaussian thinning and demonstrates its application to attention mechanisms in transformers, which aligns with model compression and efficiency breakthroughs.

**Relevance:** 9
**Novelty:** 8

---

## 5. [Exact Upper and Lower Bounds for the Output Distribution of Neural Networks with Random Inputs](https://arxiv.org/abs/2502.11672) <a id="link5"></a>

**ArXiv ID:** 2502.11672

**Authors:** Andrey Kofnov, Daniel Kapla, Ezio Bartocci, Efstathia Bura

**Abstract:** We derive exact upper and lower bounds for the cumulative distribution function (cdf) of the output of a neural network over its entire support subject to noisy (stochastic) inputs. The upper and lower bounds converge to the true cdf over its domain as the resolution increases. Our method applies to any feedforward NN using continuous monotonic piecewise differentiable activation functions (e.g., ReLU, tanh and softmax) and convolutional NNs, which were beyond the scope of competing approaches. The novelty and an instrumental tool of our approach is to bound general NNs with ReLU NNs. The ReLU NN based bounds are then used to derive upper and lower bounds of the cdf of the NN output. Experiments demonstrate that our method delivers guaranteed bounds of the predictive output distribution over its support, thus providing exact error guarantees, in contrast to competing approaches.

**Comment:** The paper derives exact bounds for neural network output distributions, which provides theoretical insights into neural network behavior and training dynamics.

**Relevance:** 9
**Novelty:** 8

---

## 6. [Does Editing Provide Evidence for Localization?](https://arxiv.org/abs/2502.11447) <a id="link6"></a>

**ArXiv ID:** 2502.11447

**Authors:** Zihao Wang, Victor Veitch

**Abstract:** A basic aspiration for interpretability research in large language models is to "localize" semantically meaningful behaviors to particular components within the LLM. There are various heuristics for finding candidate locations within the LLM. Once a candidate localization is found, it can be assessed by editing the internal representations at the corresponding localization and checking whether this induces model behavior that is consistent with the semantic interpretation of the localization. The question we address here is: how strong is the evidence provided by such edits? To assess localization, we want to assess the effect of the optimal intervention at a particular location. The key new technical tool is a way of adapting LLM alignment techniques to find such optimal localized edits. With this tool in hand, we give an example where the edit-based evidence for localization appears strong, but where localization clearly fails. Indeed, we find that optimal edits at random localizations can be as effective as aligning the full model. In aggregate, our results suggest that merely observing that localized edits induce targeted changes in behavior provides little to no evidence that these locations actually encode the target behavior.

**Comment:** The paper critiques interpretability methods in LLMs and introduces a novel tool for assessing localization, which aligns with foundational research in LLM behavior and interpretability.

**Relevance:** 9
**Novelty:** 8

---

## 7. [LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws](https://arxiv.org/abs/2502.12120) <a id="link7"></a>

**ArXiv ID:** 2502.12120

**Authors:** Prasanna Mayilvahanan, Thadd\"aus Wiedemer, Sayak Mallick, Matthias Bethge, Wieland Brendel

**Abstract:** Scaling laws guide the development of large language models (LLMs) by offering estimates for the optimal balance of model size, tokens, and compute. More recently, loss-to-loss scaling laws that relate losses across pretraining datasets and downstream tasks have emerged as a powerful tool for understanding and improving LLM performance. In this work, we investigate which factors most strongly influence loss-to-loss scaling. Our experiments reveal that the pretraining data and tokenizer determine the scaling trend. In contrast, model size, optimization hyperparameters, and even significant architectural differences, such as between transformer-based models like Llama and state-space models like Mamba, have limited impact. Consequently, practitioners should carefully curate suitable pretraining datasets for optimal downstream performance, while architectures and other settings can be freely optimized for training efficiency.

**Comment:** The paper investigates loss-to-loss scaling laws in LLMs, focusing on the impact of pretraining data and tokenizer. This aligns with the 'Large Language Models' criterion, particularly in understanding theoretical insights into LLM behavior.

**Relevance:** 9
**Novelty:** 8

---

## 8. [A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia](https://arxiv.org/abs/2312.02073) <a id="link8"></a>

**ArXiv ID:** 2312.02073

**Authors:** Giovanni Monea, Maxime Peyrard, Martin Josifoski, Vishrav Chaudhary, Jason Eisner, Emre K{\i}c{\i}man, Hamid Palangi, Barun Patra, Robert West

**Abstract:** Large language models (LLMs) have an impressive ability to draw on novel information supplied in their context. Yet the mechanisms underlying this contextual grounding remain unknown, especially in situations where contextual information contradicts factual knowledge stored in the parameters, which LLMs also excel at recalling. Favoring the contextual information is critical for retrieval-augmented generation methods, which enrich the context with up-to-date information, hoping that grounding can rectify outdated or noisy stored knowledge. We present a novel method to study grounding abilities using Fakepedia, a novel dataset of counterfactual texts constructed to clash with a model's internal parametric knowledge. In this study, we introduce Fakepedia, a counterfactual dataset designed to evaluate grounding abilities when the internal parametric knowledge clashes with the contextual information. We benchmark various LLMs with Fakepedia and conduct a causal mediation analysis of LLM components when answering Fakepedia queries, based on our Masked Grouped Causal Tracing (MGCT) method. Through this analysis, we identify distinct computational patterns between grounded and ungrounded responses. We finally demonstrate that distinguishing grounded from ungrounded responses is achievable through computational analysis alone. Our results, together with existing findings about factual recall mechanisms, provide a coherent narrative of how grounding and factual recall mechanisms interact within LLMs.

**Comment:** The paper studies grounding mechanisms in LLMs using a counterfactual dataset, aligning with the 'Large Language Models' criterion by providing theoretical insights into LLM behavior.

**Relevance:** 9
**Novelty:** 8

---

## 9. [Neuron Platonic Intrinsic Representation From Dynamics Using Contrastive Learning](https://arxiv.org/abs/2502.10425) <a id="link9"></a>

**ArXiv ID:** 2502.10425

**Authors:** Wei Wu, Can Liao, Zizhen Deng, Zhengrui Guo, Jinzhuo Wang

**Abstract:** The Platonic Representation Hypothesis suggests a universal, modality-independent reality representation behind different data modalities. Inspired by this, we view each neuron as a system and detect its multi-segment activity data under various peripheral conditions. We assume there's a time-invariant representation for the same neuron, reflecting its intrinsic properties like molecular profiles, location, and morphology. The goal of obtaining these intrinsic neuronal representations has two criteria: (I) segments from the same neuron should have more similar representations than those from different neurons; (II) the representations must generalize well to out-of-domain data. To meet these, we propose the NeurPIR (Neuron Platonic Intrinsic Representation) framework. It uses contrastive learning, with segments from the same neuron as positive pairs and those from different neurons as negative pairs. In implementation, we use VICReg, which focuses on positive pairs and separates dissimilar samples via regularization. We tested our method on Izhikevich model-simulated neuronal population dynamics data. The results accurately identified neuron types based on preset hyperparameters. We also applied it to two real-world neuron dynamics datasets with neuron type annotations from spatial transcriptomics and neuron locations. Our model's learned representations accurately predicted neuron types and locations and were robust on out-of-domain data (from unseen animals). This shows the potential of our approach for understanding neuronal systems and future neuroscience research.

**Comment:** The paper introduces a novel framework, NeurPIR, for learning intrinsic representations using contrastive learning, which aligns with the representation learning criterion.

**Relevance:** 9
**Novelty:** 8

---

## 10. [AdaSplash: Adaptive Sparse Flash Attention](https://arxiv.org/abs/2502.12082) <a id="link10"></a>

**ArXiv ID:** 2502.12082

**Authors:** Nuno Gon\c{c}alves, Marcos Treviso, Andr\'e F. T. Martins

**Abstract:** The computational cost of softmax-based attention in transformers limits their applicability to long-context tasks. Adaptive sparsity, of which $\alpha$-entmax attention is an example, offers a flexible data-dependent alternative, but existing implementations are inefficient and do not leverage the sparsity to obtain runtime and memory gains. In this work, we propose AdaSplash, which combines the efficiency of GPU-optimized algorithms with the sparsity benefits of $\alpha$-entmax. We first introduce a hybrid Halley-bisection algorithm, resulting in a 7-fold reduction in the number of iterations needed to compute the $\alpha$-entmax transformation. Then, we implement custom Triton kernels to efficiently handle adaptive sparsity. Experiments with RoBERTa and ModernBERT for text classification and single-vector retrieval, along with GPT-2 for language modeling, show that our method achieves substantial improvements in runtime and memory efficiency compared to existing $\alpha$-entmax implementations. It approaches -- and in some cases surpasses -- the efficiency of highly optimized softmax implementations like FlashAttention-2, enabling long-context training while maintaining strong task performance.

**Comment:** The paper introduces AdaSplash, a novel sparse attention mechanism with significant efficiency improvements, aligning with the model compression and efficiency criteria.

**Relevance:** 9
**Novelty:** 8

---

## 11. [Approximation of Permutation Invariant Polynomials by Transformers: Efficient Construction in Column-Size](https://arxiv.org/abs/2502.11467) <a id="link11"></a>

**ArXiv ID:** 2502.11467

**Authors:** Naoki Takeshita, Masaaki Imaizumi

**Abstract:** Transformers are a type of neural network that have demonstrated remarkable performance across various domains, particularly in natural language processing tasks. Motivated by this success, research on the theoretical understanding of transformers has garnered significant attention. A notable example is the mathematical analysis of their approximation power, which validates the empirical expressive capability of transformers. In this study, we investigate the ability of transformers to approximate column-symmetric polynomials, an extension of symmetric polynomials that take matrices as input. Consequently, we establish an explicit relationship between the size of the transformer network and its approximation capability, leveraging the parameter efficiency of transformers and their compatibility with symmetry by focusing on the algebraic properties of symmetric polynomials.

**Comment:** The paper provides a theoretical analysis of transformers' ability to approximate symmetric polynomials, aligning with the model architecture criteria.

**Relevance:** 9
**Novelty:** 8

---

## 12. [From Layers to States: A State Space Model Perspective to Deep Neural Network Layer Dynamics](https://arxiv.org/abs/2502.10463) <a id="link12"></a>

**ArXiv ID:** 2502.10463

**Authors:** Qinshuo Liu, Weiqin Zhao, Wei Huang, Yanwen Fang, Lequan Yu, Guodong Li

**Abstract:** The depth of neural networks is a critical factor for their capability, with deeper models often demonstrating superior performance. Motivated by this, significant efforts have been made to enhance layer aggregation - reusing information from previous layers to better extract features at the current layer, to improve the representational power of deep neural networks. However, previous works have primarily addressed this problem from a discrete-state perspective which is not suitable as the number of network layers grows. This paper novelly treats the outputs from layers as states of a continuous process and considers leveraging the state space model (SSM) to design the aggregation of layers in very deep neural networks. Moreover, inspired by its advancements in modeling long sequences, the Selective State Space Models (S6) is employed to design a new module called Selective State Space Model Layer Aggregation (S6LA). This module aims to combine traditional CNN or transformer architectures within a sequential framework, enhancing the representational capabilities of state-of-the-art vision networks. Extensive experiments show that S6LA delivers substantial improvements in both image classification and detection tasks, highlighting the potential of integrating SSMs with contemporary deep learning techniques.

**Comment:** The paper introduces a state-space model perspective for layer dynamics in deep networks, which aligns with representation learning and architectural insights.

**Relevance:** 9
**Novelty:** 8

---

## 13. [Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention](https://arxiv.org/abs/2502.11089) <a id="link13"></a>

**ArXiv ID:** 2502.11089

**Authors:** Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Y. X. Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, Chong Ruan, Ming Zhang, Wenfeng Liang, Wangding Zeng

**Abstract:** Long-context modeling is crucial for next-generation language models, yet the high computational cost of standard attention mechanisms poses significant computational challenges. Sparse attention offers a promising direction for improving efficiency while maintaining model capabilities. We present NSA, a Natively trainable Sparse Attention mechanism that integrates algorithmic innovations with hardware-aligned optimizations to achieve efficient long-context modeling. NSA employs a dynamic hierarchical sparse strategy, combining coarse-grained token compression with fine-grained token selection to preserve both global context awareness and local precision. Our approach advances sparse attention design with two key innovations: (1) We achieve substantial speedups through arithmetic intensity-balanced algorithm design, with implementation optimizations for modern hardware. (2) We enable end-to-end training, reducing pretraining computation without sacrificing model performance. As shown in Figure 1, experiments show the model pretrained with NSA maintains or exceeds Full Attention models across general benchmarks, long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves substantial speedups over Full Attention on 64k-length sequences across decoding, forward propagation, and backward propagation, validating its efficiency throughout the model lifecycle.

**Comment:** The paper introduces NSA, a sparse attention mechanism with hardware-aligned optimizations, aligning with model compression and efficiency criteria.

**Relevance:** 9
**Novelty:** 8

---

## 14. [Cognitive Neural Architecture Search Reveals Hierarchical Entailment](https://arxiv.org/abs/2502.11141) <a id="link14"></a>

**ArXiv ID:** 2502.11141

**Authors:** Lukas Kuhn, Sari Saba-Sadiya, Gemma Roig

**Abstract:** Recent research has suggested that the brain is more shallow than previously thought, challenging the traditionally assumed hierarchical structure of the ventral visual pathway. Here, we demonstrate that optimizing convolutional network architectures for brain-alignment via evolutionary neural architecture search results in models with clear representational hierarchies. Despite having random weights, the identified models achieve brain-alignment scores surpassing even those of pretrained classification models - as measured by both regression and representational similarity analysis. Furthermore, through traditional supervised training, architectures optimized for alignment with late ventral regions become competitive classification models. These findings suggest that hierarchical structure is a fundamental mechanism of primate visual processing. Finally, this work demonstrates the potential of neural architecture search as a framework for computational cognitive neuroscience research that could reduce the field's reliance on manually designed convolutional networks.

**Comment:** The paper uses neural architecture search to optimize brain-aligned convolutional networks, which is relevant to model architecture and introduces novel insights into hierarchical structures.

**Relevance:** 9
**Novelty:** 8

---

## 15. [QuantSpec: Self-Speculative Decoding with Hierarchical Quantized KV Cache](https://arxiv.org/abs/2502.10424) <a id="link15"></a>

**ArXiv ID:** 2502.10424

**Authors:** Rishabh Tiwari, Haocheng Xi, Aditya Tomar, Coleman Hooper, Sehoon Kim, Maxwell Horton, Mahyar Najibi, Michael W. Mahoney, Kurt Keutzer, Amir Gholami

**Abstract:** Large Language Models (LLMs) are increasingly being deployed on edge devices for long-context settings, creating a growing need for fast and efficient long-context inference. In these scenarios, the Key-Value (KV) cache is the primary bottleneck in terms of both GPU memory and latency, as the full KV cache must be loaded for each decoding step. While speculative decoding is a widely accepted technique to accelerate autoregressive decoding, existing methods often struggle to achieve significant speedups due to inefficient KV cache optimization strategies and result in low acceptance rates. To address these challenges, we propose a novel self-speculative decoding framework, QuantSpec, where the draft model shares the architecture of the target model but employs a hierarchical 4-bit quantized KV cache and 4-bit quantized weights for acceleration. QuantSpec maintains high acceptance rates ($>$90%) and reliably provides consistent end-to-end speedups upto $\sim2.5\times$, outperforming other self-speculative decoding methods that use sparse KV cache for long-context LLM inference. QuantSpec also reduces the memory requirements by $\sim 1.3\times$ compared to these alternatives.

**Comment:** The paper introduces a novel hierarchical quantized KV cache for efficient LLM inference, which aligns with the model compression criterion, specifically focusing on memory and latency optimization.

**Relevance:** 9
**Novelty:** 8

---

## 16. [How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training](https://arxiv.org/abs/2502.11196) <a id="link16"></a>

**ArXiv ID:** 2502.11196

**Authors:** Yixin Ou, Yunzhi Yao, Ningyu Zhang, Hui Jin, Jiacheng Sun, Shumin Deng, Zhenguo Li, Huajun Chen

**Abstract:** Despite exceptional capabilities in knowledge-intensive tasks, Large Language Models (LLMs) face a critical gap in understanding how they internalize new knowledge, particularly how to structurally embed acquired knowledge in their neural computations. We address this issue through the lens of knowledge circuit evolution, identifying computational subgraphs that facilitate knowledge storage and processing. Our systematic analysis of circuit evolution throughout continual pre-training reveals several key findings: (1) the acquisition of new knowledge is influenced by its relevance to pre-existing knowledge; (2) the evolution of knowledge circuits exhibits a distinct phase shift from formation to optimization; (3) the evolution of knowledge circuits follows a deep-to-shallow pattern. These insights not only advance our theoretical understanding of the mechanisms of new knowledge acquisition in LLMs, but also provide potential implications for improving continual pre-training strategies to enhance model performance. Code and data will be available at https://github.com/zjunlp/DynamicKnowledgeCircuits.

**Comment:** The paper provides theoretical insights into how LLMs acquire and structure new knowledge during continual pre-training, aligning with the representation learning criterion.

**Relevance:** 9
**Novelty:** 8

---

## 17. [Atom of Thoughts for Markov LLM Test-Time Scaling](https://arxiv.org/abs/2502.12018) <a id="link17"></a>

**ArXiv ID:** 2502.12018

**Authors:** Fengwei Teng, Zhaoyang Yu, Quan Shi, Jiayi Zhang, Chenglin Wu, Yuyu Luo

**Abstract:** Large Language Models (LLMs) achieve superior performance through training-time scaling, and test-time scaling further enhances their capabilities by conducting effective reasoning during inference. However, as the scale of reasoning increases, existing test-time scaling methods suffer from accumulated historical information, which not only wastes computational resources but also interferes with effective reasoning. To address this issue, we observe that complex reasoning progress is often achieved by solving a sequence of independent subquestions, each being self-contained and verifiable. These subquestions are essentially atomic questions, relying primarily on their current state rather than accumulated history, similar to the memoryless transitions in a Markov process. Based on this observation, we propose Atom of Thoughts (AoT), where each state transition in the reasoning process consists of decomposing the current question into a dependency-based directed acyclic graph and contracting its subquestions, forming a new atomic question state. This iterative decomposition-contraction process continues until reaching directly solvable atomic questions, naturally realizing Markov transitions between question states. Furthermore, these atomic questions can be seamlessly integrated into existing test-time scaling methods, enabling AoT to serve as a plug-in enhancement for improving reasoning capabilities. Experiments across six benchmarks demonstrate the effectiveness of AoT both as a standalone framework and a plug-in enhancement. Notably, on HotpotQA, when applied to gpt-4o-mini, AoT achieves an 80.6% F1 score, surpassing o3-mini by 3.4% and DeepSeek-R1 by 10.6%. The code will be available at https://github.com/qixucen/atom.

**Comment:** The paper introduces 'Atom of Thoughts' (AoT), a novel framework for test-time scaling in LLMs, which aligns with the 'Large Language Models' criterion by providing theoretical insights into reasoning processes and test-time behavior.

**Relevance:** 9
**Novelty:** 8

---

## 18. [Towards Efficient Pre-training: Exploring FP4 Precision in Large Language Models](https://arxiv.org/abs/2502.11458) <a id="link18"></a>

**ArXiv ID:** 2502.11458

**Authors:** Jiecheng Zhou, Ding Tang, Rong Fu, Boni Hu, Haoran Xu, Yi Wang, Zhilin Pei, Zhongling Su, Liang Liu, Xingcheng Zhang, Weiming Zhang

**Abstract:** The burgeoning computational demands for training large language models (LLMs) necessitate efficient methods, including quantized training, which leverages low-bit arithmetic operations to reduce costs. While FP8 precision has shown potential, leveraging FP4 remains challenging due to inherent quantization errors and limited representation capability. Based on the Transformer architecture, we present an FP4 training scheme for LLMs, overcoming these obstacles through mixed-precision quantization strategies tailed for different modules and training stages. This allows us to apply the precision level suitable to distinct components within the model, ensuring that multi-head attention and linear layers are handled appropriately. Our pretraining recipe ensures stability in backpropagation by incorporating fine-grained quantization methods with a target precision training schedule. Experimental results demonstrate that our FP4 training scheme achieves accuracy comparable to BF16 and FP8, with smaller theoretical computational cost. With the advent of next-generation hardware supporting FP4, our method sets the foundation for efficient ultra-low precision training.

**Comment:** The paper focuses on ultra-low precision training (FP4) for LLMs, which aligns with the model compression criterion, specifically quantization and efficiency breakthroughs.

**Relevance:** 9
**Novelty:** 8

---

## 19. [CoLA: Compute-Efficient Pre-Training of LLMs via Low-Rank Activation](https://arxiv.org/abs/2502.10940) <a id="link19"></a>

**ArXiv ID:** 2502.10940

**Authors:** Ziyue Liu, Ruijie Zhang, Zhengyang Wang, Zi Yang, Paul Hovland, Bogdan Nicolae, Franck Cappello, Zheng Zhang

**Abstract:** Large language models (LLMs) are revolutionizing many science and engineering fields. However, their huge model sizes impose extremely demanding needs of computational resources in the pre-training stage. Although low-rank factorizations can reduce model parameters, their direct application in LLM pre-training often lead to non-negligible performance loss. To address this fundamental challenge, we introduce CoLA and its memory-efficient implementation, CoLA-M. We leverage the low-rank structure observed widely in model activations, enforcing non-linear transformations between factorized weight matrices to reduce model size, boost model capacity and training efficiency. Experiments on LLaMA models with 60 million to 7 billion parameters show that CoLA reduces the computing cost by $\bf 2\pmb{\times}$ and improves training throughput by $\bf 1.86\pmb{\times}$ while maintaining full-rank level performance. CoLA-M further squeezes memory cost without sacrificing throughput, offering a pre-training approach with collectively superior parameter, computing, and memory efficiency. The LLMs produced are also $\bf 2\pmb{\times}$ smaller, enabling faster inference with lower memory cost on resource-constrained platforms

**Comment:** The paper introduces a low-rank activation method (CoLA) for efficient LLM pretraining, aligning with the model compression criterion, particularly low-rank approaches.

**Relevance:** 9
**Novelty:** 8

---

## 20. [Unlocking the Power of Function Vectors for Characterizing and Mitigating Catastrophic Forgetting in Continual Instruction Tuning](https://arxiv.org/abs/2502.11019) <a id="link20"></a>

**ArXiv ID:** 2502.11019

**Authors:** Gangwei Jiang, Caigao Jiang, Zhaoyi Li, Siqiao Xue, Jun Zhou, Linqi Song, Defu Lian, Yin Wei

**Abstract:** Catastrophic forgetting (CF) poses a significant challenge in machine learning, where a model forgets previously learned information upon learning new tasks. Despite the advanced capabilities of Large Language Models (LLMs), they continue to face challenges with CF during continual learning. The majority of existing research focuses on analyzing forgetting patterns through a singular training sequence, thereby overlooking the intricate effects that diverse tasks have on model behavior. Our study explores CF across various settings, discovering that model forgetting is influenced by both the specific training tasks and the models themselves. To this end, we interpret forgetting by examining the function vector (FV), a compact representation of functions in LLMs, offering a model-dependent indicator for the occurrence of CF. Through theoretical and empirical analyses, we demonstrated that CF in LLMs primarily stems from biases in function activation rather than the overwriting of task processing functions. Leveraging these insights, we propose a novel function vector guided training methodology, incorporating a regularization technique to stabilize the FV and mitigate forgetting. Empirical tests on four benchmarks confirm the effectiveness of our proposed training method, substantiating our theoretical framework concerning CF and model function dynamics. We plan to make our code publicly accessible in the near future.

**Comment:** The paper provides theoretical insights into catastrophic forgetting in LLMs using function vectors, which aligns with foundational research in representation learning and training dynamics.

**Relevance:** 9
**Novelty:** 8

---

## 21. [An Efficient Row-Based Sparse Fine-Tuning](https://arxiv.org/abs/2502.11439) <a id="link21"></a>

**ArXiv ID:** 2502.11439

**Authors:** Cen-Jhih Li, Aditya Bhaskara

**Abstract:** Fine-tuning is an important step in adapting foundation models such as large language models to downstream tasks. To make this step more accessible to users with limited computational budgets, it is crucial to develop fine-tuning methods that are memory and computationally efficient. Sparse Fine-tuning (SFT) and Low-rank adaptation (LoRA) are two frameworks that have emerged for addressing this problem and have been adopted widely in practice. In this work, we develop a new SFT framework, based on ideas from neural network pruning. At a high level, we first identify "important" neurons/nodes using feature importance metrics from network pruning (specifically, we use the structural pruning method), and then perform fine-tuning by restricting to weights involving these neurons. Using experiments on common language tasks, we demonstrate that our method significantly improves the memory efficiency of SFT without increasing training time complexity and implementation complexity, while achieving accuracy comparable to state-of-the-art methods such as LoRA and its variants.

**Comment:** The paper introduces a new sparse fine-tuning framework inspired by neural network pruning, which aligns with the model compression criterion, specifically sparsity and pruning. It also compares its method to state-of-the-art approaches like LoRA, adding theoretical and practical insights.

**Relevance:** 9
**Novelty:** 8

---

## 22. [MoHAVE: Mixture of Hierarchical Audio-Visual Experts for Robust Speech Recognition](https://arxiv.org/abs/2502.10447) <a id="link22"></a>

**ArXiv ID:** 2502.10447

**Authors:** Sungnyun Kim, Kangwook Jang, Sangmin Bae, Sungwoo Cho, Se-Young Yun

**Abstract:** Audio-visual speech recognition (AVSR) has become critical for enhancing speech recognition in noisy environments by integrating both auditory and visual modalities. However, existing AVSR systems struggle to scale up without compromising computational efficiency. In this study, we introduce MoHAVE (Mixture of Hierarchical Audio-Visual Experts), a novel robust AVSR framework designed to address these scalability constraints. By leveraging a Mixture-of-Experts (MoE) architecture, MoHAVE activates modality-specific expert groups, ensuring dynamic adaptation to various audio-visual inputs with minimal computational overhead. Key contributions of MoHAVE include: (1) a sparse MoE framework that efficiently scales AVSR model capacity, (2) a hierarchical gating mechanism that dynamically utilizes the expert groups based on input context, enhancing adaptability and robustness, and (3) remarkable performance across robust AVSR benchmarks, including LRS3 and MuAViC transcription and translation tasks, setting a new standard for scalable speech recognition systems.

**Comment:** The paper introduces a Mixture-of-Experts (MoE) framework for audio-visual speech recognition, aligning with the model architecture criterion, particularly MoE innovations.

**Relevance:** 9
**Novelty:** 7

---

## 23. [LEAPS: A discrete neural sampler via locally equivariant networks](https://arxiv.org/abs/2502.10843) <a id="link23"></a>

**ArXiv ID:** 2502.10843

**Authors:** Peter Holderrieth, Michael S. Albergo, Tommi Jaakkola

**Abstract:** We propose LEAPS, an algorithm to sample from discrete distributions known up to normalization by learning a rate matrix of a continuous-time Markov chain (CTMC). LEAPS can be seen as a continuous-time formulation of annealed importance sampling and sequential Monte Carlo methods, extended so that the variance of the importance weights is offset by the inclusion of the CTMC. To derive these importance weights, we introduce a set of Radon-Nikodym derivatives of CTMCs over their path measures. Because the computation of these weights is intractable with standard neural network parameterizations of rate matrices, we devise a new compact representation for rate matrices via what we call locally equivariant functions. To parameterize them, we introduce a family of locally equivariant multilayer perceptrons, attention layers, and convolutional networks, and provide an approach to make deep networks that preserve the local equivariance. This property allows us to propose a scalable training algorithm for the rate matrix such that the variance of the importance weights associated to the CTMC are minimal. We demonstrate the efficacy of LEAPS on problems in statistical physics.

**Comment:** The paper introduces LEAPS, a discrete neural sampler with locally equivariant networks, which is a novel approach to parameterizing rate matrices and aligns with emerging trends in representation learning.

**Relevance:** 8
**Novelty:** 8

---

## 24. [Raising the Bar in Graph OOD Generalization: Invariant Learning Beyond Explicit Environment Modeling](https://arxiv.org/abs/2502.10706) <a id="link24"></a>

**ArXiv ID:** 2502.10706

**Authors:** Xu Shen, Yixin Liu, Yili Wang, Rui Miao, Yiwei Dai, Shirui Pan, Xin Wang

**Abstract:** Out-of-distribution (OOD) generalization has emerged as a critical challenge in graph learning, as real-world graph data often exhibit diverse and shifting environments that traditional models fail to generalize across. A promising solution to address this issue is graph invariant learning (GIL), which aims to learn invariant representations by disentangling label-correlated invariant subgraphs from environment-specific subgraphs. However, existing GIL methods face two major challenges: (1) the difficulty of capturing and modeling diverse environments in graph data, and (2) the semantic cliff, where invariant subgraphs from different classes are difficult to distinguish, leading to poor class separability and increased misclassifications. To tackle these challenges, we propose a novel method termed Multi-Prototype Hyperspherical Invariant Learning (MPHIL), which introduces two key innovations: (1) hyperspherical invariant representation extraction, enabling robust and highly discriminative hyperspherical invariant feature extraction, and (2) multi-prototype hyperspherical classification, which employs class prototypes as intermediate variables to eliminate the need for explicit environment modeling in GIL and mitigate the semantic cliff issue. Derived from the theoretical framework of GIL, we introduce two novel objective functions: the invariant prototype matching loss to ensure samples are matched to the correct class prototypes, and the prototype separation loss to increase the distinction between prototypes of different classes in the hyperspherical space. Extensive experiments on 11 OOD generalization benchmark datasets demonstrate that MPHIL achieves state-of-the-art performance, significantly outperforming existing methods across graph data from various domains and with different distribution shifts.

**Comment:** The paper proposes a novel invariant learning method for graph OOD generalization, which aligns with representation learning and emerging trends in robust graph learning.

**Relevance:** 8
**Novelty:** 8

---

## 25. [Why Domain Generalization Fail? A View of Necessity and Sufficiency](https://arxiv.org/abs/2502.10716) <a id="link25"></a>

**ArXiv ID:** 2502.10716

**Authors:** Long-Tung Vuong, Vy Vo, Hien Dang, Van-Anh Nguyen, Thanh-Toan Do, Mehrtash Harandi, Trung Le, Dinh Phung

**Abstract:** Despite a strong theoretical foundation, empirical experiments reveal that existing domain generalization (DG) algorithms often fail to consistently outperform the ERM baseline. We argue that this issue arises because most DG studies focus on establishing theoretical guarantees for generalization under unrealistic assumptions, such as the availability of sufficient, diverse (or even infinite) domains or access to target domain knowledge. As a result, the extent to which domain generalization is achievable in scenarios with limited domains remains largely unexplored. This paper seeks to address this gap by examining generalization through the lens of the conditions necessary for its existence and learnability. Specifically, we systematically establish a set of necessary and sufficient conditions for generalization. Our analysis highlights that existing DG methods primarily act as regularization mechanisms focused on satisfying sufficient conditions, while often neglecting necessary ones. However, sufficient conditions cannot be verified in settings with limited training domains. In such cases, regularization targeting sufficient conditions aims to maximize the likelihood of generalization, whereas regularization targeting necessary conditions ensures its existence. Using this analysis, we reveal the shortcomings of existing DG algorithms by showing that, while they promote sufficient conditions, they inadvertently violate necessary conditions. To validate our theoretical insights, we propose a practical method that promotes the sufficient condition while maintaining the necessary conditions through a novel subspace representation alignment strategy. This approach highlights the advantages of preserving the necessary conditions on well-established DG benchmarks.

**Comment:** The paper critiques domain generalization methods and proposes a novel subspace alignment strategy, which aligns with representation learning and foundational research.

**Relevance:** 8
**Novelty:** 8

---

## 26. [PEA: Enhancing LLM Performance on Computational-Reasoning Tasks](https://arxiv.org/abs/2502.10938) <a id="link26"></a>

**ArXiv ID:** 2502.10938

**Authors:** Zi Wang, Shiwei Weng, Mohannad Alhanahnah, Somesh Jha, Tom Reps

**Abstract:** Large Language Models (LLMs) have exhibited remarkable capabilities across diverse domains, prompting investigations into their potential as generic reasoning engines. While recent studies have explored inference-time computation to enhance model performance on complex problems, current research lacks a formal framework to characterize the complexity of reasoning tasks. This study introduces the Predicate-Enumeration-Aggregation (PEA) framework, a formal approach to describe and solve a class of important reasoning tasks termed computational reasoning problems. The PEA framework decomposes these problems into predicate and enumeration components, using LLMs to synthesize programs based on specified predicates, enumeration, and aggregation rules. These synthesized programs are then executed to obtain solutions to the computational tasks. We demonstrate the framework's efficacy on benchmark tasks including Boolean satisfiability problems, game of $24$, and planning problems. Empirical evaluation reveals that PEA substantially enhances the performance of underlying models on benchmark computational problems, yielding an average accuracy improvement of approximately $50\%$, coupled with increased efficiency.

**Comment:** The PEA framework introduces a formal approach to computational reasoning tasks, which is relevant to foundational research in LLMs.

**Relevance:** 8
**Novelty:** 8

---

## 27. [Towards Data-Efficient Pretraining for Atomic Property Prediction](https://arxiv.org/abs/2502.11085) <a id="link27"></a>

**ArXiv ID:** 2502.11085

**Authors:** Yasir Ghunaim, Hasan Abed Al Kader Hammoud, Bernard Ghanem

**Abstract:** This paper challenges the recent paradigm in atomic property prediction that links progress to growing dataset sizes and computational resources. We show that pretraining on a carefully selected, task-relevant dataset can match or even surpass large-scale pretraining, while using as little as 1/24th of the computational cost. We introduce the Chemical Similarity Index (CSI), a novel metric inspired by computer vision's Fr\'echet Inception Distance, for molecular graphs which quantifies the alignment between upstream pretraining datasets and downstream tasks. By selecting the most relevant dataset with minimal CSI distance, we show that models pretrained on a smaller, focused dataset consistently outperform those pretrained on massive, mixed datasets such as JMP, even when those larger datasets include the relevant dataset. Counterintuitively, we also find that indiscriminately adding more data can degrade model performance when the additional data poorly aligns with the task at hand. Our findings highlight that quality often outperforms quantity in pretraining for atomic property prediction.

**Comment:** The paper challenges the paradigm of large-scale pretraining for atomic property prediction and introduces a novel metric (CSI) for dataset alignment, which is relevant to representation learning and efficiency.

**Relevance:** 8
**Novelty:** 8

---

## 28. [Sharp-PINNs: staggered hard-constrained physics-informed neural networks for phase field modelling of corrosion](https://arxiv.org/abs/2502.11942) <a id="link28"></a>

**ArXiv ID:** 2502.11942

**Authors:** Nanxi Chen, Chuanjie Cui, Rujin Ma, Airong Chen, Sifan Wang

**Abstract:** Physics-informed neural networks have shown significant potential in solving partial differential equations (PDEs) across diverse scientific fields. However, their performance often deteriorates when addressing PDEs with intricate and strongly coupled solutions. In this work, we present a novel Sharp-PINN framework to tackle complex phase field corrosion problems. Instead of minimizing all governing PDE residuals simultaneously, the Sharp-PINNs introduce a staggered training scheme that alternately minimizes the residuals of Allen-Cahn and Cahn-Hilliard equations, which govern the corrosion system. To further enhance its efficiency and accuracy, we design an advanced neural network architecture that integrates random Fourier features as coordinate embeddings, employs a modified multi-layer perceptron as the primary backbone, and enforces hard constraints in the output layer. This framework is benchmarked through simulations of corrosion problems with multiple pits, where the staggered training scheme and network architecture significantly improve both the efficiency and accuracy of PINNs. Moreover, in three-dimensional cases, our approach is 5-10 times faster than traditional finite element methods while maintaining competitive accuracy, demonstrating its potential for real-world engineering applications in corrosion prediction.

**Comment:** The paper introduces a novel PINN framework with architectural innovations for solving PDEs, which aligns with foundational research in model architecture and efficiency.

**Relevance:** 8
**Novelty:** 8

---

## 29. [Learning the Exact Time Integration Algorithm for Initial Value Problems by Randomized Neural Networks](https://arxiv.org/abs/2502.10949) <a id="link29"></a>

**ArXiv ID:** 2502.10949

**Authors:** Suchuan Dong, Naxian Ni

**Abstract:** We present a method leveraging extreme learning machine (ELM) type randomized neural networks (NNs) for learning the exact time integration algorithm for initial value problems (IVPs). The exact time integration algorithm for non-autonomous systems can be represented by an algorithmic function in higher dimensions, which satisfies an associated system of partial differential equations with corresponding boundary conditions. Our method learns the algorithmic function by solving this associated system using ELM with a physics informed approach. The trained ELM network serves as the learned algorithm and can be used to solve the IVP with arbitrary initial data or step sizes from some domain. When the right hand side of the non-autonomous system exhibits a periodicity with respect to any of its arguments, while the solution itself to the problem is not periodic, we show that the algorithmic function is either periodic, or when it is not, satisfies a well-defined relation for different periods. This property can greatly simplify the algorithm learning in many problems. We consider explicit and implicit NN formulations, leading to explicit or implicit time integration algorithms, and discuss how to train the ELM network by the nonlinear least squares method. Extensive numerical experiments with benchmark problems, including non-stiff, stiff and chaotic systems, show that the learned NN algorithm produces highly accurate solutions in long-time simulations, with its time-marching errors decreasing nearly exponentially with increasing degrees of freedom in the neural network. We compare extensively the computational performance (accuracy vs.~cost) between the current NN algorithm and the leading traditional time integration algorithms. The learned NN algorithm is computationally competitive, markedly outperforming the traditional algorithms in many problems.

**Comment:** The paper introduces a novel method for learning time integration algorithms using randomized neural networks, which is relevant to efficiency and foundational research in neural network training.

**Relevance:** 8
**Novelty:** 8

---

## 30. [Preconditioned Inexact Stochastic ADMM for Deep Model](https://arxiv.org/abs/2502.10784) <a id="link30"></a>

**ArXiv ID:** 2502.10784

**Authors:** Shenglong Zhou, Ouya Wang, Ziyan Luo, Yongxu Zhu, Geoffrey Ye Li

**Abstract:** The recent advancement of foundation models (FMs) has brought about a paradigm shift, revolutionizing various sectors worldwide. The popular optimizers used to train these models are stochastic gradient descent-based algorithms, which face inherent limitations, such as slow convergence and stringent assumptions for convergence. In particular, data heterogeneity arising from distributed settings poses significant challenges to their theoretical and numerical performance. This paper develops an algorithm, PISA ({P}reconditioned {I}nexact {S}tochastic {A}lternating Direction Method of Multipliers), which enables scalable parallel computing and supports various second-moment schemes. Grounded in rigorous theoretical guarantees, the algorithm converges under the sole assumption of Lipschitz continuity of the gradient, thereby removing the need for other conditions commonly imposed by stochastic methods. This capability enables PISA to tackle the challenge of data heterogeneity effectively. Comprehensive experimental evaluations for training or fine-tuning diverse FMs, including vision models, large language models, reinforcement learning models, generative adversarial networks, and recurrent neural networks, demonstrate its superior numerical performance compared to various state-of-the-art optimizers.

**Comment:** The paper proposes a new stochastic ADMM algorithm for training foundation models, which aligns with 'Emerging Trends' by addressing optimization challenges in distributed settings.

**Relevance:** 8
**Novelty:** 8

---

## 31. [LIMR: Less is More for RL Scaling](https://arxiv.org/abs/2502.11886) <a id="link31"></a>

**ArXiv ID:** 2502.11886

**Authors:** Xuefeng Li, Haoyang Zou, Pengfei Liu

**Abstract:** In this paper, we ask: what truly determines the effectiveness of RL training data for enhancing language models' reasoning capabilities? While recent advances like o1, Deepseek R1, and Kimi1.5 demonstrate RL's potential, the lack of transparency about training data requirements has hindered systematic progress. Starting directly from base models without distillation, we challenge the assumption that scaling up RL training data inherently improves performance. we demonstrate that a strategically selected subset of just 1,389 samples can outperform the full 8,523-sample dataset. We introduce Learning Impact Measurement (LIM), an automated method to evaluate and prioritize training samples based on their alignment with model learning trajectories, enabling efficient resource utilization and scalable implementation. Our method achieves comparable or even superior performance using only 1,389 samples versus the full 8,523 samples dataset. Notably, while recent data-efficient approaches (e.g., LIMO and s1) show promise with 32B-scale models, we find it significantly underperforms at 7B-scale through supervised fine-tuning (SFT). In contrast, our RL-based LIMR achieves 16.7% higher accuracy on AIME24 and outperforms LIMO and s1 by 13.0% and 22.2% on MATH500. These results fundamentally reshape our understanding of RL scaling in LLMs, demonstrating that precise sample selection, rather than data scale, may be the key to unlocking enhanced reasoning capabilities. For reproducible research and future innovation, we are open-sourcing LIMR, including implementation of LIM, training and evaluation code, curated datasets, and trained models at https://github.com/GAIR-NLP/LIMR.

**Comment:** The paper explores RL scaling for LLMs and introduces a novel sample selection method, which provides insights into training dynamics and efficiency improvements.

**Relevance:** 8
**Novelty:** 8

---

## 32. [Adversarial Alignment for LLMs Requires Simpler, Reproducible, and More Measurable Objectives](https://arxiv.org/abs/2502.11910) <a id="link32"></a>

**ArXiv ID:** 2502.11910

**Authors:** Leo Schwinn, Yan Scholten, Tom Wollschl\"ager, Sophie Xhonneux, Stephen Casper, Stephan G\"unnemann, Gauthier Gidel

**Abstract:** Misaligned research objectives have considerably hindered progress in adversarial robustness research over the past decade. For instance, an extensive focus on optimizing target metrics, while neglecting rigorous standardized evaluation, has led researchers to pursue ad-hoc heuristic defenses that were seemingly effective. Yet, most of these were exposed as flawed by subsequent evaluations, ultimately contributing little measurable progress to the field. In this position paper, we illustrate that current research on the robustness of large language models (LLMs) risks repeating past patterns with potentially worsened real-world implications. To address this, we argue that realigned objectives are necessary for meaningful progress in adversarial alignment. To this end, we build on established cybersecurity taxonomy to formally define differences between past and emerging threat models that apply to LLMs. Using this framework, we illustrate that progress requires disentangling adversarial alignment into addressable sub-problems and returning to core academic principles, such as measureability, reproducibility, and comparability. Although the field presents significant challenges, the fresh start on adversarial robustness offers the unique opportunity to build on past experience while avoiding previous mistakes.

**Comment:** The paper discusses the need for simpler and more measurable objectives in adversarial alignment for LLMs, which aligns with emerging trends in improving robustness and interpretability.

**Relevance:** 8
**Novelty:** 7

---

## 33. [Dynamic Chain-of-Thought: Towards Adaptive Deep Reasoning](https://arxiv.org/abs/2502.10428) <a id="link33"></a>

**ArXiv ID:** 2502.10428

**Authors:** Libo Wang

**Abstract:** To reduce the cost and consumption of computing resources caused by computational redundancy and delayed reward assignment in long CoT, this research proposes the dynamic chain-of-thought with adaptive reasoning time and steps. The researcher used simulation experiment to simulate the integration of D-CoT through Python 3.13 IDLE combined with a Python simulator based on GPTs. At the same time, the researcher used DeepSeek R1 as a control group to test and compare the performance of the D-CoT simulator in processing MIT OpenCourseWare's linear algebra exam questions. Experimental results show that D-CoT is better than DeepSeek R1 based on long CoT in three indicators: reasoning time, CoT length (reasoning steps) and token count, which achieves a significant reduction in computing resource consumption. In addition, this research has potential value in deep reasoning optimization and can be used as a reference for future dynamic deep reasoning frameworks.

**Comment:** The paper proposes a dynamic chain-of-thought reasoning framework, which aligns with conditional/dynamic networks and optimization of reasoning steps, making it relevant to model architecture innovations.

**Relevance:** 8
**Novelty:** 7

---

## 34. [Deep Subspace Learning for Surface Anomaly Classification Based on 3D Point Cloud Data](https://arxiv.org/abs/2502.11669) <a id="link34"></a>

**ArXiv ID:** 2502.11669

**Authors:** Xuanming Cao, Chengyu Tao, Juan Du

**Abstract:** Surface anomaly classification is critical for manufacturing system fault diagnosis and quality control. However, the following challenges always hinder accurate anomaly classification in practice: (i) Anomaly patterns exhibit intra-class variation and inter-class similarity, presenting challenges in the accurate classification of each sample. (ii) Despite the predefined classes, new types of anomalies can occur during production that require to be detected accurately. (iii) Anomalous data is rare in manufacturing processes, leading to limited data for model learning. To tackle the above challenges simultaneously, this paper proposes a novel deep subspace learning-based 3D anomaly classification model. Specifically, starting from a lightweight encoder to extract the latent representations, we model each class as a subspace to account for the intra-class variation, while promoting distinct subspaces of different classes to tackle the inter-class similarity. Moreover, the explicit modeling of subspaces offers the capability to detect out-of-distribution samples, i.e., new types of anomalies, and the regularization effect with much fewer learnable parameters of our proposed subspace classifier, compared to the popular Multi-Layer Perceptions (MLPs). Extensive numerical experiments demonstrate our method achieves better anomaly classification results than benchmark methods, and can effectively identify the new types of anomalies.

**Comment:** The paper proposes a deep subspace learning method for anomaly classification, which involves representation learning through subspace modeling, making it relevant to foundational research.

**Relevance:** 8
**Novelty:** 7

---

## 35. [Dictionary-Learning-Based Data Pruning for System Identification](https://arxiv.org/abs/2502.11484) <a id="link35"></a>

**ArXiv ID:** 2502.11484

**Authors:** Tingna Wang (Department of Bridge Engineering, Tongji University, Shanghai, China, Shanghai Qi Zhi Institute, Shanghai, China), Sikai Zhang (Baosight Software), Limin Sun (Department of Bridge Engineering, Tongji University, Shanghai, China, State Key Laboratory of Disaster Reduction in Civil Engineering, Tongji University, Shanghai, China)

**Abstract:** System identification is normally involved in augmenting time series data by time shifting and nonlinearisation (via polynomial basis), which introduce redundancy both feature-wise and sample-wise. Many research works focus on reducing redundancy feature-wise, while less attention is paid to sample-wise redundancy. This paper proposes a novel data pruning method, called (mini-batch) FastCan, to reduce sample-wise redundancy based on dictionary learning. Time series data is represented by some representative samples, called atoms, via dictionary learning. The useful samples are selected based on their correlation with the atoms. The method is tested on one simulated dataset and two benchmark datasets. The R-squared between the coefficients of models trained on the full and the coefficients of models trained on pruned datasets is adopted to evaluate the performance of data pruning methods. It is found that the proposed method significantly outperforms the random pruning method.

**Comment:** The paper proposes a dictionary-learning-based data pruning method, which aligns with model compression and sparsity techniques.

**Relevance:** 8
**Novelty:** 7

---

## 36. [Towards Reasoning Ability of Small Language Models](https://arxiv.org/abs/2502.11569) <a id="link36"></a>

**ArXiv ID:** 2502.11569

**Authors:** Gaurav Srivastava, Shuxiang Cao, Xuan Wang

**Abstract:** Reasoning has long been viewed as an emergent property of large language models (LLMs), appearing at or above a certain scale ($\sim$100B parameters). However, recent studies challenge this assumption, showing that small language models (SLMs) can also achieve competitive reasoning performance. SLMs are increasingly favored for their efficiency and deployability. However, there is a lack of systematic study on the reasoning abilities of diverse SLMs, including those trained from scratch or derived from LLMs through quantization, pruning, and distillation. This raises a critical question: Can SLMs achieve reasoning abilities comparable to LLMs? In this work, we systematically survey, benchmark, and analyze 72 SLMs from six model families across 14 reasoning benchmarks. For reliable evaluation, we examine four evaluation methods and compare four LLM judges against human evaluations on 800 data points. We repeat all experiments three times to ensure a robust performance assessment. Additionally, we analyze the impact of different prompting strategies in small models. Beyond accuracy, we also evaluate model robustness under adversarial conditions and intermediate reasoning steps. Our findings challenge the assumption that scaling is the only way to achieve strong reasoning. Instead, we foresee a future where SLMs with strong reasoning capabilities can be developed through structured training or post-training compression. They can serve as efficient alternatives to LLMs for reasoning-intensive tasks.

**Comment:** The paper systematically studies reasoning abilities in small language models, including compression techniques like pruning and quantization, aligning with the 'Model Compression' and 'Large Language Models' criteria.

**Relevance:** 8
**Novelty:** 7

---

## 37. [K-Edit: Language Model Editing with Contextual Knowledge Awareness](https://arxiv.org/abs/2502.10626) <a id="link37"></a>

**ArXiv ID:** 2502.10626

**Authors:** Elan Markowitz, Anil Ramakrishna, Ninareh Mehrabi, Charith Peris, Rahul Gupta, Kai-Wei Chang, Aram Galstyan

**Abstract:** As the world changes, we need to be able to update our models and correct false information without costly retraining. Knowledge-based model editing enables precise modifications to the weights of large language models in order to modify the information encoded within. Recent approaches have seen success in enabling recall of edited information for thousands of edits at once. However, these approaches fail to produce edits that account for associated contextual information. We present K-Edit, an effective approach to generating contextually consistent knowledge edits. By using knowledge graphs, which maintain contextual consistency when an edge is edited, we are able to generate additional \textit{contextual edits} that ensure consistency of related information in the language model. Our experiments demonstrate significant improvements in multi-hop question answering while maintaining the general effectiveness and scalability of model edits.

**Comment:** The paper introduces K-Edit, a method for contextually consistent knowledge editing in LLMs, which aligns with foundational research in LLM behavior and interpretability.

**Relevance:** 8
**Novelty:** 7

---

## 38. [A Neural Network Training Method Based on Neuron Connection Coefficient Adjustments](https://arxiv.org/abs/2502.10414) <a id="link38"></a>

**ArXiv ID:** 2502.10414

**Authors:** Kun Jiang

**Abstract:** In previous studies, we introduced a neural network framework based on symmetric differential equations, along with one of its training methods. In this article, we present another training approach for this neural network. This method leverages backward signal propagation and eliminates reliance on the traditional chain derivative rule, offering a high degree of biological interpretability. Unlike the previously introduced method, this approach does not require adjustments to the fixed points of the differential equations. Instead, it focuses solely on modifying the connection coefficients between neurons, closely resembling the training process of traditional multilayer perceptron (MLP) networks. By adopting a suitable adjustment strategy, this method effectively avoids certain potential local minima. To validate this approach, we tested it on the MNIST dataset and achieved promising results. Through further analysis, we identified certain limitations of the current neural network architecture and proposed measures for improvement.

**Comment:** The paper introduces a biologically inspired training method for neural networks, which aligns with foundational research in training dynamics and architecture.

**Relevance:** 8
**Novelty:** 7

---

## 39. [The Rotary Position Embedding May Cause Dimension Inefficiency in Attention Heads for Long-Distance Retrieval](https://arxiv.org/abs/2502.11276) <a id="link39"></a>

**ArXiv ID:** 2502.11276

**Authors:** Ting-Rui Chiang, Dani Yogatama

**Abstract:** The Rotary Position Embedding (RoPE) is widely used in the attention heads of many large language models (LLM). It rotates dimensions in the query and the key vectors by different angles according to their positions in the input sequence. For long context modeling, the range of positions may vary a lot, and thus RoPE rotates some dimensions by a great range of angles. We hypothesize that the wide range of rotation angles may prevent LLMs from utilizing those dimensions. To validate this hypothesis, we present a controlled experiment showing that applying RoPE causes low utility of certain dimensions. Our analyses on three LLMs also indicate that these dimensions do not help LLMs do long-context question answering.

**Comment:** The paper critiques RoPE embeddings in LLMs and provides insights into their inefficiencies, aligning with the model architecture criteria.

**Relevance:** 8
**Novelty:** 7

---

## 40. [Neural Networks Remember More: The Power of Parameter Isolation and Combination](https://arxiv.org/abs/2502.10966) <a id="link40"></a>

**ArXiv ID:** 2502.10966

**Authors:** Biqing Zeng, Zehan Li, Aladdin Ayesh

**Abstract:** Catastrophic forgetting is a pervasive issue for pre-trained language models (PLMs) during continual learning, where models lose previously acquired knowledge when sequentially trained on a series of tasks. The model's ability to retain old tasks is referred to as stability, while its adaptability to new tasks is called plasticity. Therefore, the key to solving this problem is to find a trade-off between the plasticity and stability of the model. To address this issue, in this paper, we propose a novel method to achieve a balance between model stability and plasticity, thereby mitigating catastrophic forgetting. More specifically, our proposed approach leverages parameter isolation and a subsequent combination strategy. Initially, in the training stage, the model adapts to each downstream task via a parameter isolation method to prevent potential interference among different tasks. We then combine all trained parameters, which contain acquired knowledge, using the task arithmetic method and finally apply them to the backbone model. Empirical evaluations on continual language learning benchmarks substantiate the effectiveness of our approach, revealing a marked enhancement over existing state-of-the-art approaches.

**Comment:** The paper proposes a parameter isolation and combination strategy to address catastrophic forgetting, which is relevant to representation learning and training dynamics.

**Relevance:** 8
**Novelty:** 7

---

## 41. [Why is prompting hard? Understanding prompts on binary sequence predictors](https://arxiv.org/abs/2502.10760) <a id="link41"></a>

**ArXiv ID:** 2502.10760

**Authors:** Li Kevin Wenliang, Anian Ruoss, Jordi Grau-Moya, Marcus Hutter, Tim Genewein

**Abstract:** Large language models (LLMs) can be prompted to do many tasks, but finding good prompts is not always easy, nor is understanding some performant prompts. We explore these issues by viewing prompting as conditioning a near-optimal sequence predictor (LLM) pretrained on diverse data sources. Through numerous prompt search experiments, we show that the unintuitive patterns in optimal prompts can be better understood given the pretraining distribution, which is often unavailable in practice. Moreover, even using exhaustive search, reliably identifying optimal prompts from practical neural predictors can be difficult. Further, we demonstrate that common prompting methods, such as using intuitive prompts or samples from the targeted task, are in fact suboptimal. Thus, this work takes an initial step towards understanding the difficulties in finding and understanding optimal prompts from a statistical and empirical perspective.

**Comment:** The paper provides theoretical insights into prompting challenges for LLMs, aligning with representation learning and interpretability of LLM behavior.

**Relevance:** 8
**Novelty:** 7

---

## 42. [Minimal Ranks, Maximum Confidence: Parameter-efficient Uncertainty Quantification for LoRA](https://arxiv.org/abs/2502.12122) <a id="link42"></a>

**ArXiv ID:** 2502.12122

**Authors:** Patryk Marsza{\l}ek, Klaudia Ba{\l}azy, Jacek Tabor, Tomasz Ku\'smierczyk

**Abstract:** Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning of large language models by decomposing weight updates into low-rank matrices, significantly reducing storage and computational overhead. While effective, standard LoRA lacks mechanisms for uncertainty quantification, leading to overconfident and poorly calibrated models. Bayesian variants of LoRA address this limitation, but at the cost of a significantly increased number of trainable parameters, partially offsetting the original efficiency gains. Additionally, these models are harder to train and may suffer from unstable convergence.   In this work, we propose a novel parameter-efficient Bayesian LoRA, demonstrating that effective uncertainty quantification can be achieved in very low-dimensional parameter spaces. The proposed method achieves strong performance with improved calibration and generalization while maintaining computational efficiency. Our empirical findings show that, with the appropriate projection of the weight space: (1) uncertainty can be effectively modeled in a low-dimensional space, and (2) weight covariances exhibit low ranks.

**Comment:** The paper proposes a Bayesian variant of LoRA for uncertainty quantification, which aligns with the 'Model Compression' criterion by addressing low-rank adaptation and efficiency.

**Relevance:** 8
**Novelty:** 7

---

## 43. [InfiR : Crafting Effective Small Language Models and Multimodal Small Language Models in Reasoning](https://arxiv.org/abs/2502.11573) <a id="link43"></a>

**ArXiv ID:** 2502.11573

**Authors:** Congkai Xie, Shuo Cai, Wenjun Wang, Pengxiang Li, Zhijie Sang, Kejing Yang, Yiming Zhang, Zhen Li, Guanghao Zhu, Zeyu Liu, Yang Yu, Yuhang Liu, Su Lu, Baoyi He, Qi Zhou, Xiaotian Han, Jianbo Yuan, Shengyu Zhang, Fei Wu, Hongxia Yang

**Abstract:** Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have made significant advancements in reasoning capabilities. However, they still face challenges such as high computational demands and privacy concerns. This paper focuses on developing efficient Small Language Models (SLMs) and Multimodal Small Language Models (MSLMs) that retain competitive reasoning abilities. We introduce a novel training pipeline that enhances reasoning capabilities and facilitates deployment on edge devices, achieving state-of-the-art performance while minimizing development costs. \InfR~ aims to advance AI systems by improving reasoning, reducing adoption barriers, and addressing privacy concerns through smaller model sizes. Resources are available at https://github. com/Reallm-Labs/InfiR.

**Comment:** The paper discusses efficient small language models for reasoning, which aligns with the 'Large Language Models' criterion by addressing computational efficiency and reasoning capabilities.

**Relevance:** 8
**Novelty:** 7

---

## 44. [Navigating the Helpfulness-Truthfulness Trade-Off with Uncertainty-Aware Instruction Fine-Tuning](https://arxiv.org/abs/2502.11962) <a id="link44"></a>

**ArXiv ID:** 2502.11962

**Authors:** Tianyi Wu, Jingwei Ni, Bryan Hooi, Jiaheng Zhang, Elliott Ash, See-Kiong Ng, Mrinmaya Sachan, Markus Leippold

**Abstract:** Instruction Fine-tuning (IFT) can enhance the helpfulness of Large Language Models (LLMs), but it may lower their truthfulness. This trade-off arises because IFT steers LLMs to generate responses with long-tail knowledge that is not well covered during pre-training, leading to more informative but less truthful answers when generalizing to unseen tasks. In this paper, we empirically demonstrate this helpfulness-truthfulness trade-off in IFT and propose $\textbf{UNIT}$, a novel IFT paradigm to address it. UNIT teaches LLMs to recognize their uncertainty and explicitly reflect it at the end of their responses. Experimental results show that UNIT-tuned models maintain their helpfulness while distinguishing between certain and uncertain claims, thereby reducing hallucinations.

**Comment:** The paper addresses the helpfulness-truthfulness trade-off in instruction fine-tuning, which aligns with 'Large Language Models' by providing insights into fine-tuning paradigms.

**Relevance:** 8
**Novelty:** 7

---

## 45. [Revealing Bias Formation in Deep Neural Networks Through the Geometric Mechanisms of Human Visual Decoupling](https://arxiv.org/abs/2502.11809) <a id="link45"></a>

**ArXiv ID:** 2502.11809

**Authors:** Yanbiao Ma, Bowei Liu, Wei Dai, Jiayi Chen, Shuo Li

**Abstract:** Deep neural networks (DNNs) often exhibit biases toward certain categories during object recognition, even under balanced training data conditions. The intrinsic mechanisms underlying these biases remain unclear. Inspired by the human visual system, which decouples object manifolds through hierarchical processing to achieve object recognition, we propose a geometric analysis framework linking the geometric complexity of class-specific perceptual manifolds in DNNs to model bias. Our findings reveal that differences in geometric complexity can lead to varying recognition capabilities across categories, introducing biases. To support this analysis, we present the Perceptual-Manifold-Geometry library, designed for calculating the geometric properties of perceptual manifolds.

**Comment:** The paper explores geometric mechanisms in DNN biases, which aligns with representation learning, particularly insights into how networks encode information.

**Relevance:** 8
**Novelty:** 7

---

## 46. [Analysis of Overparameterization in Continual Learning under a Linear Model](https://arxiv.org/abs/2502.10442) <a id="link46"></a>

**ArXiv ID:** 2502.10442

**Authors:** Daniel Goldfarb, Paul Hand

**Abstract:** Autonomous machine learning systems that learn many tasks in sequence are prone to the catastrophic forgetting problem. Mathematical theory is needed in order to understand the extent of forgetting during continual learning. As a foundational step towards this goal, we study continual learning and catastrophic forgetting from a theoretical perspective in the simple setting of gradient descent with no explicit algorithmic mechanism to prevent forgetting. In this setting, we analytically demonstrate that overparameterization alone can mitigate forgetting in the context of a linear regression model. We consider a two-task setting motivated by permutation tasks, and show that as the overparameterization ratio becomes sufficiently high, a model trained on both tasks in sequence results in a low-risk estimator for the first task. As part of this work, we establish a non-asymptotic bound of the risk of a single linear regression task, which may be of independent interest to the field of double descent theory.

**Comment:** The paper provides theoretical analysis of overparameterization in continual learning, which contributes to foundational understanding of training dynamics and representation learning.

**Relevance:** 8
**Novelty:** 7

---

## 47. [Statistical Query Hardness of Multiclass Linear Classification with Random Classification Noise](https://arxiv.org/abs/2502.11413) <a id="link47"></a>

**ArXiv ID:** 2502.11413

**Authors:** Ilias Diakonikolas, Mingchen Ma, Lisheng Ren, Christos Tzamos

**Abstract:** We study the task of Multiclass Linear Classification (MLC) in the distribution-free PAC model with Random Classification Noise (RCN). Specifically, the learner is given a set of labeled examples $(x, y)$, where $x$ is drawn from an unknown distribution on $R^d$ and the labels are generated by a multiclass linear classifier corrupted with RCN. That is, the label $y$ is flipped from $i$ to $j$ with probability $H_{ij}$ according to a known noise matrix $H$ with non-negative separation $\sigma: = \min_{i \neq j} H_{ii}-H_{ij}$. The goal is to compute a hypothesis with small 0-1 error. For the special case of two labels, prior work has given polynomial-time algorithms achieving the optimal error. Surprisingly, little is known about the complexity of this task even for three labels. As our main contribution, we show that the complexity of MLC with RCN becomes drastically different in the presence of three or more labels. Specifically, we prove super-polynomial Statistical Query (SQ) lower bounds for this problem. In more detail, even for three labels and constant separation, we give a super-polynomial lower bound on the complexity of any SQ algorithm achieving optimal error. For a larger number of labels and smaller separation, we show a super-polynomial SQ lower bound even for the weaker goal of achieving any constant factor approximation to the optimal loss or even beating the trivial hypothesis.

**Comment:** The paper provides theoretical insights into the complexity of multiclass linear classification with random noise, which aligns with 'Emerging Trends' due to its foundational nature in statistical learning theory.

**Relevance:** 7
**Novelty:** 8

---

## 48. [Generative Adversarial Networks for High-Dimensional Item Factor Analysis: A Deep Adversarial Learning Algorithm](https://arxiv.org/abs/2502.10650) <a id="link48"></a>

**ArXiv ID:** 2502.10650

**Authors:** Nanyu Luo, Feng Ji

**Abstract:** Advances in deep learning and representation learning have transformed item factor analysis (IFA) in the item response theory (IRT) literature by enabling more efficient and accurate parameter estimation. Variational Autoencoders (VAEs) have been one of the most impactful techniques in modeling high-dimensional latent variables in this context. However, the limited expressiveness of the inference model based on traditional VAEs can still hinder the estimation performance. This study introduces Adversarial Variational Bayes (AVB) algorithms as an improvement to VAEs for IFA with improved flexibility and accuracy. By bridging the strengths of VAEs and Generative Adversarial Networks (GANs), AVB incorporates an auxiliary discriminator network to reframe the estimation process as a two-player adversarial game and removes the restrictive assumption of standard normal distributions in the inference model. Theoretically, AVB can achieve similar or higher likelihood compared to VAEs. A further enhanced algorithm, Importance-weighted Adversarial Variational Bayes (IWAVB) is proposed and compared with Importance-weighted Autoencoders (IWAE). In an exploratory analysis of real empirical data, IWAVB demonstrated superior expressiveness by achieving a higher likelihood compared to IWAE. In confirmatory studies with simulated data, IWAVB achieved similar mean-square error results to IWAE while consistently achieving higher likelihoods. Moreover, in simulations where latent variables followed a multimodal distribution, IWAVB outperformed IWAE by providing more accurate parameter estimates. With its innovative use of GANs, IWAVB is shown to have the potential to extend IFA to handle large-scale data, facilitating the potential integration of psychometrics and multimodal data analysis.

**Comment:** The paper introduces adversarial variational methods for item factor analysis, which aligns with 'Representation Learning' by proposing a novel generative framework.

**Relevance:** 7
**Novelty:** 8

---

## 49. [One Class Restricted Kernel Machines](https://arxiv.org/abs/2502.10443) <a id="link49"></a>

**ArXiv ID:** 2502.10443

**Authors:** A. Quadir, M. Sajid, M. Tanveer

**Abstract:** Restricted kernel machines (RKMs) have demonstrated a significant impact in enhancing generalization ability in the field of machine learning. Recent studies have introduced various methods within the RKM framework, combining kernel functions with the least squares support vector machine (LSSVM) in a manner similar to the energy function of restricted boltzmann machines (RBM), such that a better performance can be achieved. However, RKM's efficacy can be compromised by the presence of outliers and other forms of contamination within the dataset. These anomalies can skew the learning process, leading to less accurate and reliable outcomes. To address this critical issue and to ensure the robustness of the model, we propose the novel one-class RKM (OCRKM). In the framework of OCRKM, we employ an energy function akin to that of the RBM, which integrates both visible and hidden variables in a nonprobabilistic setting. The formulation of the proposed OCRKM facilitates the seamless integration of one-class classification method with the RKM, enhancing its capability to detect outliers and anomalies effectively. The proposed OCRKM model is evaluated over UCI benchmark datasets. Experimental findings and statistical analyses consistently emphasize the superior generalization capabilities of the proposed OCRKM model over baseline models across all scenarios.

**Comment:** The paper introduces a novel one-class restricted kernel machine (OCRKM) with an energy function inspired by restricted Boltzmann machines, which aligns with representation learning through its focus on robust feature encoding.

**Relevance:** 7
**Novelty:** 7

---

## 50. [Reduced Order Modeling with Shallow Recurrent Decoder Networks](https://arxiv.org/abs/2502.10930) <a id="link50"></a>

**ArXiv ID:** 2502.10930

**Authors:** Matteo Tomasetto, Jan P. Williams, Francesco Braghin, Andrea Manzoni, J. Nathan Kutz

**Abstract:** Reduced Order Modeling is of paramount importance for efficiently inferring high-dimensional spatio-temporal fields in parametric contexts, enabling computationally tractable parametric analyses, uncertainty quantification and control. However, conventional dimensionality reduction techniques are typically limited to known and constant parameters, inefficient for nonlinear and chaotic dynamics, and uninformed to the actual system behavior. In this work, we propose sensor-driven SHallow REcurrent Decoder networks for Reduced Order Modeling (SHRED-ROM). Specifically, we consider the composition of a long short-term memory network, which encodes the temporal dynamics of limited sensor data in multiple scenarios, and a shallow decoder, which reconstructs the corresponding high-dimensional states. SHRED-ROM is a robust decoding-only strategy that circumvents the numerically unstable approximation of an inverse which is required by encoding-decoding schemes. To enhance computational efficiency and memory usage, the full-order state snapshots are reduced by, e.g., proper orthogonal decomposition, allowing for compressive training of the networks with minimal hyperparameter tuning. Through applications on chaotic and nonlinear fluid dynamics, we show that SHRED-ROM (i) accurately reconstructs the state dynamics for new parameter values starting from limited fixed or mobile sensors, independently on sensor placement, (ii) can cope with both physical, geometrical and time-dependent parametric dependencies, while being agnostic to their actual values, (iii) can accurately estimate unknown parameters, and (iv) can deal with different data sources, such as high-fidelity simulations, coupled fields and videos.

**Comment:** The paper introduces SHRED-ROM, a reduced-order modeling approach using shallow recurrent decoder networks, which aligns with representation learning and efficiency in high-dimensional systems.

**Relevance:** 7
**Novelty:** 7

---

## 51. [GraphiT: Efficient Node Classification on Text-Attributed Graphs with Prompt Optimized LLMs](https://arxiv.org/abs/2502.10522) <a id="link51"></a>

**ArXiv ID:** 2502.10522

**Authors:** Shima Khoshraftar, Niaz Abedini, Amir Hajian

**Abstract:** The application of large language models (LLMs) to graph data has attracted a lot of attention recently. LLMs allow us to use deep contextual embeddings from pretrained models in text-attributed graphs, where shallow embeddings are often used for the text at- tributes of nodes. However, it is still challenging to efficiently en- code the graph structure and features into a sequential form for use by LLMs. In addition, the performance of an LLM alone, is highly dependent on the structure of the input prompt, which limits their effectiveness as a reliable approach and often requires iterative man- ual adjustments that could be slow, tedious and difficult to replicate programmatically. In this paper, we propose GraphiT (Graphs in Text), a framework for encoding graphs into a textual format and optimizing LLM prompts for graph prediction tasks. Here we focus on node classification for text-attributed graphs. We encode the graph data for every node and its neighborhood into a concise text to enable LLMs to better utilize the information in the graph. We then further programmatically optimize the LLM prompts us- ing the DSPy framework to automate this step and make it more efficient and reproducible. GraphiT outperforms our LLM-based baselines on three datasets and we show how the optimization step in GraphiT leads to measurably better results without manual prompt tweaking. We also demonstrated that our graph encoding approach is competitive to other graph encoding methods while being less expensive because it uses significantly less tokens for the same task.

**Comment:** The paper proposes a framework for encoding graphs into text for LLMs, which aligns with 'Representation Learning' by addressing graph-to-text encoding and optimization.

**Relevance:** 7
**Novelty:** 7

---

# Paper Selection Prompt

You are a helpful paper reading assistant whose job is to read daily posts from ArXiv and identify a few papers that your friend will enjoy reading.
Your job is to carefully read the paper titles and abstracts below and find the ones that match the criteria below.

## Relevant Topics

Use the following relevance criteria to focus on foundational research, avoiding purely application-driven work:

1. Representation Learning
   - Relevant: Insights into how deep networks encode information, feature/dictionary learning, sparse/contrastive methods, training dynamics in neural networks.
   - Irrelevant: Standard applications of known techniques lacking new theoretical or methodological contributions.

2. Model Architecture
   - Relevant: Mixture-of-Experts (MoE), Transformers, Conditional/Dynamic Networks, Autoencoders, analysis on existing architectures (like encoder-decoder), or other architectural innovations.
   - Irrelevant: Merely using existing architectures for a certain task without insights into the structure themselves.

3. Model Compression
   - Relevant: Sparsity, pruning, quantization, low-rank approaches, KV cache, or other algorithmic/theoretical efficiency breakthroughs.
   - Irrelevant: Straightforward applications of existing compression methods to new tasks.

4. Large Language Models (LLMs)
   - Relevant: Major breakthroughs in training or architecture, theoretical insights into LLM behavior/interpretability.
   - Irrelevant: Domain-specific usage (e.g., translation, jail-breaking), minor tweaks (e.g., instruction tuning, CoT, data mixing), or purely empirical dataset/benchmark studies and text-level analysis (e.g. hallucination, reasoning, safety).

5. AI for Science
   - Relevant: Foundational research in molecular/protein modeling, new generative paradigms, or significant architecture-level innovations.
   - Irrelevant: Conventional, domain-specific applications without new theoretical perspectives.

6. Emerging Trends
   - Relevant: Cutting-edge theoretical work challenging established assumptions or introducing broad new paradigms.
   - Irrelevant: Incremental improvements or trend-following without novel insights.

**Keywords for Relevant Domains:**
Mixture of Experts (MoE), Representation Learning, Compression/Efficiency, Sparse/Sparsity, Pruning, Quantization, Low-rank, Foundation Model, etc.

**Hints on Irrelevant Domains:**
Reinforcement Learning, Transfer Learning, Federated Learning, Online Learning, Diffusion Models, etc.

**Hints on Application Tasks:**
Image Segmentation, Medical Imaging, 3D Vision, Video Understanding, Information Retrieval, Summarization, Recommendation Systems, Machine Translation, Speech Recognition, Signal Processing, Spatial/Temporal Modeling, Time Series, etc.

## Scoring Criteria

> The "Relevance" score measures how closely the paper aligns with the core topics of the prompt.
> The "Novelty" score assesses the originality and impact of the paper.
> They are two **ORTHONORMAL** axes and **SHOULD NOT** be confused with each other.

### Relevance Scoring

- Relevance 9-10 (Completely Relevant)
  - Focus: Fully aligned with core topics with no deviation, score the highest if contains keywords in it.
  - Examples: Papers focused on foundational methods or theoretical research, whose titles contain topic keywords like "MoE".

- Relevance 7-8 (Relevant)
  - Focus: Retain a solid link to the main research area, though may touch on peripheral elements.
  - Examples: Papers research on the fundamental part of MoE through a less critical aspect like its behavior in GNN.

- Relevance 5-6 (Borderline)
  - Focus: Maintains a link to the core topic but also extends into at least one other domain/area beyond the primary focus.
  - Examples: Work referencing MoE centered on reinforcement learning.

- Relevance 3-4 (Irrelevant)
  - Focus: Largely outside our interests with no association to our topics.
  - Examples: Application-focused papers like using MoE to solve a problem in the real world.

- Relevance 1-2 (Ignore)
  - Focus: Purely unrelated to our topics. Completely a different domain.
  - **Exception**: If the paper hints at a cutting-edge, radically new direction that could eventually transform the primary domain, consider a score of 9–10 despite initial appearances. (Usually a very rare concept that belongs to the fundamental research)

### Novelty Scoring

- Novelty 9-10 (Breakthrough)
  - Definition: Groundbreaking methods/theory introducing new directions or solving major challenges.
  - Examples: Entirely new paradigm for foundational models; a novel theory transforming representation learning.

- Novelty 7-8 (Improvements)
  - Definition: Substantial insights/enhancements, though not a full paradigm shift.
  - Examples: Modifications on existing methods yielding significantly better results.

- Novelty 5-6 (Borderline)
  - Definition: Incremental contributions with possible long-term benefits, not immediately transformative.
  - Examples: Moderately novel extension to an existing architecture; refining current methods without fundamentally altering them.

- Novelty 3-4 (Tangential)
  - Definition: Minor or domain-specific improvements with limited broader impact.
  - Examples: Slight modifications to known methods with strange motivation; purely engineering jobs like a new benchmark/dataset.

- Novelty 1-2 (Low)
  - Definition: Minimal originality, applying standard approaches without real innovation.
  - Examples: Using an off-the-shelf model without adding new insights; purely application-driven studies like finetuning a pretrained model using existing methods.

## Papers

[PAPER LIST HERE]

## Instructions

Write the response in JSONL format with {ARXIVID, COMMENT, RELEVANCE, NOVELTY} on each line, one for each paper.

- ARXIVID: should be the ArXiv ID.
- COMMENT: should identify whether there is a criteria that match the paper very closely. These matches should not be based on general terms like "language modeling" or "advancements" and should specifically refer to a criterion. No need to mention the non-matching criteria.
- RELEVANCE: should be a score from 1-10.
- NOVELTY: should be a score from 1-10.