{
    "2507.19465": {
        "SCORE": 17,
        "ARXIVID": "2507.19465",
        "COMMENT": "The paper introduces a linearly convergent algorithm for optimizing piecewise smooth functions, which is a significant theoretical contribution in optimization.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Zhe Zhang",
            "Suvrit Sra"
        ],
        "title": "Linearly Convergent Algorithms for Nonsmooth Problems with Unknown Smooth Pieces",
        "abstract": "We develop efficient algorithms for optimizing piecewise smooth (PWS) functions where the underlying partition of the domain into smooth pieces is \\emph{unknown}. For PWS functions satisfying a quadratic growth (QG) condition, we propose a bundle-level (BL) type method that achieves global linear convergence -- to our knowledge, the first such result for any algorithm for this problem class. We extend this method to handle approximately PWS functions and to solve weakly-convex PWS problems, improving the state-of-the-art complexity to match the benchmark for smooth non-convex optimization. Furthermore, we introduce the first verifiable and accurate termination criterion for PWS optimization. Similar to the gradient norm in smooth optimization, this certificate tightly characterizes the optimality gap under the QG condition, and can moreover be evaluated without knowledge of any problem parameters. We develop a search subroutine for this certificate and embed it within a guess-and-check framework, resulting in an almost parameter-free algorithm for both the convex QG and weakly-convex settings.",
        "arxiv_id": "2507.19465"
    },
    "2507.19321": {
        "SCORE": 17,
        "ARXIVID": "2507.19321",
        "COMMENT": "The paper introduces a method for sparse information disentanglement in neural networks, contributing to explainable AI and representation learning.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Viktar Dubovik",
            "{\\L}ukasz Struski",
            "Jacek Tabor",
            "Dawid Rymarczyk"
        ],
        "title": "SIDE: Sparse Information Disentanglement for Explainable Artificial Intelligence",
        "abstract": "Understanding the decisions made by deep neural networks is essential in high-stakes domains such as medical imaging and autonomous driving. Yet, these models often lack transparency, particularly in computer vision. Prototypical-parts-based neural networks have emerged as a promising solution by offering concept-level explanations. However, most are limited to fine-grained classification tasks, with few exceptions such as InfoDisent. InfoDisent extends prototypical models to large-scale datasets like ImageNet, but produces complex explanations.   We introduce Sparse Information Disentanglement for Explainability (SIDE), a novel method that improves the interpretability of prototypical parts through a dedicated training and pruning scheme that enforces sparsity. Combined with sigmoid activations in place of softmax, this approach allows SIDE to associate each class with only a small set of relevant prototypes. Extensive experiments show that SIDE matches the accuracy of existing methods while reducing explanation size by over $90\\%$, substantially enhancing the understandability of prototype-based explanations.",
        "arxiv_id": "2507.19321"
    },
    "2507.18794": {
        "SCORE": 17,
        "ARXIVID": "2507.18794",
        "COMMENT": "The paper proposes a contrastive learning framework to disentangle task-relevant and task-irrelevant features, which is relevant to representation learning.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Minghui Sun",
            "Benjamin A. Goldstein",
            "Matthew M. Engelhard"
        ],
        "title": "CLEAR: Unlearning Spurious Style-Content Associations with Contrastive LEarning with Anti-contrastive Regularization",
        "abstract": "Learning representations unaffected by superficial characteristics is important to ensure that shifts in these characteristics at test time do not compromise downstream prediction performance. For instance, in healthcare applications, we might like to learn features that contain information about pathology yet are unaffected by race, sex, and other sources of physiologic variability, thereby ensuring predictions are equitable and generalizable across all demographics. Here we propose Contrastive LEarning with Anti-contrastive Regularization (CLEAR), an intuitive and easy-to-implement framework that effectively separates essential (i.e., task-relevant) characteristics from superficial (i.e., task-irrelevant) characteristics during training, leading to better performance when superficial characteristics shift at test time. We begin by supposing that data representations can be semantically separated into task-relevant content features, which contain information relevant to downstream tasks, and task-irrelevant style features, which encompass superficial attributes that are irrelevant to these tasks, yet may degrade performance due to associations with content present in training data that do not generalize. We then prove that our anti-contrastive penalty, which we call Pair-Switching (PS), minimizes the Mutual Information between the style attributes and content labels. Finally, we instantiate CLEAR in the latent space of a Variational Auto-Encoder (VAE), then perform experiments to quantitatively and qualitatively evaluate the resulting CLEAR-VAE over several image datasets. Our results show that CLEAR-VAE allows us to: (a) swap and interpolate content and style between any pair of samples, and (b) improve downstream classification performance in the presence of previously unseen combinations of content and style. Our code will be made publicly available.",
        "arxiv_id": "2507.18794"
    },
    "2507.18903": {
        "SCORE": 17,
        "ARXIVID": "2507.18903",
        "COMMENT": "The paper introduces the PACC Discovery framework, extending PAC learning principles to causal discovery, which is a novel theoretical contribution.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Mian Wei",
            "Somesh Jha",
            "David Page"
        ],
        "title": "Probably Approximately Correct Causal Discovery",
        "abstract": "The discovery of causal relationships is a foundational problem in artificial intelligence, statistics, epidemiology, economics, and beyond. While elegant theories exist for accurate causal discovery given infinite data, real-world applications are inherently resource-constrained. Effective methods for inferring causal relationships from observational data must perform well under finite data and time constraints, where \"performing well\" implies achieving high, though not perfect accuracy. In his seminal paper A Theory of the Learnable, Valiant highlighted the importance of resource constraints in supervised machine learning, introducing the concept of Probably Approximately Correct (PAC) learning as an alternative to exact learning. Inspired by Valiant's work, we propose the Probably Approximately Correct Causal (PACC) Discovery framework, which extends PAC learning principles to the causal field. This framework emphasizes both computational and sample efficiency for established causal methods such as propensity score techniques and instrumental variable approaches. Furthermore, we show that it can also provide theoretical guarantees for other widely used methods, such as the Self-Controlled Case Series (SCCS) method, which had previously lacked such guarantees.",
        "arxiv_id": "2507.18903"
    },
    "2507.18671": {
        "SCORE": 17,
        "ARXIVID": "2507.18671",
        "COMMENT": "The paper introduces a novel approach to convert a dense LLM into a Mixture-of-Experts model, which is relevant to model architecture and large language models.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Ning Liao",
            "Xiaoxing Wang",
            "Zehao Lin",
            "Weiyang Guo",
            "Feng Hong",
            "Shixiang Song",
            "Geng Yu",
            "Zihua Zhao",
            "Sitao Xie",
            "Longxuan Wei",
            "Xiangqi Jin",
            "Xiaohan Qin",
            "Jiale Ma",
            "Kai Chen",
            "Jiangchao Yao",
            "Zhouhan Lin",
            "Junchi Yan",
            "Zhiyu Li",
            "Feiyu Xiong",
            "Yanfeng Wang",
            "Linfeng Zhang"
        ],
        "title": "Innovator: Scientific Continued Pretraining with Fine-grained MoE Upcycling",
        "abstract": "A large language model (LLM) with knowledge in both scientific and general tasks is the foundation of science general intelligence. However, directly continued pretraining an LLM using science data usually leads to catastrophic forgetting, which indicates severe degradation in general ability. In this report, we present Innovator, which solves this problem by upcycling a pre-trained dense LLM into a fine-grained Mixtures-of-Experts model during continued pretraining, where different experts are expected to learn science knowledge in different disciplines, and a shared expert is utilized for general tasks. Innovator introduces a four-stage upcycle training paradigm: (1) Scientific Expert Induction on discipline-specific data, (2) Fine-grained Expert Splitting via FFN dimension decomposition, (3) Science-Aware Routing warmup, and (4) Generalist-Scientist Integration training on hybrid datasets. Such a paradigm enables knowledge in the general domain, and different scientific disciplines can be decoupled, avoiding the negative influence among knowledge in different domains. With 53.3B total parameters and 13.3B activated, Innovator extends Qwen2.5-7B using a shared general expert and 64 specialized scientific experts with 8 activated. Trained on 300B tokens with tri-level quality-controlled data, Innovator achieves 25% average improvement across 30 scientific tasks with a win rate as 70%, while retaining 99% performance in general tasks. Furthermore, Innovator-Reason, which is post-trained from Innovator for reasoning boosting, exhibits excellent reasoning performance in solving complex scientific problems with improvements over 30%.",
        "arxiv_id": "2507.18671"
    },
    "2507.18725": {
        "SCORE": 17,
        "ARXIVID": "2507.18725",
        "COMMENT": "The paper introduces a novel concept of 'un-pruning' in sparse models, which is relevant to model compression and sparsity.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Yang Xiao",
            "Gen Li",
            "Jie Ji",
            "Ruimeng Ye",
            "Xiaolong Ma",
            "Bo Hui"
        ],
        "title": "The Right to be Forgotten in Pruning: Unveil Machine Unlearning on Sparse Models",
        "abstract": "Machine unlearning aims to efficiently eliminate the memory about deleted data from trained models and address the right to be forgotten. Despite the success of existing unlearning algorithms, unlearning in sparse models has not yet been well studied. In this paper, we empirically find that the deleted data has an impact on the pruned topology in a sparse model. Motivated by the observation and the right to be forgotten, we define a new terminology ``un-pruning\" to eliminate the impact of deleted data on model pruning. Then we propose an un-pruning algorithm to approximate the pruned topology driven by retained data. We remark that any existing unlearning algorithm can be integrated with the proposed un-pruning workflow and the error of un-pruning is upper-bounded in theory. Also, our un-pruning algorithm can be applied to both structured sparse models and unstructured sparse models. In the experiment, we further find that Membership Inference Attack (MIA) accuracy is unreliable for assessing whether a model has forgotten deleted data, as a small change in the amount of deleted data can produce arbitrary MIA results. Accordingly, we devise new performance metrics for sparse models to evaluate the success of un-pruning. Lastly, we conduct extensive experiments to verify the efficacy of un-pruning with various pruning methods and unlearning algorithms. Our code is released at https://anonymous.4open.science/r/UnlearningSparseModels-FBC5/.",
        "arxiv_id": "2507.18725"
    },
    "2507.19427": {
        "SCORE": 17,
        "ARXIVID": "2507.19427",
        "COMMENT": "The paper introduces a novel attention mechanism and system co-design for LLMs, relevant to model architecture and efficiency improvements.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "StepFun",
            ":",
            "Bin Wang",
            "Bojun Wang",
            "Changyi Wan",
            "Guanzhe Huang",
            "Hanpeng Hu",
            "Haonan Jia",
            "Hao Nie",
            "Mingliang Li",
            "Nuo Chen",
            "Siyu Chen",
            "Song Yuan",
            "Wuxun Xie",
            "Xiaoniu Song",
            "Xing Chen",
            "Xingping Yang",
            "Xuelin Zhang",
            "Yanbo Yu",
            "Yaoyu Wang",
            "Yibo Zhu",
            "Yimin Jiang",
            "Yu Zhou",
            "Yuanwei Lu",
            "Houyi Li",
            "Jingcheng Hu",
            "Ka Man Lo",
            "Ailin Huang",
            "Binxing Jiao",
            "Bo Li",
            "Boyu Chen",
            "Changxin Miao",
            "Chang Lou",
            "Chen Hu",
            "Chen Xu",
            "Chenfeng Yu",
            "Chengyuan Yao",
            "Daokuan Lv",
            "Dapeng Shi",
            "Deshan Sun",
            "Ding Huang",
            "Dingyuan Hu",
            "Dongqing Pang",
            "Enle Liu",
            "Fajie Zhang",
            "Fanqi Wan",
            "Gulin Yan",
            "Han Zhang",
            "Han Zhou",
            "Hanghao Wu",
            "Hangyu Guo",
            "Hanqi Chen",
            "Hanshan Zhang",
            "Hao Wu",
            "Haocheng Zhang",
            "Haolong Yan",
            "Haoran Lv",
            "Haoran Wei",
            "Hebin Zhou",
            "Heng Wang",
            "Heng Wang",
            "Hongxin Li",
            "Hongyu Zhou",
            "Hongyuan Wang",
            "Huiyong Guo",
            "Jia Wang",
            "Jiahao Gong",
            "Jialing Xie",
            "Jian Zhou",
            "Jianjian Sun",
            "Jiaoren Wu",
            "Jiaran Zhang",
            "Jiayu Liu",
            "Jie Cheng",
            "Jie Luo",
            "Jie Yan",
            "Jie Yang",
            "Jieyi Hou",
            "Jinguang Zhang",
            "Jinlan Cao",
            "Jisheng Yin",
            "Junfeng Liu",
            "Junhao Huang",
            "Junzhe Lin",
            "Kaijun Tan",
            "Kaixiang Li",
            "Kang An",
            "Kangheng Lin",
            "Kenkun Liu",
            "Lei Yang",
            "Liang Zhao",
            "Liangyu Chen",
            "Lieyu Shi",
            "Liguo Tan",
            "Lin Lin",
            "Lin Zhang",
            "Lina Chen",
            "Liwen Huang",
            "Liying Shi",
            "Longlong Gu",
            "Mei Chen",
            "Mengqiang Ren",
            "Ming Li",
            "Mingzhe Chen",
            "Na Wang",
            "Nan Wu",
            "Qi Han",
            "Qian Zhao",
            "Qiang Zhang",
            "Qianni Liu",
            "Qiaohui Chen",
            "Qiling Wu",
            "Qinglin He",
            "Qinyuan Tan",
            "Qiufeng Wang",
            "Qiuping Wu",
            "Qiuyan Liang",
            "Quan Sun",
            "Rui Li",
            "Ruihang Miao",
            "Ruosi Wan",
            "Ruyan Guo",
            "Shangwu Zhong",
            "Shaoliang Pang",
            "Shengjie Fan",
            "Shijie Shang",
            "Shilei Jiang",
            "Shiliang Yang",
            "Shiming Hao",
            "Shuli Gao",
            "Siming Huang",
            "Siqi Liu",
            "Tiancheng Cao",
            "Tianhao Cheng",
            "Tianhao Peng",
            "Wang You",
            "Wei Ji",
            "Wen Sun",
            "Wenjin Deng",
            "Wenqing He",
            "Wenzhen Zheng",
            "Xi Chen",
            "Xiangwen Kong",
            "Xianzhen Luo",
            "Xiaobo Yang",
            "Xiaojia Liu",
            "Xiaoxiao Ren",
            "Xin Han",
            "Xin Li",
            "Xin Wu",
            "Xu Zhao",
            "Yanan Wei",
            "Yang Li",
            "Yangguang Li",
            "Yangshijie Xu",
            "Yanming Xu",
            "Yaqiang Shi",
            "Yeqing Shen",
            "Yi Yang",
            "Yifei Yang",
            "Yifeng Gong",
            "Yihan Chen",
            "Yijing Yang",
            "Yinmin Zhang",
            "Yizhuang Zhou",
            "Yuanhao Ding",
            "Yuantao Fan",
            "Yuanzhen Yang",
            "Yuchu Luo",
            "Yue Peng",
            "Yufan Lu",
            "Yuhang Deng",
            "Yuhe Yin",
            "Yujie Liu",
            "Yukun Chen",
            "Yuling Zhao",
            "Yun Mou",
            "Yunlong Li",
            "Yunzhou Ju",
            "Yusheng Li",
            "Yuxiang Yang",
            "Yuxiang Zhang",
            "Yuyang Chen",
            "Zejia Weng",
            "Zhe Xie",
            "Zheng Ge",
            "Zheng Gong",
            "Zhenyi Lu",
            "Zhewei Huang",
            "Zhichao Chang",
            "Zhiguo Huang",
            "Zhirui Wang",
            "Zidong Yang",
            "Zili Wang",
            "Ziqi Wang",
            "Zixin Zhang",
            "Binxing Jiao",
            "Daxin Jiang",
            "Heung-Yeung Shum",
            "Xiangyu Zhang"
        ],
        "title": "Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding",
        "abstract": "Large language models (LLMs) face low hardware efficiency during decoding, especially for long-context reasoning tasks. This paper introduces Step-3, a 321B-parameter VLM with hardware-aware model-system co-design optimized for minimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel Multi-Matrix Factorization Attention (MFA) mechanism that significantly reduces both KV cache size and computation while maintaining high attention expressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed inference system that decouples attention and Feed-Forward Network (FFN) layers into specialized subsystems. This co-design achieves unprecedented cost efficiency: Step-3 significantly reduces theoretical decoding costs compared with models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at longer context. Step-3 achieves low cost while activating 38B parameters per token (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that hardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are critical to cost-effectiveness. We perform a head-to-head comparison with DeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs achieves a decoding throughput of up to 4,039 tokens per second per GPU under 50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324 in the same setup and sets a new Pareto frontier for LLM decoding.",
        "arxiv_id": "2507.19427"
    },
    "2507.19290": {
        "SCORE": 17,
        "ARXIVID": "2507.19290",
        "COMMENT": "The paper discusses learning structured matrix approximations, which is relevant to model compression and efficiency.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Noah Amsel",
            "Pratyush Avi",
            "Tyler Chen",
            "Feyza Duman Keles",
            "Chinmay Hegde",
            "Cameron Musco",
            "Christopher Musco",
            "David Persson"
        ],
        "title": "Query Efficient Structured Matrix Learning",
        "abstract": "We study the problem of learning a structured approximation (low-rank, sparse, banded, etc.) to an unknown matrix $A$ given access to matrix-vector product (matvec) queries of the form $x \\rightarrow Ax$ and $x \\rightarrow A^Tx$. This problem is of central importance to algorithms across scientific computing and machine learning, with applications to fast multiplication and inversion for structured matrices, building preconditioners for first-order optimization, and as a model for differential operator learning. Prior work focuses on obtaining query complexity upper and lower bounds for learning specific structured matrix families that commonly arise in applications.   We initiate the study of the problem in greater generality, aiming to understand the query complexity of learning approximations from general matrix families. Our main result focuses on finding a near-optimal approximation to $A$ from any finite-sized family of matrices, $\\mathcal{F}$. Standard results from matrix sketching show that $O(\\log|\\mathcal{F}|)$ matvec queries suffice in this setting. This bound can also be achieved, and is optimal, for vector-matrix-vector queries of the form $x,y\\rightarrow x^TAy$, which have been widely studied in work on rank-$1$ matrix sensing.   Surprisingly, we show that, in the matvec model, it is possible to obtain a nearly quadratic improvement in complexity, to $\\tilde{O}(\\sqrt{\\log|\\mathcal{F}|})$. Further, we prove that this bound is tight up to log-log factors.Via covering number arguments, our result extends to well-studied infinite families. As an example, we establish that a near-optimal approximation from any \\emph{linear matrix family} of dimension $q$ can be learned with $\\tilde{O}(\\sqrt{q})$ matvec queries, improving on an $O(q)$ bound achievable via sketching techniques and vector-matrix-vector queries.",
        "arxiv_id": "2507.19290"
    },
    "2507.19046": {
        "SCORE": 16,
        "ARXIVID": "2507.19046",
        "COMMENT": "The paper introduces a novel framework for reservoir computing, which is a foundational research area in representation learning.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Charlotte Geier (Dynamics Group",
            "Hamburg University of Technology)",
            "Merten Stender (Chair of Cyber-Physical Systems in Mechanical Engineering",
            "Technische Universit\\\"at Berlin",
            "Germany)"
        ],
        "title": "Dynamics-Informed Reservoir Computing with Visibility Graphs",
        "abstract": "Accurate prediction of complex and nonlinear time series remains a challenging problem across engineering and scientific disciplines. Reservoir computing (RC) offers a computationally efficient alternative to traditional deep learning by training only the read-out layer while employing a randomly structured and fixed reservoir network. Despite its advantages, the largely random reservoir graph architecture often results in suboptimal and oversized networks with poorly understood dynamics. Addressing this issue, we propose a novel Dynamics-Informed Reservoir Computing (DyRC) framework that systematically infers the reservoir network structure directly from the input training sequence. This work proposes to employ the visibility graph (VG) technique, which converts time series data into networks by representing measurement points as nodes linked by mutual visibility. The reservoir network is constructed by directly adopting the VG network from a training data sequence, leveraging the parameter-free visibility graph approach to avoid expensive hyperparameter tuning. This process results in a reservoir that is directly informed by the specific dynamics of the prediction task under study. We assess the DyRC-VG method through prediction tasks involving the canonical nonlinear Duffing oscillator, evaluating prediction accuracy and consistency. Compared to an Erd\\H{o}s-R\\'enyi graph of the same size, spectral radius, and comparable density, we observe higher prediction quality and more consistent performance over repeated implementations in the DyRC-VG.",
        "arxiv_id": "2507.19046"
    },
    "2507.18807": {
        "SCORE": 16,
        "ARXIVID": "2507.18807",
        "COMMENT": "The paper explores approximating the Fisher Information Matrix using existing gradient accumulators, which is relevant to model efficiency and compression.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "YuXin Li",
            "Felix Dangel",
            "Derek Tam",
            "Colin Raffel"
        ],
        "title": "Fishers for Free? Approximating the Fisher Information Matrix by Recycling the Squared Gradient Accumulator",
        "abstract": "The diagonal of a model's Fisher Information Matrix (the \"Fisher diagonal\") has frequently been used as a way to measure parameter sensitivity. Typically, the Fisher diagonal is estimated via squared sampled gradients of the model's likelihood with respect to its parameters, averaged over a few hundred or thousand examples -- a process which incurs nontrivial computational costs. At the same time, adaptive gradient methods like the ubiquitous Adam optimizer compute a moving average of the squared gradient over the course of training. This paper therefore explores whether an approximation of the Fisher diagonal can be obtained \"for free\" by recycling the squared gradient accumulator that has already been computed over the course of training. Through a comprehensive set of experiments covering five applications of the Fisher diagonal, we demonstrate that the \"Squisher\" (SQUared gradient accumulator as an approximation of the FISHER) consistently performs similarly to the Fisher diagonal while outperforming baseline methods. Additionally, we clarify the exact differences between the Squisher and the Fisher diagonal and provide empirical quantification of their respective impact.",
        "arxiv_id": "2507.18807"
    },
    "2507.18803": {
        "SCORE": 16,
        "ARXIVID": "2507.18803",
        "COMMENT": "The paper provides theoretical insights into the eigenvalues of graph Laplacians, which is relevant to foundational research in representation learning.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Chenghui Li",
            "Nicol\\'as Garc\\'ia Trillos",
            "Housen Li",
            "Leo Suchan"
        ],
        "title": "Central limit theorems for the eigenvalues of graph Laplacians on data clouds",
        "abstract": "Given i.i.d.\\ samples $X_n =\\{ x_1, \\dots, x_n \\}$ from a distribution supported on a low dimensional manifold ${M}$ embedded in Eucliden space, we consider the graph Laplacian operator $\\Delta_n$ associated to an $\\varepsilon$-proximity graph over $X_n$ and study the asymptotic fluctuations of its eigenvalues around their means. In particular, letting $\\hat{\\lambda}_l^\\varepsilon$ denote the $l$-th eigenvalue of $\\Delta_n$, and under suitable assumptions on the data generating model and on the rate of decay of $\\varepsilon$, we prove that $\\sqrt{n } (\\hat{\\lambda}_{l}^\\varepsilon - \\mathbb{E}[\\hat{\\lambda}_{l}^\\varepsilon] )$ is asymptotically Gaussian with a variance that we can explicitly characterize. A formal argument allows us to interpret this asymptotic variance as the dissipation of a gradient flow of a suitable energy with respect to the Fisher-Rao geometry. This geometric interpretation allows us to give, in turn, a statistical interpretation of the asymptotic variance in terms of a Cramer-Rao lower bound for the estimation of the eigenvalues of certain weighted Laplace-Beltrami operator. The latter interpretation suggests a form of asymptotic statistical efficiency for the eigenvalues of the graph Laplacian. We also present CLTs for multiple eigenvalues and through several numerical experiments explore the validity of our results when some of the assumptions that we make in our theoretical analysis are relaxed.",
        "arxiv_id": "2507.18803"
    },
    "2507.18813": {
        "SCORE": 16,
        "ARXIVID": "2507.18813",
        "COMMENT": "The paper proposes a scale-consistent learning approach for PDEs, which is relevant to emerging trends in AI for science and foundational research.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Zongyi Li",
            "Samuel Lanthaler",
            "Catherine Deng",
            "Michael Chen",
            "Yixuan Wang",
            "Kamyar Azizzadenesheli",
            "Anima Anandkumar"
        ],
        "title": "Scale-Consistent Learning for Partial Differential Equations",
        "abstract": "Machine learning (ML) models have emerged as a promising approach for solving partial differential equations (PDEs) in science and engineering. Previous ML models typically cannot generalize outside the training data; for example, a trained ML model for the Navier-Stokes equations only works for a fixed Reynolds number ($Re$) on a pre-defined domain. To overcome these limitations, we propose a data augmentation scheme based on scale-consistency properties of PDEs and design a scale-informed neural operator that can model a wide range of scales. Our formulation leverages the facts: (i) PDEs can be rescaled, or more concretely, a given domain can be re-scaled to unit size, and the parameters and the boundary conditions of the PDE can be appropriately adjusted to represent the original solution, and (ii) the solution operators on a given domain are consistent on the sub-domains. We leverage these facts to create a scale-consistency loss that encourages matching the solutions evaluated on a given domain and the solution obtained on its sub-domain from the rescaled PDE. Since neural operators can fit to multiple scales and resolutions, they are the natural choice for incorporating scale-consistency loss during training of neural PDE solvers. We experiment with scale-consistency loss and the scale-informed neural operator model on the Burgers' equation, Darcy Flow, Helmholtz equation, and Navier-Stokes equations. With scale-consistency, the model trained on $Re$ of 1000 can generalize to $Re$ ranging from 250 to 10000, and reduces the error by 34% on average of all datasets compared to baselines.",
        "arxiv_id": "2507.18813"
    },
    "2507.19353": {
        "SCORE": 15,
        "ARXIVID": "2507.19353",
        "COMMENT": "The paper discusses a novel method for improving the performance of Recurrent LLMs on long-context tasks, which aligns with the interest in foundational research on LLM architectures.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Kai Liu",
            "Zhan Su",
            "Peijie Dong",
            "Fengran Mo",
            "Jianfei Gao",
            "ShaoTing Zhang",
            "Kai Chen"
        ],
        "title": "Smooth Reading: Bridging the Gap of Recurrent LLM to Self-Attention LLM on Long-Context Tasks",
        "abstract": "Recently, recurrent large language models (Recurrent LLMs) with linear computational complexity have re-emerged as efficient alternatives to self-attention-based LLMs (Self-Attention LLMs), which have quadratic complexity. However, Recurrent LLMs often underperform on long-context tasks due to their limited fixed-size memory. Previous research has primarily focused on enhancing the memory capacity of Recurrent LLMs through architectural innovations, but these approaches have not yet enabled Recurrent LLMs to match the performance of Self-Attention LLMs on long-context tasks. We argue that this limitation arises because processing the entire context at once is not well-suited for Recurrent LLMs. In this paper, we propose Smooth Reading, a chunk-wise inference method inspired by human reading strategies. Smooth Reading processes context in chunks and iteratively summarizes the contextual information, thereby reducing memory demands and making the approach more compatible with Recurrent LLMs. Our experimental results show that this method substantially narrows the performance gap between Recurrent and Self-Attention LLMs on long-context tasks, while preserving the efficiency advantages of Recurrent LLMs. Our Smooth Reading boosts SWA-3B-4k (a Recurrent LLM) from 5.68% lower to 3.61% higher performance than Self-Attention LLMs on LongBench. Besides, our method maintains the high efficiency, training 3x faster and inferring 2x faster at 64k context compared to Self-Attention LLMs. To our knowledge, this is the first work to achieve comparable performance using Recurrent LLMs compared with Self-Attention LLMs on long-context tasks. We hope our method will inspire future research in this area. To facilitate further progress, we will release code and dataset.",
        "arxiv_id": "2507.19353"
    },
    "2507.19423": {
        "SCORE": 15,
        "ARXIVID": "2507.19423",
        "COMMENT": "The paper presents a tensor-based methodology for perfect clustering in sparse networks, contributing to foundational research in network models.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Marianna Pensky"
        ],
        "title": "Perfect Clustering in Very Sparse Diverse Multiplex Networks",
        "abstract": "The paper studies the DIverse MultiPLEx Signed Generalized Random Dot Product Graph (DIMPLE-SGRDPG) network model (Pensky (2024)), where all layers of the network have the same collection of nodes. In addition, all layers can be partitioned into groups such that the layers in the same group are embedded in the same ambient subspace but otherwise matrices of connection probabilities can be all different. This setting includes majority of multilayer network models as its particular cases. The key task in this model is to recover the groups of layers with unique subspace structures, since the case where all layers of the network are embedded in the same subspace has been fairly well studied. Until now, clustering of layers in such networks was based on the layer-per-layer analysis, which required the multilayer network to be sufficiently dense. Nevertheless, in this paper we succeeded in pooling information in all layers together and providing a tensor-based methodology that ensures perfect clustering for a much sparser network. Our theoretical results, established under intuitive non-restrictive assumptions, assert that the new technique achieves perfect clustering under sparsity conditions that, up to logarithmic factors, coincide with the computational lower bound derived for a much simpler model.",
        "arxiv_id": "2507.19423"
    },
    "2507.19420": {
        "SCORE": 15,
        "ARXIVID": "2507.19420",
        "COMMENT": "The paper provides insights into the internal reasoning mechanisms of large vision-language models, which is relevant to understanding model behavior and representation learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Yiming Zhang",
            "Chengzhang Yu",
            "Zhuokai Zhao",
            "Kun Wang",
            "Qiankun Li",
            "Zihan Chen",
            "Yang Liu",
            "Zenghui Ding",
            "Yining Sun"
        ],
        "title": "CircuitProbe: Dissecting Spatiotemporal Visual Semantics with Circuit Tracing",
        "abstract": "The processing mechanisms underlying language and image understanding in large vision-language models (LVLMs) have been extensively studied. However, the internal reasoning mechanisms of LVLMs for spatiotemporal understanding remain poorly understood. In this work, we introduce a systematic, circuit-based framework designed to investigate how spatiotemporal visual semantics are represented and processed within these LVLMs. Specifically, our framework comprises three circuits: visual auditing circuit, semantic tracing circuit, and attention flow circuit. Through the lens of these circuits, we discover that visual semantics are highly localized to specific object tokens--removing these tokens can degrade model performance by up to 92.6%. Furthermore, we identify that interpretable concepts of objects and actions emerge and become progressively refined in the middle-to-late layers of LVLMs. In contrary to the current works that solely focus on objects in one image, we reveal that the middle-to-late layers of LVLMs exhibit specialized functional localization for spatiotemporal semantics. Our findings offer significant mechanistic insights into spatiotemporal semantics analysis of LVLMs, laying a foundation for designing more robust and interpretable models.",
        "arxiv_id": "2507.19420"
    },
    "2507.19031": {
        "SCORE": 15,
        "ARXIVID": "2507.19031",
        "COMMENT": "The paper presents a framework for knowledge distillation from GNNs to MLPs, which involves model compression and efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Weigang Lu",
            "Ziyu Guan",
            "Wei Zhao",
            "Yaming Yang",
            "Yujie Sun",
            "Zheng Liang",
            "Yibing Zhan",
            "Dapeng Tao"
        ],
        "title": "ProGMLP: A Progressive Framework for GNN-to-MLP Knowledge Distillation with Efficient Trade-offs",
        "abstract": "GNN-to-MLP (G2M) methods have emerged as a promising approach to accelerate Graph Neural Networks (GNNs) by distilling their knowledge into simpler Multi-Layer Perceptrons (MLPs). These methods bridge the gap between the expressive power of GNNs and the computational efficiency of MLPs, making them well-suited for resource-constrained environments. However, existing G2M methods are limited by their inability to flexibly adjust inference cost and accuracy dynamically, a critical requirement for real-world applications where computational resources and time constraints can vary significantly. To address this, we introduce a Progressive framework designed to offer flexible and on-demand trade-offs between inference cost and accuracy for GNN-to-MLP knowledge distillation (ProGMLP). ProGMLP employs a Progressive Training Structure (PTS), where multiple MLP students are trained in sequence, each building on the previous one. Furthermore, ProGMLP incorporates Progressive Knowledge Distillation (PKD) to iteratively refine the distillation process from GNNs to MLPs, and Progressive Mixup Augmentation (PMA) to enhance generalization by progressively generating harder mixed samples. Our approach is validated through comprehensive experiments on eight real-world graph datasets, demonstrating that ProGMLP maintains high accuracy while dynamically adapting to varying runtime scenarios, making it highly effective for deployment in diverse application settings.",
        "arxiv_id": "2507.19031"
    }
}