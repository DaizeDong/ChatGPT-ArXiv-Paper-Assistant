{
    "2505.01557": {
        "SCORE": 19,
        "ARXIVID": "2505.01557",
        "COMMENT": "The paper introduces a novel theoretical framework for representation learning, directly addressing the 'Representation Learning' criterion with a focus on foundational insights.",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "Runtian Zhai",
            "Kai Yang",
            "Che-Ping Tsai",
            "Burak Varici",
            "Zico Kolter",
            "Pradeep Ravikumar"
        ],
        "title": "Contextures: Representations from Contexts",
        "abstract": "Despite the empirical success of foundation models, we do not have a systematic characterization of the representations that these models learn. In this paper, we establish the contexture theory. It shows that a large class of representation learning methods can be characterized as learning from the association between the input and a context variable. Specifically, we show that many popular methods aim to approximate the top-d singular functions of the expectation operator induced by the context, in which case we say that the representation learns the contexture. We demonstrate the generality of the contexture theory by proving that representation learning within various learning paradigms -- supervised, self-supervised, and manifold learning -- can all be studied from such a perspective. We also prove that the representations that learn the contexture are optimal on those tasks that are compatible with the context. One important implication of the contexture theory is that once the model is large enough to approximate the top singular functions, further scaling up the model size yields diminishing returns. Therefore, scaling is not all we need, and further improvement requires better contexts. To this end, we study how to evaluate the usefulness of a context without knowing the downstream tasks. We propose a metric and show by experiments that it correlates well with the actual performance of the encoder on many real datasets.",
        "arxiv_id": "2505.01557"
    },
    "2505.03335": {
        "SCORE": 18,
        "ARXIVID": "2505.03335",
        "COMMENT": "The paper introduces a self-evolving reasoning paradigm for LLMs, which aligns with the 'Large Language Models' criterion for foundational innovations in reasoning capabilities.",
        "RELEVANCE": 9,
        "NOVELTY": 9,
        "authors": [
            "Andrew Zhao",
            "Yiran Wu",
            "Yang Yue",
            "Tong Wu",
            "Quentin Xu",
            "Yang Yue",
            "Matthieu Lin",
            "Shenzhi Wang",
            "Qingyun Wu",
            "Zilong Zheng",
            "Gao Huang"
        ],
        "title": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
        "abstract": "Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from outcome-based rewards. Recent RLVR works that operate under the zero setting avoid supervision in labeling the reasoning process, but still depend on manually curated collections of questions and answers for training. The scarcity of high-quality, human-produced examples raises concerns about the long-term scalability of relying on human supervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a hypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited learning potential for a superintelligent system. To address these concerns, we propose a new RLVR paradigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own learning progress and improves reasoning by solving them, without relying on any external data. Under this paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training curriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks and verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded learning. Despite being trained entirely without external data, AZR achieves overall SOTA performance on coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens of thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be effectively applied across different model scales and is compatible with various model classes.",
        "arxiv_id": "2505.03335"
    },
    "2505.02581": {
        "SCORE": 18,
        "ARXIVID": "2505.02581",
        "COMMENT": "The paper provides a theoretical argument about AI alignment and introduces a novel perspective on misalignment as a strategy, which aligns with emerging trends in foundational AI research.",
        "RELEVANCE": 9,
        "NOVELTY": 9,
        "authors": [
            "Alberto Hern\\'andez-Espinosa",
            "Felipe S. Abrah\\~ao",
            "Olaf Witkowski",
            "Hector Zenil"
        ],
        "title": "Agentic Neurodivergence as a Contingent Solution to the AI Alignment Problem",
        "abstract": "The AI alignment problem, which focusses on ensuring that artificial intelligence (AI), including AGI and ASI, systems act according to human values, presents profound challenges. With the progression from narrow AI to Artificial General Intelligence (AGI) and Superintelligence, fears about control and existential risk have escalated. This paper demonstrates that achieving complete alignment is inherently unattainable due to mathematical principles rooted in the foundations of predicate logic and computability, in particular Turing's computational universality, G\\\"odel's incompleteness and Chaitin's randomness. Instead, we argue that embracing AI misalignment or agent's `neurodivergence' as a contingent strategy, defined as fostering a dynamic ecosystem of competing, partially aligned agents, is a possible only viable path to mitigate risks. Through mathematical proofs and an experimental design, we explore how misalignment may serve and should be promoted as a counterbalancing mechanism to team up with whichever agents are most aligned AI to human values, ensuring that no single system dominates destructively. The main premise of our contribution is that misalignment is inevitable because full AI-human alignment is a mathematical impossibility from Turing-complete systems which we also prove in this paper, a feature then inherited to AGI and ASI systems. We introduce and test `change-of-opinion' attacks based on this kind of perturbation and intervention analysis to study how agents may neutralise friendly or unfriendly AIs through cooperation, competition or malice.",
        "arxiv_id": "2505.02581"
    },
    "2505.03648": {
        "SCORE": 18,
        "ARXIVID": "2505.03648",
        "COMMENT": "The paper introduces a theoretical framework combining oscillatory and threshold units, which aligns with foundational research on neural coding and architecture-level innovations.",
        "RELEVANCE": 9,
        "NOVELTY": 9,
        "authors": [
            "Vladimir Fanaskov",
            "Ivan Oseledets"
        ],
        "title": "Binding threshold units with artificial oscillatory neurons",
        "abstract": "Artificial Kuramoto oscillatory neurons were recently introduced as an alternative to threshold units. Empirical evidence suggests that oscillatory units outperform threshold units in several tasks including unsupervised object discovery and certain reasoning problems. The proposed coupling mechanism for these oscillatory neurons is heterogeneous, combining a generalized Kuramoto equation with standard coupling methods used for threshold units. In this research note, we present a theoretical framework that clearly distinguishes oscillatory neurons from threshold units and establishes a coupling mechanism between them. We argue that, from a biological standpoint, oscillatory and threshold units realise distinct aspects of neural coding: roughly, threshold units model intensity of neuron firing, while oscillatory units facilitate information exchange by frequency modulation. To derive interaction between these two types of units, we constrain their dynamics by focusing on dynamical systems that admit Lyapunov functions. For threshold units, this leads to Hopfield associative memory model, and for oscillatory units it yields a specific form of generalized Kuramoto model. The resulting dynamical systems can be naturally coupled to form a Hopfield-Kuramoto associative memory model, which also admits a Lyapunov function. Various forms of coupling are possible. Notably, oscillatory neurons can be employed to implement a low-rank correction to the weight matrix of a Hopfield network. This correction can be viewed either as a form of Hebbian learning or as a popular LoRA method used for fine-tuning of large language models. We demonstrate the practical realization of this particular coupling through illustrative toy experiments.",
        "arxiv_id": "2505.03648"
    },
    "2505.03205": {
        "SCORE": 17,
        "ARXIVID": "2505.03205",
        "COMMENT": "The paper provides theoretical insights into how transformers leverage low-dimensional structures in noisy data, aligning with the 'Model Architecture' criterion for foundational analysis of transformers.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Zhaiming Shen",
            "Alex Havrilla",
            "Rongjie Lai",
            "Alexander Cloninger",
            "Wenjing Liao"
        ],
        "title": "Transformers for Learning on Noisy and Task-Level Manifolds: Approximation and Generalization Insights",
        "abstract": "Transformers serve as the foundational architecture for large language and video generation models, such as GPT, BERT, SORA and their successors. Empirical studies have demonstrated that real-world data and learning tasks exhibit low-dimensional structures, along with some noise or measurement error. The performance of transformers tends to depend on the intrinsic dimension of the data/tasks, though theoretical understandings remain largely unexplored for transformers. This work establishes a theoretical foundation by analyzing the performance of transformers for regression tasks involving noisy input data on a manifold. Specifically, the input data are in a tubular neighborhood of a manifold, while the ground truth function depends on the projection of the noisy data onto the manifold. We prove approximation and generalization errors which crucially depend on the intrinsic dimension of the manifold. Our results demonstrate that transformers can leverage low-complexity structures in learning task even when the input data are perturbed by high-dimensional noise. Our novel proof technique constructs representations of basic arithmetic operations by transformers, which may hold independent interest.",
        "arxiv_id": "2505.03205"
    },
    "2505.03373": {
        "SCORE": 17,
        "ARXIVID": "2505.03373",
        "COMMENT": "This paper proposes SPAP, a structured pruning framework for LLMs, which aligns with the model compression criterion by introducing optimization-driven pruning methods.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Hanyu Hu",
            "Xiaoming Yuan"
        ],
        "title": "SPAP: Structured Pruning via Alternating Optimization and Penalty Methods",
        "abstract": "The deployment of large language models (LLMs) is often constrained by their substantial computational and memory demands. While structured pruning presents a viable approach by eliminating entire network components, existing methods suffer from performance degradation, reliance on heuristic metrics, or expensive finetuning. To address these challenges, we propose SPAP (Structured Pruning via Alternating Optimization and Penalty Methods), a novel and efficient structured pruning framework for LLMs grounded in optimization theory. SPAP formulates the pruning problem through a mixed-integer optimization model, employs a penalty method that effectively makes pruning decisions to minimize pruning errors, and introduces an alternating minimization algorithm tailored to the splittable problem structure for efficient weight updates and performance recovery. Extensive experiments on OPT, LLaMA-3/3.1/3.2, and Qwen2.5 models demonstrate SPAP's superiority over state-of-the-art methods, delivering linear inference speedups (1.29$\\times$ at 30% sparsity) and proportional memory reductions. Our work offers a practical, optimization-driven solution for pruning LLMs while preserving model performance.",
        "arxiv_id": "2505.03373"
    },
    "2505.03005": {
        "SCORE": 17,
        "ARXIVID": "2505.03005",
        "COMMENT": "The paper proposes a method for distilling softmax attention transformers into linear attention decoders, aligning with model compression and efficiency breakthroughs.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Daniel Goldstein",
            "Eric Alcaide",
            "Janna Lu",
            "Eugene Cheah"
        ],
        "title": "RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale",
        "abstract": "We present Rapid Attention Distillation to Linear Attention Decoders at Scale (RADLADS), a protocol for rapidly converting softmax attention transformers into linear attention decoder models, along with two new RWKV-variant architectures, and models converted from popular Qwen2.5 open source models in 7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens, less than 0.005% of the token count used to train the original teacher models. Converting to our 72B linear attention model costs less than \\$2,000 USD at today's prices, yet quality at inference remains close to the original transformer. These models achieve state-of-the-art downstream performance across a set of standard benchmarks for linear attention models of their size. We release all our models on HuggingFace under the Apache 2.0 license, with the exception of our 72B models which are also governed by the Qwen License Agreement.   Models at https://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102 Training Code at https://github.com/recursal/RADLADS-paper",
        "arxiv_id": "2505.03005"
    },
    "2505.01855": {
        "SCORE": 17,
        "ARXIVID": "2505.01855",
        "COMMENT": "The paper introduces intra-layer recurrence in transformers, which aligns with architectural innovations and efficiency improvements in transformer models.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Anthony Nguyen",
            "Wenjun Lin"
        ],
        "title": "Intra-Layer Recurrence in Transformers for Language Modeling",
        "abstract": "Transformer models have established new benchmarks in natural language processing; however, their increasing depth results in substantial growth in parameter counts. While existing recurrent transformer methods address this issue by reprocessing layers multiple times, they often apply recurrence indiscriminately across entire blocks of layers. In this work, we investigate Intra-Layer Recurrence (ILR), a more targeted approach that applies recurrence selectively to individual layers within a single forward pass. Our experiments show that allocating more iterations to earlier layers yields optimal results. These findings suggest that ILR offers a promising direction for optimizing recurrent structures in transformer architectures.",
        "arxiv_id": "2505.01855"
    },
    "2505.03717": {
        "SCORE": 17,
        "ARXIVID": "2505.03717",
        "COMMENT": "The paper investigates theoretical properties of low-rank matrix recovery with nonnegative constraints, which aligns with the model compression topic, particularly low-rank approaches.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Richard Y. Zhang"
        ],
        "title": "Nonnegative Low-rank Matrix Recovery Can Have Spurious Local Minima",
        "abstract": "The classical low-rank matrix recovery problem is well-known to exhibit \\emph{benign nonconvexity} under the restricted isometry property (RIP): local optimization is guaranteed to converge to the global optimum, where the ground truth is recovered. We investigate whether benign nonconvexity continues to hold when the factor matrices are constrained to be elementwise nonnegative -- a common practical requirement. In the simple setting of a rank-1 nonnegative ground truth, we confirm that benign nonconvexity holds in the fully-observed case with RIP constant $\\delta=0$. Surprisingly, however, this property fails to extend to the partially-observed case with any arbitrarily small RIP constant $\\delta\\to0^{+}$, irrespective of rank overparameterization. This finding exposes a critical theoretical gap: the continuity argument widely used to explain the empirical robustness of low-rank matrix recovery fundamentally breaks down once nonnegative constraints are imposed.",
        "arxiv_id": "2505.03717"
    },
    "2505.02922": {
        "SCORE": 17,
        "ARXIVID": "2505.02922",
        "COMMENT": "The paper proposes RetroInfer, a novel system for efficient long-context LLM inference, which aligns with model compression and efficiency topics.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Yaoqi Chen",
            "Jinkai Zhang",
            "Baotong Lu",
            "Qianxi Zhang",
            "Chengruidong Zhang",
            "Jingjia Luo",
            "Di Liu",
            "Huiqiang Jiang",
            "Qi Chen",
            "Jing Liu",
            "Bailu Ding",
            "Xiao Yan",
            "Jiawei Jiang",
            "Chen Chen",
            "Mingxing Zhang",
            "Yuqing Yang",
            "Fan Yang",
            "Mao Yang"
        ],
        "title": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM Inference",
        "abstract": "The growing context lengths of large language models (LLMs) pose significant challenges for efficient inference, primarily due to GPU memory and bandwidth constraints. We present RetroInfer, a novel system that reconceptualizes the key-value (KV) cache as a vector storage system which exploits the inherent attention sparsity to accelerate long-context LLM inference. At its core is the wave index, an Attention-aWare VEctor index that enables efficient and accurate retrieval of critical tokens through techniques such as tripartite attention approximation, accuracy-bounded attention estimation, and segmented clustering. Complementing this is the wave buffer, which coordinates KV cache placement and overlaps computation and data transfer across GPU and CPU to sustain high throughput. Unlike prior sparsity-based methods that struggle with token selection and hardware coordination, RetroInfer delivers robust performance without compromising model accuracy. Experiments on long-context benchmarks show up to 4.5X speedup over full attention within GPU memory limits and up to 10.5X over sparse attention baselines when KV cache is extended to CPU memory, all while preserving full-attention-level accuracy.",
        "arxiv_id": "2505.02922"
    },
    "2505.02072": {
        "SCORE": 17,
        "ARXIVID": "2505.02072",
        "COMMENT": "The paper provides theoretical insights into the interpretation of LLM probabilities, aligning with the LLM behavior/interpretability criterion.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Eitan Wagner",
            "Omri Abend"
        ],
        "title": "What do Language Model Probabilities Represent? From Distribution Estimation to Response Prediction",
        "abstract": "The notion of language modeling has gradually shifted in recent years from a distribution over finite-length strings to general-purpose prediction models for textual inputs and outputs, following appropriate alignment phases. This paper analyzes the distinction between distribution estimation and response prediction in the context of LLMs, and their often conflicting goals. We examine the training phases of LLMs, which include pretraining, in-context learning, and preference tuning, and also the common use cases for their output probabilities, which include completion probabilities and explicit probabilities as output. We argue that the different settings lead to three distinct intended output distributions. We demonstrate that NLP works often assume that these distributions should be similar, which leads to misinterpretations of their experimental findings. Our work sets firmer formal foundations for the interpretation of LLMs, which will inform ongoing work on the interpretation and use of LLMs' induced distributions.",
        "arxiv_id": "2505.02072"
    },
    "2505.03531": {
        "SCORE": 17,
        "ARXIVID": "2505.03531",
        "COMMENT": "The paper discusses efficiency optimization for sparse Mixture of Experts (MoE) models, which aligns closely with the model architecture and efficiency criteria.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Haoqi Yang",
            "Luohe Shi",
            "Qiwei Li",
            "Zuchao Li",
            "Ping Wang",
            "Bo Du",
            "Mengjia Shen",
            "Hai Zhao"
        ],
        "title": "Faster MoE LLM Inference for Extremely Large Models",
        "abstract": "Sparse Mixture of Experts (MoE) large language models (LLMs) are gradually becoming the mainstream approach for ultra-large-scale models. Existing optimization efforts for MoE models have focused primarily on coarse-grained MoE architectures. With the emergence of DeepSeek Models, fine-grained MoE models are gaining popularity, yet research on them remains limited. Therefore, we want to discuss the efficiency dynamic under different service loads. Additionally, fine-grained models allow deployers to reduce the number of routed experts, both activated counts and total counts, raising the question of how this reduction affects the trade-off between MoE efficiency and performance. Our findings indicate that while deploying MoE models presents greater challenges, it also offers significant optimization opportunities. Reducing the number of activated experts can lead to substantial efficiency improvements in certain scenarios, with only minor performance degradation. Reducing the total number of experts provides limited efficiency gains but results in severe performance degradation. Our method can increase throughput by at least 10\\% without any performance degradation. Overall, we conclude that MoE inference optimization remains an area with substantial potential for exploration and improvement.",
        "arxiv_id": "2505.03531"
    },
    "2505.02369": {
        "SCORE": 17,
        "ARXIVID": "2505.02369",
        "COMMENT": "The paper introduces ZSharp, an improvement to Sharpness-Aware Minimization, which directly contributes to foundational research in optimization and generalization in neural networks.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Juyoung Yun"
        ],
        "title": "Sharpness-Aware Minimization with Z-Score Gradient Filtering for Neural Networks",
        "abstract": "Generalizing well in deep neural networks remains a core challenge, particularly due to their tendency to converge to sharp minima that degrade robustness. Sharpness-Aware Minimization (SAM) mitigates this by seeking flatter minima but perturbs parameters using the full gradient, which can include statistically insignificant directions. We propose ZSharp, a simple yet effective extension to SAM that applies layer-wise Z-score normalization followed by percentile-based filtering to retain only statistically significant gradient components. This selective perturbation aligns updates with curvature-sensitive directions, enhancing generalization without requiring architectural changes. ZSharp introduces only one additional hyperparameter, the percentile threshold, and remains fully compatible with existing SAM variants. Experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet using ResNet, VGG, and Vision Transformers show that ZSharp consistently outperforms SAM and its variants in test accuracy, particularly on deeper and transformer-based models. These results demonstrate that ZSharp is a principled and lightweight improvement for sharpness-aware optimization.",
        "arxiv_id": "2505.02369"
    },
    "2505.01459": {
        "SCORE": 17,
        "ARXIVID": "2505.01459",
        "COMMENT": "The paper introduces MoxE, a novel architecture combining xLSTM and Mixture of Experts (MoE) with an entropy-aware routing mechanism. This aligns closely with the 'Model Architecture' criterion, particularly MoE and sparsity.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Abdoul Majid O. Thiombiano",
            "Brahim Hnich",
            "Ali Ben Mrad",
            "Mohamed Wiem Mkaouer"
        ],
        "title": "MoxE: Mixture of xLSTM Experts with Entropy-Aware Routing for Efficient Language Modeling",
        "abstract": "This paper introduces MoxE, a novel architecture that synergistically combines the Extended Long Short-Term Memory (xLSTM) with the Mixture of Experts (MoE) framework to address critical scalability and efficiency challenges in large language models (LLMs). The proposed method effectively leverages xLSTM's innovative memory structures while strategically introducing sparsity through MoE to substantially reduce computational overhead. At the heart of our approach is a novel entropy-based routing mechanism, designed to dynamically route tokens to specialized experts, thereby ensuring efficient and balanced resource utilization. This entropy awareness enables the architecture to effectively manage both rare and common tokens, with mLSTM blocks being favored to handle rare tokens. To further enhance generalization, we introduce a suite of auxiliary losses, including entropy-based and group-wise balancing losses, ensuring robust performance and efficient training. Theoretical analysis and empirical evaluations rigorously demonstrate that MoxE achieves significant efficiency gains and enhanced effectiveness compared to existing approaches, marking a notable advancement in scalable LLM architectures.",
        "arxiv_id": "2505.01459"
    },
    "2505.03281": {
        "SCORE": 16,
        "ARXIVID": "2505.03281",
        "COMMENT": "The paper proposes a novel recurrent architecture inspired by physics, which aligns with the 'Model Architecture' criterion for foundational innovations in sequence modeling.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Zhou Wu",
            "Junyi An",
            "Baile Xu",
            "Furao Shen",
            "Jian Zhao"
        ],
        "title": "Physics-inspired Energy Transition Neural Network for Sequence Learning",
        "abstract": "Recently, the superior performance of Transformers has made them a more robust and scalable solution for sequence modeling than traditional recurrent neural networks (RNNs). However, the effectiveness of Transformer in capturing long-term dependencies is primarily attributed to their comprehensive pair-modeling process rather than inherent inductive biases toward sequence semantics. In this study, we explore the capabilities of pure RNNs and reassess their long-term learning mechanisms. Inspired by the physics energy transition models that track energy changes over time, we propose a effective recurrent structure called the``Physics-inspired Energy Transition Neural Network\" (PETNN). We demonstrate that PETNN's memory mechanism effectively stores information over long-term dependencies. Experimental results indicate that PETNN outperforms transformer-based methods across various sequence tasks. Furthermore, owing to its recurrent nature, PETNN exhibits significantly lower complexity. Our study presents an optimal foundational recurrent architecture and highlights the potential for developing effective recurrent neural networks in fields currently dominated by Transformer.",
        "arxiv_id": "2505.03281"
    },
    "2505.03176": {
        "SCORE": 16,
        "ARXIVID": "2505.03176",
        "COMMENT": "The paper introduces seq-JEPA, a self-supervised learning framework for invariant and equivariant representations. It aligns with representation learning and architectural innovations.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Hafez Ghaemi",
            "Eilif Muller",
            "Shahab Bakhtiari"
        ],
        "title": "seq-JEPA: Autoregressive Predictive Learning of Invariant-Equivariant World Models",
        "abstract": "Current self-supervised algorithms mostly rely on transformations such as data augmentation and masking to learn visual representations. This is achieved by inducing invariance or equivariance with respect to these transformations after encoding two views of an image. This dominant two-view paradigm can limit the flexibility of learned representations for downstream adaptation by creating performance trade-offs between invariance-related tasks such as image classification and more fine-grained equivariance-related tasks. In this work, we introduce \\emph{seq-JEPA}, a world modeling paradigm based on joint-embedding predictive architecture that leverages architectural inductive biases to resolve this trade-off. Without requiring an additional equivariance predictor or loss term, seq-JEPA simultaneously learns two architecturally segregated representations: one equivariant to the specified transformations and another invariant to them and suited for tasks such as classification. To do so, our model processes a short sequence of different views (observations) of an input image. Each encoded view is concatenated with embeddings corresponding to the relative transformation (action) producing the next observation in the sequence. A transformer encoder outputs an aggregate representation of this sequence, which is subsequently conditioned on the action leading to the next observation to predict its representation. Empirically, seq-JEPA achieves strong performance on equivariant benchmarks and image classification without sacrificing one for the other. Additionally, our framework excels at tasks that inherently require aggregating a sequence of observations, such as path integration across actions and predictive learning across eye movements.",
        "arxiv_id": "2505.03176"
    },
    "2505.02972": {
        "SCORE": 16,
        "ARXIVID": "2505.02972",
        "COMMENT": "The paper introduces a geometry-aware MTL framework, which aligns with representation learning and foundational innovations in multi-task learning.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Aoran Chen",
            "Yang Feng"
        ],
        "title": "GeoERM: Geometry-Aware Multi-Task Representation Learning on Riemannian Manifolds",
        "abstract": "Multi-Task Learning (MTL) seeks to boost statistical power and learning efficiency by discovering structure shared across related tasks. State-of-the-art MTL representation methods, however, usually treat the latent representation matrix as a point in ordinary Euclidean space, ignoring its often non-Euclidean geometry, thus sacrificing robustness when tasks are heterogeneous or even adversarial. We propose GeoERM, a geometry-aware MTL framework that embeds the shared representation on its natural Riemannian manifold and optimizes it via explicit manifold operations. Each training cycle performs (i) a Riemannian gradient step that respects the intrinsic curvature of the search space, followed by (ii) an efficient polar retraction to remain on the manifold, guaranteeing geometric fidelity at every iteration. The procedure applies to a broad class of matrix-factorized MTL models and retains the same per-iteration cost as Euclidean baselines. Across a set of synthetic experiments with task heterogeneity and on a wearable-sensor activity-recognition benchmark, GeoERM consistently improves estimation accuracy, reduces negative transfer, and remains stable under adversarial label noise, outperforming leading MTL and single-task alternatives.",
        "arxiv_id": "2505.02972"
    },
    "2505.02884": {
        "SCORE": 16,
        "ARXIVID": "2505.02884",
        "COMMENT": "The paper introduces a novel unlearning method for LLMs, which aligns with emerging trends in ethical AI and foundational aspects of LLM behavior.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Guangzhi Sun",
            "Potsawee Manakul",
            "Xiao Zhan",
            "Mark Gales"
        ],
        "title": "Unlearning vs. Obfuscation: Are We Truly Removing Knowledge?",
        "abstract": "Unlearning has emerged as a critical capability for large language models (LLMs) to support data privacy, regulatory compliance, and ethical AI deployment. Recent techniques often rely on obfuscation by injecting incorrect or irrelevant information to suppress knowledge. Such methods effectively constitute knowledge addition rather than true removal, often leaving models vulnerable to probing. In this paper, we formally distinguish unlearning from obfuscation and introduce a probing-based evaluation framework to assess whether existing approaches genuinely remove targeted information. Moreover, we propose DF-MCQ, a novel unlearning method that flattens the model predictive distribution over automatically generated multiple-choice questions using KL-divergence, effectively removing knowledge about target individuals and triggering appropriate refusal behaviour. Experimental results demonstrate that DF-MCQ achieves unlearning with over 90% refusal rate and a random choice-level uncertainty that is much higher than obfuscation on probing questions.",
        "arxiv_id": "2505.02884"
    },
    "2505.02322": {
        "SCORE": 16,
        "ARXIVID": "2505.02322",
        "COMMENT": "The paper proposes a novel reasoning paradigm for LLMs using hierarchical planning, which aligns with foundational research on improving LLM reasoning capabilities.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Runquan Gui",
            "Zhihai Wang",
            "Jie Wang",
            "Chi Ma",
            "Huiling Zhen",
            "Mingxuan Yuan",
            "Jianye Hao",
            "Defu Lian",
            "Enhong Chen",
            "Feng Wu"
        ],
        "title": "HyperTree Planning: Enhancing LLM Reasoning via Hierarchical Thinking",
        "abstract": "Recent advancements have significantly enhanced the performance of large language models (LLMs) in tackling complex reasoning tasks, achieving notable success in domains like mathematical and logical reasoning. However, these methods encounter challenges with complex planning tasks, primarily due to extended reasoning steps, diverse constraints, and the challenge of handling multiple distinct sub-tasks. To address these challenges, we propose HyperTree Planning (HTP), a novel reasoning paradigm that constructs hypertree-structured planning outlines for effective planning. The hypertree structure enables LLMs to engage in hierarchical thinking by flexibly employing the divide-and-conquer strategy, effectively breaking down intricate reasoning steps, accommodating diverse constraints, and managing multiple distinct sub-tasks in a well-organized manner. We further introduce an autonomous planning framework that completes the planning process by iteratively refining and expanding the hypertree-structured planning outlines. Experiments demonstrate the effectiveness of HTP, achieving state-of-the-art accuracy on the TravelPlanner benchmark with Gemini-1.5-Pro, resulting in a 3.6 times performance improvement over o1-preview.",
        "arxiv_id": "2505.02322"
    },
    "2505.03052": {
        "SCORE": 16,
        "ARXIVID": "2505.03052",
        "COMMENT": "The paper proposes a pretraining paradigm (SLUNG) to handle high-risk data, which aligns with foundational research on LLM behavior and interpretability.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Ryan Wang",
            "Matthew Finlayson",
            "Luca Soldaini",
            "Swabha Swayamdipta",
            "Robin Jia"
        ],
        "title": "Teaching Models to Understand (but not Generate) High-risk Data",
        "abstract": "Language model developers typically filter out high-risk content -- such as toxic or copyrighted text -- from their pre-training data to prevent models from generating similar outputs. However, removing such data altogether limits models' ability to recognize and appropriately respond to harmful or sensitive content. In this paper, we introduce Selective Loss to Understand but Not Generate (SLUNG), a pre-training paradigm through which models learn to understand high-risk data without learning to generate it. Instead of uniformly applying the next-token prediction loss, SLUNG selectively avoids incentivizing the generation of high-risk tokens while ensuring they remain within the model's context window. As the model learns to predict low-risk tokens that follow high-risk ones, it is forced to understand the high-risk content. Through our experiments, we show that SLUNG consistently improves models' understanding of high-risk data (e.g., ability to recognize toxic content) without increasing its generation (e.g., toxicity of model responses). Overall, our SLUNG paradigm enables models to benefit from high-risk text that would otherwise be filtered out.",
        "arxiv_id": "2505.03052"
    },
    "2505.01618": {
        "SCORE": 16,
        "ARXIVID": "2505.01618",
        "COMMENT": "The paper introduces CompleteP, a parameterization method for efficient LLM training that avoids lazy learning. This aligns with foundational research in LLM training dynamics and efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Nolan Dey",
            "Bin Claire Zhang",
            "Lorenzo Noci",
            "Mufan Li",
            "Blake Bordelon",
            "Shane Bergsma",
            "Cengiz Pehlevan",
            "Boris Hanin",
            "Joel Hestness"
        ],
        "title": "Don't be lazy: CompleteP enables compute-efficient deep transformers",
        "abstract": "We study compute efficiency of LLM training when using different parameterizations, i.e., rules for adjusting model and optimizer hyperparameters (HPs) as model size changes. Some parameterizations fail to transfer optimal base HPs (such as learning rate) across changes in model depth, requiring practitioners to either re-tune these HPs as they scale up (expensive), or accept sub-optimal training when re-tuning is prohibitive. Even when they achieve HP transfer, we develop theory to show parameterizations may still exist in the lazy learning regime where layers learn only features close to their linearization, preventing effective use of depth and nonlinearity. Finally, we identify and adopt the unique parameterization we call CompleteP that achieves both depth-wise HP transfer and non-lazy learning in all layers. CompleteP enables a wider range of model width/depth ratios to remain compute-efficient, unlocking shapes better suited for different hardware settings and operational contexts. Moreover, CompleteP enables 12-34\\% compute efficiency improvements over the prior state-of-the-art.",
        "arxiv_id": "2505.01618"
    },
    "2505.03069": {
        "SCORE": 16,
        "ARXIVID": "2505.03069",
        "COMMENT": "The paper introduces BiLipREN, a robustly invertible recurrent neural model with theoretical guarantees. This aligns with foundational research in representation learning and dynamic models.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Yurui Zhang",
            "Ruigang Wang",
            "Ian R. Manchester"
        ],
        "title": "Robustly Invertible Nonlinear Dynamics and the BiLipREN: Contracting Neural Models with Contracting Inverses",
        "abstract": "We study the invertibility of nonlinear dynamical systems from the perspective of contraction and incremental stability analysis and propose a new invertible recurrent neural model: the BiLipREN. In particular, we consider a nonlinear state space model to be robustly invertible if an inverse exists with a state space realisation, and both the forward model and its inverse are contracting, i.e. incrementally exponentially stable, and Lipschitz, i.e. have bounded incremental gain. This property of bi-Lipschitzness implies both robustness in the sense of sensitivity to input perturbations, as well as robust distinguishability of different inputs from their corresponding outputs, i.e. the inverse model robustly reconstructs the input sequence despite small perturbations to the initial conditions and measured output. Building on this foundation, we propose a parameterization of neural dynamic models: bi-Lipschitz recurrent equilibrium networks (biLipREN), which are robustly invertible by construction. Moreover, biLipRENs can be composed with orthogonal linear systems to construct more general bi-Lipschitz dynamic models, e.g., a nonlinear analogue of minimum-phase/all-pass (inner/outer) factorization. We illustrate the utility of our proposed approach with numerical examples.",
        "arxiv_id": "2505.03069"
    },
    "2505.02537": {
        "SCORE": 15,
        "ARXIVID": "2505.02537",
        "COMMENT": "The paper provides theoretical advancements in monotonic neural networks, aligning with the 'Model Architecture' criterion for foundational innovations.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Davide Sartor",
            "Alberto Sinigaglia",
            "Gian Antonio Susto"
        ],
        "title": "Advancing Constrained Monotonic Neural Networks: Achieving Universal Approximation Beyond Bounded Activations",
        "abstract": "Conventional techniques for imposing monotonicity in MLPs by construction involve the use of non-negative weight constraints and bounded activation functions, which pose well-known optimization challenges. In this work, we generalize previous theoretical results, showing that MLPs with non-negative weight constraint and activations that saturate on alternating sides are universal approximators for monotonic functions. Additionally, we show an equivalence between the saturation side in the activations and the sign of the weight constraint. This connection allows us to prove that MLPs with convex monotone activations and non-positive constrained weights also qualify as universal approximators, in contrast to their non-negative constrained counterparts. Our results provide theoretical grounding to the empirical effectiveness observed in previous works while leading to possible architectural simplification. Moreover, to further alleviate the optimization difficulties, we propose an alternative formulation that allows the network to adjust its activations according to the sign of the weights. This eliminates the requirement for weight reparameterization, easing initialization and improving training stability. Experimental evaluation reinforces the validity of the theoretical results, showing that our novel approach compares favourably to traditional monotonic architectures.",
        "arxiv_id": "2505.02537"
    },
    "2505.01997": {
        "SCORE": 15,
        "ARXIVID": "2505.01997",
        "COMMENT": "The paper addresses calibration issues in LLMs and proposes a novel fine-tuning approach, aligning with the 'Large Language Models' criterion for foundational insights into model behavior.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Jiancong Xiao",
            "Bojian Hou",
            "Zhanliang Wang",
            "Ruochen Jin",
            "Qi Long",
            "Weijie J. Su",
            "Li Shen"
        ],
        "title": "Restoring Calibration for Aligned Large Language Models: A Calibration-Aware Fine-Tuning Approach",
        "abstract": "One of the key technologies for the success of Large Language Models (LLMs) is preference alignment. However, a notable side effect of preference alignment is poor calibration: while the pre-trained models are typically well-calibrated, LLMs tend to become poorly calibrated after alignment with human preferences. In this paper, we investigate why preference alignment affects calibration and how to address this issue. For the first question, we observe that the preference collapse issue in alignment undesirably generalizes to the calibration scenario, causing LLMs to exhibit overconfidence and poor calibration. To address this, we demonstrate the importance of fine-tuning with domain-specific knowledge to alleviate the overconfidence issue. To further analyze whether this affects the model's performance, we categorize models into two regimes: calibratable and non-calibratable, defined by bounds of Expected Calibration Error (ECE). In the calibratable regime, we propose a calibration-aware fine-tuning approach to achieve proper calibration without compromising LLMs' performance. However, as models are further fine-tuned for better performance, they enter the non-calibratable regime. For this case, we develop an EM-algorithm-based ECE regularization for the fine-tuning loss to maintain low calibration error. Extensive experiments validate the effectiveness of the proposed methods.",
        "arxiv_id": "2505.01997"
    },
    "2505.02130": {
        "SCORE": 15,
        "ARXIVID": "2505.02130",
        "COMMENT": "The paper explores attention mechanisms in LLMs for graph-structured data, aligning with the criterion of analyzing LLM behavior and interpretability.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Zhong Guan",
            "Likang Wu",
            "Hongke Zhao",
            "Ming He",
            "Jianpin Fan"
        ],
        "title": "Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data",
        "abstract": "Attention mechanisms are critical to the success of large language models (LLMs), driving significant advancements in multiple fields. However, for graph-structured data, which requires emphasis on topological connections, they fall short compared to message-passing mechanisms on fixed links, such as those employed by Graph Neural Networks (GNNs). This raises a question: ``Does attention fail for graphs in natural language settings?'' Motivated by these observations, we embarked on an empirical study from the perspective of attention mechanisms to explore how LLMs process graph-structured data. The goal is to gain deeper insights into the attention behavior of LLMs over graph structures. We uncovered unique phenomena regarding how LLMs apply attention to graph-structured data and analyzed these findings to improve the modeling of such data by LLMs. The primary findings of our research are: 1) While LLMs can recognize graph data and capture text-node interactions, they struggle to model inter-node relationships within graph structures due to inherent architectural constraints. 2) The attention distribution of LLMs across graph nodes does not align with ideal structural patterns, indicating a failure to adapt to graph topology nuances. 3) Neither fully connected attention nor fixed connectivity is optimal; each has specific limitations in its application scenarios. Instead, intermediate-state attention windows improve LLM training performance and seamlessly transition to fully connected windows during inference. Source code: \\href{https://github.com/millioniron/LLM_exploration}{LLM4Exploration}",
        "arxiv_id": "2505.02130"
    },
    "2505.02811": {
        "SCORE": 15,
        "ARXIVID": "2505.02811",
        "COMMENT": "The paper proposes a framework for improving multi-round RAG systems, which aligns with foundational improvements in LLM behavior and self-awareness.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Diji Yang",
            "Linda Zeng",
            "Jinmeng Rao",
            "Yi Zhang"
        ],
        "title": "Knowing You Don't Know: Learning When to Continue Search in Multi-round RAG through Self-Practicing",
        "abstract": "Retrieval Augmented Generation (RAG) has shown strong capability in enhancing language models' knowledge and reducing AI generative hallucinations, driving its widespread use. However, complex tasks requiring multi-round retrieval remain challenging, and early attempts tend to be overly optimistic without a good sense of self-skepticism. Current multi-round RAG systems may continue searching even when enough information has already been retrieved, or they may provide incorrect answers without having sufficient information or knowledge. Existing solutions either require large amounts of expensive human-labeled process supervision data or lead to subpar performance.   This paper aims to address these limitations by introducing a new framework, \\textbf{SIM-RAG}, to explicitly enhance RAG systems' self-awareness and multi-round retrieval capabilities. To train SIM-RAG, we first let a RAG system self-practice multi-round retrieval, augmenting existing question-answer pairs with intermediate inner monologue reasoning steps to generate synthetic training data. For each pair, the system may explore multiple retrieval paths, which are labeled as successful if they reach the correct answer and unsuccessful otherwise. Using this data, we train a lightweight information sufficiency Critic. At inference time, the Critic evaluates whether the RAG system has retrieved sufficient information at each round, guiding retrieval decisions and improving system-level self-awareness through in-context reinforcement learning.   Experiments across multiple prominent RAG benchmarks show that SIM-RAG is an effective multi-round RAG solution. Furthermore, this framework is system-efficient, adding a lightweight component to RAG without requiring modifications to existing LLMs or search engines, and data-efficient, eliminating the need for costly human-annotated mid-step retrieval process supervision data.",
        "arxiv_id": "2505.02811"
    },
    "2505.02533": {
        "SCORE": 15,
        "ARXIVID": "2505.02533",
        "COMMENT": "The paper proposes a resource-aware partitioning algorithm for LLM inference, which aligns with model compression and efficiency topics.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Dimitrios Kafetzis",
            "Ramin Khalili",
            "Iordanis Koutsopoulos"
        ],
        "title": "Large Language Model Partitioning for Low-Latency Inference at the Edge",
        "abstract": "Large Language Models (LLMs) based on autoregressive, decoder-only Transformers generate text one token at a time, where a token represents a discrete unit of text. As each newly produced token is appended to the partial output sequence, the length grows and so does the memory and compute load, due to the expanding key-value caches, which store intermediate representations of all previously generated tokens in the multi-head attention (MHA) layer. As this iterative process steadily increases memory and compute demands, layer-based partitioning in resource-constrained edge environments often results in memory overload or high inference latency. To address this and reduce inference latency, we propose a resource-aware Transformer architecture partitioning algorithm, where the partitioning decision is updated at regular intervals during token generation. The approach is myopic in that it is based on instantaneous information about device resource availability and network link bandwidths. When first executed, the algorithm places blocks on devices, and in later executions, it migrates these blocks among devices so that the sum of migration delay and inference delay remains low. Our approach partitions the decoder at the attention head level, co-locating each attention head with its key-value cache and allowing dynamic migrations whenever resources become tight. By allocating different attention heads to different devices, we exploit parallel execution of attention heads and thus achieve substantial reductions in inference delays. Our experiments show that in small-scale settings (3-5 devices), the proposed method achieves within 15 to 20 percent of an exact optimal solver's latency, while in larger-scale tests it achieves notable improvements in inference speed and memory usage compared to state-of-the-art layer-based partitioning approaches.",
        "arxiv_id": "2505.02533"
    },
    "2505.02390": {
        "SCORE": 15,
        "ARXIVID": "2505.02390",
        "COMMENT": "The paper discusses quantization techniques for large models, which aligns with the model compression criterion. The introduction of DQ3_K_M as a novel 3-bit quantization method adds some methodological contribution.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Enbo Zhao",
            "Yi Shen",
            "Shuming Shi",
            "Jieyun Huang",
            "Zhihao Chen",
            "Ning Wang",
            "Siqi Xiao",
            "Jian Zhang",
            "Kai Wang",
            "Shiguo Lian"
        ],
        "title": "Quantitative Analysis of Performance Drop in DeepSeek Model Quantization",
        "abstract": "Recently, there is a high demand for deploying DeepSeek-R1 and V3 locally, possibly because the official service often suffers from being busy and some organizations have data privacy concerns. While single-machine deployment offers infrastructure simplicity, the models' 671B FP8 parameter configuration exceeds the practical memory limits of a standard 8-GPU machine. Quantization is a widely used technique that helps reduce model memory consumption. However, it is unclear what the performance of DeepSeek-R1 and V3 will be after being quantized. This technical report presents the first quantitative evaluation of multi-bitwidth quantization across the complete DeepSeek model spectrum. Key findings reveal that 4-bit quantization maintains little performance degradation versus FP8 while enabling single-machine deployment on standard NVIDIA GPU devices. We further propose DQ3_K_M, a dynamic 3-bit quantization method that significantly outperforms traditional Q3_K_M variant on various benchmarks, which is also comparable with 4-bit quantization (Q4_K_M) approach in most tasks. Moreover, DQ3_K_M supports single-machine deployment configurations for both NVIDIA H100/A100 and Huawei 910B. Our implementation of DQ3\\_K\\_M is released at https://github.com/UnicomAI/DeepSeek-Eval, containing optimized 3-bit quantized variants of both DeepSeek-R1 and DeepSeek-V3.",
        "arxiv_id": "2505.02390"
    }
}