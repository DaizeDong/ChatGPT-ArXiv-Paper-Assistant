{
    "2506.10130": {
        "SCORE": 18,
        "ARXIVID": "2506.10130",
        "COMMENT": "The paper introduces a conjecture on a fundamental trade-off in AI systems, which aligns with emerging trends and theoretical insights.",
        "RELEVANCE": 9,
        "NOVELTY": 9,
        "authors": [
            "Luciano Floridi"
        ],
        "title": "A Conjecture on a Fundamental Trade-Off between Certainty and Scope in Symbolic and Generative AI",
        "abstract": "This article introduces a conjecture that formalises a fundamental trade-off between provable correctness and broad data-mapping capacity in Artificial Intelligence (AI) systems. When an AI system is engineered for deductively watertight guarantees (demonstrable certainty about the error-free nature of its outputs) -- as in classical symbolic AI -- its operational domain must be narrowly circumscribed and pre-structured. Conversely, a system that can input high-dimensional data to produce rich information outputs -- as in contemporary generative models -- necessarily relinquishes the possibility of zero-error performance, incurring an irreducible risk of errors or misclassification. By making this previously implicit trade-off explicit and open to rigorous verification, the conjecture significantly reframes both engineering ambitions and philosophical expectations for AI. After reviewing the historical motivations for this tension, the article states the conjecture in information-theoretic form and contextualises it within broader debates in epistemology, formal verification, and the philosophy of technology. It then offers an analysis of its implications and consequences, drawing on notions of underdetermination, prudent epistemic risk, and moral responsibility. The discussion clarifies how, if correct, the conjecture would help reshape evaluation standards, governance frameworks, and hybrid system design. The conclusion underscores the importance of eventually proving or refuting the inequality for the future of trustworthy AI.",
        "arxiv_id": "2506.10130"
    },
    "2506.10887": {
        "SCORE": 18,
        "ARXIVID": "2506.10887",
        "COMMENT": "The paper provides a theoretical foundation for understanding out-of-context reasoning in transformers, which aligns with theoretical insights into LLM behavior.",
        "RELEVANCE": 9,
        "NOVELTY": 9,
        "authors": [
            "Yixiao Huang",
            "Hanlin Zhu",
            "Tianyu Guo",
            "Jiantao Jiao",
            "Somayeh Sojoudi",
            "Michael I. Jordan",
            "Stuart Russell",
            "Song Mei"
        ],
        "title": "Generalization or Hallucination? Understanding Out-of-Context Reasoning in Transformers",
        "abstract": "Large language models (LLMs) can acquire new knowledge through fine-tuning, but this process exhibits a puzzling duality: models can generalize remarkably from new facts, yet are also prone to hallucinating incorrect information. However, the reasons for this phenomenon remain poorly understood. In this work, we argue that both behaviors stem from a single mechanism known as out-of-context reasoning (OCR): the ability to deduce implications by associating concepts, even those without a causal link. Our experiments across five prominent LLMs confirm that OCR indeed drives both generalization and hallucination, depending on whether the associated concepts are causally related. To build a rigorous theoretical understanding of this phenomenon, we then formalize OCR as a synthetic factual recall task. We empirically show that a one-layer single-head attention-only transformer with factorized output and value matrices can learn to solve this task, while a model with combined weights cannot, highlighting the crucial role of matrix factorization. Our theoretical analysis shows that the OCR capability can be attributed to the implicit bias of gradient descent, which favors solutions that minimize the nuclear norm of the combined output-value matrix. This mathematical structure explains why the model learns to associate facts and implications with high sample efficiency, regardless of whether the correlation is causal or merely spurious. Ultimately, our work provides a theoretical foundation for understanding the OCR phenomenon, offering a new lens for analyzing and mitigating undesirable behaviors from knowledge injection.",
        "arxiv_id": "2506.10887"
    },
    "2506.10972": {
        "SCORE": 18,
        "ARXIVID": "2506.10972",
        "COMMENT": "The paper introduces a refined scaling law for LLMs, which is relevant to large language models and offers theoretical insights into their behavior.",
        "RELEVANCE": 9,
        "NOVELTY": 9,
        "authors": [
            "Houyi Li",
            "Wenzhen Zheng",
            "Qiufeng Wang",
            "Zhenyu Ding",
            "Haoying Wang",
            "Zili Wang",
            "Shijie Xuyang",
            "Ning Ding",
            "Shuigeng Zhou",
            "Xiangyu Zhang",
            "Daxin Jiang"
        ],
        "title": "Farseer: A Refined Scaling Law in Large Language Models",
        "abstract": "Training Large Language Models (LLMs) is prohibitively expensive, creating a critical scaling gap where insights from small-scale experiments often fail to transfer to resource-intensive production systems, thereby hindering efficient innovation. To bridge this, we introduce Farseer, a novel and refined scaling law offering enhanced predictive accuracy across scales. By systematically constructing a model loss surface $L(N,D)$, Farseer achieves a significantly better fit to empirical data than prior laws (e.g., Chinchilla's law). Our methodology yields accurate, robust, and highly generalizable predictions, demonstrating excellent extrapolation capabilities, improving upon Chinchilla's law by reducing extrapolation error by 433\\%. This allows for the reliable evaluation of competing training strategies across all $(N,D)$ settings, enabling conclusions from small-scale ablation studies to be confidently extrapolated to predict large-scale performance. Furthermore, Farseer provides new insights into optimal compute allocation, better reflecting the nuanced demands of modern LLM training. To validate our approach, we trained an extensive suite of approximately 1,000 LLMs across diverse scales and configurations, consuming roughly 3 million NVIDIA H100 GPU hours. We are comprehensively open-sourcing all models, data, results, and logs at https://github.com/Farseer-Scaling-Law/Farseer to foster further research.",
        "arxiv_id": "2506.10972"
    },
    "2506.10205": {
        "SCORE": 17,
        "ARXIVID": "2506.10205",
        "COMMENT": "The paper focuses on model compression through activation-aware weight pruning and quantization, which aligns with the model compression criterion.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Jing Liu",
            "Toshiaki Koike-Akino",
            "Ye Wang",
            "Hassan Mansour",
            "Matthew Brand"
        ],
        "title": "AWP: Activation-Aware Weight Pruning and Quantization with Projected Gradient Descent",
        "abstract": "To address the enormous size of Large Language Models (LLMs), model compression methods, such as quantization and pruning, are often deployed, especially on edge devices. In this work, we focus on layer-wise post-training quantization and pruning. Drawing connections between activation-aware weight pruning and sparse approximation problems, and motivated by the success of Iterative Hard Thresholding (IHT), we propose a unified method for Activation-aware Weight pruning and quantization via Projected gradient descent (AWP). Our experiments demonstrate that AWP outperforms state-of-the-art LLM pruning and quantization methods. Theoretical convergence guarantees of the proposed method for pruning are also provided.",
        "arxiv_id": "2506.10205"
    },
    "2506.10378": {
        "SCORE": 17,
        "ARXIVID": "2506.10378",
        "COMMENT": "The paper uses causal representation learning to uncover latent capabilities of language models, aligning with representation learning and providing theoretical insights into LLM behavior.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Jikai Jin",
            "Vasilis Syrgkanis",
            "Sham Kakade",
            "Hanlin Zhang"
        ],
        "title": "Discovering Hierarchical Latent Capabilities of Language Models via Causal Representation Learning",
        "abstract": "Faithful evaluation of language model capabilities is crucial for deriving actionable insights that can inform model development. However, rigorous causal evaluations in this domain face significant methodological challenges, including complex confounding effects and prohibitive computational costs associated with extensive retraining. To tackle these challenges, we propose a causal representation learning framework wherein observed benchmark performance is modeled as a linear transformation of a few latent capability factors. Crucially, these latent factors are identified as causally interrelated after appropriately controlling for the base model as a common confounder. Applying this approach to a comprehensive dataset encompassing over 1500 models evaluated across six benchmarks from the Open LLM Leaderboard, we identify a concise three-node linear causal structure that reliably explains the observed performance variations. Further interpretation of this causal structure provides substantial scientific insights beyond simple numerical rankings: specifically, we reveal a clear causal direction starting from general problem-solving capabilities, advancing through instruction-following proficiency, and culminating in mathematical reasoning ability. Our results underscore the essential role of carefully controlling base model variations during evaluation, a step critical to accurately uncovering the underlying causal relationships among latent model capabilities.",
        "arxiv_id": "2506.10378"
    },
    "2506.10918": {
        "SCORE": 17,
        "ARXIVID": "2506.10918",
        "COMMENT": "The paper discusses a broad class of neural sequence models and introduces Prefix-Scannable Models, which is relevant to model architecture innovations.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Morris Yau",
            "Sharut Gupta",
            "Valerie Engelmayer",
            "Kazuki Irie",
            "Stefanie Jegelka",
            "Jacob Andreas"
        ],
        "title": "Sequential-Parallel Duality in Prefix Scannable Models",
        "abstract": "Modern neural sequence models are designed to meet the dual mandate of parallelizable training and fast sequential inference. Recent developments have given rise to various models, such as Gated Linear Attention (GLA) and Mamba, that achieve such ``sequential-parallel duality.'' This raises a natural question: can we characterize the full class of neural sequence models that support near-constant-time parallel evaluation and linear-time, constant-space sequential inference? We begin by describing a broad class of such models -- state space models -- as those whose state updates can be computed using the classic parallel prefix scan algorithm with a custom associative aggregation operator. We then define a more general class, Prefix-Scannable Models (PSMs), by relaxing the state aggregation operator to allow arbitrary (potentially non-associative) functions such as softmax attention. This generalization unifies many existing architectures, including element-wise RNNs (e.g., Mamba) and linear transformers (e.g., GLA, Mamba2, mLSTM), while also introducing new models with softmax-like operators that achieve O(1) amortized compute per token and log(N) memory for sequence length N. We empirically evaluate such models on illustrative small-scale language modeling and canonical synthetic tasks, including state tracking and associative recall. Empirically, we find that PSMs retain the expressivity of transformer-based architectures while matching the inference efficiency of state space models -- in some cases exhibiting better length generalization than either.",
        "arxiv_id": "2506.10918"
    },
    "2506.10959": {
        "SCORE": 17,
        "ARXIVID": "2506.10959",
        "COMMENT": "The paper provides a theoretical study of in-context learning on structured manifolds, which is relevant to representation learning and provides foundational insights.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Zhaiming Shen",
            "Alexander Hsu",
            "Rongjie Lai",
            "Wenjing Liao"
        ],
        "title": "Understanding In-Context Learning on Structured Manifolds: Bridging Attention to Kernel Methods",
        "abstract": "While in-context learning (ICL) has achieved remarkable success in natural language and vision domains, its theoretical understanding--particularly in the context of structured geometric data--remains unexplored. In this work, we initiate a theoretical study of ICL for regression of H\\\"older functions on manifolds. By establishing a novel connection between the attention mechanism and classical kernel methods, we derive generalization error bounds in terms of the prompt length and the number of training tasks. When a sufficient number of training tasks are observed, transformers give rise to the minimax regression rate of H\\\"older functions on manifolds, which scales exponentially with the intrinsic dimension of the manifold, rather than the ambient space dimension. Our result also characterizes how the generalization error scales with the number of training tasks, shedding light on the complexity of transformers as in-context algorithm learners. Our findings provide foundational insights into the role of geometry in ICL and novels tools to study ICL of nonlinear models.",
        "arxiv_id": "2506.10959"
    },
    "2506.10139": {
        "SCORE": 17,
        "ARXIVID": "2506.10139",
        "COMMENT": "The paper presents an unsupervised algorithm for fine-tuning language models, which is relevant to large language models and representation learning.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Jiaxin Wen",
            "Zachary Ankner",
            "Arushi Somani",
            "Peter Hase",
            "Samuel Marks",
            "Jacob Goldman-Wetzler",
            "Linda Petrini",
            "Henry Sleight",
            "Collin Burns",
            "He He",
            "Shi Feng",
            "Ethan Perez",
            "Jan Leike"
        ],
        "title": "Unsupervised Elicitation of Language Models",
        "abstract": "To steer pretrained language models for downstream tasks, today's post-training paradigm relies on humans to specify desired behaviors. However, for models with superhuman capabilities, it is difficult or impossible to get high-quality human supervision. To address this challenge, we introduce a new unsupervised algorithm, Internal Coherence Maximization (ICM), to fine-tune pretrained language models on their own generated labels, \\emph{without external supervision}. On GSM8k-verification, TruthfulQA, and Alpaca reward modeling tasks, our method matches the performance of training on golden supervision and outperforms training on crowdsourced human supervision. On tasks where LMs' capabilities are strongly superhuman, our method can elicit those capabilities significantly better than training on human labels. Finally, we show that our method can improve the training of frontier LMs: we use our method to train an unsupervised reward model and use reinforcement learning to train a Claude 3.5 Haiku-based assistant. Both the reward model and the assistant outperform their human-supervised counterparts.",
        "arxiv_id": "2506.10139"
    },
    "2506.10159": {
        "SCORE": 17,
        "ARXIVID": "2506.10159",
        "COMMENT": "The paper introduces a probabilistic approach to contrastive learning, which is relevant to representation learning and offers a new theoretical perspective.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Minoh Jeong",
            "Seonho Kim",
            "Alfred Hero"
        ],
        "title": "Probabilistic Variational Contrastive Learning",
        "abstract": "Deterministic embeddings learned by contrastive learning (CL) methods such as SimCLR and SupCon achieve state-of-the-art performance but lack a principled mechanism for uncertainty quantification. We propose Variational Contrastive Learning (VCL), a decoder-free framework that maximizes the evidence lower bound (ELBO) by interpreting the InfoNCE loss as a surrogate reconstruction term and adding a KL divergence regularizer to a uniform prior on the unit hypersphere. We model the approximate posterior $q_\\theta(z|x)$ as a projected normal distribution, enabling the sampling of probabilistic embeddings. Our two instantiations--VSimCLR and VSupCon--replace deterministic embeddings with samples from $q_\\theta(z|x)$ and incorporate a normalized KL term into the loss. Experiments on multiple benchmarks demonstrate that VCL mitigates dimensional collapse, enhances mutual information with class labels, and matches or outperforms deterministic baselines in classification accuracy, all the while providing meaningful uncertainty estimates through the posterior model. VCL thus equips contrastive learning with a probabilistic foundation, serving as a new basis for contrastive approaches.",
        "arxiv_id": "2506.10159"
    },
    "2506.10920": {
        "SCORE": 17,
        "ARXIVID": "2506.10920",
        "COMMENT": "The paper focuses on representation learning by decomposing MLP activations into interpretable features using semi-nonnegative matrix factorization, which aligns with insights into how deep networks encode information.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Or Shafran",
            "Atticus Geiger",
            "Mor Geva"
        ],
        "title": "Decomposing MLP Activations into Interpretable Features via Semi-Nonnegative Matrix Factorization",
        "abstract": "A central goal for mechanistic interpretability has been to identify the right units of analysis in large language models (LLMs) that causally explain their outputs. While early work focused on individual neurons, evidence that neurons often encode multiple concepts has motivated a shift toward analyzing directions in activation space. A key question is how to find directions that capture interpretable features in an unsupervised manner. Current methods rely on dictionary learning with sparse autoencoders (SAEs), commonly trained over residual stream activations to learn directions from scratch. However, SAEs often struggle in causal evaluations and lack intrinsic interpretability, as their learning is not explicitly tied to the computations of the model. Here, we tackle these limitations by directly decomposing MLP activations with semi-nonnegative matrix factorization (SNMF), such that the learned features are (a) sparse linear combinations of co-activated neurons, and (b) mapped to their activating inputs, making them directly interpretable. Experiments on Llama 3.1, Gemma 2 and GPT-2 show that SNMF derived features outperform SAEs and a strong supervised baseline (difference-in-means) on causal steering, while aligning with human-interpretable concepts. Further analysis reveals that specific neuron combinations are reused across semantically-related features, exposing a hierarchical structure in the MLP's activation space. Together, these results position SNMF as a simple and effective tool for identifying interpretable features and dissecting concept representations in LLMs.",
        "arxiv_id": "2506.10920"
    },
    "2506.10911": {
        "SCORE": 17,
        "ARXIVID": "2506.10911",
        "COMMENT": "The paper proposes a novel optimization method, NoLoCo, for low communication training of large models, which aligns with model compression and efficiency breakthroughs.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Jari Kolehmainen",
            "Nikolay Blagoev",
            "John Donaghy",
            "O\\u{g}uzhan Ersoy",
            "Christopher Nies"
        ],
        "title": "NoLoCo: No-all-reduce Low Communication Training Method for Large Models",
        "abstract": "Training large language models is generally done via optimization methods on clusters containing tens of thousands of accelerators, communicating over a high-bandwidth interconnect. Scaling up these clusters is expensive and can become impractical, imposing limits on the size of models that can be trained. Several recent studies have proposed training methods that are less communication intensive, avoiding the need for a highly connected compute cluster. These state-of-the-art low communication training methods still employ a synchronization step for model parameters, which, when performed over all model replicas, can become costly on a low-bandwidth network.   In this work, we propose a novel optimization method, NoLoCo, that does not explicitly synchronize all model parameters during training and, as a result, does not require any collective communication. NoLoCo implicitly synchronizes model weights via a novel variant of the Nesterov momentum optimizer by partially averaging model weights with a randomly selected other one. We provide both a theoretical convergence analysis for our proposed optimizer as well as empirical results from language model training.   We benchmark NoLoCo on a wide range of accelerator counts and model sizes, between 125M to 6.8B parameters. Our method requires significantly less communication overhead than fully sharded data parallel training or even widely used low communication training method, DiLoCo. The synchronization step itself is estimated to be one magnitude faster than the all-reduce used in DiLoCo for few hundred accelerators training over the internet. We also do not have any global blocking communication that reduces accelerator idling time. Compared to DiLoCo, we also observe up to $4\\%$ faster convergence rate with wide range of model sizes and accelerator counts.",
        "arxiv_id": "2506.10911"
    },
    "2506.09967": {
        "SCORE": 17,
        "ARXIVID": "2506.09967",
        "COMMENT": "The paper introduces Resa, a reasoning model using sparse autoencoder tuning, which aligns with representation learning and insights into how deep networks encode information.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Shangshang Wang",
            "Julian Asilis",
            "\\\"Omer Faruk Akg\\\"ul",
            "Enes Burak Bilgin",
            "Ollie Liu",
            "Deqing Fu",
            "Willie Neiswanger"
        ],
        "title": "Resa: Transparent Reasoning Models via SAEs",
        "abstract": "How cost-effectively can we elicit strong reasoning in language models by leveraging their underlying representations? We answer this question with Resa, a family of 1.5B reasoning models trained via a novel and efficient sparse autoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to capture reasoning abilities from a source model, and then uses the trained SAE to guide a standard supervised fine-tuning process to elicit such abilities in a target model, all using verified question-answer data without any reasoning traces. Notably, when applied to certain base models before further RL post-training, SAE-Tuning retains >97% of its RL-trained counterpart's reasoning performance while reducing training costs by >2000x to roughly \\$1 and training time by >450x to around 20 minutes. Furthermore, when applied to lightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning performance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only around \\$1 additional cost. Surprisingly, the reasoning abilities extracted via SAEs are potentially both generalizable and modular. Generality means abilities extracted from one dataset still elevate performance on a larger and overlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math can be attached to the R1-Distill model at test time, without any retraining, and yield comparable gains. Extensive ablations validate these findings and all artifacts are fully open-sourced.",
        "arxiv_id": "2506.09967"
    },
    "2506.10801": {
        "SCORE": 17,
        "ARXIVID": "2506.10801",
        "COMMENT": "The paper proposes a novel energy function for Dense Associative Memory networks, which aligns with emerging trends in foundational research by introducing a new paradigm for memory storage.",
        "RELEVANCE": 8,
        "NOVELTY": 9,
        "authors": [
            "Benjamin Hoover",
            "Zhaoyang Shi",
            "Krishnakumar Balasubramanian",
            "Dmitry Krotov",
            "Parikshit Ram"
        ],
        "title": "Dense Associative Memory with Epanechnikov Energy",
        "abstract": "We propose a novel energy function for Dense Associative Memory (DenseAM) networks, the log-sum-ReLU (LSR), inspired by optimal kernel density estimation. Unlike the common log-sum-exponential (LSE) function, LSR is based on the Epanechnikov kernel and enables exact memory retrieval with exponential capacity without requiring exponential separation functions. Moreover, it introduces abundant additional \\emph{emergent} local minima while preserving perfect pattern recovery -- a characteristic previously unseen in DenseAM literature. Empirical results show that LSR energy has significantly more local minima (memories) that have comparable log-likelihood to LSE-based models. Analysis of LSR's emergent memories on image datasets reveals a degree of creativity and novelty, hinting at this method's potential for both large-scale memory storage and generative tasks.",
        "arxiv_id": "2506.10801"
    },
    "2504.15777": {
        "SCORE": 16,
        "ARXIVID": "2504.15777",
        "COMMENT": "The paper discusses low-rank adaptation (LoRA) for efficient reasoning in language models, which is relevant to model compression and efficiency.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Shangshang Wang",
            "Julian Asilis",
            "\\\"Omer Faruk Akg\\\"ul",
            "Enes Burak Bilgin",
            "Ollie Liu",
            "Willie Neiswanger"
        ],
        "title": "Tina: Tiny Reasoning Models via LoRA",
        "abstract": "How cost-effectively can strong reasoning abilities be achieved in language models? Driven by this fundamental question, we present Tina, a family of tiny reasoning models achieved with high cost-efficiency. Notably, Tina demonstrates that substantial reasoning performance can be developed using only minimal resources, by applying parameter-efficient updates during reinforcement learning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B parameter base model. This minimalist approach produces models that achieve reasoning performance which is competitive with, and sometimes surpasses, SOTA RL reasoning models built upon the same base model. Crucially, this is achieved at a tiny fraction of the computational post-training cost employed by existing SOTA models. In fact, the best Tina model achieves a >20\\% reasoning performance increase and 43.33\\% Pass@1 accuracy on AIME24, at only \\$9 USD post-training and evaluation cost (i.e., an estimated 260x cost reduction). Our work reveals the surprising effectiveness of efficient RL reasoning via LoRA. We validate this across multiple open-source reasoning datasets and various ablation settings starting with a single, fixed set of hyperparameters. Furthermore, we hypothesize that this effectiveness and efficiency stem from LoRA rapidly adapting the model to the structural format of reasoning rewarded by RL, while largely preserving the base model's underlying knowledge. In service of accessibility and open research, we fully open-source all code, training logs, and model weights \\& checkpoints.",
        "arxiv_id": "2504.15777"
    },
    "2506.10341": {
        "SCORE": 16,
        "ARXIVID": "2506.10341",
        "COMMENT": "The paper formalizes the Learning from Language Feedback problem and introduces a new complexity measure, which aligns with emerging trends in theoretical insights.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Wanqiao Xu",
            "Allen Nie",
            "Ruijie Zheng",
            "Aditya Modi",
            "Adith Swaminathan",
            "Ching-An Cheng"
        ],
        "title": "Provably Learning from Language Feedback",
        "abstract": "Interactively learning from observation and language feedback is an increasingly studied area driven by the emergence of large language model (LLM) agents. While impressive empirical demonstrations have been shown, so far a principled framing of these decision problems remains lacking. In this paper, we formalize the Learning from Language Feedback (LLF) problem, assert sufficient assumptions to enable learning despite latent rewards, and introduce $\\textit{transfer eluder dimension}$ as a complexity measure to characterize the hardness of LLF problems. We show that transfer eluder dimension captures the intuition that information in the feedback changes the learning complexity of the LLF problem. We demonstrate cases where learning from rich language feedback can be exponentially faster than learning from reward. We develop a no-regret algorithm, called $\\texttt{HELiX}$, that provably solves LLF problems through sequential interactions, with performance guarantees that scale with the transfer eluder dimension of the problem. Across several empirical domains, we show that $\\texttt{HELiX}$ performs well even when repeatedly prompting LLMs does not work reliably. Our contributions mark a first step towards designing principled interactive learning algorithms from generic language feedback.",
        "arxiv_id": "2506.10341"
    },
    "2506.10914": {
        "SCORE": 16,
        "ARXIVID": "2506.10914",
        "COMMENT": "The paper introduces a framework for training foundation models for causal inference, which aligns with foundational research in AI for science.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Yuchen Ma",
            "Dennis Frauen",
            "Emil Javurek",
            "Stefan Feuerriegel"
        ],
        "title": "Foundation Models for Causal Inference via Prior-Data Fitted Networks",
        "abstract": "Prior-data fitted networks (PFNs) have recently been proposed as a promising way to train tabular foundation models. PFNs are transformers that are pre-trained on synthetic data generated from a prespecified prior distribution and that enable Bayesian inference through in-context learning. In this paper, we introduce CausalFM, a comprehensive framework for training PFN-based foundation models in various causal inference settings. First, we formalize the construction of Bayesian priors for causal inference based on structural causal models (SCMs) in a principled way and derive necessary criteria for the validity of such priors. Building on this, we propose a novel family of prior distributions using causality-inspired Bayesian neural networks that enable CausalFM to perform Bayesian causal inference in various settings, including back-door, front-door, and instrumental variable adjustment. Finally, we instantiate CausalFM and explicitly train a foundation model for estimating conditional average treatment effects (CATEs) using back-door adjustment. We show that CausalFM performs competitively for CATE estimation using various synthetic and semi-synthetic benchmarks. In sum, our framework can be used as a general recipe to train foundation models for various causal inference settings. In contrast to the current state-of-the-art in causal inference, CausalFM offers a novel paradigm with the potential to fundamentally change how practitioners perform causal inference in medicine, economics, and other disciplines.",
        "arxiv_id": "2506.10914"
    },
    "2506.10177": {
        "SCORE": 16,
        "ARXIVID": "2506.10177",
        "COMMENT": "The paper reveals geometric regularity in diffusion-based generative models, which aligns with emerging trends in theoretical insights.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Defang Chen",
            "Zhenyu Zhou",
            "Can Wang",
            "Siwei Lyu"
        ],
        "title": "Geometric Regularity in Deterministic Sampling of Diffusion-based Generative Models",
        "abstract": "Diffusion-based generative models employ stochastic differential equations (SDEs) and their equivalent probability flow ordinary differential equations (ODEs) to establish a smooth transformation between complex high-dimensional data distributions and tractable prior distributions. In this paper, we reveal a striking geometric regularity in the deterministic sampling dynamics: each simulated sampling trajectory lies within an extremely low-dimensional subspace, and all trajectories exhibit an almost identical ''boomerang'' shape, regardless of the model architecture, applied conditions, or generated content. We characterize several intriguing properties of these trajectories, particularly under closed-form solutions based on kernel-estimated data modeling. We also demonstrate a practical application of the discovered trajectory regularity by proposing a dynamic programming-based scheme to better align the sampling time schedule with the underlying trajectory structure. This simple strategy requires minimal modification to existing ODE-based numerical solvers, incurs negligible computational overhead, and achieves superior image generation performance, especially in regions with only $5 \\sim 10$ function evaluations.",
        "arxiv_id": "2506.10177"
    },
    "2506.10275": {
        "SCORE": 16,
        "ARXIVID": "2506.10275",
        "COMMENT": "The paper introduces a novel hybrid quantum-classical architecture, VQC-MLPNet, which enhances representation capabilities and training stability, aligning with representation learning and model architecture criteria.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Jun Qi",
            "Chao-Han Yang",
            "Pin-Yu Chen",
            "Min-Hsiu Hsieh"
        ],
        "title": "VQC-MLPNet: An Unconventional Hybrid Quantum-Classical Architecture for Scalable and Robust Quantum Machine Learning",
        "abstract": "Variational Quantum Circuits (VQCs) offer a novel pathway for quantum machine learning, yet their practical application is hindered by inherent limitations such as constrained linear expressivity, optimization challenges, and acute sensitivity to quantum hardware noise. This work introduces VQC-MLPNet, a scalable and robust hybrid quantum-classical architecture designed to overcome these obstacles. By innovatively employing quantum circuits to dynamically generate parameters for classical Multi-Layer Perceptrons (MLPs) via amplitude encoding and parameterized quantum operations, VQC-MLPNet substantially expands representation capabilities and augments training stability. We provide rigorous theoretical guarantees via statistical learning techniques and Neural Tangent Kernel analysis, explicitly deriving upper bounds on approximation, uniform deviation, and optimization errors. These theoretical insights demonstrate exponential improvements in representation capacity relative to quantum circuit depth and the number of qubits, providing clear computational advantages over standalone quantum circuits and existing hybrid quantum architectures. Our theoretical claims are empirically corroborated through extensive experiments, including classifying semiconductor quantum-dot charge states and predicting genomic transcription factor binding sites, demonstrating resilient performance even under realistic IBM quantum noise simulations. This research establishes a theoretically sound and practically robust framework, advancing the frontiers of quantum-enhanced learning for unconventional computing paradigms in the Noisy Intermediate-Scale Quantum era and beyond.",
        "arxiv_id": "2506.10275"
    },
    "2506.10967": {
        "SCORE": 16,
        "ARXIVID": "2506.10967",
        "COMMENT": "The paper proposes a novel token pruning method for MLLMs, aligning with model compression and efficiency breakthroughs.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Qizhe Zhang",
            "Mengzhen Liu",
            "Lichen Li",
            "Ming Lu",
            "Yuan Zhang",
            "Junwen Pan",
            "Qi She",
            "Shanghang Zhang"
        ],
        "title": "Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs",
        "abstract": "In multimodal large language models (MLLMs), the length of input visual tokens is often significantly greater than that of their textual counterparts, leading to a high inference cost. Many works aim to address this issue by removing redundant visual tokens. However, current approaches either rely on attention-based pruning, which retains numerous duplicate tokens, or use similarity-based pruning, overlooking the instruction relevance, consequently causing suboptimal performance. In this paper, we go beyond attention or similarity by proposing a novel visual token pruning method named CDPruner, which maximizes the conditional diversity of retained tokens. We first define the conditional similarity between visual tokens conditioned on the instruction, and then reformulate the token pruning problem with determinantal point process (DPP) to maximize the conditional diversity of the selected subset. The proposed CDPruner is training-free and model-agnostic, allowing easy application to various MLLMs. Extensive experiments across diverse MLLMs show that CDPruner establishes new state-of-the-art on various vision-language benchmarks. By maximizing conditional diversity through DPP, the selected subset better represents the input images while closely adhering to user instructions, thereby preserving strong performance even with high reduction ratios. When applied to LLaVA, CDPruner reduces FLOPs by 95\\% and CUDA latency by 78\\%, while maintaining 94\\% of the original accuracy. Our code is available at https://github.com/Theia-4869/CDPruner.",
        "arxiv_id": "2506.10967"
    },
    "2506.10269": {
        "SCORE": 16,
        "ARXIVID": "2506.10269",
        "COMMENT": "The paper addresses a fundamental challenge in neural network verification using semidefinite relaxations, which is relevant to understanding model behavior and efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Ryota Ueda",
            "Takami Sato",
            "Ken Kobayashi",
            "Kazuhide Nakata"
        ],
        "title": "Interior-Point Vanishing Problem in Semidefinite Relaxations for Neural Network Verification",
        "abstract": "Semidefinite programming (SDP) relaxation has emerged as a promising approach for neural network verification, offering tighter bounds than other convex relaxation methods for deep neural networks (DNNs) with ReLU activations. However, we identify a critical limitation in the SDP relaxation when applied to deep networks: interior-point vanishing, which leads to the loss of strict feasibility -- a crucial condition for the numerical stability and optimality of SDP. Through rigorous theoretical and empirical analysis, we demonstrate that as the depth of DNNs increases, the strict feasibility is likely to be lost, creating a fundamental barrier to scaling SDP-based verification. To address the interior-point vanishing, we design and investigate five solutions to enhance the feasibility conditions of the verification problem. Our methods can successfully solve 88% of the problems that could not be solved by existing methods, accounting for 41% of the total. Our analysis also reveals that the valid constraints for the lower and upper bounds for each ReLU unit are traditionally inherited from prior work without solid reasons, but are actually not only unbeneficial but also even harmful to the problem's feasibility. This work provides valuable insights into the fundamental challenges of SDP-based DNN verification and offers practical solutions to improve its applicability to deeper neural networks, contributing to the development of more reliable and secure systems with DNNs.",
        "arxiv_id": "2506.10269"
    },
    "2506.10395": {
        "SCORE": 15,
        "ARXIVID": "2506.10395",
        "COMMENT": "The paper introduces a novel decoupled visual encoding architecture for a multimodal foundation model, which relates to model architecture innovations.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Zhiyang Xu",
            "Jiuhai Chen",
            "Zhaojiang Lin",
            "Xichen Pan",
            "Lifu Huang",
            "Tianyi Zhou",
            "Madian Khabsa",
            "Qifan Wang",
            "Di Jin",
            "Michihiro Yasunaga",
            "Lili Yu",
            "Xi Victoria Lin",
            "Shaoliang Nie"
        ],
        "title": "Pisces: An Auto-regressive Foundation Model for Image Understanding and Generation",
        "abstract": "Recent advances in large language models (LLMs) have enabled multimodal foundation models to tackle both image understanding and generation within a unified framework. Despite these gains, unified models often underperform compared to specialized models in either task. A key challenge in developing unified models lies in the inherent differences between the visual features needed for image understanding versus generation, as well as the distinct training processes required for each modality. In this work, we introduce Pisces, an auto-regressive multimodal foundation model that addresses this challenge through a novel decoupled visual encoding architecture and tailored training techniques optimized for multimodal generation. Combined with meticulous data curation, pretraining, and finetuning, Pisces achieves competitive performance in both image understanding and image generation. We evaluate Pisces on over 20 public benchmarks for image understanding, where it demonstrates strong performance across a wide range of tasks. Additionally, on GenEval, a widely adopted benchmark for image generation, Pisces exhibits robust generative capabilities. Our extensive analysis reveals the synergistic relationship between image understanding and generation, and the benefits of using separate visual encoders, advancing the field of unified multimodal models.",
        "arxiv_id": "2506.10395"
    },
    "2506.10946": {
        "SCORE": 15,
        "ARXIVID": "2506.10946",
        "COMMENT": "The paper introduces a novel framework for unlearning in LLMs, which aligns with foundational research in LLM behavior and interpretability.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Evelyn Ma",
            "Duo Zhou",
            "Peizhi Niu",
            "Huiting Zhou",
            "Huan Zhang",
            "Olgica Milenkovic",
            "S. Rasoul Etesami"
        ],
        "title": "GUARD: Guided Unlearning and Retention via Data Attribution for Large Language Models",
        "abstract": "Unlearning in large language models (LLMs) is becoming increasingly important due to regulatory compliance, copyright protection, and privacy concerns. However, a key challenge in LLM unlearning is unintended forgetting, where the removal of specific data inadvertently impairs the utility of the model and its retention of valuable, desired information. While prior work has primarily focused on architectural innovations, the influence of data-level factors on unlearning performance remains underexplored. As a result, existing methods often suffer from degraded retention when forgetting high-impact data. To address this, we propose GUARD-a novel framework for Guided Unlearning And Retention via Data attribution. At its core, GUARD introduces a lightweight proxy data attribution metric tailored for LLM unlearning, which quantifies the \"alignment\" between the forget and retain sets while remaining computationally efficient. Building on this, we design a novel unlearning objective that assigns adaptive, nonuniform unlearning weights to samples, inversely proportional to their proxy attribution scores. Through such a reallocation of unlearning power, GUARD mitigates unintended losses in retention. We provide rigorous theoretical guarantees that GUARD significantly enhances retention while maintaining forgetting metrics comparable to prior methods. Extensive experiments on the TOFU benchmark across multiple LLM architectures demonstrate that GUARD substantially improves utility preservation while ensuring effective unlearning. Notably, GUARD reduces utility sacrifice on the Retain Set by up to 194.92% in terms of Truth Ratio when forgetting 10% of the training data.",
        "arxiv_id": "2506.10946"
    },
    "2506.10572": {
        "SCORE": 15,
        "ARXIVID": "2506.10572",
        "COMMENT": "The paper introduces a novel box-constrained softmax function, which is a foundational contribution to model architecture and calibration methods.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Kyohei Atarashi",
            "Satoshi Oyama",
            "Hiromi Arai",
            "Hisashi Kashima"
        ],
        "title": "Box-Constrained Softmax Function and Its Application for Post-Hoc Calibration",
        "abstract": "Controlling the output probabilities of softmax-based models is a common problem in modern machine learning. Although the $\\mathrm{Softmax}$ function provides soft control via its temperature parameter, it lacks the ability to enforce hard constraints, such as box constraints, on output probabilities, which can be critical in certain applications requiring reliable and trustworthy models. In this work, we propose the box-constrained softmax ($\\mathrm{BCSoftmax}$) function, a novel generalization of the $\\mathrm{Softmax}$ function that explicitly enforces lower and upper bounds on output probabilities. While $\\mathrm{BCSoftmax}$ is formulated as the solution to a box-constrained optimization problem, we develop an exact and efficient computation algorithm for $\\mathrm{BCSoftmax}$. As a key application, we introduce two post-hoc calibration methods based on $\\mathrm{BCSoftmax}$. The proposed methods mitigate underconfidence and overconfidence in predictive models by learning the lower and upper bounds of the output probabilities or logits after model training, thereby enhancing reliability in downstream decision-making tasks. We demonstrate the effectiveness of our methods experimentally using the TinyImageNet, CIFAR-100, and 20NewsGroups datasets, achieving improvements in calibration metrics.",
        "arxiv_id": "2506.10572"
    },
    "2506.10973": {
        "SCORE": 15,
        "ARXIVID": "2506.10973",
        "COMMENT": "The paper discusses extending neural architectures to function spaces for operator learning, which is relevant to foundational research in model architecture.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Julius Berner",
            "Miguel Liu-Schiaffini",
            "Jean Kossaifi",
            "Valentin Duruisseaux",
            "Boris Bonev",
            "Kamyar Azizzadenesheli",
            "Anima Anandkumar"
        ],
        "title": "Principled Approaches for Extending Neural Architectures to Function Spaces for Operator Learning",
        "abstract": "A wide range of scientific problems, such as those described by continuous-time dynamical systems and partial differential equations (PDEs), are naturally formulated on function spaces. While function spaces are typically infinite-dimensional, deep learning has predominantly advanced through applications in computer vision and natural language processing that focus on mappings between finite-dimensional spaces. Such fundamental disparities in the nature of the data have limited neural networks from achieving a comparable level of success in scientific applications as seen in other fields. Neural operators are a principled way to generalize neural networks to mappings between function spaces, offering a pathway to replicate deep learning's transformative impact on scientific problems. For instance, neural operators can learn solution operators for entire classes of PDEs, e.g., physical systems with different boundary conditions, coefficient functions, and geometries. A key factor in deep learning's success has been the careful engineering of neural architectures through extensive empirical testing. Translating these neural architectures into neural operators allows operator learning to enjoy these same empirical optimizations. However, prior neural operator architectures have often been introduced as standalone models, not directly derived as extensions of existing neural network architectures. In this paper, we identify and distill the key principles for constructing practical implementations of mappings between infinite-dimensional function spaces. Using these principles, we propose a recipe for converting several popular neural architectures into neural operators with minimal modifications. This paper aims to guide practitioners through this process and details the steps to make neural operators work in practice. Our code can be found at https://github.com/neuraloperator/NNs-to-NOs",
        "arxiv_id": "2506.10973"
    },
    "2506.10060": {
        "SCORE": 15,
        "ARXIVID": "2506.10060",
        "COMMENT": "The paper introduces a Bayesian approach to uncertainty quantification in LLMs, which is relevant to foundational research in LLM behavior and interpretability.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Brendan Leigh Ross",
            "No\\\"el Vouitsis",
            "Atiyeh Ashari Ghomi",
            "Rasa Hosseinzadeh",
            "Ji Xin",
            "Zhaoyan Liu",
            "Yi Sui",
            "Shiyi Hou",
            "Kin Kwan Leung",
            "Gabriel Loaiza-Ganem",
            "Jesse C. Cresswell"
        ],
        "title": "Textual Bayes: Quantifying Uncertainty in LLM-Based Systems",
        "abstract": "Although large language models (LLMs) are becoming increasingly capable of solving challenging real-world tasks, accurately quantifying their uncertainty remains a critical open problem, which limits their applicability in high-stakes domains. This challenge is further compounded by the closed-source, black-box nature of many state-of-the-art LLMs. Moreover, LLM-based systems can be highly sensitive to the prompts that bind them together, which often require significant manual tuning (i.e., prompt engineering). In this work, we address these challenges by viewing LLM-based systems through a Bayesian lens. We interpret prompts as textual parameters in a statistical model, allowing us to use a small training dataset to perform Bayesian inference over these prompts. This novel perspective enables principled uncertainty quantification over both the model's textual parameters and its downstream predictions, while also incorporating prior beliefs about these parameters expressed in free-form text. To perform Bayesian inference, a difficult problem even for well-studied data modalities, we introduce Metropolis-Hastings through LLM Proposals (MHLP), a novel Markov chain Monte Carlo (MCMC) algorithm that combines prompt optimization techniques with standard MCMC methods. MHLP is a turnkey modification to existing LLM pipelines, including those that rely exclusively on closed-source models. Empirically, we demonstrate that our method yields improvements in both predictive accuracy and uncertainty quantification (UQ) on a range of LLM benchmarks and UQ tasks. More broadly, our work demonstrates a viable path for incorporating methods from the rich Bayesian literature into the era of LLMs, paving the way for more reliable and calibrated LLM-based systems.",
        "arxiv_id": "2506.10060"
    },
    "2506.10355": {
        "SCORE": 15,
        "ARXIVID": "2506.10355",
        "COMMENT": "The paper introduces TreeLoRA, a novel approach for efficient continual learning in large pre-trained models, which is relevant to model compression and efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Yu-Yang Qian",
            "Yuan-Ze Xu",
            "Zhen-Yu Zhang",
            "Peng Zhao",
            "Zhi-Hua Zhou"
        ],
        "title": "TreeLoRA: Efficient Continual Learning via Layer-Wise LoRAs Guided by a Hierarchical Gradient-Similarity Tree",
        "abstract": "Many real-world applications collect data in a streaming environment, where learning tasks are encountered sequentially. This necessitates continual learning (CL) to update models online, enabling adaptation to new tasks while preserving past knowledge to prevent catastrophic forgetting. Nowadays, with the flourish of large pre-trained models (LPMs), efficiency has become increasingly critical for CL, due to their substantial computational demands and growing parameter sizes. In this paper, we introduce TreeLoRA (K-D Tree of Low-Rank Adapters), a novel approach that constructs layer-wise adapters by leveraging hierarchical gradient similarity to enable efficient CL, particularly for LPMs. To reduce the computational burden of task similarity estimation, we employ bandit techniques to develop an algorithm based on lower confidence bounds to efficiently explore the task structure. Furthermore, we use sparse gradient updates to facilitate parameter optimization, making the approach better suited for LPMs. Theoretical analysis is provided to justify the rationale behind our approach, and experiments on both vision transformers (ViTs) and large language models (LLMs) demonstrate the effectiveness and efficiency of our approach across various domains, including vision and natural language processing tasks.",
        "arxiv_id": "2506.10355"
    },
    "2506.10089": {
        "SCORE": 15,
        "ARXIVID": "2506.10089",
        "COMMENT": "The paper introduces a framework for optimizing latent dimension allocation in hierarchical VAEs, which is relevant to model architecture and representation learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Dane Williamson",
            "Yangfeng Ji",
            "Matthew Dwyer"
        ],
        "title": "Optimizing Latent Dimension Allocation in Hierarchical VAEs: Balancing Attenuation and Information Retention for OOD Detection",
        "abstract": "Out-of-distribution (OOD) detection is a critical task in machine learning, particularly for safety-critical applications where unexpected inputs must be reliably flagged. While hierarchical variational autoencoders (HVAEs) offer improved representational capacity over traditional VAEs, their performance is highly sensitive to how latent dimensions are distributed across layers. Existing approaches often allocate latent capacity arbitrarily, leading to ineffective representations or posterior collapse. In this work, we introduce a theoretically grounded framework for optimizing latent dimension allocation in HVAEs, drawing on principles from information theory to formalize the trade-off between information loss and representational attenuation. We prove the existence of an optimal allocation ratio $r^{\\ast}$ under a fixed latent budget, and empirically show that tuning this ratio consistently improves OOD detection performance across datasets and architectures. Our approach outperforms baseline HVAE configurations and provides practical guidance for principled latent structure design, leading to more robust OOD detection with deep generative models.",
        "arxiv_id": "2506.10089"
    },
    "2506.10943": {
        "SCORE": 15,
        "ARXIVID": "2506.10943",
        "COMMENT": "The paper introduces a framework for self-adapting LLMs, which is relevant to foundational research in LLM architecture and behavior.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Adam Zweiger",
            "Jyothish Pari",
            "Han Guo",
            "Ekin Aky\\\"urek",
            "Yoon Kim",
            "Pulkit Agrawal"
        ],
        "title": "Self-Adapting Language Models",
        "abstract": "Large language models (LLMs) are powerful but static; they lack mechanisms to adapt their weights in response to new tasks, knowledge, or examples. We introduce Self-Adapting LLMs (SEAL), a framework that enables LLMs to self-adapt by generating their own finetuning data and update directives. Given a new input, the model produces a self-edit-a generation that may restructure the information in different ways, specify optimization hyperparameters, or invoke tools for data augmentation and gradient-based updates. Through supervised finetuning (SFT), these self-edits result in persistent weight updates, enabling lasting adaptation. To train the model to produce effective self-edits, we use a reinforcement learning loop with the downstream performance of the updated model as the reward signal. Unlike prior approaches that rely on separate adaptation modules or auxiliary networks, SEAL directly uses the model's own generation to control its adaptation process. Experiments on knowledge incorporation and few-shot generalization show that SEAL is a promising step toward language models capable of self-directed adaptation. Our website and code is available at https://jyopari.github.io/posts/seal.",
        "arxiv_id": "2506.10943"
    },
    "2506.10552": {
        "SCORE": 15,
        "ARXIVID": "2506.10552",
        "COMMENT": "The paper explores the role of non-linear latent features in bipartite generative neural networks, focusing on architectural choices and activation functions, which aligns with the interest in model architecture and representation learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Tony Bonnaire",
            "Giovanni Catania",
            "Aur\\'elien Decelle",
            "Beatriz Seoane"
        ],
        "title": "On the role of non-linear latent features in bipartite generative neural networks",
        "abstract": "We investigate the phase diagram and memory retrieval capabilities of bipartite energy-based neural networks, namely Restricted Boltzmann Machines (RBMs), as a function of the prior distribution imposed on their hidden units - including binary, multi-state, and ReLU-like activations. Drawing connections to the Hopfield model and employing analytical tools from statistical physics of disordered systems, we explore how the architectural choices and activation functions shape the thermodynamic properties of these models. Our analysis reveals that standard RBMs with binary hidden nodes and extensive connectivity suffer from reduced critical capacity, limiting their effectiveness as associative memories. To address this, we examine several modifications, such as introducing local biases and adopting richer hidden unit priors. These adjustments restore ordered retrieval phases and markedly improve recall performance, even at finite temperatures. Our theoretical findings, supported by finite-size Monte Carlo simulations, highlight the importance of hidden unit design in enhancing the expressive power of RBMs.",
        "arxiv_id": "2506.10552"
    },
    "2506.10888": {
        "SCORE": 15,
        "ARXIVID": "2506.10888",
        "COMMENT": "The paper discusses adversarial attacks on mixtures of classifiers, which is relevant to mixture-of-experts and model robustness.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Lucas Gnecco-Heredia",
            "Benjamin Negrevergne",
            "Yann Chevaleyre"
        ],
        "title": "Lattice Climber Attack: Adversarial attacks for randomized mixtures of classifiers",
        "abstract": "Finite mixtures of classifiers (a.k.a. randomized ensembles) have been proposed as a way to improve robustness against adversarial attacks. However, existing attacks have been shown to not suit this kind of classifier. In this paper, we discuss the problem of attacking a mixture in a principled way and introduce two desirable properties of attacks based on a geometrical analysis of the problem (effectiveness and maximality). We then show that existing attacks do not meet both of these properties. Finally, we introduce a new attack called {\\em lattice climber attack} with theoretical guarantees in the binary linear setting, and demonstrate its performance by conducting experiments on synthetic and real datasets.",
        "arxiv_id": "2506.10888"
    },
    "2506.10885": {
        "SCORE": 15,
        "ARXIVID": "2506.10885",
        "COMMENT": "The paper investigates parameter-efficient methods for fine-tuning LLMs, which is relevant to model compression and efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Qingda (Michael)",
            "Mai"
        ],
        "title": "Slimming Down LLMs Without Losing Their Minds",
        "abstract": "This paper investigates and validates the impact of fine-tuning on large language model performance, focusing on parameter-efficient methods (LoRA and QLoRA). We evaluate model capabilities across three key domains: (1) commonsense reasoning (HellaSwag), (2) mathematical reasoning (GSM8K), and (3) multi-domain knowledge (MMLU-CS).   Our findings demonstrate that: (1) LoRA-based methods effectively improve task-specific performance while maintaining computational efficiency, and (2) performance strongly depends on alignment between fine-tuning dataset and benchmark tasks. The study provides both theoretical insights into parameter-efficient mechanisms and practical guidance for developers implementing efficient LLM adaptation with limited resources.",
        "arxiv_id": "2506.10885"
    },
    "2506.10200": {
        "SCORE": 15,
        "ARXIVID": "2506.10200",
        "COMMENT": "The paper introduces a novel framework, DynaSubVAE, which performs representation learning and adaptive OOD detection, aligning with representation learning and model architecture.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Tina Behrouzi",
            "Sana Tonekaboni",
            "Rahul G. Krishnan",
            "Anna Goldenberg"
        ],
        "title": "DynaSubVAE: Adaptive Subgrouping for Scalable and Robust OOD Detection",
        "abstract": "Real-world observational data often contain existing or emerging heterogeneous subpopulations that deviate from global patterns. The majority of models tend to overlook these underrepresented groups, leading to inaccurate or even harmful predictions. Existing solutions often rely on detecting these samples as Out-of-domain (OOD) rather than adapting the model to new emerging patterns. We introduce DynaSubVAE, a Dynamic Subgrouping Variational Autoencoder framework that jointly performs representation learning and adaptive OOD detection. Unlike conventional approaches, DynaSubVAE evolves with the data by dynamically updating its latent structure to capture new trends. It leverages a novel non-parametric clustering mechanism, inspired by Gaussian Mixture Models, to discover and model latent subgroups based on embedding similarity. Extensive experiments show that DynaSubVAE achieves competitive performance in both near-OOD and far-OOD detection, and excels in class-OOD scenarios where an entire class is missing during training. We further illustrate that our dynamic subgrouping mechanism outperforms standalone clustering methods such as GMM and KMeans++ in terms of both OOD accuracy and regret precision.",
        "arxiv_id": "2506.10200"
    }
}