{
    "2510.06477": {
        "COMMENT": "Author match",
        "SCORE": 20.0,
        "authors": [
            "Enrique Queipo-de-Llano",
            "\\'Alvaro Arroyo",
            "Federico Barbero",
            "Xiaowen Dong",
            "Michael Bronstein",
            "Yann LeCun",
            "Ravid Shwartz-Ziv"
        ],
        "title": "Attention Sinks and Compression Valleys in LLMs are Two Sides of the Same Coin",
        "abstract": "Attention sinks and compression valleys have attracted significant attention as two puzzling phenomena in large language models, but have been studied in isolation. In this work, we present a surprising connection between attention sinks and compression valleys, tracing both to the formation of massive activations in the residual stream. We prove theoretically that massive activations necessarily produce representational compression and establish bounds on the resulting entropy reduction. Through experiments across several models (410M-120B parameters), we confirm that when the beginning-of-sequence token develops extreme activation norms in the middle layers, both compression valleys and attention sinks emerge simultaneously. Targeted ablation studies validate our theoretical predictions. This unified view motivates us to propose the Mix-Compress-Refine theory of information flow, as an attempt to explain how LLMs organize their computation in depth by controlling attention and representational compression via massive activations. Specifically, we posit that Transformer-based LLMs process tokens in three distinct phases: (1) broad mixing in the early layers, (2) compressed computation with limited mixing in the middle layers, and (3) selective refinement in the late layers. Our framework helps explain why embedding tasks perform best at intermediate layers, whereas generation tasks benefit from full-depth processing, clarifying differences in task-dependent representations.",
        "arxiv_id": "2510.06477"
    },
    "2510.06685": {
        "SCORE": 19,
        "ARXIVID": "2510.06685",
        "COMMENT": "Provides a rigorous random-matrix-theoretic analysis of self-attention spectra, advancing theoretical understanding of Transformer architecture and representation dynamics.",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "Tomohiro Hayase",
            "Beno\\^it Collins",
            "Ryo Karakida"
        ],
        "title": "Gaussian Equivalence for Self-Attention: Asymptotic Spectral Analysis of Attention Matrix",
        "abstract": "Self-attention layers have become fundamental building blocks of modern deep neural networks, yet their theoretical understanding remains limited, particularly from the perspective of random matrix theory. In this work, we provide a rigorous analysis of the singular value spectrum of the attention matrix and establish the first Gaussian equivalence result for attention. In a natural regime where the inverse temperature remains of constant order, we show that the singular value distribution of the attention matrix is asymptotically characterized by a tractable linear model. We further demonstrate that the distribution of squared singular values deviates from the Marchenko-Pastur law, which has been believed in previous work. Our proof relies on two key ingredients: precise control of fluctuations in the normalization term and a refined linearization that leverages favorable Taylor expansions of the exponential. This analysis also identifies a threshold for linearization and elucidates why attention, despite not being an entrywise operation, admits a rigorous Gaussian equivalence in this regime.",
        "arxiv_id": "2510.06685"
    },
    "2510.06662": {
        "SCORE": 19,
        "ARXIVID": "2510.06662",
        "COMMENT": "Model Architecture theory: establishes upper and lower bounds on transformer approximation as a function of attention head count, including a first rigorous lower bound in a nonlinear practical setting.",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "Penghao Yu",
            "Haotian Jiang",
            "Zeyu Bao",
            "Ruoxi Yu",
            "Qianxiao Li"
        ],
        "title": "The Effect of Attention Head Count on Transformer Approximation",
        "abstract": "Transformer has become the dominant architecture for sequence modeling, yet a detailed understanding of how its structural parameters influence expressive power remains limited. In this work, we study the approximation properties of transformers, with particular emphasis on the role of the number of attention heads. Our analysis begins with the introduction of a generalized $D$-retrieval task, which we prove to be dense in the space of continuous functions, thereby providing the basis for our theoretical framework. We then establish both upper and lower bounds on the parameter complexity required for $\\epsilon$-approximation. Specifically, we show that transformers with sufficiently many heads admit efficient approximation, whereas with too few heads, the number of parameters must scale at least as $O(1/\\epsilon^{cT})$, for some constant $c$ and sequence length $T$. To the best of our knowledge, this constitutes the first rigorous lower bound of this type in a nonlinear and practically relevant setting. We further examine the single-head case and demonstrate that an embedding dimension of order $O(T)$ allows complete memorization of the input, where approximation is entirely achieved by the feed-forward block. Finally, we validate our theoretical findings with experiments on both synthetic data and real-world tasks, illustrating the practical relevance of our results.",
        "arxiv_id": "2510.06662"
    },
    "2510.07205": {
        "SCORE": 19,
        "ARXIVID": "2510.07205",
        "COMMENT": "Matches Model Architecture (MoE) and Representation Learning: provable joint training dynamics for soft-routed MoE; also includes post-training pruning with convergence guarantees (Model Compression/Efficiency).",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "Fangshuo Liao",
            "Anastasios Kyrillidis"
        ],
        "title": "Guided by the Experts: Provable Feature Learning Dynamic of Soft-Routed Mixture-of-Experts",
        "abstract": "Mixture-of-Experts (MoE) architectures have emerged as a cornerstone of modern AI systems. In particular, MoEs route inputs dynamically to specialized experts whose outputs are aggregated through weighted summation. Despite their widespread application, theoretical understanding of MoE training dynamics remains limited to either separate expert-router optimization or only top-1 routing scenarios with carefully constructed datasets. This paper advances MoE theory by providing convergence guarantees for joint training of soft-routed MoE models with non-linear routers and experts in a student-teacher framework. We prove that, with moderate over-parameterization, the student network undergoes a feature learning phase, where the router's learning process is ``guided'' by the experts, that recovers the teacher's parameters. Moreover, we show that a post-training pruning can effectively eliminate redundant neurons, followed by a provably convergent fine-tuning process that reaches global optimality. To our knowledge, our analysis is the first to bring novel insights in understanding the optimization landscape of the MoE architecture.",
        "arxiv_id": "2510.07205"
    },
    "2510.06557": {
        "SCORE": 19,
        "ARXIVID": "2510.06557",
        "COMMENT": "High-Performance/Algorithmic Efficiency: redesigns the reasoning environment to a Markovian, constant-state setup enabling linear compute and constant memory for very long thinking.",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "Milad Aghajohari",
            "Kamran Chitsaz",
            "Amirhossein Kazemnejad",
            "Sarath Chandar",
            "Alessandro Sordoni",
            "Aaron Courville",
            "Siva Reddy"
        ],
        "title": "The Markovian Thinker",
        "abstract": "Reinforcement learning (RL) has recently become a strong recipe for training reasoning LLMs that produce long chains of thought (LongCoT). Yet the standard RL \"thinking environment\", where the state is the prompt plus all prior reasoning tokens, makes the state unbounded and forces attention-based policies to pay quadratic compute as thoughts lengthen. We revisit the environment itself. We propose Markovian Thinking, a paradigm in which the policy advances reasoning while conditioning on a constant-size state, decoupling thinking length from context size. As an immediate consequence this yields linear compute with constant memory. We instantiate this idea with Delethink, an RL environment that structures reasoning into fixed-size chunks. Within each chunk, the model thinks as usual; at the boundary, the environment resets the context and reinitializes the prompt with a short carryover. Through RL, the policy learns to write a textual state near the end of each chunk sufficient for seamless continuation of reasoning after reset. Trained in this environment, an R1-Distill 1.5B model reasons in 8K-token chunks yet thinks up to 24K tokens, matching or surpassing LongCoT-RL trained with a 24K budget. With test-time scaling, Delethink continues to improve where LongCoT plateaus. The effect of linear compute is substantial: we empirically estimate at 96K average thinking length LongCoT-RL costs 27 H100-months vs. 7 for Delethink. Analysis at RL initialization shows off-the-shelf reasoning models (1.5B-120B) often sample Markovian traces zero-shot across diverse benchmarks, providing positive samples that make RL effective at scale. Our results show that redesigning the thinking environment is a powerful lever: it enables very long reasoning without quadratic overhead and opens a path toward efficient, scalable reasoning LLMs.",
        "arxiv_id": "2510.06557"
    },
    "2510.06303": {
        "SCORE": 19,
        "ARXIVID": "2510.06303",
        "COMMENT": "Model Architecture and Efficiency: introduces a hybrid AR\u2013diffusion decoding paradigm enabling blockwise parallel generation and reports scaling across dense and MoE models.",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "Shuang Cheng",
            "Yihan Bian",
            "Dawei Liu",
            "Yuhua Jiang",
            "Yihao Liu",
            "Linfeng Zhang",
            "Wenhai Wang",
            "Qipeng Guo",
            "Kai Chen",
            "Biqing Qi",
            "Bowen Zhou"
        ],
        "title": "SDAR: A Synergistic Diffusion-AutoRegression Paradigm for Scalable Sequence Generation",
        "abstract": "We propose SDAR, a Synergistic Diffusion-Autoregression paradigm that unifies the training efficiency of autoregressive models with the parallel inference capability of diffusion. Instead of costly end-to-end diffusion training, SDAR performs a lightweight paradigm conversion that transforms a well-trained autoregressive (AR) model into a blockwise diffusion model through brief, data-efficient adaptation. During inference, SDAR generates sequences autoregressively across blocks for global coherence while decoding all tokens within each block in parallel via a discrete diffusion process. Extensive experiments show that AR models remain substantially more compute-efficient than masked diffusion models, providing a strong foundation for adaptation. Building on this insight, SDAR achieves efficient AR-to-diffusion conversion with minimal cost, preserving AR-level performance while enabling parallel generation. Scaling studies across dense and Mixture-of-Experts architectures confirm that SDAR scales without compromise: larger models exhibit stronger robustness to block size and decoding thresholds, yielding greater speedups without accuracy loss. Beyond efficiency, SDAR demonstrates enhanced reasoning and domain adaptability. Our 30B MoE model surpasses its AR counterpart on challenging scientific reasoning benchmarks such as GPQA and ChemBench, and gains further improvements under test-time scaling methods like majority voting and pass@k. Together, these results establish SDAR as a practical paradigm that combines the strengths of autoregression and diffusion for scalable, high-throughput reasoning.",
        "arxiv_id": "2510.06303"
    },
    "2510.06954": {
        "SCORE": 18,
        "ARXIVID": "2510.06954",
        "COMMENT": "Representation Learning/Training Dynamics: theoretical two-stage analysis of Transformer attention training (condensation then rank collapse) under gradient flow.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Zheng-An Chen",
            "Tao Luo"
        ],
        "title": "From Condensation to Rank Collapse: A Two-Stage Analysis of Transformer Training Dynamics",
        "abstract": "Although transformer-based models have shown exceptional empirical performance, the fundamental principles governing their training dynamics are inadequately characterized beyond configuration-specific studies. Inspired by empirical evidence showing improved reasoning capabilities under small initialization scales in language models, we employ the gradient flow analytical framework established in [Zhou et al. NeurIPS 2022] to systematically investigate linearized Transformer training dynamics. Our theoretical analysis dissects the dynamics of attention modules into two distinct stages. In the first stage, asymmetric weight perturbations from random initialization sustain non-degenerate gradient dynamics in parameter matrices, facilitating systematic escape from small initialization regimes. Subsequently, these matrices undergo condensation, progressively aligning toward the target orientation. In the second stage, the previously static key-query matrices actively participate in training, driving the normalized matrices toward asymptotic rank collapse. This two-stage framework generalizes classical directional convergence results.",
        "arxiv_id": "2510.06954"
    },
    "2510.07318": {
        "SCORE": 18,
        "ARXIVID": "2510.07318",
        "COMMENT": "Model Architecture and Efficiency: hybrid memory design combining Transformer KV cache with learnable RNN-like compressive long-term memory (AHN) to cut FLOPs and cache.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Yunhao Fang",
            "Weihao Yu",
            "Shu Zhong",
            "Qinghao Ye",
            "Xuehan Xiong",
            "Lai Wei"
        ],
        "title": "Artificial Hippocampus Networks for Efficient Long-Context Modeling",
        "abstract": "Long-sequence modeling faces a fundamental trade-off between the efficiency of compressive fixed-size memory in RNN-like models and the fidelity of lossless growing memory in attention-based Transformers. Inspired by the Multi-Store Model in cognitive science, we introduce a memory framework of artificial neural networks. Our method maintains a sliding window of the Transformer's KV cache as lossless short-term memory, while a learnable module termed Artificial Hippocampus Network (AHN) recurrently compresses out-of-window information into a fixed-size compact long-term memory. To validate this framework, we instantiate AHNs using modern RNN-like architectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate that AHN-augmented models consistently outperform sliding window baselines and achieve performance comparable or even superior to full-attention models, while substantially reducing computational and memory requirements. For instance, augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5% and memory cache by 74.0%, while improving its average score on LV-Eval (128k sequence length) from 4.41 to 5.88. Code is available at: https://github.com/ByteDance-Seed/AHN.",
        "arxiv_id": "2510.07318"
    },
    "2510.06377": {
        "SCORE": 17,
        "ARXIVID": "2510.06377",
        "COMMENT": "Proposes a new Transformer variant with Relational Attention over rows/columns/PK\u2013FK links, a clear architecture innovation for relational data and representation learning.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Rishabh Ranjan",
            "Valter Hudovernik",
            "Mark Znidar",
            "Charilaos Kanatsoulis",
            "Roshan Upendra",
            "Mahmoud Mohammadi",
            "Joe Meyer",
            "Tom Palczewski",
            "Carlos Guestrin",
            "Jure Leskovec"
        ],
        "title": "Relational Transformer: Toward Zero-Shot Foundation Models for Relational Data",
        "abstract": "Pretrained transformers readily adapt to new sequence modeling tasks via zero-shot prompting, but relational domains still lack architectures that transfer across datasets and tasks. The core challenge is the diversity of relational data, with varying heterogeneous schemas, graph structures and functional dependencies. In this paper, we present the Relational Transformer (RT) architecture, which can be pretrained on diverse relational databases and directly applied to unseen datasets and tasks without task- or dataset-specific fine-tuning, or retrieval of in-context examples. RT (i) tokenizes cells with table/column metadata, (ii) is pretrained via masked token prediction, and (iii) utilizes a novel \\textit{Relational Attention} mechanism over columns, rows, and primary-foreign key links. Pretrained on RelBench datasets spanning tasks such as churn and sales forecasting, RT attains strong zero-shot performance, averaging 94% of fully supervised AUROC on binary classification tasks with a single forward pass of a 22M parameter model, as opposed to 84% for a 27B LLM. Fine-tuning yields state-of-the-art results with high sample efficiency. Our experiments show that RT's zero-shot transfer harnesses task-table context, relational attention patterns and schema semantics. Overall, RT provides a practical path toward foundation models for relational data.",
        "arxiv_id": "2510.06377"
    },
    "2510.06824": {
        "SCORE": 17,
        "ARXIVID": "2510.06824",
        "COMMENT": "Efficiency/Architecture: proposes single-token number embeddings (BitTokens) via IEEE 754 to reduce tokenization overhead and enable efficient arithmetic in LLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Linus Kreitner",
            "Paul Hager",
            "Jonathan Mengedoht",
            "Georgios Kaissis",
            "Daniel Rueckert",
            "Martin J. Menten"
        ],
        "title": "Efficient numeracy in language models through single-token number embeddings",
        "abstract": "To drive progress in science and engineering, large language models (LLMs) must be able to process large amounts of numerical data and solve long calculations efficiently. This is currently only possible through the use of external tools or extensive reasoning chains, either limiting the numerical intuition of LLMs or limiting the length of problems they can solve. We show that frontier LLMs require excessive amounts of reasoning tokens to solve even basic calculations, which is exacerbated by their tokenization strategies that split single numbers into multiple tokens. This motivates the need for efficient and effective single-token number encodings. We introduce a set of desiderata for such encodings and show that existing approaches fail to fulfill them. To address these shortcomings, we propose BitTokens, a novel tokenization strategy that embeds any number into a single token using its IEEE 754 binary floating-point representation. Through extensive experiments we show that our BitTokens allow even small language models to learn algorithms that solve basic arithmetic operations nearly perfectly. This newly gained efficiency could expand the length and complexity of problems language models can solve.",
        "arxiv_id": "2510.06824"
    },
    "2510.06548": {
        "SCORE": 17,
        "ARXIVID": "2510.06548",
        "COMMENT": "Training Dynamics/Efficiency: establishes a scaling law for multi-stage (bootstrapped) pretraining, guiding efficient reuse of overtrained base models.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Seng Pei Liew",
            "Takuya Kato"
        ],
        "title": "From Acceleration to Saturation: Scaling Behavior of Bootstrapped Language Model Pretraining",
        "abstract": "Bootstrapped pretraining, i.e., the reuse of a pretrained base model for further pretraining, such as continual pretraining or model growth, is promising at reducing the cost of training language models from scratch. However, its effectiveness remains unclear, especially when applied to overtrained base models. In this work, we empirically study the scaling behavior of bootstrapped pretraining and find that its scaling efficiency diminishes in a predictable manner: The scaling exponent with respect to second-stage pretraining tokens decreases logarithmically with the number of tokens used to pretrain the base model. The joint dependence on first- and second-stage tokens is accurately modeled by a simple scaling law. Such saturation effect reveals a fundamental trade-off in multi-stage pretraining strategies: the more extensively a model is pretrained, the less additional benefit bootstrapping provides. Our findings provide practical insights for efficient language model training and raise important considerations for the reuse of overtrained models.",
        "arxiv_id": "2510.06548"
    },
    "2510.07213": {
        "SCORE": 17,
        "ARXIVID": "2510.07213",
        "COMMENT": "Matches Representation Learning and Sparsity: identifies and manipulates sparse, layer-consistent dimensions governing multilingual control without training.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Chengzhi Zhong",
            "Fei Cheng",
            "Qianying Liu",
            "Yugo Murawaki",
            "Chenhui Chu",
            "Sadao Kurohashi"
        ],
        "title": "Language Lives in Sparse Dimensions: Toward Interpretable and Efficient Multilingual Control for Large Language Models",
        "abstract": "Large language models exhibit strong multilingual capabilities despite limited exposure to non-English data. Prior studies show that English-centric large language models map multilingual content into English-aligned representations at intermediate layers and then project them back into target-language token spaces in the final layer. From this observation, we hypothesize that this cross-lingual transition is governed by a small and sparse set of dimensions, which occur at consistent indices across the intermediate to final layers. Building on this insight, we introduce a simple, training-free method to identify and manipulate these dimensions, requiring only as few as 50 sentences of either parallel or monolingual data. Experiments on a multilingual generation control task reveal the interpretability of these dimensions, demonstrating that the interventions in these dimensions can switch the output language while preserving semantic content, and that it surpasses the performance of prior neuron-based approaches at a substantially lower cost.",
        "arxiv_id": "2510.07213"
    },
    "2510.06527": {
        "SCORE": 17,
        "ARXIVID": "2510.06527",
        "COMMENT": "Matches Representation Learning/training dynamics: theoretical condition for near-independent outputs in wide nets via zero-mean activations, informing architectural design.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "John Dunbar",
            "Scott Aaronson"
        ],
        "title": "Wide Neural Networks as a Baseline for the Computational No-Coincidence Conjecture",
        "abstract": "We establish that randomly initialized neural networks, with large width and a natural choice of hyperparameters, have nearly independent outputs exactly when their activation function is nonlinear with zero mean under the Gaussian measure: $\\mathbb{E}_{z \\sim \\mathcal{N}(0,1)}[\\sigma(z)]=0$. For example, this includes ReLU and GeLU with an additive shift, as well as tanh, but not ReLU or GeLU by themselves. Because of their nearly independent outputs, we propose neural networks with zero-mean activation functions as a promising candidate for the Alignment Research Center's computational no-coincidence conjecture -- a conjecture that aims to measure the limits of AI interpretability.",
        "arxiv_id": "2510.06527"
    },
    "2510.06401": {
        "SCORE": 17,
        "ARXIVID": "2510.06401",
        "COMMENT": "Representation Learning: analyzes information content of hidden representations and training dynamics under label noise using an information-theoretic proxy.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Ali Hussaini Umar",
            "Franky Kevin Nando Tezoh",
            "Jean Barbier",
            "Santiago Acevedo",
            "Alessandro Laio"
        ],
        "title": "The Effect of Label Noise on the Information Content of Neural Representations",
        "abstract": "In supervised classification tasks, models are trained to predict a label for each data point. In real-world datasets, these labels are often noisy due to annotation errors. While the impact of label noise on the performance of deep learning models has been widely studied, its effects on the networks' hidden representations remain poorly understood. We address this gap by systematically comparing hidden representations using the Information Imbalance, a computationally efficient proxy of conditional mutual information. Through this analysis, we observe that the information content of the hidden representations follows a double descent as a function of the number of network parameters, akin to the behavior of the test error. We further demonstrate that in the underparameterized regime, representations learned with noisy labels are more informative than those learned with clean labels, while in the overparameterized regime, these representations are equally informative. Our results indicate that the representations of overparameterized networks are robust to label noise. We also found that the information imbalance between the penultimate and pre-softmax layers decreases with cross-entropy loss in the overparameterized regime. This offers a new perspective on understanding generalization in classification tasks. Extending our analysis to representations learned from random labels, we show that these perform worse than random features. This indicates that training on random labels drives networks much beyond lazy learning, as weights adapt to encode labels information.",
        "arxiv_id": "2510.06401"
    },
    "2510.06949": {
        "SCORE": 17,
        "ARXIVID": "2510.06949",
        "COMMENT": "Model Architecture: introduces grouped differential attention with ratio-aware head allocation and selective expansion for more compute-efficient Transformers.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Junghwan Lim",
            "Sungmin Lee",
            "Dongseok Kim",
            "Wai Ting Cheung",
            "Beomgyu Kim",
            "Taehwan Kim",
            "Haesol Lee",
            "Junhyeok Lee",
            "Dongpin Oh",
            "Eunhwan Park"
        ],
        "title": "Grouped Differential Attention",
        "abstract": "The self-attention mechanism, while foundational to modern Transformer architectures, suffers from a critical inefficiency: it frequently allocates substantial attention to redundant or noisy context. Differential Attention addressed this by using subtractive attention maps for signal and noise, but its required balanced head allocation imposes rigid constraints on representational flexibility and scalability.   To overcome this, we propose Grouped Differential Attention (GDA), a novel approach that introduces unbalanced head allocation between signal-preserving and noise-control groups. GDA significantly enhances signal focus by strategically assigning more heads to signal extraction and fewer to noise-control, stabilizing the latter through controlled repetition (akin to GQA). This design achieves stronger signal fidelity with minimal computational overhead. We further extend this principle to group-differentiated growth, a scalable strategy that selectively replicates only the signal-focused heads, thereby ensuring efficient capacity expansion.   Through large-scale pretraining and continual training experiments, we demonstrate that moderate imbalance ratios in GDA yield substantial improvements in generalization and stability compared to symmetric baselines. Our results collectively establish that ratio-aware head allocation and selective expansion offer an effective and practical path toward designing scalable, computation-efficient Transformer architectures.",
        "arxiv_id": "2510.06949"
    },
    "2510.06372": {
        "SCORE": 16,
        "ARXIVID": "2510.06372",
        "COMMENT": "Model Architecture theory: provides a constructive upper bound on neurons needed in shallow networks to approximate continuous functions on compact sets.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Frantisek Hakl",
            "Vit Fojtik"
        ],
        "title": "A General Constructive Upper Bound on Shallow Neural Nets Complexity",
        "abstract": "We provide an upper bound on the number of neurons required in a shallow   neural network to approximate a continuous function on a compact set with a   given accuracy. This method, inspired by a specific proof of the   Stone-Weierstrass theorem, is constructive and more general than previous   bounds of this character, as it applies to any continuous function on any   compact set.",
        "arxiv_id": "2510.06372"
    },
    "2510.07227": {
        "SCORE": 16,
        "ARXIVID": "2510.07227",
        "COMMENT": "Compression/Efficiency: selects structurally sparse subnetwork initializations via evolutionary search and uses distillation to accelerate pretraining, achieving 9.2x fewer tokens for comparable perplexity.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Arjun Krishnakumar",
            "Rhea Sanjay Sukthanker",
            "Hannan Javed Mahadik",
            "Gabriela Kadlecov\\'a",
            "Vladyslav Moroshan",
            "Timur Carstensen",
            "Frank Hutter",
            "Aaron Klein"
        ],
        "title": "Where to Begin: Efficient Pretraining via Subnetwork Selection and Distillation",
        "abstract": "Small Language models (SLMs) offer an efficient and accessible alternative to Large Language Models (LLMs), delivering strong performance while using far fewer resources. We introduce a simple and effective framework for pretraining SLMs that brings together three complementary ideas. First, we identify structurally sparse sub-network initializations that consistently outperform randomly initialized models of similar size under the same compute budget. Second, we use evolutionary search to automatically discover high-quality sub-network initializations, providing better starting points for pretraining. Third, we apply knowledge distillation from larger teacher models to speed up training and improve generalization. Together, these components make SLM pretraining substantially more efficient: our best model, discovered using evolutionary search and initialized with LLM weights, matches the validation perplexity of a comparable Pythia SLM while requiring 9.2x fewer pretraining tokens. We release all code and models at https://github.com/whittle-org/whittle/, offering a practical and reproducible path toward cost-efficient small language model development at scale.",
        "arxiv_id": "2510.07227"
    },
    "2510.07019": {
        "SCORE": 16,
        "ARXIVID": "2510.07019",
        "COMMENT": "Matches Model Architecture and Efficiency: proposes a hybrid linear+softmax attention layer with sliding-window control for long-context sequence modeling, reducing quadratic attention cost.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Jusen Du",
            "Jiaxi Hu",
            "Tao Zhang",
            "Weigao Sun",
            "Yu Cheng"
        ],
        "title": "Native Hybrid Attention for Efficient Sequence Modeling",
        "abstract": "Transformers excel at sequence modeling but face quadratic complexity, while linear attention offers improved efficiency but often compromises recall accuracy over long contexts. In this work, we introduce Native Hybrid Attention (NHA), a novel hybrid architecture of linear and full attention that integrates both intra \\& inter-layer hybridization into a unified layer design. NHA maintains long-term context in key-value slots updated by a linear RNN, and augments them with short-term tokens from a sliding window. A single \\texttt{softmax attention} operation is then applied over all keys and values, enabling per-token and per-head context-dependent weighting without requiring additional fusion parameters. The inter-layer behavior is controlled through a single hyperparameter, the sliding window size, which allows smooth adjustment between purely linear and full attention while keeping all layers structurally uniform. Experimental results show that NHA surpasses Transformers and other hybrid baselines on recall-intensive and commonsense reasoning tasks. Furthermore, pretrained LLMs can be structurally hybridized with NHA, achieving competitive accuracy while delivering significant efficiency gains. Code is available at https://github.com/JusenD/NHA.",
        "arxiv_id": "2510.07019"
    },
    "2510.06640": {
        "SCORE": 16,
        "ARXIVID": "2510.06640",
        "COMMENT": "Matches Representation Learning: token- and layer-level analysis of representation propagation and oversmoothing in SSMs vs Transformers, revealing inductive biases and training dynamics.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Nhat M. Hoang",
            "Do Xuan Long",
            "Cong-Duy Nguyen",
            "Min-Yen Kan",
            "Luu Anh Tuan"
        ],
        "title": "A Comparative Analysis of Contextual Representation Flow in State-Space and Transformer Architectures",
        "abstract": "State Space Models (SSMs) have recently emerged as efficient alternatives to Transformer-Based Models (TBMs) for long-sequence processing, offering linear scaling and lower memory use. Yet, how contextual information flows across layers and tokens in these architectures remains understudied. We present the first unified, token- and layer-level analysis of representation propagation in SSMs and TBMs. Using centered kernel alignment, stability metrics, and probing, we characterize how representations evolve within and across layers. We find a key divergence: TBMs rapidly homogenize token representations, with diversity reemerging only in later layers, while SSMs preserve token uniqueness early but converge to homogenization deeper. Theoretical analysis and parameter randomization further reveal that oversmoothing in TBMs stems from architectural design, whereas in SSMs it arises mainly from training dynamics. These insights clarify the inductive biases of both architectures and inform future model and training designs for long-context reasoning.",
        "arxiv_id": "2510.06640"
    },
    "2510.07018": {
        "SCORE": 16,
        "ARXIVID": "2510.07018",
        "COMMENT": "Matches Model Compression/Efficiency: zero-shot quantization with sharpness-aware synthetic data generation and supporting theory for better generalization.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Dung Hoang-Anh",
            "Cuong Pham Trung Le",
            "Jianfei Cai",
            "Thanh-Toan Do"
        ],
        "title": "Sharpness-Aware Data Generation for Zero-shot Quantization",
        "abstract": "Zero-shot quantization aims to learn a quantized model from a pre-trained full-precision model with no access to original real training data. The common idea in zero-shot quantization approaches is to generate synthetic data for quantizing the full-precision model. While it is well-known that deep neural networks with low sharpness have better generalization ability, none of the previous zero-shot quantization works considers the sharpness of the quantized model as a criterion for generating training data. This paper introduces a novel methodology that takes into account quantized model sharpness in synthetic data generation to enhance generalization. Specifically, we first demonstrate that sharpness minimization can be attained by maximizing gradient matching between the reconstruction loss gradients computed on synthetic and real validation data, under certain assumptions. We then circumvent the problem of the gradient matching without real validation set by approximating it with the gradient matching between each generated sample and its neighbors. Experimental evaluations on CIFAR-100 and ImageNet datasets demonstrate the superiority of the proposed method over the state-of-the-art techniques in low-bit quantization settings.",
        "arxiv_id": "2510.07018"
    },
    "2510.07195": {
        "SCORE": 16,
        "ARXIVID": "2510.07195",
        "COMMENT": "High Performance Computing/Efficiency: fully coherent quantum implementation of multilayer neural inference with provable speedups under quantum data access assumptions.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Arthur G. Rattew",
            "Po-Wei Huang",
            "Naixu Guo",
            "Lirand\\\"e Pira",
            "Patrick Rebentrost"
        ],
        "title": "Accelerating Inference for Multilayer Neural Networks with Quantum Computers",
        "abstract": "Fault-tolerant Quantum Processing Units (QPUs) promise to deliver exponential speed-ups in select computational tasks, yet their integration into modern deep learning pipelines remains unclear. In this work, we take a step towards bridging this gap by presenting the first fully-coherent quantum implementation of a multilayer neural network with non-linear activation functions. Our constructions mirror widely used deep learning architectures based on ResNet, and consist of residual blocks with multi-filter 2D convolutions, sigmoid activations, skip-connections, and layer normalizations. We analyse the complexity of inference for networks under three quantum data access regimes. Without any assumptions, we establish a quadratic speedup over classical methods for shallow bilinear-style networks. With efficient quantum access to the weights, we obtain a quartic speedup over classical methods. With efficient quantum access to both the inputs and the network weights, we prove that a network with an $N$-dimensional vectorized input, $k$ residual block layers, and a final residual-linear-pooling layer can be implemented with an error of $\\epsilon$ with $O(\\text{polylog}(N/\\epsilon)^k)$ inference cost.",
        "arxiv_id": "2510.07195"
    },
    "2510.06434": {
        "SCORE": 16,
        "ARXIVID": "2510.06434",
        "COMMENT": "Representation Learning/Training Theory: Hellinger localization framework yields near instance-optimal MLE rates for multi-trajectory sequential models, including linear-attention sequence models.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Eliot Shekhtman",
            "Yichen Zhou",
            "Ingvar Ziemann",
            "Nikolai Matni",
            "Stephen Tu"
        ],
        "title": "Nearly Instance-Optimal Parameter Recovery from Many Trajectories via Hellinger Localization",
        "abstract": "Learning from temporally-correlated data is a core facet of modern machine learning. Yet our understanding of sequential learning remains incomplete, particularly in the multi-trajectory setting where data consists of many independent realizations of a time-indexed stochastic process. This important regime both reflects modern training pipelines such as for large foundation models, and offers the potential for learning without the typical mixing assumptions made in the single-trajectory case. However, instance-optimal bounds are known only for least-squares regression with dependent covariates; for more general models or loss functions, the only broadly applicable guarantees result from a reduction to either i.i.d. learning, with effective sample size scaling only in the number of trajectories, or an existing single-trajectory result when each individual trajectory mixes, with effective sample size scaling as the full data budget deflated by the mixing-time.   In this work, we significantly broaden the scope of instance-optimal rates in multi-trajectory settings via the Hellinger localization framework, a general approach for maximum likelihood estimation. Our method proceeds by first controlling the squared Hellinger distance at the path-measure level via a reduction to i.i.d. learning, followed by localization as a quadratic form in parameter space weighted by the trajectory Fisher information. This yields instance-optimal bounds that scale with the full data budget under a broad set of conditions. We instantiate our framework across four diverse case studies: a simple mixture of Markov chains, dependent linear regression under non-Gaussian noise, generalized linear models with non-monotonic activations, and linear-attention sequence models. In all cases, our bounds nearly match the instance-optimal rates from asymptotic normality, substantially improving over standard reductions.",
        "arxiv_id": "2510.06434"
    },
    "2510.06673": {
        "SCORE": 16,
        "ARXIVID": "2510.06673",
        "COMMENT": "Model Architecture: introduces a causal Transformer with a novel \u201cnext 2D distribution prediction\u201d objective and a reconstruction-focused visual tokenizer, unifying autoregressive modeling with masked autoencoding.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Yongxin Zhu",
            "Jiawei Chen",
            "Yuanzhe Chen",
            "Zhuo Chen",
            "Dongya Jia",
            "Jian Cong",
            "Xiaobin Zhuang",
            "Yuping Wang",
            "Yuxuan Wang"
        ],
        "title": "Heptapod: Language Modeling on Visual Signals",
        "abstract": "We introduce Heptapod, an image autoregressive model that adheres to the foundational principles of language modeling. Heptapod employs \\textbf{causal attention}, \\textbf{eliminates reliance on CFG}, and \\textbf{eschews the trend of semantic tokenizers}. Our key innovation is \\textit{next 2D distribution prediction}: a causal Transformer with reconstruction-focused visual tokenizer, learns to predict the distribution over the entire 2D spatial grid of images at each timestep. This learning objective unifies the sequential modeling of autoregressive framework with the holistic self-supervised learning of masked autoencoding, enabling the model to capture comprehensive image semantics via generative training. On the ImageNet generation benchmark, Heptapod achieves an FID of $2.70$, significantly outperforming previous causal autoregressive approaches. We hope our work inspires a principled rethinking of language modeling on visual signals and beyond.",
        "arxiv_id": "2510.06673"
    },
    "2510.06293": {
        "SCORE": 15,
        "ARXIVID": "2510.06293",
        "COMMENT": "Introduces a frame-level autoregressive Transformer with space\u2013time factorization and batched tokenization, improving architectural efficiency (notably faster inference).",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Cristian Meo",
            "Varun Sarathchandran",
            "Avijit Majhi",
            "Shao Hung",
            "Carlo Saccardi",
            "Ruben Imhoff",
            "Roberto Deidda",
            "Remko Uijlenhoet",
            "Justin Dauwels"
        ],
        "title": "BlockGPT: Spatio-Temporal Modelling of Rainfall via Frame-Level Autoregression",
        "abstract": "Predicting precipitation maps is a highly complex spatiotemporal modeling task, critical for mitigating the impacts of extreme weather events. Short-term precipitation forecasting, or nowcasting, requires models that are not only accurate but also computationally efficient for real-time applications. Current methods, such as token-based autoregressive models, often suffer from flawed inductive biases and slow inference, while diffusion models can be computationally intensive. To address these limitations, we introduce BlockGPT, a generative autoregressive transformer using batched tokenization (Block) method that predicts full two-dimensional fields (frames) at each time step. Conceived as a model-agnostic paradigm for video prediction, BlockGPT factorizes space-time by using self-attention within each frame and causal attention across frames; in this work, we instantiate it for precipitation nowcasting. We evaluate BlockGPT on two precipitation datasets, viz. KNMI (Netherlands) and SEVIR (U.S.), comparing it to state-of-the-art baselines including token-based (NowcastingGPT) and diffusion-based (DiffCast+Phydnet) models. The results show that BlockGPT achieves superior accuracy, event localization as measured by categorical metrics, and inference speeds up to 31x faster than comparable baselines.",
        "arxiv_id": "2510.06293"
    },
    "2510.06907": {
        "SCORE": 15,
        "ARXIVID": "2510.06907",
        "COMMENT": "Representation Learning: proposes a geometric angular embedding (SpherePair loss) with theoretical guarantees, decoupling representation learning from clustering.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Shaojie Zhang",
            "Ke Chen"
        ],
        "title": "Angular Constraint Embedding via SpherePair Loss for Constrained Clustering",
        "abstract": "Constrained clustering integrates domain knowledge through pairwise constraints. However, existing deep constrained clustering (DCC) methods are either limited by anchors inherent in end-to-end modeling or struggle with learning discriminative Euclidean embedding, restricting their scalability and real-world applicability. To avoid their respective pitfalls, we propose a novel angular constraint embedding approach for DCC, termed SpherePair. Using the SpherePair loss with a geometric formulation, our method faithfully encodes pairwise constraints and leads to embeddings that are clustering-friendly in angular space, effectively separating representation learning from clustering. SpherePair preserves pairwise relations without conflict, removes the need to specify the exact number of clusters, generalizes to unseen data, enables rapid inference of the number of clusters, and is supported by rigorous theoretical guarantees. Comparative evaluations with state-of-the-art DCC methods on diverse benchmarks, along with empirical validation of theoretical insights, confirm its superior performance, scalability, and overall real-world effectiveness. Code is available at \\href{https://github.com/spherepaircc/SpherePairCC/tree/main}{our repository}.",
        "arxiv_id": "2510.06907"
    },
    "2510.06502": {
        "SCORE": 15,
        "ARXIVID": "2510.06502",
        "COMMENT": "Matches Model Compression and Efficiency: parameter-space guided initialization/distillation (GUIDE) improves teacher\u2013student transfer with no training/inference overhead.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Khoa Trinh",
            "Gaurav Menghani",
            "Erik Vee"
        ],
        "title": "GUIDE: Guided Initialization and Distillation of Embeddings",
        "abstract": "Algorithmic efficiency techniques such as distillation (\\cite{hinton2015distillation}) are useful in improving model quality without increasing serving costs, provided a larger teacher model is available for a smaller student model to learn from during training. Standard distillation methods are limited to only forcing the student to match the teacher's outputs. Given the costs associated with training a large model, we believe we should be extracting more useful information from a teacher model than by just making the student match the teacher's outputs.   In this paper, we introduce \\guide (Guided Initialization and Distillation of Embeddings). \\guide can be considered a distillation technique that forces the student to match the teacher in the parameter space. Using \\guide we show 25-26\\% reduction in the teacher-student quality gap when using large student models (400M - 1B parameters) trained on $\\approx$ 20B tokens. We also present a thorough analysis demonstrating that \\guide can be combined with knowledge distillation with near additive improvements. Furthermore, we show that applying \\guide alone leads to substantially better model quality than applying knowledge distillation by itself.   Most importantly, \\guide introduces no training or inference overhead and hence any model quality gains from our method are virtually free.",
        "arxiv_id": "2510.06502"
    },
    "2510.06632": {
        "SCORE": 15,
        "ARXIVID": "2510.06632",
        "COMMENT": "Matches Representation Learning and Low-rank methods: multi-layer \u03b1-divergence NMF with a convergence-stabilizing scheme and rigorous asymptotic analysis.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Yasaman Torabi",
            "Shahram Shirani",
            "James P. Reilly"
        ],
        "title": "Chem-NMF: Multi-layer $\\alpha$-divergence Non-Negative Matrix Factorization for Cardiorespiratory Disease Clustering, with Improved Convergence Inspired by Chemical Catalysts and Rigorous Asymptotic Analysis",
        "abstract": "Non-Negative Matrix Factorization (NMF) is an unsupervised learning method offering low-rank representations across various domains such as audio processing, biomedical signal analysis, and image recognition. The incorporation of $\\alpha$-divergence in NMF formulations enhances flexibility in optimization, yet extending these methods to multi-layer architectures presents challenges in ensuring convergence. To address this, we introduce a novel approach inspired by the Boltzmann probability of the energy barriers in chemical reactions to theoretically perform convergence analysis. We introduce a novel method, called Chem-NMF, with a bounding factor which stabilizes convergence. To our knowledge, this is the first study to apply a physical chemistry perspective to rigorously analyze the convergence behaviour of the NMF algorithm. We start from mathematically proven asymptotic convergence results and then show how they apply to real data. Experimental results demonstrate that the proposed algorithm improves clustering accuracy by 5.6% $\\pm$ 2.7% on biomedical signals and 11.1% $\\pm$ 7.2% on face images (mean $\\pm$ std).",
        "arxiv_id": "2510.06632"
    },
    "2510.07304": {
        "SCORE": 15,
        "ARXIVID": "2510.07304",
        "COMMENT": "High Performance Computing: hardware\u2013software co-design (precomputed correlated DP noise, near-memory processing) to reduce training overheads for large models/embeddings.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Donghwan Kim",
            "Xin Gu",
            "Jinho Baek",
            "Timothy Lo",
            "Younghoon Min",
            "Kwangsik Shin",
            "Jongryool Kim",
            "Jongse Park",
            "Kiwan Maeng"
        ],
        "title": "Cocoon: A System Architecture for Differentially Private Training with Correlated Noises",
        "abstract": "Machine learning (ML) models memorize and leak training data, causing serious privacy issues to data owners. Training algorithms with differential privacy (DP), such as DP-SGD, have been gaining attention as a solution. However, DP-SGD adds a noise at each training iteration, which degrades the accuracy of the trained model. To improve accuracy, a new family of approaches adds carefully designed correlated noises, so that noises cancel out each other across iterations. We performed an extensive characterization study of these new mechanisms, for the first time to the best of our knowledge, and show they incur non-negligible overheads when the model is large or uses large embedding tables. Motivated by the analysis, we propose Cocoon, a hardware-software co-designed framework for efficient training with correlated noises. Cocoon accelerates models with embedding tables through pre-computing and storing correlated noises in a coalesced format (Cocoon-Emb), and supports large models through a custom near-memory processing device (Cocoon-NMP). On a real system with an FPGA-based NMP device prototype, Cocoon improves the performance by 2.33-10.82x(Cocoon-Emb) and 1.55-3.06x (Cocoon-NMP).",
        "arxiv_id": "2510.07304"
    },
    "2510.06834": {
        "SCORE": 15,
        "ARXIVID": "2510.06834",
        "COMMENT": "High Performance Computing/Efficiency: vectorized FlashAttention on RISC\u2011V with low-cost exponential approximation and tiling to improve memory locality and throughput.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Vasileios Titopoulos",
            "Kosmas Alexandridis",
            "Giorgos Dimitrakopoulos"
        ],
        "title": "Vectorized FlashAttention with Low-cost Exponential Computation in RISC-V Vector Processors",
        "abstract": "Attention is a core operation in numerous machine learning and artificial intelligence models. This work focuses on the acceleration of attention kernel using FlashAttention algorithm, in vector processors, particularly those based on the RISC-V instruction set architecture (ISA). This work represents the first effort to vectorize FlashAttention, minimizing scalar code and simplifying the computational complexity of evaluating exponentials needed by softmax used in attention. By utilizing a low-cost approximation for exponentials in floating-point arithmetic, we reduce the cost of computing the exponential function without the need to extend baseline vector ISA with new custom instructions. Also, appropriate tiling strategies are explored with the goal to improve memory locality. Experimental results highlight the scalability of our approach, demonstrating significant performance gains with the vectorized implementations when processing attention layers in practical applications.",
        "arxiv_id": "2510.06834"
    }
}