{
    "2505.21073": {
        "SCORE": 18,
        "ARXIVID": "2505.21073",
        "COMMENT": "The paper presents a novel differentiable optimization framework for bridging arbitrary and tree metrics, which is a cutting-edge theoretical work challenging established assumptions.",
        "RELEVANCE": 9,
        "NOVELTY": 9,
        "authors": [
            "Pierre Houedry",
            "Nicolas Courty",
            "Florestan Martin-Baillon",
            "Laetitia Chapel",
            "Titouan Vayer"
        ],
        "title": "Bridging Arbitrary and Tree Metrics via Differentiable Gromov Hyperbolicity",
        "abstract": "Trees and the associated shortest-path tree metrics provide a powerful framework for representing hierarchical and combinatorial structures in data. Given an arbitrary metric space, its deviation from a tree metric can be quantified by Gromov's $\\delta$-hyperbolicity. Nonetheless, designing algorithms that bridge an arbitrary metric to its closest tree metric is still a vivid subject of interest, as most common approaches are either heuristical and lack guarantees, or perform moderately well. In this work, we introduce a novel differentiable optimization framework, coined DeltaZero, that solves this problem. Our method leverages a smooth surrogate for Gromov's $\\delta$-hyperbolicity which enables a gradient-based optimization, with a tractable complexity. The corresponding optimization procedure is derived from a problem with better worst case guarantees than existing bounds, and is justified statistically. Experiments on synthetic and real-world datasets demonstrate that our method consistently achieves state-of-the-art distortion.",
        "arxiv_id": "2505.21073"
    },
    "2505.21251": {
        "SCORE": 18,
        "ARXIVID": "2505.21251",
        "COMMENT": "The paper introduces copresheaf topological neural networks, a generalized deep learning framework, which aligns with emerging trends and architectural innovation.",
        "RELEVANCE": 9,
        "NOVELTY": 9,
        "authors": [
            "Mustafa Hajij",
            "Lennart Bastian",
            "Sarah Osentoski",
            "Hardik Kabaria",
            "John L. Davenport",
            "Sheik Dawood",
            "Balaji Cherukuri",
            "Joseph G. Kocheemoolayil",
            "Nastaran Shahmansouri",
            "Adrian Lew",
            "Theodore Papamarkou",
            "Tolga Birdal"
        ],
        "title": "Copresheaf Topological Neural Networks: A Generalized Deep Learning Framework",
        "abstract": "We introduce copresheaf topological neural networks (CTNNs), a powerful and unifying framework that encapsulates a wide spectrum of deep learning architectures, designed to operate on structured data: including images, point clouds, graphs, meshes, and topological manifolds. While deep learning has profoundly impacted domains ranging from digital assistants to autonomous systems, the principled design of neural architectures tailored to specific tasks and data types remains one of the field's most persistent open challenges. CTNNs address this gap by grounding model design in the language of copresheaves, a concept from algebraic topology that generalizes and subsumes most practical deep learning models in use today. This abstract yet constructive formulation yields a rich design space from which theoretically sound and practically effective solutions can be derived to tackle core challenges in representation learning: long-range dependencies, oversmoothing, heterophily, and non-Euclidean domains. Our empirical results on structured data benchmarks demonstrate that CTNNs consistently outperform conventional baselines, particularly in tasks requiring hierarchical or localized sensitivity. These results underscore CTNNs as a principled, multi-scale foundation for the next generation of deep learning architectures.",
        "arxiv_id": "2505.21251"
    },
    "2505.21422": {
        "SCORE": 18,
        "ARXIVID": "2505.21422",
        "COMMENT": "The paper provides theoretical insights into distribution shifts and hidden confounding, which aligns with emerging trends and foundational research.",
        "RELEVANCE": 9,
        "NOVELTY": 9,
        "authors": [
            "Abbavaram Gowtham Reddy",
            "Celia Rubio-Madrigal",
            "Rebekka Burkholz",
            "Krikamol Muandet"
        ],
        "title": "When Shift Happens - Confounding Is to Blame",
        "abstract": "Distribution shifts introduce uncertainty that undermines the robustness and generalization capabilities of machine learning models. While conventional wisdom suggests that learning causal-invariant representations enhances robustness to such shifts, recent empirical studies present a counterintuitive finding: (i) empirical risk minimization (ERM) can rival or even outperform state-of-the-art out-of-distribution (OOD) generalization methods, and (ii) its OOD generalization performance improves when all available covariates, not just causal ones, are utilized. Drawing on both empirical and theoretical evidence, we attribute this phenomenon to hidden confounding. Shifts in hidden confounding induce changes in data distributions that violate assumptions commonly made by existing OOD generalization approaches. Under such conditions, we prove that effective generalization requires learning environment-specific relationships, rather than relying solely on invariant ones. Furthermore, we show that models augmented with proxies for hidden confounders can mitigate the challenges posed by hidden confounding shifts. These findings offer new theoretical insights and practical guidance for designing robust OOD generalization algorithms and principled covariate selection strategies.",
        "arxiv_id": "2505.21422"
    },
    "2505.21364": {
        "SCORE": 17,
        "ARXIVID": "2505.21364",
        "COMMENT": "The paper introduces Mixture of Decoders (MxDs) for interpretable dense layer decomposition, relevant to model architecture and sparsity.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "James Oldfield",
            "Shawn Im",
            "Yixuan Li",
            "Mihalis A. Nicolaou",
            "Ioannis Patras",
            "Grigorios G Chrysos"
        ],
        "title": "Towards Interpretability Without Sacrifice: Faithful Dense Layer Decomposition with Mixture of Decoders",
        "abstract": "Multilayer perceptrons (MLPs) are an integral part of large language models, yet their dense representations render them difficult to understand, edit, and steer. Recent methods learn interpretable approximations via neuron-level sparsity, yet fail to faithfully reconstruct the original mapping--significantly increasing model's next-token cross-entropy loss. In this paper, we advocate for moving to layer-level sparsity to overcome the accuracy trade-off in sparse layer approximation. Under this paradigm, we introduce Mixture of Decoders (MxDs). MxDs generalize MLPs and Gated Linear Units, expanding pre-trained dense layers into tens of thousands of specialized sublayers. Through a flexible form of tensor factorization, each sparsely activating MxD sublayer implements a linear transformation with full-rank weights--preserving the original decoders' expressive capacity even under heavy sparsity. Experimentally, we show that MxDs significantly outperform state-of-the-art methods (e.g., Transcoders) on the sparsity-accuracy frontier in language models with up to 3B parameters. Further evaluations on sparse probing and feature steering demonstrate that MxDs learn similarly specialized features of natural language--opening up a promising new avenue for designing interpretable yet faithful decompositions. Our code is included at: https://github.com/james-oldfield/MxD/.",
        "arxiv_id": "2505.21364"
    },
    "2505.21475": {
        "SCORE": 17,
        "ARXIVID": "2505.21475",
        "COMMENT": "The paper provides a theoretical analysis of learning real-valued Multi-Index Models, which is relevant to representation learning and emerging trends.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Ilias Diakonikolas",
            "Giannis Iakovidis",
            "Daniel M. Kane",
            "Lisheng Ren"
        ],
        "title": "Algorithms and SQ Lower Bounds for Robustly Learning Real-valued Multi-index Models",
        "abstract": "We study the complexity of learning real-valued Multi-Index Models (MIMs) under the Gaussian distribution. A $K$-MIM is a function $f:\\mathbb{R}^d\\to \\mathbb{R}$ that depends only on the projection of its input onto a $K$-dimensional subspace. We give a general algorithm for PAC learning a broad class of MIMs with respect to the square loss, even in the presence of adversarial label noise. Moreover, we establish a nearly matching Statistical Query (SQ) lower bound, providing evidence that the complexity of our algorithm is qualitatively optimal as a function of the dimension. Specifically, we consider the class of bounded variation MIMs with the property that degree at most $m$ distinguishing moments exist with respect to projections onto any subspace. In the presence of adversarial label noise, the complexity of our learning algorithm is $d^{O(m)}2^{\\mathrm{poly}(K/\\epsilon)}$. For the realizable and independent noise settings, our algorithm incurs complexity $d^{O(m)}2^{\\mathrm{poly}(K)}(1/\\epsilon)^{O(K)}$. To complement our upper bound, we show that if for some subspace degree-$m$ distinguishing moments do not exist, then any SQ learner for the corresponding class of MIMs requires complexity $d^{\\Omega(m)}$. As an application, we give the first efficient learner for the class of positive-homogeneous $L$-Lipschitz $K$-MIMs. The resulting algorithm has complexity $\\mathrm{poly}(d) 2^{\\mathrm{poly}(KL/\\epsilon)}$. This gives a new PAC learning algorithm for Lipschitz homogeneous ReLU networks with complexity independent of the network size, removing the exponential dependence incurred in prior work.",
        "arxiv_id": "2505.21475"
    },
    "2505.20433": {
        "SCORE": 17,
        "ARXIVID": "2505.20433",
        "COMMENT": "The paper introduces kernel quantile embeddings, which is relevant to representation learning and emerging trends.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Masha Naslidnyk",
            "Siu Lun Chau",
            "Fran\\c{c}ois-Xavier Briol",
            "Krikamol Muandet"
        ],
        "title": "Kernel Quantile Embeddings and Associated Probability Metrics",
        "abstract": "Embedding probability distributions into reproducing kernel Hilbert spaces (RKHS) has enabled powerful nonparametric methods such as the maximum mean discrepancy (MMD), a statistical distance with strong theoretical and computational properties. At its core, the MMD relies on kernel mean embeddings to represent distributions as mean functions in RKHS. However, it remains unclear if the mean function is the only meaningful RKHS representation. Inspired by generalised quantiles, we introduce the notion of kernel quantile embeddings (KQEs). We then use KQEs to construct a family of distances that: (i) are probability metrics under weaker kernel conditions than MMD; (ii) recover a kernelised form of the sliced Wasserstein distance; and (iii) can be efficiently estimated with near-linear cost. Through hypothesis testing, we show that these distances offer a competitive alternative to MMD and its fast approximations.",
        "arxiv_id": "2505.20433"
    },
    "2505.20666": {
        "SCORE": 17,
        "ARXIVID": "2505.20666",
        "COMMENT": "The paper introduces a novel attention mechanism using PDEs for long-sequence transformers, which is relevant to model architecture.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Yukun Zhang",
            "Xueqing Zhou"
        ],
        "title": "Continuous-Time Attention: PDE-Guided Mechanisms for Long-Sequence Transformers",
        "abstract": "We propose a novel framework, Continuous_Time Attention, which infuses partial differential equations (PDEs) into the Transformer's attention mechanism to address the challenges of extremely long input sequences. Instead of relying solely on a static attention matrix, we allow attention weights to evolve over a pseudo_time dimension via diffusion, wave, or reaction_diffusion dynamics. This mechanism systematically smooths local noise, enhances long_range dependencies, and stabilizes gradient flow. Theoretically, our analysis shows that PDE_based attention leads to better optimization landscapes and polynomial rather than exponential decay of distant interactions. Empirically, we benchmark our method on diverse experiments_demonstrating consistent gains over both standard and specialized long sequence Transformer variants. Our findings highlight the potential of PDE_based formulations to enrich attention mechanisms with continuous_time dynamics and global coherence.",
        "arxiv_id": "2505.20666"
    },
    "2505.20896": {
        "SCORE": 17,
        "ARXIVID": "2505.20896",
        "COMMENT": "The paper investigates how Transformers learn variable binding, providing insights into how deep networks encode information, which aligns with representation learning.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Yiwei Wu",
            "Atticus Geiger",
            "Rapha\\\"el Milli\\`ere"
        ],
        "title": "How Do Transformers Learn Variable Binding in Symbolic Programs?",
        "abstract": "Variable binding -- the ability to associate variables with values -- is fundamental to symbolic computation and cognition. Although classical architectures typically implement variable binding via addressable memory, it is not well understood how modern neural networks lacking built-in binding operations may acquire this capacity. We investigate this by training a Transformer to dereference queried variables in symbolic programs where variables are assigned either numerical constants or other variables. Each program requires following chains of variable assignments up to four steps deep to find the queried value, and also contains irrelevant chains of assignments acting as distractors. Our analysis reveals a developmental trajectory with three distinct phases during training: (1) random prediction of numerical constants, (2) a shallow heuristic prioritizing early variable assignments, and (3) the emergence of a systematic mechanism for dereferencing assignment chains. Using causal interventions, we find that the model learns to exploit the residual stream as an addressable memory space, with specialized attention heads routing information across token positions. This mechanism allows the model to dynamically track variable bindings across layers, resulting in accurate dereferencing. Our results show how Transformer models can learn to implement systematic variable binding without explicit architectural support, bridging connectionist and symbolic approaches.",
        "arxiv_id": "2505.20896"
    },
    "2505.21077": {
        "SCORE": 17,
        "ARXIVID": "2505.21077",
        "COMMENT": "The paper introduces Neural Block Linearization, a novel framework for accelerating LLM inference, aligning with model compression and efficiency breakthroughs.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Mete Erdogan",
            "Francesco Tonin",
            "Volkan Cevher"
        ],
        "title": "Efficient Large Language Model Inference with Neural Block Linearization",
        "abstract": "The high inference demands of transformer-based Large Language Models (LLMs) pose substantial challenges in their deployment. To this end, we introduce Neural Block Linearization (NBL), a novel framework for accelerating transformer model inference by replacing self-attention layers with linear approximations derived from Linear Minimum Mean Squared Error estimators. NBL leverages Canonical Correlation Analysis to compute a theoretical upper bound on the approximation error. Then, we use this bound as a criterion for substitution, selecting the LLM layers with the lowest linearization error. NBL can be efficiently applied to pre-trained LLMs without the need for fine-tuning. In experiments, NBL achieves notable computational speed-ups while preserving competitive accuracy on multiple reasoning benchmarks. For instance, applying NBL to 12 self-attention layers in DeepSeek-R1-Distill-Llama-8B increases the inference speed by 32% with less than 1% accuracy trade-off, making it a flexible and promising solution to improve the inference efficiency of LLMs.",
        "arxiv_id": "2505.21077"
    },
    "2505.20925": {
        "SCORE": 17,
        "ARXIVID": "2505.20925",
        "COMMENT": "The paper introduces Hierarchical Mixture-of-Experts for LLM alignment, which involves architectural innovation and aligns with the model architecture criterion.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Zhuo Li",
            "Guodong Du",
            "Weiyang Guo",
            "Yigeng Zhou",
            "Xiucheng Li",
            "Wenya Wang",
            "Fangming Liu",
            "Yequan Wang",
            "Deheng Ye",
            "Min Zhang",
            "Jing Li"
        ],
        "title": "Multi-objective Large Language Model Alignment with Hierarchical Experts",
        "abstract": "Aligning large language models (LLMs) to simultaneously satisfy multiple objectives remains a significant challenge, especially given the diverse and often conflicting nature of human preferences. Existing alignment methods struggle to balance trade-offs effectively, often requiring costly retraining or yielding suboptimal results across the Pareto frontier of preferences. In this paper, we introduce \\textit{HoE}(Hierarchical Mixture-of-Experts), a \\textit{lightweight}, \\textit{parameter-efficient}, and \\textit{plug-and-play} approach that eliminates the need for model training, while enabling LLMs to adapt across the entire Pareto frontier and accommodate diverse user preferences. In particular, \\textit{HoE} consists of three hierarchical components: LoRA Experts, Router Experts and Preference Routing, reaching optimal Pareto frontiers and achieving a trade-off between parameter size, training cost, and performance. We evaluate \\textit{HoE} across various tasks on 14 objectives and 200 different preferences among 6 benchmarks, demonstrating superior performance over 15 recent baselines. Code is available in the supplementary materials.",
        "arxiv_id": "2505.20925"
    },
    "2505.21399": {
        "SCORE": 17,
        "ARXIVID": "2505.21399",
        "COMMENT": "The paper investigates factual self-awareness in LLMs, providing theoretical insights into LLM behavior, aligning with the large language models criterion.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Hovhannes Tamoyan",
            "Subhabrata Dutta",
            "Iryna Gurevych"
        ],
        "title": "Factual Self-Awareness in Language Models: Representation, Robustness, and Scaling",
        "abstract": "Factual incorrectness in generated content is one of the primary concerns in ubiquitous deployment of large language models (LLMs). Prior findings suggest LLMs can (sometimes) detect factual incorrectness in their generated content (i.e., fact-checking post-generation). In this work, we provide evidence supporting the presence of LLMs' internal compass that dictate the correctness of factual recall at the time of generation. We demonstrate that for a given subject entity and a relation, LLMs internally encode linear features in the Transformer's residual stream that dictate whether it will be able to recall the correct attribute (that forms a valid entity-relation-attribute triplet). This self-awareness signal is robust to minor formatting variations. We investigate the effects of context perturbation via different example selection strategies. Scaling experiments across model sizes and training dynamics highlight that self-awareness emerges rapidly during training and peaks in intermediate layers. These findings uncover intrinsic self-monitoring capabilities within LLMs, contributing to their interpretability and reliability.",
        "arxiv_id": "2505.21399"
    },
    "2505.20802": {
        "SCORE": 17,
        "ARXIVID": "2505.20802",
        "COMMENT": "The paper challenges the belief that bigger transformers are better by proposing a redesign with more heads and less depth, aligning with model architecture innovation.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Hemanth Saratchandran",
            "Damien Teney",
            "Simon Lucey"
        ],
        "title": "Leaner Transformers: More Heads, Less Depth",
        "abstract": "Transformers have reshaped machine learning by utilizing attention mechanisms to capture complex patterns in large datasets, leading to significant improvements in performance. This success has contributed to the belief that \"bigger means better\", leading to ever-increasing model sizes. This paper challenge this ideology by showing that many existing transformers might be unnecessarily oversized. We discover a theoretical principle that redefines the role of multi-head attention. An important benefit of the multiple heads is in improving the conditioning of the attention block. We exploit this theoretical insight and redesign popular architectures with an increased number of heads. The improvement in the conditioning proves so significant in practice that model depth can be decreased, reducing the parameter count by up to 30-50% while maintaining accuracy. We obtain consistent benefits across a variety of transformer-based architectures of various scales, on tasks in computer vision (ImageNet-1k) as well as language and sequence modeling (GLUE benchmark, TinyStories, and the Long-Range Arena benchmark).",
        "arxiv_id": "2505.20802"
    },
    "2505.21226": {
        "SCORE": 17,
        "ARXIVID": "2505.21226",
        "COMMENT": "The paper provides a theoretical analysis of model merging, focusing on the scalability and parameter space constraints, which is relevant to model architecture and efficiency.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Zijing Wang",
            "Xingle Xu",
            "Yongkang Liu",
            "Yiqun Zhang",
            "Peiqin Lin",
            "Shi Feng",
            "Xiaocui Yang",
            "Daling Wang",
            "Hinrich Sch\\\"utze"
        ],
        "title": "Why Do More Experts Fail? A Theoretical Analysis of Model Merging",
        "abstract": "Model merging dramatically reduces storage and computational resources by combining multiple expert models into a single multi-task model. Although recent model merging methods have shown promising results, they struggle to maintain performance gains as the number of merged models increases. In this paper, we investigate the key obstacles that limit the scalability of model merging when integrating a large number of expert models. First, we prove that there is an upper bound on model merging. Further theoretical analysis reveals that the limited effective parameter space imposes a strict constraint on the number of models that can be successfully merged. Gaussian Width shows that the marginal benefit of merging additional models diminishes according to a strictly concave function. This implies that the effective parameter space becomes rapidly saturated as the number of merged models increases. Furthermore, using Approximate Kinematics Theory, we prove the existence of a unique optimal threshold beyond which adding more models does not yield significant performance improvements. At the same time, we introduce a straightforward Reparameterized Heavy-Tailed method (RHT) to extend the coverage of the merged model, thereby enhancing its performance. Empirical results on 12 benchmarks, including both knowledge-intensive and general-purpose tasks, validate our theoretical analysis. We believe that these results spark further research beyond the current scope of model merging. The source code is in the anonymous Github repository https://github.com/wzj1718/ModelMergingAnalysis.",
        "arxiv_id": "2505.21226"
    },
    "2505.20698": {
        "SCORE": 17,
        "ARXIVID": "2505.20698",
        "COMMENT": "The paper introduces a novel sparsification method for state-space models, which aligns with the model compression criterion by focusing on token pruning and efficiency improvements.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Woomin Song",
            "Jihoon Tack",
            "Sangwoo Mo",
            "Seunghyuk Oh",
            "Jinwoo Shin"
        ],
        "title": "Sparsified State-Space Models are Efficient Highway Networks",
        "abstract": "State-space models (SSMs) offer a promising architecture for sequence modeling, providing an alternative to Transformers by replacing expensive self-attention with linear recurrences. In this paper, we propose a simple yet effective trick to enhance SSMs within given computational budgets by sparsifying them. Our intuition is that tokens in SSMs are highly redundant due to gradual recurrent updates, and dense recurrence operations block the delivery of past information. In particular, we observe that upper layers of SSMs tend to be more redundant as they encode global information, while lower layers encode local information. Motivated by this, we introduce Simba, a hierarchical sparsification method for SSMs based on token pruning. Simba sparsifies upper layers more than lower layers, encouraging the upper layers to behave like highways. To achieve this, we propose a novel token pruning criterion for SSMs, measuring the global impact of tokens on the final output by accumulating local recurrences. We demonstrate that Simba outperforms the baseline model, Mamba, with the same FLOPS in various natural language tasks. Moreover, we illustrate the effect of highways, showing that Simba not only enhances efficiency but also improves the information flow across long sequences. Code is available at https://github.com/woominsong/Simba.",
        "arxiv_id": "2505.20698"
    },
    "2505.20674": {
        "SCORE": 17,
        "ARXIVID": "2505.20674",
        "COMMENT": "The paper introduces a novel pondering mechanism for language models, which could be considered a new paradigm in LLM pretraining and architecture.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Boyi Zeng",
            "Shixiang Song",
            "Siyuan Huang",
            "Yixuan Wang",
            "He Li",
            "Ziwei He",
            "Xinbing Wang",
            "Zhiyu Li",
            "Zhouhan Lin"
        ],
        "title": "Pretraining Language Models to Ponder in Continuous Space",
        "abstract": "Humans ponder before articulating complex sentence elements, enabling deeper cognitive processing through focused effort. In this work, we introduce this pondering process into language models by repeatedly invoking the forward process within a single token generation step. During pondering, instead of generating an actual token sampled from the prediction distribution, the model ponders by yielding a weighted sum of all token embeddings according to the predicted token distribution. The generated embedding is then fed back as input for another forward pass. We show that the model can learn to ponder in this way through self-supervised learning, without any human annotations. Our method is straightforward and can be seamlessly integrated with various existing language models. Experiments across three widely used open-source architectures-GPT-2, Pythia, and LLaMA-and extensive downstream task evaluations demonstrate the effectiveness and generality of our method. For language modeling tasks, pondering language models achieve performance comparable to vanilla models with twice the number of parameters. On 9 downstream benchmarks, our pondering-enhanced Pythia models significantly outperform the official Pythia models. Notably, pondering-enhanced Pythia-1B is comparable to TinyLlama-1.1B, which is trained on 10 times more data. The code is available at https://github.com/LUMIA-Group/PonderingLM.",
        "arxiv_id": "2505.20674"
    },
    "2505.20633": {
        "SCORE": 17,
        "ARXIVID": "2505.20633",
        "COMMENT": "The paper proposes a test-time learning paradigm for LLMs, which aligns with the large language models criterion by focusing on theoretical insights into LLM behavior.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Jinwu Hu",
            "Zhitian Zhang",
            "Guohao Chen",
            "Xutao Wen",
            "Chao Shuai",
            "Wei Luo",
            "Bin Xiao",
            "Yuanqing Li",
            "Mingkui Tan"
        ],
        "title": "Test-Time Learning for Large Language Models",
        "abstract": "While Large Language Models (LLMs) have exhibited remarkable emergent capabilities through extensive pre-training, they still face critical limitations in generalizing to specialized domains and handling diverse linguistic variations, known as distribution shifts. In this paper, we propose a Test-Time Learning (TTL) paradigm for LLMs, namely TLM, which dynamically adapts LLMs to target domains using only unlabeled test data during testing. Specifically, we first provide empirical evidence and theoretical insights to reveal that more accurate predictions from LLMs can be achieved by minimizing the input perplexity of the unlabeled test data. Based on this insight, we formulate the Test-Time Learning process of LLMs as input perplexity minimization, enabling self-supervised enhancement of LLM performance. Furthermore, we observe that high-perplexity samples tend to be more informative for model optimization. Accordingly, we introduce a Sample Efficient Learning Strategy that actively selects and emphasizes these high-perplexity samples for test-time updates. Lastly, to mitigate catastrophic forgetting and ensure adaptation stability, we adopt Low-Rank Adaptation (LoRA) instead of full-parameter optimization, which allows lightweight model updates while preserving more original knowledge from the model. We introduce the AdaptEval benchmark for TTL and demonstrate through experiments that TLM improves performance by at least 20% compared to original LLMs on domain knowledge adaptation.",
        "arxiv_id": "2505.20633"
    },
    "2505.21289": {
        "SCORE": 17,
        "ARXIVID": "2505.21289",
        "COMMENT": "The paper introduces a novel low-rank adaptation method that aligns with the model compression criterion by focusing on parameter-efficient fine-tuning.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Nurbek Tastan",
            "Stefanos Laskaridis",
            "Martin Takac",
            "Karthik Nandakumar",
            "Samuel Horvath"
        ],
        "title": "LoFT: Low-Rank Adaptation That Behaves Like Full Fine-Tuning",
        "abstract": "Large pre-trained models are commonly adapted to downstream tasks using parameter-efficient fine-tuning methods such as Low-Rank Adaptation (LoRA), which injects small trainable low-rank matrices instead of updating all weights. While LoRA dramatically reduces trainable parameters with little overhead, it can still underperform full fine-tuning in accuracy and often converges more slowly. We introduce LoFT, a novel low-rank adaptation method that behaves like full fine-tuning by aligning the optimizer's internal dynamics with those of updating all model weights. LoFT not only learns weight updates in a low-rank subspace (like LoRA) but also properly projects the optimizer's first and second moments (Adam's momentum and variance) into the same subspace, mirroring full-model updates. By aligning the low-rank update itself with the full update, LoFT eliminates the need for tuning extra hyperparameters, e.g., LoRA scaling factor $\\alpha$. Empirically, this approach substantially narrows the performance gap between adapter-based tuning and full fine-tuning and consistently outperforms standard LoRA-style methods, all without increasing inference cost.",
        "arxiv_id": "2505.21289"
    },
    "2505.21024": {
        "SCORE": 17,
        "ARXIVID": "2505.21024",
        "COMMENT": "The paper provides theoretical insights into the expressivity of Transformers with pause tokens, which aligns with the model architecture criterion.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Charles London",
            "Varun Kanade"
        ],
        "title": "Pause Tokens Strictly Increase the Expressivity of Constant-Depth Transformers",
        "abstract": "Pause tokens, simple filler symbols such as \"...\", consistently improve Transformer performance on both language and mathematical tasks, yet their theoretical effect remains unexplained. We provide the first formal separation result, proving that adding pause tokens to constant-depth, logarithmic-width Transformers strictly increases their computational expressivity. With bounded-precision activations, Transformers without pause tokens compute only a strict subset of $\\mathsf{AC}^0$ functions, while adding a polynomial number of pause tokens allows them to express the entire class. For logarithmic-precision Transformers, we show that adding pause tokens achieves expressivity equivalent to $\\mathsf{TC}^0$, matching known upper bounds. Empirically, we demonstrate that two-layer causally masked Transformers can learn parity when supplied with pause tokens, a function that they appear unable to learn without them. Our results provide a rigorous theoretical explanation for prior empirical findings, clarify how pause tokens interact with width, depth, and numeric precision, and position them as a distinct mechanism, complementary to chain-of-thought prompting, for enhancing Transformer reasoning.",
        "arxiv_id": "2505.21024"
    },
    "2505.20993": {
        "SCORE": 17,
        "ARXIVID": "2505.20993",
        "COMMENT": "The paper investigates the reasoning capabilities of LLMs, providing insights into model behavior and interpretability, which is relevant to foundational research in LLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Jie Shao",
            "Jianxin Wu"
        ],
        "title": "Who Reasons in the Large Language Models?",
        "abstract": "Despite the impressive performance of large language models (LLMs), the process of endowing them with new capabilities--such as mathematical reasoning--remains largely empirical and opaque. A critical open question is whether reasoning abilities stem from the entire model, specific modules, or are merely artifacts of overfitting. In this work, we hypothesize that the reasoning capabilities in well-trained LLMs are primarily attributed to the output projection module (oproj) in the Transformer's multi-head self-attention (MHSA) mechanism. To support this hypothesis, we introduce Stethoscope for Networks (SfN), a suite of diagnostic tools designed to probe and analyze the internal behaviors of LLMs. Using SfN, we provide both circumstantial and empirical evidence suggesting that oproj plays a central role in enabling reasoning, whereas other modules contribute more to fluent dialogue. These findings offer a new perspective on LLM interpretability and open avenues for more targeted training strategies, potentially enabling more efficient and specialized LLMs.",
        "arxiv_id": "2505.20993"
    },
    "2505.20524": {
        "SCORE": 17,
        "ARXIVID": "2505.20524",
        "COMMENT": "The paper introduces a new class of LLM architectures supporting FP8 computation, relevant to model architecture and efficiency in LLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Alejandro Hern\\'andez-Cano",
            "Dhia Garbaya",
            "Imanol Schlag",
            "Martin Jaggi"
        ],
        "title": "Towards Fully FP8 GEMM LLM Training at Scale",
        "abstract": "Despite the significant potential of FP8 data formats for large language model (LLM) pre-training, their adoption has been limited due to challenges in maintaining stability at scale. Existing approaches often rely on suboptimal fine-grained FP8 kernels or fall back to higher-precision matrix multiplications (GEMMs) in sensitive components, such as attention projections, compromising potential throughput gains. We introduce a new class of LLM architectures that, for the first time, support FP8 computation for all GEMMs within transformer blocks during both forward and backward passes. This enables unprecedented throughput gains, particularly at scale, while matching the downstream performance of standard BF16 training. Our architecture design reduces large outlier activations, promoting stable long-term FP8 training. In addition, we identify key metrics to monitor low-precision training and predict potential future divergences.",
        "arxiv_id": "2505.20524"
    },
    "2505.21444": {
        "SCORE": 17,
        "ARXIVID": "2505.21444",
        "COMMENT": "The paper explores self-training in large reasoning models, which touches on foundational aspects of LLM behavior and interpretability.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Sheikh Shafayat",
            "Fahim Tajwar",
            "Ruslan Salakhutdinov",
            "Jeff Schneider",
            "Andrea Zanette"
        ],
        "title": "Can Large Reasoning Models Self-Train?",
        "abstract": "Scaling the performance of large language models (LLMs) increasingly depends on methods that reduce reliance on human supervision. Reinforcement learning from automated verification offers an alternative, but it incurs scalability limitations due to dependency upon human-designed verifiers. Self-training, where the model's own judgment provides the supervisory signal, presents a compelling direction. We propose an online self-training reinforcement learning algorithm that leverages the model's self-consistency to infer correctness signals and train without any ground-truth supervision. We apply the algorithm to challenging mathematical reasoning tasks and show that it quickly reaches performance levels rivaling reinforcement-learning methods trained explicitly on gold-standard answers. Additionally, we analyze inherent limitations of the algorithm, highlighting how the self-generated proxy reward initially correlated with correctness can incentivize reward hacking, where confidently incorrect outputs are favored. Our results illustrate how self-supervised improvement can achieve significant performance gains without external labels, while also revealing its fundamental challenges.",
        "arxiv_id": "2505.21444"
    },
    "2505.20589": {
        "SCORE": 17,
        "ARXIVID": "2505.20589",
        "COMMENT": "Prot2Token presents a unified framework for protein modeling using next-token prediction, aligning with foundational research in AI for science and representation learning.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Mahdi Pourmirzaei",
            "Farzaneh Esmaili",
            "Salhuldin Alqarghuli",
            "Mohammadreza Pourmirzaei",
            "Ye Han",
            "Kai Chen",
            "Mohsen Rezaei",
            "Duolin Wang",
            "Dong Xu"
        ],
        "title": "Prot2Token: A Unified Framework for Protein Modeling via Next-Token Prediction",
        "abstract": "The diverse nature of protein prediction tasks has traditionally necessitated specialized models, hindering the development of broadly applicable and computationally efficient Protein Language Models (PLMs). In this work, we introduce Prot2Token, a unified framework that overcomes these challenges by converting a wide spectrum of protein-related predictions, from sequence-level properties and residue-specific attributes to complex inter-protein interactions, into a standardized next-token prediction format. At its core, Prot2Token employs an autoregressive decoder, conditioned on embeddings from pre-trained protein encoders and guided by learnable task tokens, to perform diverse predictions. This architecture uniquely facilitates multi-task learning, enabling a single model to master numerous tasks with improved efficiency. We present extensive experimental validation across a variety of benchmarks, demonstrating Prot2Tokens strong predictive power in different types of protein-prediction tasks. Key results include significant speedups (e.g., near 1000x over AlphaFold2 with MSA) and performance often matching or exceeding specialized approaches. Beyond that, we introduce an auxiliary self-supervised decoder pre-training approach to improve spatially sensitive task performance. Prot2Token thus offers a significant step towards a versatile, high-throughput paradigm for protein modeling, promising to accelerate biological discovery and the development of novel therapeutics. The code is available at https://github.com/mahdip72/prot2token .",
        "arxiv_id": "2505.20589"
    },
    "2505.21218": {
        "SCORE": 16,
        "ARXIVID": "2505.21218",
        "COMMENT": "The paper studies how LLMs capture uncertainty, providing insights into model behavior and interpretability, relevant to foundational research in LLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Roi Cohen",
            "Omri Fahn",
            "Gerard de Melo"
        ],
        "title": "Pretrained LLMs Learn Multiple Types of Uncertainty",
        "abstract": "Large Language Models are known to capture real-world knowledge, allowing them to excel in many downstream tasks. Despite recent advances, these models are still prone to what are commonly known as hallucinations, causing them to emit unwanted and factually incorrect text. In this work, we study how well LLMs capture uncertainty, without explicitly being trained for that. We show that, if considering uncertainty as a linear concept in the model's latent space, it might indeed be captured, even after only pretraining. We further show that, though unintuitive, LLMs appear to capture several different types of uncertainty, each of which can be useful to predict the correctness for a specific task or benchmark. Furthermore, we provide in-depth results such as demonstrating a correlation between our correction prediction and the model's ability to abstain from misinformation using words, and the lack of impact of model scaling for capturing uncertainty. Finally, we claim that unifying the uncertainty types as a single one using instruction-tuning or [IDK]-token tuning is helpful for the model in terms of correctness prediction.",
        "arxiv_id": "2505.21218"
    },
    "2505.20334": {
        "SCORE": 16,
        "ARXIVID": "2505.20334",
        "COMMENT": "The paper proposes a novel KV cache eviction framework for LLMs, which is relevant to model compression and efficiency improvements.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Yixuan Wang",
            "Shiyu Ji",
            "Yijun Liu",
            "Yuzhuang Xu",
            "Yang Xu",
            "Qingfu Zhu",
            "Wanxiang Che"
        ],
        "title": "Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query",
        "abstract": "Large language models (LLMs) rely on key-value cache (KV cache) to accelerate decoding by reducing redundant computations. However, the KV cache memory usage grows substantially with longer text sequences, posing challenges for efficient deployment. Existing KV cache eviction methods prune tokens using prefilling-stage attention scores, causing inconsistency with actual inference queries, especially under tight memory budgets. In this paper, we propose Lookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost pseudo lookahead queries to better approximate the true decoding-stage queries. By using these lookahead queries as the observation window for importance estimation, LAQ achieves more consistent and accurate KV cache eviction aligned with real inference scenarios. Experimental results on LongBench and Needle-in-a-Haystack benchmarks show that LAQ outperforms existing methods across various budget levels, achieving a 1 $\\sim$ 4 point improvement on LongBench under limited cache budget. Moreover, LAQ is complementary to existing approaches and can be flexibly combined to yield further improvements.",
        "arxiv_id": "2505.20334"
    },
    "2505.20435": {
        "SCORE": 16,
        "ARXIVID": "2505.20435",
        "COMMENT": "The paper uses topological data analysis to study LLMs under adversarial conditions, offering insights into representational dynamics, relevant to representation learning.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Aideen Fay",
            "In\\'es Garc\\'ia-Redondo",
            "Qiquan Wang",
            "Haim Dubossarsky",
            "Anthea Monod"
        ],
        "title": "Holes in Latent Space: Topological Signatures Under Adversarial Influence",
        "abstract": "Understanding how adversarial conditions affect language models requires techniques that capture both global structure and local detail within high-dimensional activation spaces. We propose persistent homology (PH), a tool from topological data analysis, to systematically characterize multiscale latent space dynamics in LLMs under two distinct attack modes -- backdoor fine-tuning and indirect prompt injection. By analyzing six state-of-the-art LLMs, we show that adversarial conditions consistently compress latent topologies, reducing structural diversity at smaller scales while amplifying dominant features at coarser ones. These topological signatures are statistically robust across layers, architectures, model sizes, and align with the emergence of adversarial effects deeper in the network. To capture finer-grained mechanisms underlying these shifts, we introduce a neuron-level PH framework that quantifies how information flows and transforms within and across layers. Together, our findings demonstrate that PH offers a principled and unifying approach to interpreting representational dynamics in LLMs, particularly under distributional shift.",
        "arxiv_id": "2505.20435"
    },
    "2505.20646": {
        "SCORE": 16,
        "ARXIVID": "2505.20646",
        "COMMENT": "The paper applies algorithmic information theory to Binarized Neural Networks, offering a new perspective on training dynamics, relevant to representation learning.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Eduardo Y. Sakabe",
            "Felipe S. Abrah\\~ao",
            "Alexandre Sim\\~oes",
            "Esther Colombini",
            "Paula Costa",
            "Ricardo Gudwin",
            "Hector Zenil"
        ],
        "title": "Evaluating Training in Binarized Neural Networks Through the Lens of Algorithmic Information Theory",
        "abstract": "Understanding and controlling the informational complexity of neural networks is a central challenge in machine learning, with implications for generalization, optimization, and model capacity. While most approaches rely on entropy-based loss functions and statistical metrics, these measures often fail to capture deeper, causally relevant algorithmic regularities embedded in network structure. We propose a shift toward algorithmic information theory, using Binarized Neural Networks (BNNs) as a first proxy. Grounded in algorithmic probability (AP) and the universal distribution it defines, our approach characterizes learning dynamics through a formal, causally grounded lens. We apply the Block Decomposition Method (BDM) -- a scalable approximation of algorithmic complexity based on AP -- and demonstrate that it more closely tracks structural changes during training than entropy, consistently exhibiting stronger correlations with training loss across varying model sizes and randomized training runs. These results support the view of training as a process of algorithmic compression, where learning corresponds to the progressive internalization of structured regularities. In doing so, our work offers a principled estimate of learning progression and suggests a framework for complexity-aware learning and regularization, grounded in first principles from information theory, complexity, and computability.",
        "arxiv_id": "2505.20646"
    },
    "2505.20473": {
        "SCORE": 15,
        "ARXIVID": "2505.20473",
        "COMMENT": "The paper introduces a novel stochastic preconditioning method for neural field optimization, which is relevant to representation learning and training dynamics.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Selena Ling",
            "Merlin Nimier-David",
            "Alec Jacobson",
            "Nicholas Sharp"
        ],
        "title": "Stochastic Preconditioning for Neural Field Optimization",
        "abstract": "Neural fields are a highly effective representation across visual computing. This work observes that fitting these fields is greatly improved by incorporating spatial stochasticity during training, and that this simple technique can replace or even outperform custom-designed hierarchies and frequency space constructions. The approach is formalized as implicitly operating on a blurred version of the field, evaluated in-expectation by sampling with Gaussian-distributed offsets. Querying the blurred field during optimization greatly improves convergence and robustness, akin to the role of preconditioners in numerical linear algebra. This implicit, sampling-based perspective fits naturally into the neural field paradigm, comes at no additional cost, and is extremely simple to implement. We describe the basic theory of this technique, including details such as handling boundary conditions, and extending to a spatially-varying blur. Experiments demonstrate this approach on representations including coordinate MLPs, neural hashgrids, triplanes, and more, across tasks including surface reconstruction and radiance fields. In settings where custom-designed hierarchies have already been developed, stochastic preconditioning nearly matches or improves their performance with a simple and unified approach; in settings without existing hierarchies it provides an immediate boost to quality and robustness.",
        "arxiv_id": "2505.20473"
    },
    "2505.20318": {
        "SCORE": 15,
        "ARXIVID": "2505.20318",
        "COMMENT": "The paper proposes a method for dynamic vector construction from latent representations, relevant to representation learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Wang Cai",
            "Hsiu-Yuan Huang",
            "Zhixiang Wang",
            "Yunfang Wu"
        ],
        "title": "Beyond Demonstrations: Dynamic Vector Construction from Latent Representations",
        "abstract": "In-Context derived Vector (ICV) methods extract task-relevant representations from large language models (LLMs) and reinject them during inference, achieving comparable performance to few-shot In-Context Learning (ICL) without repeated demonstration processing. However, existing ICV methods remain sensitive to ICL-specific factors, often use coarse or semantically fragmented representations as the source of the vector, and rely on heuristic-based injection positions, limiting their applicability.   To address these issues, we propose Dynamic Vector (DyVec), which incorporates an Exhaustive Query Rotation (EQR) strategy to extract robust semantically aggregated latent representations by mitigating variance introduced by ICL. It then applies Dynamic Latent Segmentation and Injection to adaptively partition representations based on task complexity and leverages REINFORCE-based optimization to learn optimal injection positions for each segment.   Experiments results show that DyVec outperforms few-shot ICL, LoRA, and prior ICV baselines. Further analysis highlights the effectiveness of dynamically segmenting and injecting semantically aggregated latent representations. DyVec provides a lightweight and data-efficient solution for inference-time task adaptation.",
        "arxiv_id": "2505.20318"
    },
    "2505.20535": {
        "SCORE": 15,
        "ARXIVID": "2505.20535",
        "COMMENT": "The paper presents Rotary Masked Autoencoders, an extension of MAE for representation learning across various modalities.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Uros Zivanovic",
            "Serafina Di Gioia",
            "Andre Scaffidi",
            "Mart\\'in de los Rios",
            "Gabriella Contardo",
            "Roberto Trotta"
        ],
        "title": "Rotary Masked Autoencoders are Versatile Learners",
        "abstract": "Applying Transformers to irregular time-series typically requires specializations to their baseline architecture, which can result in additional computational overhead and increased method complexity. We present the Rotary Masked Autoencoder (RoMAE), which utilizes the popular Rotary Positional Embedding (RoPE) method for continuous positions. RoMAE is an extension to the Masked Autoencoder (MAE) that enables representation learning with multidimensional continuous positional information while avoiding any time-series-specific architectural specializations. We showcase RoMAE's performance on a variety of modalities including irregular and multivariate time-series, images, and audio, demonstrating that RoMAE surpasses specialized time-series architectures on difficult datasets such as the DESC ELAsTiCC Challenge while maintaining MAE's usual performance across other modalities. In addition, we investigate RoMAE's ability to reconstruct the embedded continuous positions, demonstrating that including learned embeddings in the input sequence breaks RoPE's relative position property.",
        "arxiv_id": "2505.20535"
    },
    "2505.20992": {
        "SCORE": 15,
        "ARXIVID": "2505.20992",
        "COMMENT": "The paper introduces a spectral-based random feature aggregation method for graph embedding, relevant to representation learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Meng Qin",
            "Jiahong Liu",
            "Irwin King"
        ],
        "title": "Efficient Identity and Position Graph Embedding via Spectral-Based Random Feature Aggregation",
        "abstract": "Graph neural networks (GNNs), which capture graph structures via a feature aggregation mechanism following the graph embedding framework, have demonstrated a powerful ability to support various tasks. According to the topology properties (e.g., structural roles or community memberships of nodes) to be preserved, graph embedding can be categorized into identity and position embedding. However, it is unclear for most GNN-based methods which property they can capture. Some of them may also suffer from low efficiency and scalability caused by several time- and space-consuming procedures (e.g., feature extraction and training). From a perspective of graph signal processing, we find that high- and low-frequency information in the graph spectral domain may characterize node identities and positions, respectively. Based on this investigation, we propose random feature aggregation (RFA) for efficient identity and position embedding, serving as an extreme ablation study regarding GNN feature aggregation. RFA (i) adopts a spectral-based GNN without learnable parameters as its backbone, (ii) only uses random noises as inputs, and (iii) derives embeddings via just one feed-forward propagation (FFP). Inspired by degree-corrected spectral clustering, we further introduce a degree correction mechanism to the GNN backbone. Surprisingly, our experiments demonstrate that two variants of RFA with high- and low-pass filters can respectively derive informative identity and position embeddings via just one FFP (i.e., without any training). As a result, RFA can achieve a better trade-off between quality and efficiency for both identity and position embedding over various baselines.",
        "arxiv_id": "2505.20992"
    },
    "2505.20888": {
        "SCORE": 15,
        "ARXIVID": "2505.20888",
        "COMMENT": "The paper presents EasyDistill, a toolkit for knowledge distillation of LLMs, which is relevant to model compression and efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Chengyu Wang",
            "Junbing Yan",
            "Wenrui Cai",
            "Yuanhao Yue",
            "Jun Huang"
        ],
        "title": "EasyDistill: A Comprehensive Toolkit for Effective Knowledge Distillation of Large Language Models",
        "abstract": "In this paper, we present EasyDistill, a comprehensive toolkit designed for effective black-box and white-box knowledge distillation (KD) of large language models (LLMs). Our framework offers versatile functionalities, including data synthesis, supervised fine-tuning, ranking optimization, and reinforcement learning techniques specifically tailored for KD scenarios. The toolkit accommodates KD functionalities for both System 1 (fast, intuitive) and System 2 (slow, analytical) models. With its modular design and user-friendly interface, EasyDistill empowers researchers and industry practitioners to seamlessly experiment with and implement state-of-the-art KD strategies for LLMs. In addition, EasyDistill provides a series of robust distilled models and KD-based industrial solutions developed by us, along with the corresponding open-sourced datasets, catering to a variety of use cases. Furthermore, we describe the seamless integration of EasyDistill into Alibaba Cloud's Platform for AI (PAI). Overall, the EasyDistill toolkit makes advanced KD techniques for LLMs more accessible and impactful within the NLP community.",
        "arxiv_id": "2505.20888"
    },
    "2505.20322": {
        "SCORE": 15,
        "ARXIVID": "2505.20322",
        "COMMENT": "The paper discusses a novel method for controlling language model generation using sparse autoencoders, which is relevant to representation learning and model architecture.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Mengru Wang",
            "Ziwen Xu",
            "Shengyu Mao",
            "Shumin Deng",
            "Zhaopeng Tu",
            "Huajun Chen",
            "Ningyu Zhang"
        ],
        "title": "Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering Target Atoms",
        "abstract": "Precise control over language model generation is vital for ensuring both safety and reliability. Although prompt engineering and steering are commonly used to intervene in model behaviors, the vast number of parameters in models often results in highly intertwined internal representations. This interdependency can limit control precision and sometimes lead to unintended side effects. Recent research has explored the use of sparse autoencoders (SAE) to disentangle knowledge in high-dimensional spaces for steering. However, these applications have been limited to toy tasks owing to the nontrivial issue of locating atomic knowledge components. In this paper, we propose Steering Target Atoms (STA), a novel method that isolates and manipulates disentangled knowledge components to enhance safety. Comprehensive experiments demonstrate the effectiveness of our approach. Further analysis reveals that steering exhibits superior robustness and flexibility, particularly in adversarial scenarios. We also apply the steering strategy to the large reasoning model, confirming its effectiveness in precise reasoning control.",
        "arxiv_id": "2505.20322"
    },
    "2505.20807": {
        "SCORE": 15,
        "ARXIVID": "2505.20807",
        "COMMENT": "The paper presents a new graph distillation approach, which is relevant to model compression and representation learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Yurui Lai",
            "Taiyan Zhang",
            "Renchi Yang"
        ],
        "title": "Simple yet Effective Graph Distillation via Clustering",
        "abstract": "Despite plentiful successes achieved by graph representation learning in various domains, the training of graph neural networks (GNNs) still remains tenaciously challenging due to the tremendous computational overhead needed for sizable graphs in practice. Recently, graph data distillation (GDD), which seeks to distill large graphs into compact and informative ones, has emerged as a promising technique to enable efficient GNN training. However, most existing GDD works rely on heuristics that align model gradients or representation distributions on condensed and original graphs, leading to compromised result quality, expensive training for distilling large graphs, or both. Motivated by this, this paper presents an efficient and effective GDD approach, ClustGDD. Under the hood, ClustGDD resorts to synthesizing the condensed graph and node attributes through fast and theoretically-grounded clustering that minimizes the within-cluster sum of squares and maximizes the homophily on the original graph. The fundamental idea is inspired by our empirical and theoretical findings unveiling the connection between clustering and empirical condensation quality using Fr\\'echet Inception Distance, a well-known quality metric for synthetic images. Furthermore, to mitigate the adverse effects caused by the homophily-based clustering, ClustGDD refines the nodal attributes of the condensed graph with a small augmentation learned via class-aware graph sampling and consistency loss. Our extensive experiments exhibit that GNNs trained over condensed graphs output by ClustGDD consistently achieve superior or comparable performance to state-of-the-art GDD methods in terms of node classification on five benchmark datasets, while being orders of magnitude faster.",
        "arxiv_id": "2505.20807"
    },
    "2505.20817": {
        "SCORE": 15,
        "ARXIVID": "2505.20817",
        "COMMENT": "The paper provides convergence bounds for Clip-SGD under heavy-tailed noise, which is relevant to model compression and efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Savelii Chezhegov",
            "Aleksandr Beznosikov",
            "Samuel Horv\\'ath",
            "Eduard Gorbunov"
        ],
        "title": "Convergence of Clipped-SGD for Convex $(L_0,L_1)$-Smooth Optimization with Heavy-Tailed Noise",
        "abstract": "Gradient clipping is a widely used technique in Machine Learning and Deep Learning (DL), known for its effectiveness in mitigating the impact of heavy-tailed noise, which frequently arises in the training of large language models. Additionally, first-order methods with clipping, such as Clip-SGD, exhibit stronger convergence guarantees than SGD under the $(L_0,L_1)$-smoothness assumption, a property observed in many DL tasks. However, the high-probability convergence of Clip-SGD under both assumptions -- heavy-tailed noise and $(L_0,L_1)$-smoothness -- has not been fully addressed in the literature. In this paper, we bridge this critical gap by establishing the first high-probability convergence bounds for Clip-SGD applied to convex $(L_0,L_1)$-smooth optimization with heavy-tailed noise. Our analysis extends prior results by recovering known bounds for the deterministic case and the stochastic setting with $L_1 = 0$ as special cases. Notably, our rates avoid exponentially large factors and do not rely on restrictive sub-Gaussian noise assumptions, significantly broadening the applicability of gradient clipping.",
        "arxiv_id": "2505.20817"
    },
    "2505.20836": {
        "SCORE": 15,
        "ARXIVID": "2505.20836",
        "COMMENT": "The paper proposes a novel Hybrid Architecture Distillation approach, which involves architectural innovation and model compression, aligning with model architecture and compression criteria.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Hexiong Yang",
            "Mingrui Chen",
            "Huaibo Huang",
            "Junxian Duan",
            "Jie Cao",
            "Zhen Zhou",
            "Ran He"
        ],
        "title": "HAD: Hybrid Architecture Distillation Outperforms Teacher in Genomic Sequence Modeling",
        "abstract": "Inspired by the great success of Masked Language Modeling (MLM) in the natural language domain, the paradigm of self-supervised pre-training and fine-tuning has also achieved remarkable progress in the field of DNA sequence modeling. However, previous methods often relied on massive pre-training data or large-scale base models with huge parameters, imposing a significant computational burden. To address this, many works attempted to use more compact models to achieve similar outcomes but still fell short by a considerable margin. In this work, we propose a Hybrid Architecture Distillation (HAD) approach, leveraging both distillation and reconstruction tasks for more efficient and effective pre-training. Specifically, we employ the NTv2-500M as the teacher model and devise a grouping masking strategy to align the feature embeddings of visible tokens while concurrently reconstructing the invisible tokens during MLM pre-training. To validate the effectiveness of our proposed method, we conducted comprehensive experiments on the Nucleotide Transformer Benchmark and Genomic Benchmark. Compared to models with similar parameters, our model achieved excellent performance. More surprisingly, it even surpassed the distillation ceiling-teacher model on some sub-tasks, which is more than 500 $\\times$ larger. Lastly, we utilize t-SNE for more intuitive visualization, which shows that our model can gain a sophisticated understanding of the intrinsic representation pattern in genomic sequences.",
        "arxiv_id": "2505.20836"
    },
    "2505.21208": {
        "SCORE": 15,
        "ARXIVID": "2505.21208",
        "COMMENT": "The paper presents a new input convex neural network architecture, which involves architectural innovation, aligning with model architecture criteria.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Thomas Deschatre",
            "Xavier Warin"
        ],
        "title": "Input Convex Kolmogorov Arnold Networks",
        "abstract": "This article presents an input convex neural network architecture using Kolmogorov-Arnold networks (ICKAN). Two specific networks are presented: the first is based on a low-order, linear-by-part, representation of functions, and a universal approximation theorem is provided. The second is based on cubic splines, for which only numerical results support convergence. We demonstrate on simple tests that these networks perform competitively with classical input convex neural networks (ICNNs). In a second part, we use the networks to solve some optimal transport problems needing a convex approximation of functions and demonstrate their effectiveness. Comparisons with ICNNs show that cubic ICKANs produce results similar to those of classical ICNNs.",
        "arxiv_id": "2505.21208"
    },
    "2505.21382": {
        "SCORE": 15,
        "ARXIVID": "2505.21382",
        "COMMENT": "The paper presents a novel algorithm for low-rank adaptation in decentralized settings, which aligns with model compression and efficiency improvements.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Nastaran Saadati",
            "Zhanhong Jiang",
            "Joshua R. Waite",
            "Shreyan Ganguly",
            "Aditya Balu",
            "Chinmay Hegde",
            "Soumik Sarkar"
        ],
        "title": "DeCAF: Decentralized Consensus-And-Factorization for Low-Rank Adaptation of Foundation Models",
        "abstract": "Low-Rank Adaptation (LoRA) has emerged as one of the most effective, computationally tractable fine-tuning approaches for training Vision-Language Models (VLMs) and Large Language Models (LLMs). LoRA accomplishes this by freezing the pre-trained model weights and injecting trainable low-rank matrices, allowing for efficient learning of these foundation models even on edge devices. However, LoRA in decentralized settings still remains under explored, particularly for the theoretical underpinnings due to the lack of smoothness guarantee and model consensus interference (defined formally below). This work improves the convergence rate of decentralized LoRA (DLoRA) to match the rate of decentralized SGD by ensuring gradient smoothness. We also introduce DeCAF, a novel algorithm integrating DLoRA with truncated singular value decomposition (TSVD)-based matrix factorization to resolve consensus interference. Theoretical analysis shows TSVD's approximation error is bounded and consensus differences between DLoRA and DeCAF vanish as rank increases, yielding DeCAF's matching convergence rate. Extensive experiments across vision/language tasks demonstrate our algorithms outperform local training and rivals federated learning under both IID and non-IID data distributions.",
        "arxiv_id": "2505.21382"
    },
    "2505.21487": {
        "SCORE": 15,
        "ARXIVID": "2505.21487",
        "COMMENT": "The paper proposes hardware-efficient attention mechanisms for LLMs, focusing on improving decoding efficiency, which is relevant to model compression and efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Ted Zadouri",
            "Hubert Strauss",
            "Tri Dao"
        ],
        "title": "Hardware-Efficient Attention for Fast Decoding",
        "abstract": "LLM decoding is bottlenecked for large batches and long contexts by loading the key-value (KV) cache from high-bandwidth memory, which inflates per-token latency, while the sequential nature of decoding limits parallelism. We analyze the interplay among arithmetic intensity, parallelization, and model quality and question whether current architectures fully exploit modern hardware. This work redesigns attention to perform more computation per byte loaded from memory to maximize hardware efficiency without trading off parallel scalability. We first propose Grouped-Tied Attention (GTA), a simple variant that combines and reuses key and value states, reducing memory transfers without compromising model quality. We then introduce Grouped Latent Attention (GLA), a parallel-friendly latent attention paired with low-level optimizations for fast decoding while maintaining high model quality. Experiments show that GTA matches Grouped-Query Attention (GQA) quality while using roughly half the KV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier to shard. Our optimized GLA kernel is up to 2$\\times$ faster than FlashMLA, for example, in a speculative decoding setting when the query length exceeds one. Furthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end latency and increases throughput in online serving benchmarks by up to 2$\\times$.",
        "arxiv_id": "2505.21487"
    },
    "2505.20892": {
        "SCORE": 15,
        "ARXIVID": "2505.20892",
        "COMMENT": "The paper proposes a method for resilient learning without weight transport, which aligns with model architecture and training dynamics.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Jeonghwan Cheon",
            "Jaehyuk Bae",
            "Se-Bum Paik"
        ],
        "title": "One-Time Soft Alignment Enables Resilient Learning without Weight Transport",
        "abstract": "Backpropagation is the cornerstone of deep learning, but its reliance on symmetric weight transport and global synchronization makes it computationally expensive and biologically implausible. Feedback alignment offers a promising alternative by approximating error gradients through fixed random feedback, thereby avoiding symmetric weight transport. However, this approach often struggles with poor learning performance and instability, especially in deep networks. Here, we show that a one-time soft alignment between forward and feedback weights at initialization enables deep networks to achieve performance comparable to backpropagation, without requiring weight transport during learning. This simple initialization condition guides stable error minimization in the loss landscape, improving network trainability. Spectral analyses further reveal that initial alignment promotes smoother gradient flow and convergence to flatter minima, resulting in better generalization and robustness. Notably, we also find that allowing moderate deviations from exact weight symmetry can improve adversarial robustness compared to standard backpropagation. These findings demonstrate that a simple initialization strategy can enable effective learning in deep networks in a biologically plausible and resource-efficient manner.",
        "arxiv_id": "2505.20892"
    },
    "2505.21136": {
        "SCORE": 15,
        "ARXIVID": "2505.21136",
        "COMMENT": "The paper focuses on efficiency improvements in attention mechanisms, which aligns with the model compression criterion.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Jintao Zhang",
            "Xiaoming Xu",
            "Jia Wei",
            "Haofeng Huang",
            "Pengle Zhang",
            "Chendong Xiang",
            "Jun Zhu",
            "Jianfei Chen"
        ],
        "title": "SageAttention2++: A More Efficient Implementation of SageAttention2",
        "abstract": "The efficiency of attention is critical because its time complexity grows quadratically with sequence length. SageAttention2 addresses this by utilizing quantization to accelerate matrix multiplications (Matmul) in attention. To further accelerate SageAttention2, we propose to utilize the faster instruction of FP8 Matmul accumulated in FP16. The instruction is 2x faster than the FP8 Matmul used in SageAttention2. Our experiments show that SageAttention2++ achieves a 3.9x speedup over FlashAttention while maintaining the same attention accuracy as SageAttention2. This means SageAttention2++ effectively accelerates various models, including those for language, image, and video generation, with negligible end-to-end metrics loss. The code will be available at https://github.com/thu-ml/SageAttention.",
        "arxiv_id": "2505.21136"
    },
    "2505.21109": {
        "SCORE": 15,
        "ARXIVID": "2505.21109",
        "COMMENT": "The paper introduces a lightweight multi-expert generative language model system, which aligns with the model architecture criterion by proposing a novel graph-based structure.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Bogdan Bogachov",
            "Yaoyao Fiona Zhao"
        ],
        "title": "A Lightweight Multi-Expert Generative Language Model System for Engineering Information and Knowledge Extraction",
        "abstract": "Despite recent advancements in domain adaptation techniques for large language models, these methods remain computationally intensive, and the resulting models can still exhibit hallucination issues. Most existing adaptation methods do not prioritize reducing the computational resources required for fine-tuning and inference of language models. Hallucination issues have gradually decreased with each new model release. However, they remain prevalent in engineering contexts, where generating well-structured text with minimal errors and inconsistencies is critical. This work introduces a novel approach called the Small Language Graph (SLG), which is a lightweight adaptation solution designed to address the two key challenges outlined above. The system is structured in the form of a graph, where each node represents a lightweight expert - a small language model fine-tuned on specific and concise texts. The results of this study have shown that SLG was able to surpass conventional fine-tuning methods on the Exact Match metric by 3 times. Additionally, the fine-tuning process was 1.7 times faster compared to that of a larger stand-alone language model. These findings introduce a potential for small to medium-sized engineering companies to confidently use generative AI technologies, such as LLMs, without the necessity to invest in expensive computational resources. Also, the graph architecture and the small size of expert nodes offer a possible opportunity for distributed AI systems, thus potentially diverting the global need for expensive centralized compute clusters.",
        "arxiv_id": "2505.21109"
    },
    "2505.20628": {
        "SCORE": 15,
        "ARXIVID": "2505.20628",
        "COMMENT": "The paper argues for the use of constrained optimization methods over penalties in deep learning, which aligns with foundational research in model architecture and training dynamics.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Juan Ramirez",
            "Meraj Hashemizadeh",
            "Simon Lacoste-Julien"
        ],
        "title": "Position: Adopt Constraints Over Penalties in Deep Learning",
        "abstract": "Recent efforts toward developing trustworthy AI systems with accountability guarantees have led to a growing reliance on machine learning formulations that incorporate external requirements, or constraints. These requirements are often enforced through penalization--adding fixed-weight terms to the task loss. We argue that this approach is ill-suited, and that tailored constrained optimization methods should be adopted instead. In particular, no penalty coefficient may yield a solution that both satisfies the constraints and achieves good performance--i.e., one solving the constrained problem. Moreover, tuning these coefficients is costly, incurring significant time and computational overhead. In contrast, tailored constrained methods--such as the Lagrangian approach, which optimizes the penalization \"coefficients\" (the Lagrange multipliers) alongside the model--(i) truly solve the constrained problem and add accountability, (ii) eliminate the need for extensive penalty tuning, and (iii) integrate seamlessly with modern deep learning pipelines.",
        "arxiv_id": "2505.20628"
    }
}