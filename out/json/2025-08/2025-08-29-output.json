{
    "2508.20211": {
        "SCORE": 17,
        "ARXIVID": "2508.20211",
        "COMMENT": "The paper provides insights into transformers by connecting them with classical nonlinear filtering theory, which aligns with the model architecture criterion by offering a theoretical perspective on transformer operations.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Heng-Sheng Chang",
            "Prashant G. Mehta"
        ],
        "title": "What can we learn from signals and systems in a transformer? Insights for probabilistic modeling and inference architecture",
        "abstract": "In the 1940s, Wiener introduced a linear predictor, where the future prediction is computed by linearly combining the past data. A transformer generalizes this idea: it is a nonlinear predictor where the next-token prediction is computed by nonlinearly combining the past tokens. In this essay, we present a probabilistic model that interprets transformer signals as surrogates of conditional measures, and layer operations as fixed-point updates. An explicit form of the fixed-point update is described for the special case when the probabilistic model is a hidden Markov model (HMM). In part, this paper is in an attempt to bridge the classical nonlinear filtering theory with modern inference architectures.",
        "arxiv_id": "2508.20211"
    },
    "2508.21038": {
        "SCORE": 17,
        "ARXIVID": "2508.21038",
        "COMMENT": "The paper discusses the theoretical limitations of embedding-based retrieval, which aligns with the representation learning criterion by addressing fundamental challenges in vector embeddings.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Orion Weller",
            "Michael Boratko",
            "Iftekhar Naim",
            "Jinhyuk Lee"
        ],
        "title": "On the Theoretical Limitations of Embedding-Based Retrieval",
        "abstract": "Vector embeddings have been tasked with an ever-increasing set of retrieval tasks over the years, with a nascent rise in using them for reasoning, instruction-following, coding, and more. These new benchmarks push embeddings to work for any query and any notion of relevance that could be given. While prior works have pointed out theoretical limitations of vector embeddings, there is a common assumption that these difficulties are exclusively due to unrealistic queries, and those that are not can be overcome with better training data and larger models. In this work, we demonstrate that we may encounter these theoretical limitations in realistic settings with extremely simple queries. We connect known results in learning theory, showing that the number of top-k subsets of documents capable of being returned as the result of some query is limited by the dimension of the embedding. We empirically show that this holds true even if we restrict to k=2, and directly optimize on the test set with free parameterized embeddings. We then create a realistic dataset called LIMIT that stress tests models based on these theoretical results, and observe that even state-of-the-art models fail on this dataset despite the simple nature of the task. Our work shows the limits of embedding models under the existing single vector paradigm and calls for future research to develop methods that can resolve this fundamental limitation.",
        "arxiv_id": "2508.21038"
    },
    "2508.20766": {
        "SCORE": 17,
        "ARXIVID": "2508.20766",
        "COMMENT": "The paper introduces Rank-One Safety Injection (ROSI), a novel method for enhancing safety alignment in LLMs, which aligns with foundational research in LLM behavior and interpretability.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Harethah Abu Shairah",
            "Hasan Abed Al Kader Hammoud",
            "George Turkiyyah",
            "Bernard Ghanem"
        ],
        "title": "Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection",
        "abstract": "Safety alignment in Large Language Models (LLMs) often involves mediating internal representations to refuse harmful requests. Recent research has demonstrated that these safety mechanisms can be bypassed by ablating or removing specific representational directions within the model. In this paper, we propose the opposite approach: Rank-One Safety Injection (ROSI), a white-box method that amplifies a model's safety alignment by permanently steering its activations toward the refusal-mediating subspace. ROSI operates as a simple, fine-tuning-free rank-one weight modification applied to all residual stream write matrices. The required safety direction can be computed from a small set of harmful and harmless instruction pairs. We show that ROSI consistently increases safety refusal rates - as evaluated by Llama Guard 3 - while preserving the utility of the model on standard benchmarks such as MMLU, HellaSwag, and Arc. Furthermore, we show that ROSI can also re-align 'uncensored' models by amplifying their own latent safety directions, demonstrating its utility as an effective last-mile safety procedure. Our results suggest that targeted, interpretable weight steering is a cheap and potent mechanism to improve LLM safety, complementing more resource-intensive fine-tuning paradigms.",
        "arxiv_id": "2508.20766"
    },
    "2508.20122": {
        "SCORE": 17,
        "ARXIVID": "2508.20122",
        "COMMENT": "The paper explores spatio-temporal pruning for Spiking LLMs, which is relevant to model compression and efficiency in large language models.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Yi Jiang",
            "Malyaban Bal",
            "Brian Matejek",
            "Susmit Jha",
            "Adam Cobb",
            "Abhronil Sengupta"
        ],
        "title": "Spatio-Temporal Pruning for Compressed Spiking Large Language Models",
        "abstract": "Large Language Models (LLMs) present significant challenges for deployment in energy-constrained environments due to their large model sizes and high inference latency. Spiking Neural Networks (SNNs), inspired by the sparse event-driven neural processing and energy-efficient information transmission in the brain, offer a promising alternative for achieving low-power computing. Integrating the event-driven efficiency of spiking neurons with the advanced capabilities of LLMs represents a promising direction for power-efficient LLMs. This work specifically delves into the design of compressed spiking LLMs. Here, we revisit spatial and temporal pruning from the perspective of SNNs and propose a novel spatio-temporal pruning framework for Spiking LLMs to optimize computational efficiency while preserving high performance. Our spatial pruning technique reduces the number of active neurons and attention heads, effectively lowering the computational complexity of the model. Meanwhile, temporal pruning minimizes inference latency by dynamically adjusting the number of timesteps required for different layers. By combining these approaches with other compression techniques, we present the first work in the domain of Spiking LLMs to jointly explore spatial pruning, temporal pruning, extreme quantization and knowledge distillation strategies. Extensive experimental evaluation of our proposed framework for SpikingBERT on the large-scale GLUE benchmark demonstrates the efficacy of our approach in terms of computational operations and inference latency. Our approach offers a compelling solution for real-time, low-power natural language processing applications, making Spiking LLMs more practical for deployment on edge devices and in power-constrained settings.",
        "arxiv_id": "2508.20122"
    },
    "2508.20395": {
        "SCORE": 16,
        "ARXIVID": "2508.20395",
        "COMMENT": "The paper examines reasoning utility in LLMs using conditional entropy, providing theoretical insights into LLM behavior.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Xu Guo"
        ],
        "title": "Measuring Reasoning Utility in LLMs via Conditional Entropy Reduction",
        "abstract": "Recent advancements in large language models (LLMs) often rely on generating intermediate reasoning steps to enhance accuracy. However, little work has examined how reasoning utility contributes to the final answer's correctness. Due to the stochastic nature of autoregressive generation, generating more context does not guarantee increased confidence in the answer. If we could predict, during generation, whether a reasoning step will be useful, we could stop early or prune ineffective steps, avoiding distractions in the final decision.   We present an oracle study on MATH dataset, using Qwen2.5-32B and GPT-4o to generate reasoning chains, and then employing a separate model (Qwen3-8B) to quantify the utility of these chains for final accuracy. Specifically, we measure the model's uncertainty on the answer span Y at each reasoning step using conditional entropy (expected negative log-likelihood over the vocabulary) with context expanding step by step. Our results show a clear pattern: conditional entropy that decreases over steps is strongly associated with correct answers, whereas flat or increasing entropy often results in wrong answers. We also corroborate that incorrect reasoning paths tend to be longer than correct ones, suggesting that longer reasoning does not necessarily yield better outcomes. These findings serve as a foundation to inspire future work on designing efficient reasoning pipelines that detect and avoid unproductive reasoning early.",
        "arxiv_id": "2508.20395"
    },
    "2508.20991": {
        "SCORE": 16,
        "ARXIVID": "2508.20991",
        "COMMENT": "The paper introduces a Mixture-of-Generative-Experts architecture for particle detector simulation, aligning with the model architecture criterion, specifically MoE.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Patryk B\\k{e}dkowski",
            "Jan Dubi\\'nski",
            "Filip Szatkowski",
            "Kamil Deja",
            "Przemys{\\l}aw Rokita",
            "Tomasz Trzci\\'nski"
        ],
        "title": "ExpertSim: Fast Particle Detector Simulation Using Mixture-of-Generative-Experts",
        "abstract": "Simulating detector responses is a crucial part of understanding the inner workings of particle collisions in the Large Hadron Collider at CERN. Such simulations are currently performed with statistical Monte Carlo methods, which are computationally expensive and put a significant strain on CERN's computational grid. Therefore, recent proposals advocate for generative machine learning methods to enable more efficient simulations. However, the distribution of the data varies significantly across the simulations, which is hard to capture with out-of-the-box methods. In this study, we present ExpertSim - a deep learning simulation approach tailored for the Zero Degree Calorimeter in the ALICE experiment. Our method utilizes a Mixture-of-Generative-Experts architecture, where each expert specializes in simulating a different subset of the data. This allows for a more precise and efficient generation process, as each expert focuses on a specific aspect of the calorimeter response. ExpertSim not only improves accuracy, but also provides a significant speedup compared to the traditional Monte-Carlo methods, offering a promising solution for high-efficiency detector simulations in particle physics experiments at CERN. We make the code available at https://github.com/patrick-bedkowski/expertsim-mix-of-generative-experts.",
        "arxiv_id": "2508.20991"
    },
    "2508.20293": {
        "SCORE": 16,
        "ARXIVID": "2508.20293",
        "COMMENT": "The paper presents Beacon, a novel algorithm for post-training quantization, which is relevant to model compression techniques.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Shihao Zhang",
            "Rayan Saab"
        ],
        "title": "Beacon: Post-Training Quantization with Integrated Grid Selection",
        "abstract": "Quantization is a widely used compression technique for reducing the memory and computation costs of large pre-trained models. A key challenge in per-channel post-training quantization (PTQ) is selecting appropriate scaling factors to replace weight values with values from a scaled quantization grid. Existing methods typically fix the scale at the outset via heuristic tuning or grid search. In this note, we propose Beacon, a simple and effective algorithm that eliminates the need for such manual tuning. Beacon performs per-channel PTQ directly using a fixed non-scaled alphabet and automatically determines the optimal scaling factors by exploiting the geometry of symmetric scalar quantization. It supports both symmetric and asymmetric quantization with minimal modifications and does not rely on back-propagation or large calibration sets. Despite its simplicity and tuning-free nature, Beacon achieves competitive performance compared to state-of-the-art methods, making it a practical solution for efficient model deployment.",
        "arxiv_id": "2508.20293"
    },
    "2508.20906": {
        "SCORE": 16,
        "ARXIVID": "2508.20906",
        "COMMENT": "The paper proposes a novel approach to turn tabular foundation models into graph foundation models, which aligns with foundational research in model architecture.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Dmitry Eremeev",
            "Gleb Bazhenov",
            "Oleg Platonov",
            "Artem Babenko",
            "Liudmila Prokhorenkova"
        ],
        "title": "Turning Tabular Foundation Models into Graph Foundation Models",
        "abstract": "While foundation models have revolutionized such fields as natural language processing and computer vision, their application and potential within graph machine learning remain largely unexplored. One of the key challenges in designing graph foundation models (GFMs) is handling diverse node features that can vary across different graph datasets. Although many works on GFMs have been focused exclusively on text-attributed graphs, the problem of handling arbitrary features of other types in GFMs has not been fully addressed. However, this problem is not unique to the graph domain, as it also arises in the field of machine learning for tabular data. In this work, motivated by the recent success of tabular foundation models like TabPFNv2, we propose G2T-FM, a simple graph foundation model that employs TabPFNv2 as a backbone. Specifically, G2T-FM augments the original node features with neighborhood feature aggregation, adds structural embeddings, and then applies TabPFNv2 to the constructed node representations. Even in a fully in-context regime, our model achieves strong results, significantly outperforming publicly available GFMs and performing on par with well-tuned GNNs trained from scratch. Moreover, after finetuning, G2T-FM surpasses well-tuned GNN baselines, highlighting the potential of the proposed approach. More broadly, our paper reveals a previously overlooked direction of utilizing tabular foundation models for graph machine learning tasks.",
        "arxiv_id": "2508.20906"
    },
    "2508.21058": {
        "SCORE": 16,
        "ARXIVID": "2508.21058",
        "COMMENT": "The paper introduces a novel sparse attention routing module for long video generation, which aligns with foundational research in model architecture.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Shengqu Cai",
            "Ceyuan Yang",
            "Lvmin Zhang",
            "Yuwei Guo",
            "Junfei Xiao",
            "Ziyan Yang",
            "Yinghao Xu",
            "Zhenheng Yang",
            "Alan Yuille",
            "Leonidas Guibas",
            "Maneesh Agrawala",
            "Lu Jiang",
            "Gordon Wetzstein"
        ],
        "title": "Mixture of Contexts for Long Video Generation",
        "abstract": "Long video generation is fundamentally a long context memory problem: models must retain and retrieve salient events across a long range without collapsing or drifting. However, scaling diffusion transformers to generate long-context videos is fundamentally limited by the quadratic cost of self-attention, which makes memory and computation intractable and difficult to optimize for long sequences. We recast long-context video generation as an internal information retrieval task and propose a simple, learnable sparse attention routing module, Mixture of Contexts (MoC), as an effective long-term memory retrieval engine. In MoC, each query dynamically selects a few informative chunks plus mandatory anchors (caption, local windows) to attend to, with causal routing that prevents loop closures. As we scale the data and gradually sparsify the routing, the model allocates compute to salient history, preserving identities, actions, and scenes over minutes of content. Efficiency follows as a byproduct of retrieval (near-linear scaling), which enables practical training and synthesis, and the emergence of memory and consistency at the scale of minutes.",
        "arxiv_id": "2508.21058"
    },
    "2508.20375": {
        "SCORE": 16,
        "ARXIVID": "2508.20375",
        "COMMENT": "CoFormer introduces a novel collaborative inference system for transformer models, focusing on architectural innovation and efficiency improvements.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Guanyu Xu",
            "Zhiwei Hao",
            "Li Shen",
            "Yong Luo",
            "Fuhui Sun",
            "Xiaoyan Wang",
            "Han Hu",
            "Yonggang Wen"
        ],
        "title": "CoFormer: Collaborating with Heterogeneous Edge Devices for Scalable Transformer Inference",
        "abstract": "The impressive performance of transformer models has sparked the deployment of intelligent applications on resource-constrained edge devices. However, ensuring high-quality service for real-time edge systems is a significant challenge due to the considerable computational demands and resource requirements of these models. Existing strategies typically either offload transformer computations to other devices or directly deploy compressed models on individual edge devices. These strategies, however, result in either considerable communication overhead or suboptimal trade-offs between accuracy and efficiency. To tackle these challenges, we propose a collaborative inference system for general transformer models, termed CoFormer. The central idea behind CoFormer is to exploit the divisibility and integrability of transformer. An off-the-shelf large transformer can be decomposed into multiple smaller models for distributed inference, and their intermediate results are aggregated to generate the final output. We formulate an optimization problem to minimize both inference latency and accuracy degradation under heterogeneous hardware constraints. DeBo algorithm is proposed to first solve the optimization problem to derive the decomposition policy, and then progressively calibrate decomposed models to restore performance. We demonstrate the capability to support a wide range of transformer models on heterogeneous edge devices, achieving up to 3.1$\\times$ inference speedup with large transformer models. Notably, CoFormer enables the efficient inference of GPT2-XL with 1.6 billion parameters on edge devices, reducing memory requirements by 76.3\\%. CoFormer can also reduce energy consumption by approximately 40\\% while maintaining satisfactory inference performance.",
        "arxiv_id": "2508.20375"
    },
    "2508.20577": {
        "SCORE": 15,
        "ARXIVID": "2508.20577",
        "COMMENT": "The paper introduces a novel optimizer, MERIT, for large-batch training in language models, focusing on improving training stability and efficiency, which aligns with the model compression and efficiency criteria.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Yang Luo",
            "Zangwei Zheng",
            "Ziheng Qin",
            "Zirui Zhu",
            "Yong Liu",
            "Yang You"
        ],
        "title": "MERIT: Maximum-normalized Element-wise Ratio for Language Model Large-batch Training",
        "abstract": "Large-batch training has become a cornerstone in accelerating the training of deep neural networks, yet it poses challenges in optimization and generalization. Existing optimizers like AdamW present performance degradation during language models' large-batch training, due to the information bottleneck in attention layers caused by the sharp increase of max attention logit. While the LAMB optimizer partially addresses this issue, some attention layers still face this issue. The reason is that $l_2$-norm-based trust ratios in LAMB are less effective in directly influencing the max value of query/key weights. Furthermore, the weight-wise trust ratio in LAMB is error-prone as it overlooks relationships of weight values within rows or columns. Building on these observations, we propose a novel optimizer, MERIT, which leverages the max-norm to calculate the trust ratio to constrain the max attention logit more effectively. Moreover, we further construct element-wise trust ratios to provide more robust update scaling by focusing on local weight structures. Extensive experiments of large-batch training across various sizes of GPT-2 models demonstrate the superior performance of MERIT. Notably, during the training of GPT-2 Medium, MERIT enables a 6k batch size without any performance degradation compared to the standard batch size (480) with 48B training tokens. This work highlights the importance of considering the max attention logit and finer-granularity trust ratio in large-batch training. It successfully improves the training stability and paves the way for larger batch usage, enabling faster development and iteration of large language models. Code is available at https://github.com/NUS-HPC-AI-Lab/MERIT.",
        "arxiv_id": "2508.20577"
    },
    "2508.20443": {
        "SCORE": 15,
        "ARXIVID": "2508.20443",
        "COMMENT": "The paper proposes a novel unlearning framework for LLMs, which aligns with foundational research in LLM behavior and interpretability.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Zhihao Liu",
            "Jian Lou",
            "Yuke Hu",
            "Xiaochen Li",
            "Tailun Chen",
            "Yitian Chen",
            "Zhan Qin"
        ],
        "title": "Towards Mitigating Excessive Forgetting in LLM Unlearning via Entanglement-Aware Unlearning with Proxy Constraint",
        "abstract": "Large language models (LLMs) are trained on massive datasets that may include private or copyrighted content. Due to growing privacy and ownership concerns, data owners may request the removal of their data from trained models. Machine unlearning provides a practical solution by removing the influence of specific data without full retraining. However, most existing methods lack a sound forgetting boundary, causing some samples to be under-forgotten, leaving residual leakage risks, while others remain over-forgotten at the expense of degraded utility.   In this work, we propose EAGLE-PC (Entanglement-Awareness Guided Loss Reweighting with Proxy Constraint), a novel unlearning framework that addresses these limitations through two key components. First, entanglement-awareness guided loss reweighting determines the forgetting effort of each sample by measuring its similarity to retain samples in the embedding space, enabling more targeted and effective unlearning. Second, a proxy constraint leveraging ICL (In-Context Learning) generated test data softly regularizes the forgetting process, effectively mitigating over-forgetting. EAGLE-PC is compatible with existing gradient-based objectives and serves as a plug-and-play enhancement. We evaluate EAGLE-PC on the TOFU and MUSE benchmarks, showing consistent improvements in the forgetting-utility trade-off across multiple LLMs. Combined with the NPO+GD optimizer, it approaches full retraining performance, offering a scalable and robust unlearning solution.",
        "arxiv_id": "2508.20443"
    },
    "2508.21022": {
        "SCORE": 15,
        "ARXIVID": "2508.21022",
        "COMMENT": "The paper provides theoretical insights into subsampled natural gradient algorithms, which is relevant to foundational research in optimization and training dynamics.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Gil Goldshlager",
            "Jiang Hu",
            "Lin Lin"
        ],
        "title": "Fast Convergence Rates for Subsampled Natural Gradient Algorithms on Quadratic Model Problems",
        "abstract": "Subsampled natural gradient descent (SNGD) has shown impressive results for parametric optimization tasks in scientific machine learning, such as neural network wavefunctions and physics-informed neural networks, but it has lacked a theoretical explanation. We address this gap by analyzing the convergence of SNGD and its accelerated variant, SPRING, for idealized parametric optimization problems where the model is linear and the loss function is strongly convex and quadratic. In the special case of a least-squares loss, namely the standard linear least-squares problem, we prove that SNGD is equivalent to a regularized Kaczmarz method while SPRING is equivalent to an accelerated regularized Kaczmarz method. As a result, by leveraging existing analyses we obtain under mild conditions (i) the first fast convergence rate for SNGD, (ii) the first convergence guarantee for SPRING in any setting, and (iii) the first proof that SPRING can accelerate SNGD. In the case of a general strongly convex quadratic loss, we extend the analysis of the regularized Kaczmarz method to obtain a fast convergence rate for SNGD under stronger conditions, providing the first explanation for the effectiveness of SNGD outside of the least-squares setting. Overall, our results illustrate how tools from randomized linear algebra can shed new light on the interplay between subsampling and curvature-aware optimization strategies.",
        "arxiv_id": "2508.21022"
    },
    "2508.20384": {
        "SCORE": 15,
        "ARXIVID": "2508.20384",
        "COMMENT": "The paper introduces Entropy Area Score (EAS), a novel metric for quantifying uncertainty in LLMs, which contributes to foundational research in LLM behavior.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Yongfu Zhu",
            "Lin Sun",
            "Guangxiang Zhao",
            "Weihong Lin",
            "Xiangzheng Zhang"
        ],
        "title": "Uncertainty Under the Curve: A Sequence-Level Entropy Area Metric for Reasoning LLM",
        "abstract": "In this work, we introduce Entropy Area Score (EAS), a simple yet effective metric to quantify uncertainty in the answer generation process of reasoning large language models (LLMs). EAS requires neither external models nor repeated sampling, it integrates token-level predictive entropy from the model itself to capture the evolution of uncertainty during generation. Empirical results show that EAS is strongly correlated with answer entropy across models and datasets. In training data selection, EAS identifies high-potential samples and consistently outperforms Pass Rate filtering under equal sample budgets, improving student model accuracy on math benchmarks. EAS is both efficient and interpretable, offering a practical tool for uncertainty modeling and data quality assessment in LLM training.",
        "arxiv_id": "2508.20384"
    },
    "2508.20978": {
        "SCORE": 15,
        "ARXIVID": "2508.20978",
        "COMMENT": "The paper introduces a differentiable neuro-symbolic architecture for solving NP-hard reasoning problems, which aligns with foundational research in representation learning and model architecture.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Marianne Defresne",
            "Romain Gambardella",
            "Sophie Barbe",
            "Thomas Schiex"
        ],
        "title": "Efficient Neuro-Symbolic Learning of Constraints and Objective",
        "abstract": "In the ongoing quest for hybridizing discrete reasoning with neural nets, there is an increasing interest in neural architectures that can learn how to solve discrete reasoning or optimization problems from natural inputs, a task that Large Language Models seem to struggle with.   Objectives: We introduce a differentiable neuro-symbolic architecture and a loss function dedicated to learning how to solve NP-hard reasoning problems.   Methods: Our new probabilistic loss allows for learning both the constraints and the objective, thus delivering a complete model that can be scrutinized and completed with side constraints. By pushing the combinatorial solver out of the training loop, our architecture also offers scalable training while exact inference gives access to maximum accuracy.   Results: We empirically show that it can efficiently learn how to solve NP-hard reasoning problems from natural inputs. On three variants of the Sudoku benchmark -- symbolic, visual, and many-solution --, our approach requires a fraction of training time of other hybrid methods. On a visual Min-Cut/Max-cut task, it optimizes the regret better than a Decision-Focused-Learning regret-dedicated loss. Finally, it efficiently learns the energy optimization formulation of the large real-world problem of designing proteins.",
        "arxiv_id": "2508.20978"
    },
    "2508.20650": {
        "SCORE": 15,
        "ARXIVID": "2508.20650",
        "COMMENT": "The paper proposes a novel framework for neural operators with a focus on efficiency and accuracy, relevant to model architecture innovations.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Juncai He",
            "Xinliang Liu",
            "Jinchao Xu"
        ],
        "title": "Self-Composing Neural Operators with Depth and Accuracy Scaling via Adaptive Train-and-Unroll Approach",
        "abstract": "In this work, we propose a novel framework to enhance the efficiency and accuracy of neural operators through self-composition, offering both theoretical guarantees and practical benefits. Inspired by iterative methods in solving numerical partial differential equations (PDEs), we design a specific neural operator by repeatedly applying a single neural operator block, we progressively deepen the model without explicitly adding new blocks, improving the model's capacity. To train these models efficiently, we introduce an adaptive train-and-unroll approach, where the depth of the neural operator is gradually increased during training. This approach reveals an accuracy scaling law with model depth and offers significant computational savings through our adaptive training strategy. Our architecture achieves state-of-the-art (SOTA) performance on standard benchmarks. We further demonstrate its efficacy on a challenging high-frequency ultrasound computed tomography (USCT) problem, where a multigrid-inspired backbone enables superior performance in resolving complex wave phenomena. The proposed framework provides a computationally tractable, accurate, and scalable solution for large-scale data-driven scientific machine learning applications.",
        "arxiv_id": "2508.20650"
    },
    "2508.20441": {
        "SCORE": 15,
        "ARXIVID": "2508.20441",
        "COMMENT": "The paper investigates the spectral bias in diagonal state space models, contributing to foundational research in model architecture and representation learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Ruben Solozabal",
            "Velibor Bojkovic",
            "Hilal AlQuabeh",
            "Kentaro Inui",
            "Martin Tak\\'a\\v{c}"
        ],
        "title": "Uncovering the Spectral Bias in Diagonal State Space Models",
        "abstract": "Current methods for initializing state space models (SSMs) parameters mainly rely on the \\textit{HiPPO framework}, which is based on an online approximation of orthogonal polynomials. Recently, diagonal alternatives have shown to reach a similar level of performance while being significantly more efficient due to the simplification in the kernel computation. However, the \\textit{HiPPO framework} does not explicitly study the role of its diagonal variants. In this paper, we take a further step to investigate the role of diagonal SSM initialization schemes from the frequency perspective. Our work seeks to systematically understand how to parameterize these models and uncover the learning biases inherent in such diagonal state-space models. Based on our observations, we propose a diagonal initialization on the discrete Fourier domain \\textit{S4D-DFouT}. The insights in the role of pole placing in the initialization enable us to further scale them and achieve state-of-the-art results on the Long Range Arena benchmark, allowing us to train from scratch on very large datasets as PathX-256.",
        "arxiv_id": "2508.20441"
    }
}