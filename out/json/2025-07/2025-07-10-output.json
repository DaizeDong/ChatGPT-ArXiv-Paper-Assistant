{
    "2507.06427": {
        "SCORE": 17,
        "ARXIVID": "2507.06427",
        "COMMENT": "The paper explores representation learning through sparse autoencoders, focusing on extracting monosemantic features from LLM neurons, which aligns with insights into how deep networks encode information.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Shun Wang",
            "Tyler Loakman",
            "Youbo Lei",
            "Yi Liu",
            "Bohao Yang",
            "Yuting Zhao",
            "Dong Yang",
            "Chenghua Lin"
        ],
        "title": "Exploring Task Performance with Interpretable Models via Sparse Auto-Encoders",
        "abstract": "Large Language Models (LLMs) are traditionally viewed as black-box algorithms, therefore reducing trustworthiness and obscuring potential approaches to increasing performance on downstream tasks. In this work, we apply an effective LLM decomposition method using a dictionary-learning approach with sparse autoencoders. This helps extract monosemantic features from polysemantic LLM neurons. Remarkably, our work identifies model-internal misunderstanding, allowing the automatic reformulation of the prompts with additional annotations to improve the interpretation by LLMs. Moreover, this approach demonstrates a significant performance improvement in downstream tasks, such as mathematical reasoning and metaphor detection.",
        "arxiv_id": "2507.06427"
    },
    "2507.06265": {
        "SCORE": 17,
        "ARXIVID": "2507.06265",
        "COMMENT": "The paper introduces SPARC, a framework for cross-model and cross-modal interpretability using sparse autoencoders, aligning with representation learning and interpretability.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Ali Nasiri-Sarvi",
            "Hassan Rivaz",
            "Mahdi S. Hosseini"
        ],
        "title": "SPARC: Concept-Aligned Sparse Autoencoders for Cross-Model and Cross-Modal Interpretability",
        "abstract": "Understanding how different AI models encode the same high-level concepts, such as objects or attributes, remains challenging because each model typically produces its own isolated representation. Existing interpretability methods like Sparse Autoencoders (SAEs) produce latent concepts individually for each model, resulting in incompatible concept spaces and limiting cross-model interpretability. To address this, we introduce SPARC (Sparse Autoencoders for Aligned Representation of Concepts), a new framework that learns a single, unified latent space shared across diverse architectures and modalities (e.g., vision models like DINO, and multimodal models like CLIP). SPARC's alignment is enforced through two key innovations: (1) a Global TopK sparsity mechanism, ensuring all input streams activate identical latent dimensions for a given concept; and (2) a Cross-Reconstruction Loss, which explicitly encourages semantic consistency between models. On Open Images, SPARC dramatically improves concept alignment, achieving a Jaccard similarity of 0.80, more than tripling the alignment compared to previous methods. SPARC creates a shared sparse latent space where individual dimensions often correspond to similar high-level concepts across models and modalities, enabling direct comparison of how different architectures represent identical concepts without requiring manual alignment or model-specific analysis. As a consequence of this aligned representation, SPARC also enables practical applications such as text-guided spatial localization in vision-only models and cross-model/cross-modal retrieval. Code and models are available at https://github.com/AtlasAnalyticsLab/SPARC.",
        "arxiv_id": "2507.06265"
    },
    "2507.06979": {
        "SCORE": 17,
        "ARXIVID": "2507.06979",
        "COMMENT": "The paper proposes a principled framework for multi-view contrastive learning, addressing limitations in current methods and providing theoretical extensions, relevant to representation learning.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Panagiotis Koromilas",
            "Efthymios Georgiou",
            "Giorgos Bouritsas",
            "Theodoros Giannakopoulos",
            "Mihalis A. Nicolaou",
            "Yannis Panagakis"
        ],
        "title": "A Principled Framework for Multi-View Contrastive Learning",
        "abstract": "Contrastive Learning (CL), a leading paradigm in Self-Supervised Learning (SSL), typically relies on pairs of data views generated through augmentation. While multiple augmentations per instance (more than two) improve generalization in supervised learning, current CL methods handle additional views suboptimally by simply aggregating different pairwise objectives. This approach suffers from four critical limitations: (L1) it utilizes multiple optimization terms per data point resulting to conflicting objectives, (L2) it fails to model all interactions across views and data points, (L3) it inherits fundamental limitations (e.g. alignment-uniformity coupling) from pairwise CL losses, and (L4) it prevents fully realizing the benefits of increased view multiplicity observed in supervised settings. We address these limitations through two novel loss functions: MV-InfoNCE, which extends InfoNCE to incorporate all possible view interactions simultaneously in one term per data point, and MV-DHEL, which decouples alignment from uniformity across views while scaling interaction complexity with view multiplicity. Both approaches are theoretically grounded - we prove they asymptotically optimize for alignment of all views and uniformity, providing principled extensions to multi-view contrastive learning. Our empirical results on ImageNet1K and three other datasets demonstrate that our methods consistently outperform existing multi-view approaches and effectively scale with increasing view multiplicity. We also apply our objectives to multimodal data and show that, in contrast to other contrastive objectives, they can scale beyond just two modalities. Most significantly, ablation studies reveal that MV-DHEL with five or more views effectively mitigates dimensionality collapse by fully utilizing the embedding space, thereby delivering multi-view benefits observed in supervised learning.",
        "arxiv_id": "2507.06979"
    },
    "2507.06516": {
        "SCORE": 17,
        "ARXIVID": "2507.06516",
        "COMMENT": "The paper proposes a novel monotonic post-hoc calibration method, which is a foundational contribution to representation learning by ensuring expressiveness, robustness, and interpretability.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Yunrui Zhang",
            "Gustavo Batista",
            "Salil S. Kanhere"
        ],
        "title": "Instance-Wise Monotonic Calibration by Constrained Transformation",
        "abstract": "Deep neural networks often produce miscalibrated probability estimates, leading to overconfident predictions. A common approach for calibration is fitting a post-hoc calibration map on unseen validation data that transforms predicted probabilities. A key desirable property of the calibration map is instance-wise monotonicity (i.e., preserving the ranking of probability outputs). However, most existing post-hoc calibration methods do not guarantee monotonicity. Previous monotonic approaches either use an under-parameterized calibration map with limited expressive ability or rely on black-box neural networks, which lack interpretability and robustness. In this paper, we propose a family of novel monotonic post-hoc calibration methods, which employs a constrained calibration map parameterized linearly with respect to the number of classes. Our proposed approach ensures expressiveness, robustness, and interpretability while preserving the relative ordering of the probability output by formulating the proposed calibration map as a constrained optimization problem. Our proposed methods achieve state-of-the-art performance across datasets with different deep neural network models, outperforming existing calibration methods while being data and computation-efficient. Our code is available at https://github.com/YunruiZhang/Calibration-by-Constrained-Transformation",
        "arxiv_id": "2507.06516"
    },
    "2507.06613": {
        "SCORE": 17,
        "ARXIVID": "2507.06613",
        "COMMENT": "The paper introduces a novel generative modeling framework for disentangled representation learning, which is a foundational contribution to representation learning.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Anshuk Uppal",
            "Yuhta Takida",
            "Chieh-Hsin Lai",
            "Yuki Mitsufuji"
        ],
        "title": "Denoising Multi-Beta VAE: Representation Learning for Disentanglement and Generation",
        "abstract": "Disentangled and interpretable latent representations in generative models typically come at the cost of generation quality. The $\\beta$-VAE framework introduces a hyperparameter $\\beta$ to balance disentanglement and reconstruction quality, where setting $\\beta > 1$ introduces an information bottleneck that favors disentanglement over sharp, accurate reconstructions. To address this trade-off, we propose a novel generative modeling framework that leverages a range of $\\beta$ values to learn multiple corresponding latent representations. First, we obtain a slew of representations by training a single variational autoencoder (VAE), with a new loss function that controls the information retained in each latent representation such that the higher $\\beta$ value prioritize disentanglement over reconstruction fidelity. We then, introduce a non-linear diffusion model that smoothly transitions latent representations corresponding to different $\\beta$ values. This model denoises towards less disentangled and more informative representations, ultimately leading to (almost) lossless representations, enabling sharp reconstructions. Furthermore, our model supports sample generation without input images, functioning as a standalone generative model. We evaluate our framework in terms of both disentanglement and generation quality. Additionally, we observe smooth transitions in the latent spaces with respect to changes in $\\beta$, facilitating consistent manipulation of generated outputs.",
        "arxiv_id": "2507.06613"
    },
    "2507.06558": {
        "SCORE": 17,
        "ARXIVID": "2507.06558",
        "COMMENT": "The paper discusses a magnitude-driven initialization scheme for low-rank adaptation, contributing to model compression and efficiency.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Zicheng Zhang",
            "Haoran Li",
            "Yifeng Zhang",
            "Guoqiang Gong",
            "Jiaxing Wang",
            "Pengzhang Liu",
            "Qixia Jiang",
            "Junxing Hu"
        ],
        "title": "The Primacy of Magnitude in Low-Rank Adaptation",
        "abstract": "Low-Rank Adaptation (LoRA) offers a parameter-efficient paradigm for tuning large models. While recent spectral initialization methods improve convergence and performance over the naive \"Noise & Zeros\" scheme, their extra computational and storage overhead undermines efficiency. In this paper, we establish update magnitude as the fundamental driver of LoRA performance and propose LoRAM, a magnitude-driven \"Basis & Basis\" initialization scheme that matches spectral methods without their inefficiencies. Our key contributions are threefold: (i) Magnitude of weight updates determines convergence. We prove low-rank structures intrinsically bound update magnitudes, unifying hyperparameter tuning in learning rate, scaling factor, and initialization as mechanisms to optimize magnitude regulation. (ii) Spectral initialization succeeds via magnitude amplification. We demystify that the presumed knowledge-driven benefit of the spectral component essentially arises from the boost in the weight update magnitude. (iii) A novel and compact initialization strategy, LoRAM, scales deterministic orthogonal bases using pretrained weight magnitudes to simulate spectral gains. Extensive experiments show that LoRAM serves as a strong baseline, retaining the full efficiency of LoRA while matching or outperforming spectral initialization across benchmarks.",
        "arxiv_id": "2507.06558"
    },
    "2507.06607": {
        "SCORE": 17,
        "ARXIVID": "2507.06607",
        "COMMENT": "The paper introduces a decoder-hybrid-decoder architecture with efficient memory sharing, contributing to model architecture innovations.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Liliang Ren",
            "Congcong Chen",
            "Haoran Xu",
            "Young Jin Kim",
            "Adam Atkinson",
            "Zheng Zhan",
            "Jiankai Sun",
            "Baolin Peng",
            "Liyuan Liu",
            "Shuohang Wang",
            "Hao Cheng",
            "Jianfeng Gao",
            "Weizhu Chen",
            "Yelong Shen"
        ],
        "title": "Decoder-Hybrid-Decoder Architecture for Efficient Reasoning with Long Generation",
        "abstract": "Recent advances in language modeling have demonstrated the effectiveness of State Space Models (SSMs) for efficient sequence modeling. While hybrid architectures such as Samba and the decoder-decoder architecture, YOCO, have shown promising performance gains over Transformers, prior works have not investigated the efficiency potential of representation sharing between SSM layers. In this paper, we introduce the Gated Memory Unit (GMU), a simple yet effective mechanism for efficient memory sharing across layers. We apply it to create SambaY, a decoder-hybrid-decoder architecture that incorporates GMUs in the cross-decoder to share memory readout states from a Samba-based self-decoder. SambaY significantly enhances decoding efficiency, preserves linear pre-filling time complexity, and boosts long-context performance, all while eliminating the need for explicit positional encoding. Through extensive scaling experiments, we demonstrate that our model exhibits a significantly lower irreducible loss compared to a strong YOCO baseline, indicating superior performance scalability under large-scale compute regimes. Our largest model enhanced with Differential Attention, Phi4-mini-Flash-Reasoning, achieves significantly better performance than Phi4-mini-Reasoning on reasoning tasks such as Math500, AIME24/25, and GPQA Diamond without any reinforcement learning, while delivering up to 10x higher decoding throughput on 2K-length prompts with 32K generation length under the vLLM inference framework. We release our training codebase on open-source data at https://github.com/microsoft/ArchScale.",
        "arxiv_id": "2507.06607"
    },
    "2507.06381": {
        "SCORE": 17,
        "ARXIVID": "2507.06381",
        "COMMENT": "The paper provides a theoretical framework for understanding gradient descent in recurrent networks, focusing on representation learning and training dynamics.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "James Hazelden",
            "Laura Driscoll",
            "Eli Shlizerman",
            "Eric Shea-Brown"
        ],
        "title": "KPFlow: An Operator Perspective on Dynamic Collapse Under Gradient Descent Training of Recurrent Networks",
        "abstract": "Gradient Descent (GD) and its variants are the primary tool for enabling efficient training of recurrent dynamical systems such as Recurrent Neural Networks (RNNs), Neural ODEs and Gated Recurrent units (GRUs). The dynamics that are formed in these models exhibit features such as neural collapse and emergence of latent representations that may support the remarkable generalization properties of networks. In neuroscience, qualitative features of these representations are used to compare learning in biological and artificial systems. Despite recent progress, there remains a need for theoretical tools to rigorously understand the mechanisms shaping learned representations, especially in finite, non-linear models. Here, we show that the gradient flow, which describes how the model's dynamics evolve over GD, can be decomposed into a product that involves two operators: a Parameter Operator, K, and a Linearized Flow Propagator, P. K mirrors the Neural Tangent Kernel in feed-forward neural networks, while P appears in Lyapunov stability and optimal control theory. We demonstrate two applications of our decomposition. First, we show how their interplay gives rise to low-dimensional latent dynamics under GD, and, specifically, how the collapse is a result of the network structure, over and above the nature of the underlying task. Second, for multi-task training, we show that the operators can be used to measure how objectives relevant to individual sub-tasks align. We experimentally and theoretically validate these findings, providing an efficient Pytorch package, \\emph{KPFlow}, implementing robust analysis tools for general recurrent architectures. Taken together, our work moves towards building a next stage of understanding of GD learning in non-linear recurrent models.",
        "arxiv_id": "2507.06381"
    },
    "2507.06415": {
        "SCORE": 17,
        "ARXIVID": "2507.06415",
        "COMMENT": "The paper introduces PERK, a parameter-efficient method for long-context reasoning using low-rank adapters, which aligns with model compression and efficiency breakthroughs.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Zeming Chen",
            "Angelika Romanou",
            "Gail Weiss",
            "Antoine Bosselut"
        ],
        "title": "PERK: Long-Context Reasoning as Parameter-Efficient Test-Time Learning",
        "abstract": "Long-context reasoning requires accurately identifying relevant information in extensive, noisy input contexts. Previous research shows that using test-time learning to encode context directly into model parameters can effectively enable reasoning over noisy information. However, meta-learning methods for enabling test-time learning are prohibitively memory-intensive, preventing their application to long context settings. In this work, we propose PERK (Parameter Efficient Reasoning over Knowledge), a scalable approach for learning to encode long input contexts using gradient updates to a lightweight model adapter at test time. Specifically, PERK employs two nested optimization loops in a meta-training phase. The inner loop rapidly encodes contexts into a low-rank adapter (LoRA) that serves as a parameter-efficient memory module for the base model. Concurrently, the outer loop learns to use the updated adapter to accurately recall and reason over relevant information from the encoded long context. Our evaluations on several long-context reasoning tasks show that PERK significantly outperforms the standard prompt-based long-context baseline, achieving average absolute performance gains of up to 90% for smaller models (GPT-2) and up to 27% for our largest evaluated model, Qwen-2.5-0.5B. In general, PERK is more robust to reasoning complexity, length extrapolation, and the locations of relevant information in contexts. Finally, we show that while PERK is memory-intensive during training, it scales more efficiently at inference time than prompt-based long-context inference.",
        "arxiv_id": "2507.06415"
    },
    "2507.06722": {
        "SCORE": 16,
        "ARXIVID": "2507.06722",
        "COMMENT": "The paper investigates the effect of uncertainty on inference dynamics in LLMs, providing theoretical insights into LLM behavior and interpretability.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Sunwoo Kim",
            "Haneul Yoo",
            "Alice Oh"
        ],
        "title": "On the Effect of Uncertainty on Layer-wise Inference Dynamics",
        "abstract": "Understanding how large language models (LLMs) internally represent and process their predictions is central to detecting uncertainty and preventing hallucinations. While several studies have shown that models encode uncertainty in their hidden states, it is underexplored how this affects the way they process such hidden states. In this work, we demonstrate that the dynamics of output token probabilities across layers for certain and uncertain outputs are largely aligned, revealing that uncertainty does not seem to affect inference dynamics. Specifically, we use the Tuned Lens, a variant of the Logit Lens, to analyze the layer-wise probability trajectories of final prediction tokens across 11 datasets and 5 models. Using incorrect predictions as those with higher epistemic uncertainty, our results show aligned trajectories for certain and uncertain predictions that both observe abrupt increases in confidence at similar layers. We balance this finding by showing evidence that more competent models may learn to process uncertainty differently. Our findings challenge the feasibility of leveraging simplistic methods for detecting uncertainty at inference. More broadly, our work demonstrates how interpretability methods may be used to investigate the way uncertainty affects inference.",
        "arxiv_id": "2507.06722"
    },
    "2507.06367": {
        "SCORE": 16,
        "ARXIVID": "2507.06367",
        "COMMENT": "The paper provides theoretical insights into the gradient flow of linear convolutional networks, contributing to the understanding of training dynamics in neural networks.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "El Mehdi Achour",
            "Kathl\\'en Kohn",
            "Holger Rauhut"
        ],
        "title": "The Riemannian Geometry associated to Gradient Flows of Linear Convolutional Networks",
        "abstract": "We study geometric properties of the gradient flow for learning deep linear convolutional networks. For linear fully connected networks, it has been shown recently that the corresponding gradient flow on parameter space can be written as a Riemannian gradient flow on function space (i.e., on the product of weight matrices) if the initialization satisfies a so-called balancedness condition. We establish that the gradient flow on parameter space for learning linear convolutional networks can be written as a Riemannian gradient flow on function space regardless of the initialization. This result holds for $D$-dimensional convolutions with $D \\geq 2$, and for $D =1$ it holds if all so-called strides of the convolutions are greater than one. The corresponding Riemannian metric depends on the initialization.",
        "arxiv_id": "2507.06367"
    },
    "2507.06502": {
        "SCORE": 16,
        "ARXIVID": "2507.06502",
        "COMMENT": "The paper introduces a Mixture of Experts (MoE) model for time-series forecasting, focusing on integrating time and frequency domain features. This aligns with the core topic of model architecture, specifically MoE.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Yiwen Liu",
            "Chenyu Zhang",
            "Junjie Song",
            "Siqi Chen",
            "Sun Yin",
            "Zihan Wang",
            "Lingming Zeng",
            "Yuji Cao",
            "Junming Jiao"
        ],
        "title": "MoFE-Time: Mixture of Frequency Domain Experts for Time-Series Forecasting Models",
        "abstract": "As a prominent data modality task, time series forecasting plays a pivotal role in diverse applications. With the remarkable advancements in Large Language Models (LLMs), the adoption of LLMs as the foundational architecture for time series modeling has gained significant attention. Although existing models achieve some success, they rarely both model time and frequency characteristics in a pretraining-finetuning paradigm leading to suboptimal performance in predictions of complex time series, which requires both modeling periodicity and prior pattern knowledge of signals. We propose MoFE-Time, an innovative time series forecasting model that integrates time and frequency domain features within a Mixture of Experts (MoE) network. Moreover, we use the pretraining-finetuning paradigm as our training framework to effectively transfer prior pattern knowledge across pretraining and finetuning datasets with different periodicity distributions. Our method introduces both frequency and time cells as experts after attention modules and leverages the MoE routing mechanism to construct multidimensional sparse representations of input signals. In experiments on six public benchmarks, MoFE-Time has achieved new state-of-the-art performance, reducing MSE and MAE by 6.95% and 6.02% compared to the representative methods Time-MoE. Beyond the existing evaluation benchmarks, we have developed a proprietary dataset, NEV-sales, derived from real-world business scenarios. Our method achieves outstanding results on this dataset, underscoring the effectiveness of the MoFE-Time model in practical commercial applications.",
        "arxiv_id": "2507.06502"
    },
    "2507.06952": {
        "SCORE": 16,
        "ARXIVID": "2507.06952",
        "COMMENT": "The paper evaluates foundation models using inductive bias probes, which aligns with the core topic of large language models and their theoretical insights.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Keyon Vafa",
            "Peter G. Chang",
            "Ashesh Rambachan",
            "Sendhil Mullainathan"
        ],
        "title": "What Has a Foundation Model Found? Using Inductive Bias to Probe for World Models",
        "abstract": "Foundation models are premised on the idea that sequence prediction can uncover deeper domain understanding, much like how Kepler's predictions of planetary motion later led to the discovery of Newtonian mechanics. However, evaluating whether these models truly capture deeper structure remains a challenge. We develop a technique for evaluating foundation models that examines how they adapt to synthetic datasets generated from some postulated world model. Our technique measures whether the foundation model's inductive bias aligns with the world model, and so we refer to it as an inductive bias probe. Across multiple domains, we find that foundation models can excel at their training tasks yet fail to develop inductive biases towards the underlying world model when adapted to new tasks. We particularly find that foundation models trained on orbital trajectories consistently fail to apply Newtonian mechanics when adapted to new physics tasks. Further analysis reveals that these models behave as if they develop task-specific heuristics that fail to generalize.",
        "arxiv_id": "2507.06952"
    },
    "2507.07024": {
        "SCORE": 16,
        "ARXIVID": "2507.07024",
        "COMMENT": "FlexOlmo employs a mixture-of-experts (MoE) architecture, which is relevant to model architecture innovations.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Weijia Shi",
            "Akshita Bhagia",
            "Kevin Farhat",
            "Niklas Muennighoff",
            "Pete Walsh",
            "Jacob Morrison",
            "Dustin Schwenk",
            "Shayne Longpre",
            "Jake Poznanski",
            "Allyson Ettinger",
            "Daogao Liu",
            "Margaret Li",
            "Dirk Groeneveld",
            "Mike Lewis",
            "Wen-tau Yih",
            "Luca Soldaini",
            "Kyle Lo",
            "Noah A. Smith",
            "Luke Zettlemoyer",
            "Pang Wei Koh",
            "Hannaneh Hajishirzi",
            "Ali Farhadi",
            "Sewon Min"
        ],
        "title": "FlexOlmo: Open Language Models for Flexible Data Use",
        "abstract": "We introduce FlexOlmo, a new class of language models (LMs) that supports (1) distributed training without data sharing, where different model parameters are independently trained on closed datasets, and (2) data-flexible inference, where these parameters along with their associated data can be flexibly included or excluded from model inferences with no further training. FlexOlmo employs a mixture-of-experts (MoE) architecture where each expert is trained independently on closed datasets and later integrated through a new domain-informed routing without any joint training. FlexOlmo is trained on FlexMix, a corpus we curate comprising publicly available datasets alongside seven domain-specific sets, representing realistic approximations of closed sets. We evaluate models with up to 37 billion parameters (20 billion active) on 31 diverse downstream tasks. We show that a general expert trained on public data can be effectively combined with independently trained experts from other data owners, leading to an average 41% relative improvement while allowing users to opt out of certain data based on data licensing or permission requirements. Our approach also outperforms prior model merging methods by 10.1% on average and surpasses the standard MoE trained without data restrictions using the same training FLOPs. Altogether, this research presents a solution for both data owners and researchers in regulated industries with sensitive or protected data. FlexOlmo enables benefiting from closed data while respecting data owners' preferences by keeping their data local and supporting fine-grained control of data access during inference.",
        "arxiv_id": "2507.07024"
    },
    "2507.06466": {
        "SCORE": 16,
        "ARXIVID": "2507.06466",
        "COMMENT": "The paper introduces Foundation-Model Self-Play, leveraging foundation models for strategy innovation, which is a novel approach in the context of large language models.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Aaron Dharna",
            "Cong Lu",
            "Jeff Clune"
        ],
        "title": "Foundation Model Self-Play: Open-Ended Strategy Innovation via Foundation Models",
        "abstract": "Multi-agent interactions have long fueled innovation, from natural predator-prey dynamics to the space race. Self-play (SP) algorithms try to harness these dynamics by pitting agents against ever-improving opponents, thereby creating an implicit curriculum toward learning high-quality solutions. However, SP often fails to produce diverse solutions and can get stuck in locally optimal behaviors. We introduce Foundation-Model Self-Play (FMSP), a new direction that leverages the code-generation capabilities and vast knowledge of foundation models (FMs) to overcome these challenges by leaping across local optima in policy space. We propose a family of approaches: (1) \\textbf{Vanilla Foundation-Model Self-Play (vFMSP)} continually refines agent policies via competitive self-play; (2) \\textbf{Novelty-Search Self-Play (NSSP)} builds a diverse population of strategies, ignoring performance; and (3) the most promising variant, \\textbf{Quality-Diveristy Self-Play (QDSP)}, creates a diverse set of high-quality policies by combining the diversity of NSSP and refinement of vFMSP. We evaluate FMSPs in Car Tag, a continuous-control pursuer-evader setting, and in Gandalf, a simple AI safety simulation in which an attacker tries to jailbreak an LLM's defenses. In Car Tag, FMSPs explore a wide variety of reinforcement learning, tree search, and heuristic-based methods, to name just a few. In terms of discovered policy quality, \\ouralgo and vFMSP surpass strong human-designed strategies. In Gandalf, FMSPs can successfully automatically red-team an LLM, breaking through and jailbreaking six different, progressively stronger levels of defense. Furthermore, FMSPs can automatically proceed to patch the discovered vulnerabilities. Overall, FMSPs represent a promising new research frontier of improving self-play with foundation models, opening fresh paths toward more creative and open-ended strategy discovery",
        "arxiv_id": "2507.06466"
    },
    "2507.06775": {
        "SCORE": 16,
        "ARXIVID": "2507.06775",
        "COMMENT": "The paper introduces topological generalization bounds free of mutual information terms, focusing on algorithmic stability, which is relevant to emerging trends in learning theory.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Mario Tuci",
            "Lennart Bastian",
            "Benjamin Dupuis",
            "Nassir Navab",
            "Tolga Birdal",
            "Umut \\c{S}im\\c{s}ekli"
        ],
        "title": "Mutual Information Free Topological Generalization Bounds via Stability",
        "abstract": "Providing generalization guarantees for stochastic optimization algorithms is a major challenge in modern learning theory. Recently, several studies highlighted the impact of the geometry of training trajectories on the generalization error, both theoretically and empirically. Among these works, a series of topological generalization bounds have been proposed, relating the generalization error to notions of topological complexity that stem from topological data analysis (TDA). Despite their empirical success, these bounds rely on intricate information-theoretic (IT) terms that can be bounded in specific cases but remain intractable for practical algorithms (such as ADAM), potentially reducing the relevance of the derived bounds. In this paper, we seek to formulate comprehensive and interpretable topological generalization bounds free of intractable mutual information terms. To this end, we introduce a novel learning theoretic framework that departs from the existing strategies via proof techniques rooted in algorithmic stability. By extending an existing notion of \\textit{hypothesis set stability}, to \\textit{trajectory stability}, we prove that the generalization error of trajectory-stable algorithms can be upper bounded in terms of (i) TDA quantities describing the complexity of the trajectory of the optimizer in the parameter space, and (ii) the trajectory stability parameter of the algorithm. Through a series of experimental evaluations, we demonstrate that the TDA terms in the bound are of great importance, especially as the number of training samples grows. This ultimately forms an explanation of the empirical success of the topological generalization bounds.",
        "arxiv_id": "2507.06775"
    },
    "2507.06752": {
        "SCORE": 16,
        "ARXIVID": "2507.06752",
        "COMMENT": "The paper introduces the Mathematical Artificial Data (MAD) framework, a new paradigm for operator learning, which could be considered a novel approach in AI for Science.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Heng Wu",
            "Benzhuo Lu"
        ],
        "title": "Mathematical artificial data for operator learning",
        "abstract": "Machine learning has emerged as a transformative tool for solving differential equations (DEs), yet prevailing methodologies remain constrained by dual limitations: data-driven methods demand costly labeled datasets while model-driven techniques face efficiency-accuracy trade-offs. We present the Mathematical Artificial Data (MAD) framework, a new paradigm that integrates physical laws with data-driven learning to facilitate large-scale operator discovery. By exploiting DEs' intrinsic mathematical structure to generate physics-embedded analytical solutions and associated synthetic data, MAD fundamentally eliminates dependence on experimental or simulated training data. This enables computationally efficient operator learning across multi-parameter systems while maintaining mathematical rigor. Through numerical demonstrations spanning 2D parametric problems where both the boundary values and source term are functions, we showcase MAD's generalizability and superior efficiency/accuracy across various DE scenarios. This physics-embedded-data-driven framework and its capacity to handle complex parameter spaces gives it the potential to become a universal paradigm for physics-informed machine intelligence in scientific computing.",
        "arxiv_id": "2507.06752"
    },
    "2507.06552": {
        "SCORE": 16,
        "ARXIVID": "2507.06552",
        "COMMENT": "The paper provides a theoretical perspective on the hardness of unsupervised domain adaptation, introducing an information-theoretic quantity to evaluate learning difficulty, which aligns with emerging trends in foundational research.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Zhiyi Dong",
            "Zixuan Liu",
            "Yongyi Mao"
        ],
        "title": "On the Hardness of Unsupervised Domain Adaptation: Optimal Learners and Information-Theoretic Perspective",
        "abstract": "This paper studies the hardness of unsupervised domain adaptation (UDA) under covariate shift. We model the uncertainty that the learner faces by a distribution $\\pi$ in the ground-truth triples $(p, q, f)$ -- which we call a UDA class -- where $(p, q)$ is the source -- target distribution pair and $f$ is the classifier. We define the performance of a learner as the overall target domain risk, averaged over the randomness of the ground-truth triple. This formulation couples the source distribution, the target distribution and the classifier in the ground truth, and deviates from the classical worst-case analyses, which pessimistically emphasize the impact of hard but rare UDA instances. In this formulation, we precisely characterize the optimal learner. The performance of the optimal learner then allows us to define the learning difficulty for the UDA class and for the observed sample. To quantify this difficulty, we introduce an information-theoretic quantity -- Posterior Target Label Uncertainty (PTLU) -- along with its empirical estimate (EPTLU) from the sample , which capture the uncertainty in the prediction for the target domain. Briefly, PTLU is the entropy of the predicted label in the target domain under the posterior distribution of ground-truth classifier given the observed source and target samples. By proving that such a quantity serves to lower-bound the risk of any learner, we suggest that these quantities can be used as proxies for evaluating the hardness of UDA learning. We provide several examples to demonstrate the advantage of PTLU, relative to the existing measures, in evaluating the difficulty of UDA learning.",
        "arxiv_id": "2507.06552"
    },
    "2507.07102": {
        "SCORE": 15,
        "ARXIVID": "2507.07102",
        "COMMENT": "The paper discusses compositional generalization in vision models, focusing on representational structure, which is relevant to representation learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Arnas Uselis",
            "Andrea Dittadi",
            "Seong Joon Oh"
        ],
        "title": "Does Data Scaling Lead to Visual Compositional Generalization?",
        "abstract": "Compositional understanding is crucial for human intelligence, yet it remains unclear whether contemporary vision models exhibit it. The dominant machine learning paradigm is built on the premise that scaling data and model sizes will improve out-of-distribution performance, including compositional generalization. We test this premise through controlled experiments that systematically vary data scale, concept diversity, and combination coverage. We find that compositional generalization is driven by data diversity, not mere data scale. Increased combinatorial coverage forces models to discover a linearly factored representational structure, where concepts decompose into additive components. We prove this structure is key to efficiency, enabling perfect generalization from few observed combinations. Evaluating pretrained models (DINO, CLIP), we find above-random yet imperfect performance, suggesting partial presence of this structure. Our work motivates stronger emphasis on constructing diverse datasets for compositional generalization, and considering the importance of representational structure that enables efficient compositional learning. Code available at https://github.com/oshapio/visual-compositional-generalization.",
        "arxiv_id": "2507.07102"
    },
    "2507.07101": {
        "SCORE": 15,
        "ARXIVID": "2507.07101",
        "COMMENT": "The paper revisits small batch size training for language models, providing insights into training dynamics and optimizer settings, which is relevant to training dynamics in neural networks.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Martin Marek",
            "Sanae Lotfi",
            "Aditya Somasundaram",
            "Andrew Gordon Wilson",
            "Micah Goldblum"
        ],
        "title": "Small Batch Size Training for Language Models: When Vanilla SGD Works, and Why Gradient Accumulation Is Wasteful",
        "abstract": "Conventional wisdom dictates that small batch sizes make language model pretraining and fine-tuning unstable, motivating gradient accumulation, which trades off the number of optimizer steps for a proportional increase in batch size. While it is common to decrease the learning rate for smaller batch sizes, other hyperparameters are often held fixed. In this work, we revisit small batch sizes all the way down to batch size one, and we propose a rule for scaling Adam hyperparameters to small batch sizes. We find that small batch sizes (1) train stably, (2) are consistently more robust to hyperparameter choices, (3) achieve equal or better per-FLOP performance than larger batch sizes, and (4) notably enable stable language model training with vanilla SGD, even without momentum, despite storing no optimizer state. Building on these results, we provide practical recommendations for selecting a batch size and setting optimizer hyperparameters. We further recommend against gradient accumulation unless training on multiple devices with multiple model replicas, bottlenecked by inter-device bandwidth.",
        "arxiv_id": "2507.07101"
    },
    "2507.06445": {
        "SCORE": 15,
        "ARXIVID": "2507.06445",
        "COMMENT": "The paper explores interpretability as a tool for predicting out-of-distribution model behavior, which aligns with theoretical insights into model behavior.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Victoria R. Li",
            "Jenny Kaufmann",
            "Martin Wattenberg",
            "David Alvarez-Melis",
            "Naomi Saphra"
        ],
        "title": "Can Interpretation Predict Behavior on Unseen Data?",
        "abstract": "Interpretability research often aims to predict how a model will respond to targeted interventions on specific mechanisms. However, it rarely predicts how a model will respond to unseen input data. This paper explores the promises and challenges of interpretability as a tool for predicting out-of-distribution (OOD) model behavior. Specifically, we investigate the correspondence between attention patterns and OOD generalization in hundreds of Transformer models independently trained on a synthetic classification task. These models exhibit several distinct systematic generalization rules OOD, forming a diverse population for correlational analysis. In this setting, we find that simple observational tools from interpretability can predict OOD performance. In particular, when in-distribution attention exhibits hierarchical patterns, the model is likely to generalize hierarchically on OOD data -- even when the rule's implementation does not rely on these hierarchical patterns, according to ablation tests. Our findings offer a proof-of-concept to motivate further interpretability work on predicting unseen model behavior.",
        "arxiv_id": "2507.06445"
    }
}