{
    "2509.09754": {
        "SCORE": 18,
        "ARXIVID": "2509.09754",
        "COMMENT": "Model Compression and Efficiency: dynamic KV-cache eviction with per-layer and per-head budget allocation derived from minimizing residual-stream information loss in Transformers.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Yiqun Shen",
            "Song Yuan",
            "Zhengze Zhang",
            "Xiaoliang Wang",
            "Daxin Jiang",
            "Nguyen Cam-Tu"
        ],
        "title": "LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation",
        "abstract": "KV Cache is commonly used to accelerate LLM inference with long contexts, yet its high memory demand drives the need for cache compression. Existing compression methods, however, are largely heuristic and lack dynamic budget allocation. To address this limitation, we introduce a unified framework for cache compression by minimizing information loss in Transformer residual streams. Building on it, we analyze the layer attention output loss and derive a new metric to compare cache entries across heads, enabling layer-wise compression with dynamic head budgets. Additionally, by contrasting cross-layer information, we also achieve dynamic layer budgets. LAVa is the first unified strategy for cache eviction and dynamic budget allocation that, unlike prior methods, does not rely on training or the combination of multiple strategies. Experiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and InfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a new insight: dynamic layer budgets are crucial for generation tasks (e.g., code completion), while dynamic head budgets play a key role in extraction tasks (e.g., extractive QA). As a fully dynamic compression method, LAVa consistently maintains top performance across task types. Our code is available at https://github.com/MGDDestiny/Lava.",
        "arxiv_id": "2509.09754"
    },
    "2509.10406": {
        "SCORE": 18,
        "ARXIVID": "2509.10406",
        "COMMENT": "Model Efficiency/Transformer Architecture: fast softmax attention via separate query/key clustering and multipole (monopole+dipole) approximations; hierarchical causal block scheme; drop-in attention replacement.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Rupert Mitchell",
            "Kristian Kersting"
        ],
        "title": "Multipole Semantic Attention: A Fast Approximation of Softmax Attention for Pretraining",
        "abstract": "We present Multipole Semantic Attention (MuSe), an efficient approximation of softmax attention that combines semantic clustering with multipole expansions from computational physics. Our method addresses the quadratic computational complexity of transformers in the context length by clustering queries and keys separately in their learned representation spaces, enabling a hierarchical two-stage attention mechanism. Unlike prior clustering approaches that group only keys or use unified clustering, we maintain separate clusterings that respect attention's asymmetric treatment of these spaces. We augment centroid-based (monopole) approximations with dipole corrections that capture directional variance within clusters, preserving richer information during training. The method operates as a drop-in replacement for standard attention, requiring only hyperparameter specification without architectural modifications. Our approach achieves $\\mathcal{O}(NCD)$ complexity for acausal attention with $C$ clusters and $\\mathcal{O}(NCD \\log N)$ for causal attention. On isolated attention layers, we demonstrate $3\\times$ speedup over CUDNN Flash Attention at 8k context length, with relative squared errors below 20%. For causal attention, we develop a hierarchical block decomposition that combines exact local computation with efficient long-range approximation. In end-to-end pretraining of a 30M parameter model on book-length texts with 16k context, we achieve 12.2% runtime reduction with only 0.36% loss degradation, establishing the viability of multipole approximations for efficient transformer pretraining.",
        "arxiv_id": "2509.10406"
    },
    "2509.10439": {
        "SCORE": 18,
        "ARXIVID": "2509.10439",
        "COMMENT": "High-Performance/Distributed Training: algorithmic analysis and design of outer optimizers (learning rate > 1, momentum, acceleration) for Local SGD with new convergence guarantees.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Ahmed Khaled",
            "Satyen Kale",
            "Arthur Douillard",
            "Chi Jin",
            "Rob Fergus",
            "Manzil Zaheer"
        ],
        "title": "Understanding Outer Optimizers in Local SGD: Learning Rates, Momentum, and Acceleration",
        "abstract": "Modern machine learning often requires training with large batch size, distributed data, and massively parallel compute hardware (like mobile and other edge devices or distributed data centers). Communication becomes a major bottleneck in such settings but methods like Local Stochastic Gradient Descent (Local SGD) show great promise in reducing this additional communication overhead. Local SGD consists of three parts: a local optimization process, an aggregation mechanism, and an outer optimizer that uses the aggregated updates from the nodes to produce a new model. While there exists an extensive literature on understanding the impact of hyperparameters in the local optimization process, the choice of outer optimizer and its hyperparameters is less clear. We study the role of the outer optimizer in Local SGD, and prove new convergence guarantees for the algorithm. In particular, we show that tuning the outer learning rate allows us to (a) trade off between optimization error and stochastic gradient noise variance, and (b) make up for ill-tuning of the inner learning rate. Our theory suggests that the outer learning rate should sometimes be set to values greater than $1$. We extend our results to settings where we use momentum in the outer optimizer, and we show a similar role for the momentum-adjusted outer learning rate. We also study acceleration in the outer optimizer and show that it improves the convergence rate as a function of the number of communication rounds, improving upon the convergence rate of prior algorithms that apply acceleration locally. Finally, we also introduce a novel data-dependent analysis of Local SGD that yields further insights on outer learning rate tuning. We conduct comprehensive experiments with standard language models and various outer optimizers to validate our theory.",
        "arxiv_id": "2509.10439"
    },
    "2509.09737": {
        "SCORE": 18,
        "ARXIVID": "2509.09737",
        "COMMENT": "Model Architecture and Representation Learning: random-access autoregressive world model with causal structure extraction and iterative integration as control tokens enabling universal prompting and improved modeling.",
        "RELEVANCE": 9,
        "NOVELTY": 9,
        "authors": [
            "Klemen Kotar",
            "Wanhee Lee",
            "Rahul Venkatesh",
            "Honglin Chen",
            "Daniel Bear",
            "Jared Watrous",
            "Simon Kim",
            "Khai Loong Aw",
            "Lilian Naing Chen",
            "Stefan Stojanov",
            "Kevin Feigelis",
            "Imran Thobani",
            "Alex Durango",
            "Khaled Jedoui",
            "Atlas Kazemian",
            "Dan Yamins"
        ],
        "title": "World Modeling with Probabilistic Structure Integration",
        "abstract": "We present Probabilistic Structure Integration (PSI), a system for learning richly controllable and flexibly promptable world models from data. PSI consists of a three-step cycle. The first step, Probabilistic prediction, involves building a probabilistic graphical model Psi of the data, in the form of a random-access autoregressive sequence model. Psi supports a complete set of learned conditional distributions describing the dependence of any variables in the data on any other set of variables. In step 2, Structure extraction, we show how to extract underlying low-dimensional properties in the data, corresponding to a diverse set of meaningful \"intermediate structures\", in a zero-shot fashion via causal inference on Psi. Step 3, Integration, completes the cycle by converting these structures into new token types that are then continually mixed back into the training diet as conditioning signals and prediction targets. Each such cycle augments the capabilities of Psi, both allowing it to model the underlying data better, and creating new control handles -- akin to an LLM-like universal prompting language. We train an instance of Psi on 1.4 trillion tokens of internet video data; we use it to perform a variety of useful video prediction and understanding inferences; we extract state-of-the-art optical flow, self-supervised depth and object segmentation; and we use these structures to support a full cycle of predictive improvements.",
        "arxiv_id": "2509.09737"
    },
    "2509.10337": {
        "SCORE": 17,
        "ARXIVID": "2509.10337",
        "COMMENT": "Representation Learning: exact generalization error characterization for broad classes of GNNs, providing theoretical insights into structure\u2013feature alignment and generalization.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Nil Ayday",
            "Mahalakshmi Sabanayagam",
            "Debarghya Ghoshdastidar"
        ],
        "title": "Why does your graph neural network fail on some graphs? Insights from exact generalisation error",
        "abstract": "Graph Neural Networks (GNNs) are widely used in learning on graph-structured data, yet a principled understanding of why they succeed or fail remains elusive. While prior works have examined architectural limitations such as over-smoothing and over-squashing, these do not explain what enables GNNs to extract meaningful representations or why performance varies drastically between similar architectures. These questions are related to the role of generalisation: the ability of a model to make accurate predictions on unlabelled data. Although several works have derived generalisation error bounds for GNNs, these are typically loose, restricted to a single architecture, and offer limited insight into what governs generalisation in practice. In this work, we take a different approach by deriving the exact generalisation error for GNNs in a transductive fixed-design setting through the lens of signal processing. From this viewpoint, GNNs can be interpreted as graph filter operators that act on node features via the graph structure. By focusing on linear GNNs while allowing non-linearity in the graph filters, we derive the first exact generalisation error for a broad range of GNNs, including convolutional, PageRank-based, and attention-based models. The exact characterisation of the generalisation error reveals that only the aligned information between node features and graph structure contributes to generalisation. Furthermore, we quantify the effect of homophily on generalisation. Our work provides a framework that explains when and why GNNs can effectively leverage structural and feature information, offering practical guidance for model selection.",
        "arxiv_id": "2509.10337"
    },
    "2509.10033": {
        "SCORE": 16,
        "ARXIVID": "2509.10033",
        "COMMENT": "Representation Learning and Efficiency: sparse dictionary learning with a low-rank coding model, sample complexity guarantees, and a convex relaxation with convergence.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Boya Ma",
            "Abram Magner",
            "Maxwell McNeil",
            "Petko Bogdanov"
        ],
        "title": "Sparse Coding Representation of 2-way Data",
        "abstract": "Sparse dictionary coding represents signals as linear combinations of a few dictionary atoms. It has been applied to images, time series, graph signals and multi-way spatio-temporal data by jointly employing temporal and spatial dictionaries. Data-agnostic analytical dictionaries, such as the discrete Fourier transform, wavelets and graph Fourier, have seen wide adoption due to efficient implementations and good practical performance. On the other hand, dictionaries learned from data offer sparser and more accurate solutions but require learning of both the dictionaries and the coding coefficients. This becomes especially challenging for multi-dictionary scenarios since encoding coefficients correspond to all atom combinations from the dictionaries. To address this challenge, we propose a low-rank coding model for 2-dictionary scenarios and study its data complexity. Namely, we establish a bound on the number of samples needed to learn dictionaries that generalize to unseen samples from the same distribution. We propose a convex relaxation solution, called AODL, whose exact solution we show also solves the original problem. We then solve this relaxation via alternating optimization between the sparse coding matrices and the learned dictionaries, which we prove to be convergent. We demonstrate its quality for data reconstruction and missing value imputation in both synthetic and real-world datasets. For a fixed reconstruction quality, AODL learns up to 90\\% sparser solutions compared to non-low-rank and analytical (fixed) dictionary baselines. In addition, the learned dictionaries reveal interpretable insights into patterns present within the samples used for training.",
        "arxiv_id": "2509.10033"
    },
    "2509.09708": {
        "SCORE": 16,
        "ARXIVID": "2509.09708",
        "COMMENT": "Matches Representation Learning criterion: uses sparse autoencoders on residual-stream activations to identify and causally manipulate refusal-mediating features, providing mechanistic insight via sparse features and interactions.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Nirmalendu Prakash",
            "Yeo Wei Jie",
            "Amir Abdullah",
            "Ranjan Satapathy",
            "Erik Cambria",
            "Roy Ka Wei Lee"
        ],
        "title": "Beyond I'm Sorry, I Can't: Dissecting Large Language Model Refusal",
        "abstract": "Refusal on harmful prompts is a key safety behaviour in instruction-tuned large language models (LLMs), yet the internal causes of this behaviour remain poorly understood. We study two public instruction-tuned models, Gemma-2-2B-IT and LLaMA-3.1-8B-IT, using sparse autoencoders (SAEs) trained on residual-stream activations. Given a harmful prompt, we search the SAE latent space for feature sets whose ablation flips the model from refusal to compliance, demonstrating causal influence and creating a jailbreak. Our search proceeds in three stages: (1) Refusal Direction: find a refusal-mediating direction and collect SAE features near that direction; (2) Greedy Filtering: prune to a minimal set; and (3) Interaction Discovery: fit a factorization machine (FM) that captures nonlinear interactions among the remaining active features and the minimal set. This pipeline yields a broad set of jailbreak-critical features, offering insight into the mechanistic basis of refusal. Moreover, we find evidence of redundant features that remain dormant unless earlier features are suppressed. Our findings highlight the potential for fine-grained auditing and targeted intervention in safety behaviours by manipulating the interpretable latent space.",
        "arxiv_id": "2509.09708"
    },
    "2509.09955": {
        "SCORE": 16,
        "ARXIVID": "2509.09955",
        "COMMENT": "Model Compression and Efficiency: training-free adaptive token merging for Transformers with per-layer similarity thresholds and Pareto-optimized trade-offs in compute/communication.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Omar Erak",
            "Omar Alhussein",
            "Hatem Abou-Zeid",
            "Mehdi Bennis",
            "Sami Muhaidat"
        ],
        "title": "Adaptive Token Merging for Efficient Transformer Semantic Communication at the Edge",
        "abstract": "Large-scale transformers are central to modern semantic communication, yet their high computational and communication costs hinder deployment on resource-constrained edge devices. This paper introduces a training-free framework for adaptive token merging, a novel mechanism that compresses transformer representations at runtime by selectively merging semantically redundant tokens under per-layer similarity thresholds. Unlike prior fixed-ratio reduction, our approach couples merging directly to input redundancy, enabling data-dependent adaptation that balances efficiency and task relevance without retraining. We cast the discovery of merging strategies as a multi-objective optimization problem and leverage Bayesian optimization to obtain Pareto-optimal trade-offs between accuracy, inference cost, and communication cost. On ImageNet classification, we match the accuracy of the unmodified transformer with 30\\% fewer floating-point operations per second and under 20\\% of the original communication cost, while for visual question answering our method achieves performance competitive with the full LLaVA model at less than one-third of the compute and one-tenth of the bandwidth. Finally, we show that our adaptive merging is robust across varying channel conditions and provides inherent privacy benefits, substantially degrading the efficacy of model inversion attacks. Our framework provides a practical and versatile solution for deploying powerful transformer models in resource-limited edge intelligence scenarios.",
        "arxiv_id": "2509.09955"
    },
    "2509.10367": {
        "SCORE": 16,
        "ARXIVID": "2509.10367",
        "COMMENT": "Representation Learning: frames dataset condensation under discrepancy-based distribution matching, providing a unified theoretical objective beyond task-specific generalization.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Tong Chen",
            "Raghavendra Selvan"
        ],
        "title": "A Discrepancy-Based Perspective on Dataset Condensation",
        "abstract": "Given a dataset of finitely many elements $\\mathcal{T} = \\{\\mathbf{x}_i\\}_{i = 1}^N$, the goal of dataset condensation (DC) is to construct a synthetic dataset $\\mathcal{S} = \\{\\tilde{\\mathbf{x}}_j\\}_{j = 1}^M$ which is significantly smaller ($M \\ll N$) such that a model trained from scratch on $\\mathcal{S}$ achieves comparable or even superior generalization performance to a model trained on $\\mathcal{T}$. Recent advances in DC reveal a close connection to the problem of approximating the data distribution represented by $\\mathcal{T}$ with a reduced set of points. In this work, we present a unified framework that encompasses existing DC methods and extend the task-specific notion of DC to a more general and formal definition using notions of discrepancy, which quantify the distance between probability distribution in different regimes. Our framework broadens the objective of DC beyond generalization, accommodating additional objectives such as robustness, privacy, and other desirable properties.",
        "arxiv_id": "2509.10367"
    },
    "2509.10363": {
        "SCORE": 16,
        "ARXIVID": "2509.10363",
        "COMMENT": "Model Architecture: structure-preserving operator learning via conditional neural Whitney forms with transformer-based attention, enforcing discrete conservation.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Benjamin David Shaffer",
            "Brooks Kinch",
            "Joseph Klobusicky",
            "M. Ani Hsieh",
            "Nathaniel Trask"
        ],
        "title": "Physics-informed sensor coverage through structure preserving machine learning",
        "abstract": "We present a machine learning framework for adaptive source localization in which agents use a structure-preserving digital twin of a coupled hydrodynamic-transport system for real-time trajectory planning and data assimilation. The twin is constructed with conditional neural Whitney forms (CNWF), coupling the numerical guarantees of finite element exterior calculus (FEEC) with transformer-based operator learning. The resulting model preserves discrete conservation, and adapts in real time to streaming sensor data. It employs a conditional attention mechanism to identify: a reduced Whitney-form basis; reduced integral balance equations; and a source field, each compatible with given sensor measurements. The induced reduced-order environmental model retains the stability and consistency of standard finite-element simulation, yielding a physically realizable, regular mapping from sensor data to the source field. We propose a staggered scheme that alternates between evaluating the digital twin and applying Lloyd's algorithm to guide sensor placement, with analysis providing conditions for monotone improvement of a coverage functional. Using the predicted source field as an importance function within an optimal-recovery scheme, we demonstrate recovery of point sources under continuity assumptions, highlighting the role of regularity as a sufficient condition for localization. Experimental comparisons with physics-agnostic transformer architectures show improved accuracy in complex geometries when physical constraints are enforced, indicating that structure preservation provides an effective inductive bias for source identification.",
        "arxiv_id": "2509.10363"
    },
    "2509.09793": {
        "SCORE": 15,
        "ARXIVID": "2509.09793",
        "COMMENT": "Optimization/Representation Learning: Plug-and-Play with denoisers trained to be explicit gradient/prox operators and associated convergent algorithms.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Vincent Herfeld",
            "Baudouin Denis de Senneville",
            "Arthur Leclaire",
            "Nicolas Papadakis"
        ],
        "title": "From the Gradient-Step Denoiser to the Proximal Denoiser and their associated convergent Plug-and-Play algorithms",
        "abstract": "In this paper we analyze the Gradient-Step Denoiser and its usage in Plug-and-Play algorithms. The Plug-and-Play paradigm of optimization algorithms uses off the shelf denoisers to replace a proximity operator or a gradient descent operator of an image prior. Usually this image prior is implicit and cannot be expressed, but the Gradient-Step Denoiser is trained to be exactly the gradient descent operator or the proximity operator of an explicit functional while preserving state-of-the-art denoising capabilities.",
        "arxiv_id": "2509.09793"
    },
    "2509.10000": {
        "SCORE": 15,
        "ARXIVID": "2509.10000",
        "COMMENT": "Training dynamics/scaling laws: empirical neural scaling laws for deep regression across architectures, providing foundational insights.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Tilen Cadez",
            "Kyoung-Min Kim"
        ],
        "title": "Neural Scaling Laws for Deep Regression",
        "abstract": "Neural scaling laws--power-law relationships between generalization errors and characteristics of deep learning models--are vital tools for developing reliable models while managing limited resources. Although the success of large language models highlights the importance of these laws, their application to deep regression models remains largely unexplored. Here, we empirically investigate neural scaling laws in deep regression using a parameter estimation model for twisted van der Waals magnets. We observe power-law relationships between the loss and both training dataset size and model capacity across a wide range of values, employing various architectures--including fully connected networks, residual networks, and vision transformers. Furthermore, the scaling exponents governing these relationships range from 1 to 2, with specific values depending on the regressed parameters and model details. The consistent scaling behaviors and their large scaling exponents suggest that the performance of deep regression models can improve substantially with increasing data size.",
        "arxiv_id": "2509.10000"
    },
    "2509.09836": {
        "SCORE": 15,
        "ARXIVID": "2509.09836",
        "COMMENT": "Model Compression and Efficiency: unified audio autoencoder using quantization (FSQ with FSQ-dropout) to produce both continuous and discrete compressed codes and enable efficient parallel decoding.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Marco Pasini",
            "Stefan Lattner",
            "George Fazekas"
        ],
        "title": "CoDiCodec: Unifying Continuous and Discrete Compressed Representations of Audio",
        "abstract": "Efficiently representing audio signals in a compressed latent space is critical for latent generative modelling. However, existing autoencoders often force a choice between continuous embeddings and discrete tokens. Furthermore, achieving high compression ratios while maintaining audio fidelity remains a challenge. We introduce CoDiCodec, a novel audio autoencoder that overcomes these limitations by both efficiently encoding global features via summary embeddings, and by producing both compressed continuous embeddings at ~ 11 Hz and discrete tokens at a rate of 2.38 kbps from the same trained model, offering unprecedented flexibility for different downstream generative tasks. This is achieved through Finite Scalar Quantization (FSQ) and a novel FSQ-dropout technique, and does not require additional loss terms beyond the single consistency loss used for end-to-end training. CoDiCodec supports both autoregressive decoding and a novel parallel decoding strategy, with the latter achieving superior audio quality and faster decoding. CoDiCodec outperforms existing continuous and discrete autoencoders at similar bitrates in terms of reconstruction audio quality. Our work enables a unified approach to audio compression, bridging the gap between continuous and discrete generative modelling paradigms.",
        "arxiv_id": "2509.09836"
    },
    "2509.09864": {
        "SCORE": 14,
        "ARXIVID": "2509.09864",
        "COMMENT": "Model Compression and Efficiency: latency- and token-aware dynamic allocation of test-time compute with per-query method selection (e.g., beam search vs best-of-N).",
        "RELEVANCE": 7,
        "NOVELTY": 7,
        "authors": [
            "Jenny Y. Huang",
            "Mehul Damani",
            "Yousef El-Kurdi",
            "Ramon Astudillo",
            "Wei Sun"
        ],
        "title": "Latency and Token-Aware Test-Time Compute",
        "abstract": "Inference-time scaling has emerged as a powerful way to improve large language model (LLM) performance by generating multiple candidate responses and selecting among them. However, existing work on dynamic allocation for test-time compute typically considers only parallel generation methods such as best-of-N, overlooking incremental decoding methods like beam search, and has largely ignored latency, focusing only on token usage. We formulate inference-time scaling as a problem of dynamic compute allocation and method selection, where the system must decide which strategy to apply and how much compute to allocate on a per-query basis. Our framework explicitly incorporates both token cost and wall-clock latency, the latter being critical for user experience and particularly for agentic workflows where models must issue multiple queries efficiently. Experiments on reasoning benchmarks show that our approach consistently outperforms static strategies, achieving favorable accuracy-cost trade-offs while remaining practical for deployment.",
        "arxiv_id": "2509.09864"
    },
    "2509.10369": {
        "SCORE": 14,
        "ARXIVID": "2509.10369",
        "COMMENT": "Representation Learning: investigates how pretraining data distribution affects contrastive representations and introduces an IDB pretraining strategy to improve OOD robustness.",
        "RELEVANCE": 7,
        "NOVELTY": 7,
        "authors": [
            "Gul Rukh Khattak",
            "Konstantinos Patlatzoglou",
            "Joseph Barker",
            "Libor Pastika",
            "Boroumand Zeidaabadi",
            "Ahmed El-Medany",
            "Hesham Aggour",
            "Yixiu Liang",
            "Antonio H. Ribeiro",
            "Jeffrey Annis",
            "Antonio Luiz Pinho Ribeiro",
            "Junbo Ge",
            "Daniel B. Kramer",
            "Jonathan W. Waks",
            "Evan Brittain",
            "Nicholas Peters",
            "Fu Siong Ng",
            "Arunashis Sau"
        ],
        "title": "Data distribution impacts the performance and generalisability of contrastive learning-based foundation models of electrocardiograms",
        "abstract": "Contrastive learning is a widely adopted self-supervised pretraining strategy, yet its dependence on cohort composition remains underexplored. We present Contrasting by Patient Augmented Electrocardiograms (CAPE) foundation model and pretrain on four cohorts (n = 5,203,352), from diverse populations across three continents (North America, South America, Asia). We systematically assess how cohort demographics, health status, and population diversity influence the downstream performance for prediction tasks also including two additional cohorts from another continent (Europe). We find that downstream performance depends on the distributional properties of the pretraining cohort, including demographics and health status. Moreover, while pretraining with a multi-centre, demographically diverse cohort improves in-distribution accuracy, it reduces out-of-distribution (OOD) generalisation of our contrastive approach by encoding cohort-specific artifacts. To address this, we propose the In-Distribution Batch (IDB) strategy, which preserves intra-cohort consistency during pretraining and enhances OOD robustness. This work provides important insights for developing clinically fair and generalisable foundation models.",
        "arxiv_id": "2509.10369"
    }
}