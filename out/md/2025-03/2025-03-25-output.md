# Personalized Daily Arxiv Papers 3/25/2025

| *[gpt-4o]*   | Prompt   | Completion   | Total   |
|:--------------:|:----------:|:--------------:|:---------:|
| **Token**    | 60688    | 8612         | 69300   |
| **Cost**     | $0.15    | $0.09        | $0.24   |

Total arXiv papers: 759

Total scanned papers: 449

Total relevant papers: 45

**Table of contents with paper titles:**

1. [Trajectory Balance with Asynchrony: Decoupling Exploration and Learning for Fast, Scalable LLM Post-Training](#user-content-link1)
**Authors:** Brian R. Bartoldson, Siddarth Venkatraman, James Diffenderfer, Moksh Jain, Tal Ben-Nun, Seanie Lee, Minsu Kim, Johan Obando-Ceron, Yoshua Bengio, Bhavya Kailkhura

2. [Feature Learning beyond the Lazy-Rich Dichotomy: Insights from Representational Geometry](#user-content-link2)
**Authors:** Chi-Ning Chou, Hang Le, Yichen Wang, SueYeon Chung

3. [Intelligence Sequencing and the Path-Dependence of Intelligence Evolution: AGI-First vs. DCI-First as Irreversible Attractors](#user-content-link3)
**Authors:** Andy E. Williams

4. [Self-Organizing Graph Reasoning Evolves into a Critical State for Continuous Discovery Through Structural-Semantic Dynamics](#user-content-link4)
**Authors:** Markus J. Buehler

5. [Learning Multi-Level Features with Matryoshka Sparse Autoencoders](#user-content-link5)
**Authors:** Bart Bussmann, Noa Nabeshima, Adam Karvonen, Neel Nanda

6. [Reasoning to Learn from Latent Thoughts](#user-content-link6)
**Authors:** Yangjun Ruan, Neil Band, Chris J. Maddison, Tatsunori Hashimoto

7. [OCRT: Boosting Foundation Models in the Open World with Object-Concept-Relation Triad](#user-content-link7)
**Authors:** Luyao Tang, Yuxuan Yuan, Chaoqi Chen, Zeyu Zhang, Yue Huang, Kun Zhang

8. [Decoupling Angles and Strength in Low-rank Adaptation](#user-content-link8)
**Authors:** Massimo Bini, Leander Girrbach, Zeynep Akata

9. [Optimal Neural Compressors for the Rate-Distortion-Perception Tradeoff](#user-content-link9)
**Authors:** Eric Lei, Hamed Hassani, Shirin Saeedi Bidokhti

10. [Improving Quantization with Post-Training Model Expansion](#user-content-link10)
**Authors:** Giuseppe Franco, Pablo Monteagudo-Lago, Ian Colbert, Nicholas Fraser, Michaela Blott

11. [xKV: Cross-Layer SVD for KV-Cache Compression](#user-content-link11)
**Authors:** Chi-Chih Chang, Chien-Yu Lin, Yash Akhauri, Wei-Cheng Lin, Kai-Chiang Wu, Luis Ceze, Mohamed S. Abdelfattah

12. [Theory-to-Practice Gap for Neural Networks and Neural Operators](#user-content-link12)
**Authors:** Philipp Grohs, Samuel Lanthaler, Margaret Trautner

13. [BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with Low-Bit KV Cache](#user-content-link13)
**Authors:** Dayou Du, Shijie Cao, Jianyi Cheng, Ting Cao, Mao Yang

14. [Variance Control via Weight Rescaling in LLM Pre-training](#user-content-link14)
**Authors:** Louis Owen, Abhay Kumar, Nilabhra Roy Chowdhury, Fabian G\"ura

15. [TopV: Compatible Token Pruning with Inference Time Optimization for Fast and Low-Memory Multimodal Vision Language Model](#user-content-link15)
**Authors:** Cheng Yang, Yang Sui, Jinqi Xiao, Lingyi Huang, Yu Gong, Chendi Li, Jinghua Yan, Yu Bai, Ponnuswamy Sadayappan, Xia Hu, Bo Yuan

16. [Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV Cache Quantization](#user-content-link16)
**Authors:** Minsu Kim, Seongmin Hong, RyeoWook Ko, Soongyu Choi, Hunjong Lee, Junsoo Kim, Joo-Young Kim, Jongse Park

17. [Feature Qualification by Deep Nets: A Constructive Approach](#user-content-link17)
**Authors:** Feilong Cao, Shao-Bo Lin

18. [Bayesian Teaching Enables Probabilistic Reasoning in Large Language Models](#user-content-link18)
**Authors:** Linlu Qiu, Fei Sha, Kelsey Allen, Yoon Kim, Tal Linzen, Sjoerd van Steenkiste

19. [Maximum Redundancy Pruning: A Principle-Driven Layerwise Sparsity Allocation for LLMs](#user-content-link19)
**Authors:** Chang Gao, Kang Zhao, Jianfei Chen, Liping Jing

20. [FFN Fusion: Rethinking Sequential Computation in Large Language Models](#user-content-link20)
**Authors:** Akhiad Bercovich, Mohammad Dabbah, Omri Puny, Ido Galil, Amnon Geifman, Yonatan Geifman, Izhak Golan, Ehud Karpas, Itay Levy, Zach Moshe, Najeeb Nabwani, Tomer Ronen, Itamar Schen, Elad Segal, Ido Shahaf, Oren Tropp, Ran Zilberstein, Ran El-Yaniv

21. [Adaptive Rank Allocation: Speeding Up Modern Transformers with RaNA Adapters](#user-content-link21)
**Authors:** Roberto Garcia, Jerry Liu, Daniel Sorvisto, Sabri Eyuboglu

22. [Efficient Knowledge Distillation via Curriculum Extraction](#user-content-link22)
**Authors:** Shivam Gupta, Sushrut Karmalkar

23. [CODA: Repurposing Continuous VAEs for Discrete Tokenization](#user-content-link23)
**Authors:** Zeyu Liu, Zanlin Ni, Yeguo Hua, Xin Deng, Xiao Ma, Cheng Zhong, Gao Huang

24. [Generative AI for Validating Physics Laws](#user-content-link24)
**Authors:** Maria Nareklishvili, Nicholas Polson, Vadim Sokolov

25. [What's Producible May Not Be Reachable: Measuring the Steerability of Generative Models](#user-content-link25)
**Authors:** Keyon Vafa, Sarah Bentley, Jon Kleinberg, Sendhil Mullainathan

26. [AutoBayes: A Compositional Framework for Generalized Variational Inference](#user-content-link26)
**Authors:** Toby St Clere Smithe, Marco Perin

27. [Does GCL Need a Large Number of Negative Samples? Enhancing Graph Contrastive Learning with Effective and Efficient Negative Sampling](#user-content-link27)
**Authors:** Yongqi Huang, Jitao Zhao, Dongxiao He, Di Jin, Yuxiao Huang, Zhen Wang

28. [On the Minimax Regret of Sequential Probability Assignment via Square-Root Entropy](#user-content-link28)
**Authors:** Zeyu Jia, Yury Polyanskiy, Alexander Rakhlin

29. [Dynamic Gradient Sparse Update for Edge Training](#user-content-link29)
**Authors:** I-Hsuan Li, Tian-Sheuan Chang

30. [Neuromorphic Principles for Efficient Large Language Models on Intel Loihi 2](#user-content-link30)
**Authors:** Steven Abreu, Sumit Bam Shrestha, Rui-Jie Zhu, Jason Eshraghian

31. [On the Optimality of Single-label and Multi-label Neural Network Decoders](#user-content-link31)
**Authors:** Yunus Can G\"ultekin, P\'eter Scheepers, Yuncheng Yuan, Federico Corradi, Alex Alvarado

32. [Improving Preference Extraction In LLMs By Identifying Latent Knowledge Through Classifying Probes](#user-content-link32)
**Authors:** Sharan Maiya, Yinhong Liu, Ramit Debnath, Anna Korhonen

33. [Generative Modeling of Class Probability for Multi-Modal Representation Learning](#user-content-link33)
**Authors:** Jungkyoo Shin, Bumsoo Kim, Eunwoo Kim

34. [Language Models May Verbatim Complete TextThey Were Not Explicitly Trained On](#user-content-link34)
**Authors:** Ken Ziyu Liu, Christopher A. Choquette-Choo, Matthew Jagielski, Peter Kairouz, Sanmi Koyejo, Percy Liang, Nicolas Papernot

35. [Towards Human-Understandable Multi-Dimensional Concept Discovery](#user-content-link35)
**Authors:** Arne Grobr\"ugge, Niklas K\"uhl, Gerhard Satzger, Philipp Spitzer

36. [Exploring Energy Landscapes for Minimal Counterfactual Explanations: Applications in Cybersecurity and Beyond](#user-content-link36)
**Authors:** Spyridon Evangelatos, Eleni Veroni, Vasilis Efthymiou, Christos Nikolopoulos, Georgios Th. Papadopoulos, Panagiotis Sarigiannidis

37. [TARDIS: Mitigate Temporal Misalignment via Representation Steering](#user-content-link37)
**Authors:** Changho Shin, Xinya Yan, Suenggwan Jo, Sungjun Cho, Shourjo Aditya Chaudhuri, Frederic Sala

38. [Interpretable Feature Interaction via Statistical Self-supervised Learning on Tabular Data](#user-content-link38)
**Authors:** Xiaochen Zhang, Haoyi Xiong

39. [Bayesian generative models can flag performance loss, bias, and out-of-distribution image content](#user-content-link39)
**Authors:** Miguel L\'opez-P\'erez, Marco Miani, Valery Naranjo, S{\o}ren Hauberg, Aasa Feragen

40. [Distil-xLSTM: Learning Attention Mechanisms through Recurrent Structures](#user-content-link40)
**Authors:** Abdoul Majid O. Thiombiano, Brahim Hnich, Ali Ben Mrad, Mohamed Wiem Mkaouer

41. [ConSol: Sequential Probability Ratio Testing to Find Consistent LLM Reasoning Paths Efficiently](#user-content-link41)
**Authors:** Jaeyeon Lee, Guantong Qi, Matthew Brady Neeley, Zhandong Liu, Hyun-Hwan Jeong

42. [Every Sample Matters: Leveraging Mixture-of-Experts and High-Quality Data for Efficient and Accurate Code LLM](#user-content-link42)
**Authors:** Codefuse, Ling Team, :, Wenting Cai, Yuchen Cao, Chaoyu Chen, Chen Chen, Siba Chen, Qing Cui, Peng Di, Junpeng Fang, Zi Gong, Ting Guo, Zhengyu He, Yang Huang, Cong Li, Jianguo Li, Zheng Li, Shijie Lian, BingChang Liu, Songshan Luo, Shuo Mao, Min Shen, Jian Wu, Jiaolong Yang, Wenjie Yang, Tong Ye, Hang Yu, Wei Zhang, Zhenduo Zhang, Hailin Zhao, Xunjin Zheng, Jun Zhou

43. [Do Your Best and Get Enough Rest for Continual Learning](#user-content-link43)
**Authors:** Hankyul Kang, Gregor Seifer, Donghyun Lee, Jongbin Ryu

44. [MoST: Efficient Monarch Sparse Tuning for 3D Representation Learning](#user-content-link44)
**Authors:** Xu Han, Yuan Tang, Jinfeng Xu, Xianzhi Li

45. [Neural Network Approach to Stochastic Dynamics for Smooth Multimodal Density Estimation](#user-content-link45)
**Authors:** Z. Zarezadeh, N. Zarezadeh

---

## 1. [Trajectory Balance with Asynchrony: Decoupling Exploration and Learning for Fast, Scalable LLM Post-Training](https://arxiv.org/abs/2503.18929) <a id="link1"></a>

**ArXiv ID:** 2503.18929

**Authors:** Brian R. Bartoldson, Siddarth Venkatraman, James Diffenderfer, Moksh Jain, Tal Ben-Nun, Seanie Lee, Minsu Kim, Johan Obando-Ceron, Yoshua Bengio, Bhavya Kailkhura

**Abstract:** Reinforcement learning (RL) is a critical component of large language model (LLM) post-training. However, existing on-policy algorithms used for post-training are inherently incompatible with the use of experience replay buffers, which can be populated scalably by distributed off-policy actors to enhance exploration as compute increases. We propose efficiently obtaining this benefit of replay buffers via Trajectory Balance with Asynchrony (TBA), a massively scalable LLM RL system. In contrast to existing approaches, TBA uses a larger fraction of compute on search, constantly generating off-policy data for a central replay buffer. A training node simultaneously samples data from this buffer based on reward or recency to update the policy using Trajectory Balance (TB), a diversity-seeking RL objective introduced for GFlowNets. TBA offers three key advantages: (1) decoupled training and search, speeding up training wall-clock time by 4x or more; (2) improved diversity through large-scale off-policy sampling; and (3) scalable search for sparse reward settings. On mathematical reasoning, preference-tuning, and automated red-teaming (diverse and representative post-training tasks), TBA produces speed and performance improvements over strong baselines.

**Comment:** Author match



---

## 2. [Feature Learning beyond the Lazy-Rich Dichotomy: Insights from Representational Geometry](https://arxiv.org/abs/2503.18114) <a id="link2"></a>

**ArXiv ID:** 2503.18114

**Authors:** Chi-Ning Chou, Hang Le, Yichen Wang, SueYeon Chung

**Abstract:** The ability to integrate task-relevant information into neural representations is a fundamental aspect of both biological and artificial intelligence. To enable theoretical analysis, recent work has examined whether a network learns task-relevant features (rich learning) or resembles a random feature model (or a kernel machine, i.e., lazy learning). However, this simple lazy-versus-rich dichotomy overlooks the possibility of various subtypes of feature learning that emerge from different architectures, learning rules, and data properties. Furthermore, most existing approaches emphasize weight matrices or neural tangent kernels, limiting their applicability to neuroscience because they do not explicitly characterize representations.   In this work, we introduce an analysis framework based on representational geometry to study feature learning. Instead of analyzing what are the learned features, we focus on characterizing how task-relevant representational manifolds evolve during the learning process. In both theory and experiment, we find that when a network learns features useful for solving a task, the task-relevant manifolds become increasingly untangled. Moreover, by tracking changes in the underlying manifold geometry, we uncover distinct learning stages throughout training, as well as different learning strategies associated with training hyperparameters, uncovering subtypes of feature learning beyond the lazy-versus-rich dichotomy. Applying our method to neuroscience and machine learning, we gain geometric insights into the structural inductive biases of neural circuits solving cognitive tasks and the mechanisms underlying out-of-distribution generalization in image classification. Our framework provides a novel geometric perspective for understanding and quantifying feature learning in both artificial and biological neural networks.

**Comment:** The paper introduces a geometric framework for analyzing feature learning, providing novel insights into representational geometry and task-relevant manifold evolution, which is highly relevant to representation learning.

**Relevance:** 10
**Novelty:** 9

---

## 3. [Intelligence Sequencing and the Path-Dependence of Intelligence Evolution: AGI-First vs. DCI-First as Irreversible Attractors](https://arxiv.org/abs/2503.17688) <a id="link3"></a>

**ArXiv ID:** 2503.17688

**Authors:** Andy E. Williams

**Abstract:** The trajectory of intelligence evolution is often framed around the emergence of artificial general intelligence (AGI) and its alignment with human values. This paper challenges that framing by introducing the concept of intelligence sequencing: the idea that the order in which AGI and decentralized collective intelligence (DCI) emerge determines the long-term attractor basin of intelligence. Using insights from dynamical systems, evolutionary game theory, and network models, it argues that intelligence follows a path-dependent, irreversible trajectory. Once development enters a centralized (AGI-first) or decentralized (DCI-first) regime, transitions become structurally infeasible due to feedback loops and resource lock-in. Intelligence attractors are modeled in functional state space as the co-navigation of conceptual and adaptive fitness spaces. Early-phase structuring constrains later dynamics, much like renormalization in physics. This has major implications for AI safety: traditional alignment assumes AGI will emerge and must be controlled after the fact, but this paper argues that intelligence sequencing is more foundational. If AGI-first architectures dominate before DCI reaches critical mass, hierarchical monopolization and existential risk become locked in. If DCI-first emerges, intelligence stabilizes around decentralized cooperative equilibrium. The paper further explores whether intelligence structurally biases itself toward an attractor based on its self-modeling method -- externally imposed axioms (favoring AGI) vs. recursive internal visualization (favoring DCI). Finally, it proposes methods to test this theory via simulations, historical lock-in case studies, and intelligence network analysis. The findings suggest that intelligence sequencing is a civilizational tipping point: determining whether the future is shaped by unbounded competition or unbounded cooperation.

**Comment:** The paper explores intelligence sequencing and path-dependence in intelligence evolution, introducing a novel theoretical framework. It aligns with emerging trends and challenges established assumptions.

**Relevance:** 9
**Novelty:** 9

---

## 4. [Self-Organizing Graph Reasoning Evolves into a Critical State for Continuous Discovery Through Structural-Semantic Dynamics](https://arxiv.org/abs/2503.18852) <a id="link4"></a>

**ArXiv ID:** 2503.18852

**Authors:** Markus J. Buehler

**Abstract:** We report fundamental insights into how agentic graph reasoning systems spontaneously evolve toward a critical state that sustains continuous semantic discovery. By rigorously analyzing structural (Von Neumann graph entropy) and semantic (embedding) entropy, we identify a subtle yet robust regime in which semantic entropy persistently dominates over structural entropy. This interplay is quantified by a dimensionless Critical Discovery Parameter that stabilizes at a small negative value, indicating a consistent excess of semantic entropy. Empirically, we observe a stable fraction (12%) of "surprising" edges, links between semantically distant concepts, providing evidence of long-range or cross-domain connections that drive continuous innovation. Concomitantly, the system exhibits scale-free and small-world topological features, alongside a negative cross-correlation between structural and semantic measures, reinforcing the analogy to self-organized criticality. These results establish clear parallels with critical phenomena in physical, biological, and cognitive complex systems, revealing an entropy-based principle governing adaptability and continuous innovation. Crucially, semantic richness emerges as the underlying driver of sustained exploration, despite not being explicitly used by the reasoning process. Our findings provide interdisciplinary insights and practical strategies for engineering intelligent systems with intrinsic capacities for long-term discovery and adaptation, and offer insights into how model training strategies can be developed that reinforce critical discovery.

**Comment:** This paper provides theoretical insights into self-organizing graph reasoning systems and their evolution into a critical state, which aligns with emerging trends and foundational research. The entropy-based principle governing adaptability and innovation is novel and interdisciplinary.

**Relevance:** 9
**Novelty:** 9

---

## 5. [Learning Multi-Level Features with Matryoshka Sparse Autoencoders](https://arxiv.org/abs/2503.17547) <a id="link5"></a>

**ArXiv ID:** 2503.17547

**Authors:** Bart Bussmann, Noa Nabeshima, Adam Karvonen, Neel Nanda

**Abstract:** Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting neural networks by extracting the concepts represented in their activations. However, choosing the size of the SAE dictionary (i.e. number of learned concepts) creates a tension: as dictionary size increases to capture more relevant concepts, sparsity incentivizes features to be split or absorbed into more specific features, leaving high-level features missing or warped. We introduce Matryoshka SAEs, a novel variant that addresses these issues by simultaneously training multiple nested dictionaries of increasing size, forcing the smaller dictionaries to independently reconstruct the inputs without using the larger dictionaries. This organizes features hierarchically - the smaller dictionaries learn general concepts, while the larger dictionaries learn more specific concepts, without incentive to absorb the high-level features. We train Matryoshka SAEs on Gemma-2-2B and TinyStories and find superior performance on sparse probing and targeted concept erasure tasks, more disentangled concept representations, and reduced feature absorption. While there is a minor tradeoff with reconstruction performance, we believe Matryoshka SAEs are a superior alternative for practical tasks, as they enable training arbitrarily large SAEs while retaining interpretable features at different levels of abstraction.

**Comment:** The introduction of Matryoshka Sparse Autoencoders directly contributes to representation learning by addressing hierarchical feature learning and disentanglement, which is highly relevant.

**Relevance:** 9
**Novelty:** 8

---

## 6. [Reasoning to Learn from Latent Thoughts](https://arxiv.org/abs/2503.18866) <a id="link6"></a>

**ArXiv ID:** 2503.18866

**Authors:** Yangjun Ruan, Neil Band, Chris J. Maddison, Tatsunori Hashimoto

**Abstract:** Compute scaling for language model (LM) pretraining has outpaced the growth of human-written texts, leading to concerns that data will become the bottleneck to LM scaling. To continue scaling pretraining in this data-constrained regime, we propose that explicitly modeling and inferring the latent thoughts that underlie the text generation process can significantly improve pretraining data efficiency. Intuitively, our approach views web text as the compressed final outcome of a verbose human thought process and that the latent thoughts contain important contextual knowledge and reasoning steps that are critical to data-efficient learning. We empirically demonstrate the effectiveness of our approach through data-constrained continued pretraining for math. We first show that synthetic data approaches to inferring latent thoughts significantly improve data efficiency, outperforming training on the same amount of raw data (5.7\% $\rightarrow$ 25.4\% on MATH). Furthermore, we demonstrate latent thought inference without a strong teacher, where an LM bootstraps its own performance by using an EM algorithm to iteratively improve the capability of the trained LM and the quality of thought-augmented pretraining data. We show that a 1B LM can bootstrap its performance across at least three iterations and significantly outperform baselines trained on raw data, with increasing gains from additional inference compute when performing the E-step. The gains from inference scaling and EM iterations suggest new opportunities for scaling data-constrained pretraining.

**Comment:** The paper explores latent thought modeling for data-efficient pretraining, which is relevant to foundational research in large language models and representation learning.

**Relevance:** 9
**Novelty:** 8

---

## 7. [OCRT: Boosting Foundation Models in the Open World with Object-Concept-Relation Triad](https://arxiv.org/abs/2503.18695) <a id="link7"></a>

**ArXiv ID:** 2503.18695

**Authors:** Luyao Tang, Yuxuan Yuan, Chaoqi Chen, Zeyu Zhang, Yue Huang, Kun Zhang

**Abstract:** Although foundation models (FMs) claim to be powerful, their generalization ability significantly decreases when faced with distribution shifts, weak supervision, or malicious attacks in the open world. On the other hand, most domain generalization or adversarial fine-tuning methods are task-related or model-specific, ignoring the universality in practical applications and the transferability between FMs. This paper delves into the problem of generalizing FMs to the out-of-domain data. We propose a novel framework, the Object-Concept-Relation Triad (OCRT), that enables FMs to extract sparse, high-level concepts and intricate relational structures from raw visual inputs. The key idea is to bind objects in visual scenes and a set of object-centric representations through unsupervised decoupling and iterative refinement. To be specific, we project the object-centric representations onto a semantic concept space that the model can readily interpret and estimate their importance to filter out irrelevant elements. Then, a concept-based graph, which has a flexible degree, is constructed to incorporate the set of concepts and their corresponding importance, enabling the extraction of high-order factors from informative concepts and facilitating relational reasoning among these concepts. Extensive experiments demonstrate that OCRT can substantially boost the generalizability and robustness of SAM and CLIP across multiple downstream tasks.

**Comment:** The paper proposes a novel framework (OCRT) for improving generalization in foundation models, which aligns with representation learning and architectural innovations.

**Relevance:** 9
**Novelty:** 8

---

## 8. [Decoupling Angles and Strength in Low-rank Adaptation](https://arxiv.org/abs/2503.18225) <a id="link8"></a>

**ArXiv ID:** 2503.18225

**Authors:** Massimo Bini, Leander Girrbach, Zeynep Akata

**Abstract:** Parameter-Efficient FineTuning (PEFT) methods have recently gained significant popularity thanks to the widespread availability of large-scale pretrained models. These methods allow for quick adaptation to downstream tasks with minimal computational cost. However, popular finetuning methods such as LoRA exhibit limited robustness when it comes to hyperparameter choices or extended training regimes, preventing optimal out-of-the-box performance. In contrast, bounded approaches, such as ETHER, provide greater robustness but are limited to extremely low-rank adaptations and fixed-strength transformations, reducing their adaptation expressive power. In this work, we propose Decoupled Low-rank Adaptation (DeLoRA), a novel finetuning method that normalizes and scales learnable low-rank matrices. By bounding the distance of the transformation, DeLoRA effectively decouples the angular learning from the adaptation strength, enhancing robustness without compromising performance. Through evaluations on subject-driven image generation, natural language understanding, and instruction tuning, we show that DeLoRA matches or surpasses performance of competing PEFT methods, while exhibiting stronger robustness. Code is available at https://github.com/ExplainableML/DeLoRA.

**Comment:** The paper introduces DeLoRA, a novel parameter-efficient fine-tuning method that aligns with model compression and efficiency breakthroughs.

**Relevance:** 9
**Novelty:** 8

---

## 9. [Optimal Neural Compressors for the Rate-Distortion-Perception Tradeoff](https://arxiv.org/abs/2503.17558) <a id="link9"></a>

**ArXiv ID:** 2503.17558

**Authors:** Eric Lei, Hamed Hassani, Shirin Saeedi Bidokhti

**Abstract:** Recent efforts in neural compression have focused on the rate-distortion-perception (RDP) tradeoff, where the perception constraint ensures the source and reconstruction distributions are close in terms of a statistical divergence. Theoretical work on RDP describes interesting properties of RDP-optimal compressors without providing constructive and low complexity solutions. While classical rate distortion theory shows that optimal compressors should efficiently pack the space, RDP theory additionally shows that infinite randomness shared between the encoder and decoder may be necessary for RDP optimality. In this paper, we propose neural compressors that are low complexity and benefit from high packing efficiency through lattice coding and shared randomness through shared dithering over the lattice cells. For two important settings, namely infinite shared and zero shared randomness, we analyze the rate, distortion, and perception achieved by our proposed neural compressors and further show optimality in the presence of infinite shared randomness. Experimentally, we investigate the roles these two components of our design, lattice coding and randomness, play in the performance of neural compressors on synthetic and real-world data. We observe that performance improves with more shared randomness and better lattice packing.

**Comment:** Addresses neural compression with a focus on the rate-distortion-perception tradeoff, providing theoretical insights and novel methods like lattice coding and shared randomness, aligning well with model compression criteria.

**Relevance:** 9
**Novelty:** 8

---

## 10. [Improving Quantization with Post-Training Model Expansion](https://arxiv.org/abs/2503.17513) <a id="link10"></a>

**ArXiv ID:** 2503.17513

**Authors:** Giuseppe Franco, Pablo Monteagudo-Lago, Ian Colbert, Nicholas Fraser, Michaela Blott

**Abstract:** The size of a model has been a strong predictor of its quality, as well as its cost. As such, the trade-off between model cost and quality has been well-studied. Post-training optimizations like quantization and pruning have typically focused on reducing the overall volume of pre-trained models to reduce inference costs while maintaining model quality. However, recent advancements have introduced optimization techniques that, interestingly, expand models post-training, increasing model size to improve quality when reducing volume. For instance, to enable 4-bit weight and activation quantization, incoherence processing often necessitates inserting online Hadamard rotations in the compute graph, and preserving highly sensitive weights often calls for additional higher precision computations. However, if application requirements cannot be met, the prevailing solution is to relax quantization constraints. In contrast, we demonstrate post-training model expansion is a viable strategy to improve model quality within a quantization co-design space, and provide theoretical justification. We show it is possible to progressively and selectively expand the size of a pre-trained large language model (LLM) to improve model quality without end-to-end retraining. In particular, when quantizing the weights and activations to 4 bits for Llama3 1B, we reduce the zero-shot accuracy gap to full precision by an average of 3% relative to both QuaRot and SpinQuant with only 5% more parameters, which is still a 3.8% reduction in volume relative to a BF16 reference model.

**Comment:** Explores post-training model expansion to improve quantization, which is a novel approach within the model compression domain and aligns with foundational research in efficiency improvements.

**Relevance:** 9
**Novelty:** 8

---

## 11. [xKV: Cross-Layer SVD for KV-Cache Compression](https://arxiv.org/abs/2503.18893) <a id="link11"></a>

**ArXiv ID:** 2503.18893

**Authors:** Chi-Chih Chang, Chien-Yu Lin, Yash Akhauri, Wei-Cheng Lin, Kai-Chiang Wu, Luis Ceze, Mohamed S. Abdelfattah

**Abstract:** Large Language Models (LLMs) with long context windows enable powerful applications but come at the cost of high memory consumption to store the Key and Value states (KV-Cache). Recent studies attempted to merge KV-cache from multiple layers into shared representations, yet these approaches either require expensive pretraining or rely on assumptions of high per-token cosine similarity across layers which generally does not hold in practice. We find that the dominant singular vectors are remarkably well-aligned across multiple layers of the KV-Cache. Exploiting this insight, we propose xKV, a simple post-training method that applies Singular Value Decomposition (SVD) on the KV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers into a shared low-rank subspace, significantly reducing KV-Cache sizes. Through extensive evaluations on the RULER long-context benchmark with widely-used LLMs (e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates than state-of-the-art inter-layer technique while improving accuracy by 2.7%. Moreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA) (e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding tasks without performance degradation. These results highlight xKV's strong capability and versatility in addressing memory bottlenecks for long-context LLM inference. Our code is publicly available at: https://github.com/abdelfattah-lab/xKV.

**Comment:** Proposes a novel KV-cache compression method using cross-layer SVD, which directly aligns with model compression and efficiency breakthroughs.

**Relevance:** 9
**Novelty:** 8

---

## 12. [Theory-to-Practice Gap for Neural Networks and Neural Operators](https://arxiv.org/abs/2503.18219) <a id="link12"></a>

**ArXiv ID:** 2503.18219

**Authors:** Philipp Grohs, Samuel Lanthaler, Margaret Trautner

**Abstract:** This work studies the sampling complexity of learning with ReLU neural networks and neural operators. For mappings belonging to relevant approximation spaces, we derive upper bounds on the best-possible convergence rate of any learning algorithm, with respect to the number of samples. In the finite-dimensional case, these bounds imply a gap between the parametric and sampling complexities of learning, known as the \emph{theory-to-practice gap}. In this work, a unified treatment of the theory-to-practice gap is achieved in a general $L^p$-setting, while at the same time improving available bounds in the literature. Furthermore, based on these results the theory-to-practice gap is extended to the infinite-dimensional setting of operator learning. Our results apply to Deep Operator Networks and integral kernel-based neural operators, including the Fourier neural operator. We show that the best-possible convergence rate in a Bochner $L^p$-norm is bounded by Monte-Carlo rates of order $1/p$.

**Comment:** Analyzes the theory-to-practice gap in neural networks and neural operators, providing theoretical insights into sampling complexity, which aligns with foundational research.

**Relevance:** 9
**Novelty:** 8

---

## 13. [BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with Low-Bit KV Cache](https://arxiv.org/abs/2503.18773) <a id="link13"></a>

**ArXiv ID:** 2503.18773

**Authors:** Dayou Du, Shijie Cao, Jianyi Cheng, Ting Cao, Mao Yang

**Abstract:** The growing adoption of long-context Large Language Models (LLMs) has introduced significant memory and computational challenges in autoregressive decoding due to the expanding Key-Value (KV) cache. KV cache quantization has emerged as a promising solution, with prior work showing that 4-bit or even 2-bit quantization can maintain model accuracy while reducing memory costs. However, despite these benefits, preliminary implementations for the low-bit KV cache struggle to deliver the expected speedup due to quantization and dequantization overheads and the lack of Tensor Cores utilization. In this work, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor Cores for efficient decoding with low-bit KV cache. Efficiently leveraging Tensor Cores for low-bit KV cache is challenging due to the dynamic nature of KV cache generation at each decoding step. BitDecoding addresses these challenges with a Tensor Cores-Centric BitFusion Scheme that ensures data layout compatibility to enable high utilization of Tensor Cores. Additionally, BitDecoding incorporates a warp-efficient parallel decoding kernel and a fine-grained asynchronous pipeline, minimizing dequantization overhead and improving computational efficiency. Experiments show that BitDecoding achieves up to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to FP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV cache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K sequence length, BitDecoding reduces single-batch decoding latency by 3x, demonstrating its effectiveness in long-context generation scenarios. The code is available at https://github.com/DD-DuDa/BitDecoding.

**Comment:** Introduces a GPU-optimized framework for low-bit KV-cache decoding, which aligns with model compression and efficiency improvements.

**Relevance:** 9
**Novelty:** 8

---

## 14. [Variance Control via Weight Rescaling in LLM Pre-training](https://arxiv.org/abs/2503.17500) <a id="link14"></a>

**ArXiv ID:** 2503.17500

**Authors:** Louis Owen, Abhay Kumar, Nilabhra Roy Chowdhury, Fabian G\"ura

**Abstract:** The outcome of Large Language Model (LLM) pre-training strongly depends on weight initialization and variance control strategies. Although the importance of initial variance control has been well documented in neural networks in general, the literature on initialization and management of its growth during LLM pre-training, specifically, is somewhat sparse. In this paper, we introduce the Layer Index Rescaling (LIR) weight initialization scheme, and the Target Variance Rescaling (TVR) variance control strategy. Experiments on a 1B parameter LLaMA model demonstrate that better variance management using these techniques yields substantial improvements in downstream task performance (up to 4.6% on common pre-training benchmarks) and reduces extreme activation values, thus mitigating challenges associated with quantization and low-precision training. Our code is available at: https://github.com/bluorion-com/weight_rescaling.

**Comment:** The paper introduces new variance control strategies (LIR and TVR) for LLM pretraining, which aligns with foundational research in LLM behavior and efficiency improvements.

**Relevance:** 9
**Novelty:** 8

---

## 15. [TopV: Compatible Token Pruning with Inference Time Optimization for Fast and Low-Memory Multimodal Vision Language Model](https://arxiv.org/abs/2503.18278) <a id="link15"></a>

**ArXiv ID:** 2503.18278

**Authors:** Cheng Yang, Yang Sui, Jinqi Xiao, Lingyi Huang, Yu Gong, Chendi Li, Jinghua Yan, Yu Bai, Ponnuswamy Sadayappan, Xia Hu, Bo Yuan

**Abstract:** Vision-Language Models (VLMs) demand substantial computational resources during inference, largely due to the extensive visual input tokens for representing visual information. Previous studies have noted that visual tokens tend to receive less attention than text tokens, suggesting their lower importance during inference and potential for pruning. However, their methods encounter several challenges: reliance on greedy heuristic criteria for token importance and incompatibility with FlashAttention and KV cache. To address these issues, we introduce \textbf{TopV}, a compatible \textbf{TO}ken \textbf{P}runing with inference Time Optimization for fast and low-memory \textbf{V}LM, achieving efficient pruning without additional training or fine-tuning. Instead of relying on attention scores, we formulate token pruning as an optimization problem, accurately identifying important visual tokens while remaining compatible with FlashAttention. Additionally, since we only perform this pruning once during the prefilling stage, it effectively reduces KV cache size. Our optimization framework incorporates a visual-aware cost function considering factors such as Feature Similarity, Relative Spatial Distance, and Absolute Central Distance, to measure the importance of each source visual token, enabling effective pruning of low-importance tokens. Extensive experiments demonstrate that our method outperforms previous token pruning methods, validating the effectiveness and efficiency of our approach.

**Comment:** The paper introduces a token pruning method for vision-language models, focusing on inference optimization and compatibility with FlashAttention, aligning with model compression and efficiency.

**Relevance:** 9
**Novelty:** 8

---

## 16. [Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV Cache Quantization](https://arxiv.org/abs/2503.18599) <a id="link16"></a>

**ArXiv ID:** 2503.18599

**Authors:** Minsu Kim, Seongmin Hong, RyeoWook Ko, Soongyu Choi, Hunjong Lee, Junsoo Kim, Joo-Young Kim, Jongse Park

**Abstract:** Modern Large Language Model serving system batches multiple requests to achieve high throughput, while batching attention operations is challenging, rendering memory bandwidth a critical bottleneck. The community relies on high-end GPUs with multiple high-bandwidth memory channels. Unfortunately, HBM's high bandwidth often comes at the expense of limited memory capacity, which reduces core utilization and increases costs. Recent advancements enabling longer contexts for LLMs have substantially increased the key-value cache size, further intensifying the pressures on memory capacity. The literature has explored KV cache quantization techniques, which commonly use low bitwidth for most values, selectively using higher bitwidth for outlier values. While this approach helps achieve high accuracy and low bitwidth simultaneously, it comes with the limitation that cost for online outlier detection is excessively high, negating the advantages. We propose Oaken, an acceleration solution that achieves high accuracy and high performance simultaneously through co-designing algorithm and hardware. To effectively find a sweet spot in the accuracy-performance trade-off space of KV cache quantization, Oaken employs an online-offline hybrid approach, setting outlier thresholds offline, which are then used to determine the quantization scale online. To translate the proposed algorithmic technique into tangible performance gains, Oaken also comes with custom quantization engines and memory management units that can be integrated with any LLM accelerators. We built an Oaken accelerator on top of an LLM accelerator, LPU, and conducted a comprehensive evaluation. Our experiments show that for a batch size of 256, Oaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU, incurring a minimal accuracy loss of only 0.54\% on average, compared to state-of-the-art KV cache quantization techniques.

**Comment:** The paper proposes a hybrid KV cache quantization method for LLMs, which aligns with model compression and efficiency breakthroughs. The co-design of algorithm and hardware adds novelty.

**Relevance:** 9
**Novelty:** 8

---

## 17. [Feature Qualification by Deep Nets: A Constructive Approach](https://arxiv.org/abs/2503.18676) <a id="link17"></a>

**ArXiv ID:** 2503.18676

**Authors:** Feilong Cao, Shao-Bo Lin

**Abstract:** The great success of deep learning has stimulated avid research activities in verifying the power of depth in theory, a common consensus of which is that deep net are versatile in approximating and learning numerous functions. Such a versatility certainly enhances the understanding of the power of depth, but makes it difficult to judge which data features are crucial in a specific learning task. This paper proposes a constructive approach to equip deep nets for the feature qualification purpose. Using the product-gate nature and localized approximation property of deep nets with sigmoid activation (deep sigmoid nets), we succeed in constructing a linear deep net operator that possesses optimal approximation performance in approximating smooth and radial functions. Furthermore, we provide theoretical evidences that the constructed deep net operator is capable of qualifying multiple features such as the smoothness and radialness of the target functions.

**Comment:** The paper provides a theoretical approach to feature qualification using deep nets, which aligns with representation learning by focusing on how features are encoded and qualified. The constructive approach adds theoretical depth.

**Relevance:** 9
**Novelty:** 8

---

## 18. [Bayesian Teaching Enables Probabilistic Reasoning in Large Language Models](https://arxiv.org/abs/2503.17523) <a id="link18"></a>

**ArXiv ID:** 2503.17523

**Authors:** Linlu Qiu, Fei Sha, Kelsey Allen, Yoon Kim, Tal Linzen, Sjoerd van Steenkiste

**Abstract:** Artificial intelligence systems based on large language models (LLMs) are increasingly used as agents that interact with users and with the world. To do so successfully, LLMs need to construct internal representations of the world and form probabilistic beliefs about those representations. To provide a user with personalized recommendations, for example, the LLM needs to gradually infer the user's preferences, over the course of multiple interactions. To evaluate whether contemporary LLMs are able to do so, we use the Bayesian inference framework from probability theory, which lays out the optimal way to update an agent's beliefs as it receives new information. We first show that the LLMs do not update their beliefs as expected from the Bayesian framework, and that consequently their predictions do not improve as expected as more information becomes available, even less so than we find is the case for humans. To address this issue, we teach the LLMs to reason in a Bayesian manner by training them to mimic the predictions of an optimal Bayesian model. We find that this approach not only significantly improves the LLM's performance on the particular recommendation task it is trained on, but also enables generalization to other tasks. This suggests that this method endows the LLM with broader Bayesian reasoning skills. More generally, our results indicate that LLMs can learn about reasoning strategies effectively and generalize those skills to new domains, which in part explains LLMs' empirical success.

**Comment:** The paper explores Bayesian reasoning in LLMs and proposes a method to improve their probabilistic reasoning capabilities, which aligns with foundational research in LLM behavior and interpretability.

**Relevance:** 9
**Novelty:** 8

---

## 19. [Maximum Redundancy Pruning: A Principle-Driven Layerwise Sparsity Allocation for LLMs](https://arxiv.org/abs/2503.18377) <a id="link19"></a>

**ArXiv ID:** 2503.18377

**Authors:** Chang Gao, Kang Zhao, Jianfei Chen, Liping Jing

**Abstract:** Large language models (LLMs) have demonstrated impressive capabilities, but their enormous size poses significant challenges for deployment in real-world applications. To address this issue, researchers have sought to apply network pruning techniques to LLMs. A critical challenge in pruning is allocation the sparsity for each layer. Recent sparsity allocation methods is often based on heuristics or search that can easily lead to suboptimal performance. In this paper, we conducted an extensive investigation into various LLMs and revealed three significant discoveries: (1) the layerwise pruning sensitivity (LPS) of LLMs is highly non-uniform, (2) the choice of pruning metric affects LPS, and (3) the performance of a sparse model is related to the uniformity of its layerwise redundancy level. Based on these observations, we propose that the layerwise sparsity of LLMs should adhere to three principles: \emph{non-uniformity}, \emph{pruning metric dependency}, and \emph{uniform layerwise redundancy level} in the pruned model. To this end, we proposed Maximum Redundancy Pruning (MRP), an iterative pruning algorithm that prunes in the most redundant layers (\emph{i.e.}, those with the highest non-outlier ratio) at each iteration. The achieved layerwise sparsity aligns with the outlined principles. We conducted extensive experiments on publicly available LLMs, including the LLaMA2 and OPT, across various benchmarks. Experimental results validate the effectiveness of MRP, demonstrating its superiority over previous methods.

**Comment:** The paper proposes a principle-driven pruning method for LLMs, which aligns with foundational research in model compression and efficiency.

**Relevance:** 9
**Novelty:** 8

---

## 20. [FFN Fusion: Rethinking Sequential Computation in Large Language Models](https://arxiv.org/abs/2503.18908) <a id="link20"></a>

**ArXiv ID:** 2503.18908

**Authors:** Akhiad Bercovich, Mohammad Dabbah, Omri Puny, Ido Galil, Amnon Geifman, Yonatan Geifman, Izhak Golan, Ehud Karpas, Itay Levy, Zach Moshe, Najeeb Nabwani, Tomer Ronen, Itamar Schen, Elad Segal, Ido Shahaf, Oren Tropp, Ran Zilberstein, Ran El-Yaniv

**Abstract:** We introduce FFN Fusion, an architectural optimization technique that reduces sequential computation in large language models by identifying and exploiting natural opportunities for parallelization. Our key insight is that sequences of Feed-Forward Network (FFN) layers, particularly those remaining after the removal of specific attention layers, can often be parallelized with minimal accuracy impact. We develop a principled methodology for identifying and fusing such sequences, transforming them into parallel operations that significantly reduce inference latency while preserving model behavior. Applying these techniques to Llama-3.1-405B-Instruct, we create Llama-Nemotron-Ultra-253B-Base (Ultra-253B-Base), an efficient and soon-to-be publicly available model that achieves a 1.71X speedup in inference latency and 35X lower per-token cost while maintaining strong performance across benchmarks. Through extensive experiments on models from 49B to 253B parameters, we demonstrate that FFN Fusion becomes increasingly effective at larger scales and can complement existing optimization techniques like quantization and pruning. Most intriguingly, we find that even full transformer blocks containing both attention and FFN layers can sometimes be parallelized, suggesting new directions for neural architecture design.

**Comment:** The paper proposes FFN Fusion, an architectural optimization technique for LLMs, which aligns with foundational research in model architecture and efficiency.

**Relevance:** 9
**Novelty:** 8

---

## 21. [Adaptive Rank Allocation: Speeding Up Modern Transformers with RaNA Adapters](https://arxiv.org/abs/2503.18216) <a id="link21"></a>

**ArXiv ID:** 2503.18216

**Authors:** Roberto Garcia, Jerry Liu, Daniel Sorvisto, Sabri Eyuboglu

**Abstract:** Large Language Models (LLMs) are computationally intensive, particularly during inference. Neuron-adaptive techniques, which selectively activate neurons in Multi-Layer Perceptron (MLP) layers, offer some speedups but suffer from limitations in modern Transformers. These include reliance on sparse activations, incompatibility with attention layers, and the use of costly neuron masking techniques. To address these issues, we propose the Adaptive Rank Allocation framework and introduce the Rank and Neuron Allocator (RaNA) adapter. RaNA adapters leverage rank adapters, which operate on linear layers by applying both low-rank matrix decompositions and adaptive masking to efficiently allocate compute without depending on activation sparsity. This enables RaNA to be generally applied to MLPs and linear components of attention modules, while eliminating the need for expensive maskers found in neuron-adaptive methods. Notably, when compared to neuron adapters, RaNA improves perplexity by up to 7 points and increases accuracy by up to 8 percentage-points when reducing FLOPs by $\sim$44% in state-of-the-art Transformer architectures. These results position RaNA as a robust solution for improving inference efficiency in modern Transformer architectures.

**Comment:** The paper introduces RaNA adapters for improving inference efficiency in Transformers, focusing on low-rank matrix decompositions and adaptive masking. This aligns with model compression and architectural efficiency.

**Relevance:** 9
**Novelty:** 8

---

## 22. [Efficient Knowledge Distillation via Curriculum Extraction](https://arxiv.org/abs/2503.17494) <a id="link22"></a>

**ArXiv ID:** 2503.17494

**Authors:** Shivam Gupta, Sushrut Karmalkar

**Abstract:** Knowledge distillation is a technique used to train a small student network using the output generated by a large teacher network, and has many empirical advantages~\citep{Hinton2015DistillingTK}. While the standard one-shot approach to distillation only uses the output of the final teacher network, recent work~\citep{panigrahi2024progressive} has shown that using intermediate checkpoints from the teacher's training process as an implicit ``curriculum'' for progressive distillation can significantly speed up training. However, such schemes require storing these checkpoints, and often require careful selection of the intermediate checkpoints to train on, which can be impractical for large-scale training.   In this paper, we show that a curriculum can be \emph{extracted} from just the fully trained teacher network, and that this extracted curriculum can give similar efficiency benefits to those of progressive distillation. Our extraction scheme is natural; we use a random projection of the hidden representations of the teacher network to progressively train the student network, before training using the output of the full network. We show that our scheme significantly outperforms one-shot distillation and achieves a performance similar to that of progressive distillation for learning sparse parities with two-layer networks, and provide theoretical guarantees for this setting. Additionally, we show that our method outperforms one-shot distillation even when using transformer-based architectures, both for sparse-parity learning, and language modeling tasks.

**Comment:** The paper introduces a curriculum extraction method for knowledge distillation, which aligns with model compression and efficiency. It provides theoretical guarantees and demonstrates practical benefits.

**Relevance:** 9
**Novelty:** 8

---

## 23. [CODA: Repurposing Continuous VAEs for Discrete Tokenization](https://arxiv.org/abs/2503.17760) <a id="link23"></a>

**ArXiv ID:** 2503.17760

**Authors:** Zeyu Liu, Zanlin Ni, Yeguo Hua, Xin Deng, Xiao Ma, Cheng Zhong, Gao Huang

**Abstract:** Discrete visual tokenizers transform images into a sequence of tokens, enabling token-based visual generation akin to language models. However, this process is inherently challenging, as it requires both compressing visual signals into a compact representation and discretizing them into a fixed set of codes. Traditional discrete tokenizers typically learn the two tasks jointly, often leading to unstable training, low codebook utilization, and limited reconstruction quality. In this paper, we introduce \textbf{CODA}(\textbf{CO}ntinuous-to-\textbf{D}iscrete \textbf{A}daptation), a framework that decouples compression and discretization. Instead of training discrete tokenizers from scratch, CODA adapts off-the-shelf continuous VAEs -- already optimized for perceptual compression -- into discrete tokenizers via a carefully designed discretization process. By primarily focusing on discretization, CODA ensures stable and efficient training while retaining the strong visual fidelity of continuous VAEs. Empirically, with $\mathbf{6 \times}$ less training budget than standard VQGAN, our approach achieves a remarkable codebook utilization of 100% and notable reconstruction FID (rFID) of $\mathbf{0.43}$ and $\mathbf{1.34}$ for $8 \times$ and $16 \times$ compression on ImageNet 256$\times$ 256 benchmark.

**Comment:** Proposes a novel framework for adapting continuous VAEs into discrete tokenizers, which aligns with foundational research in representation learning and autoencoders.

**Relevance:** 8
**Novelty:** 8

---

## 24. [Generative AI for Validating Physics Laws](https://arxiv.org/abs/2503.17894) <a id="link24"></a>

**ArXiv ID:** 2503.17894

**Authors:** Maria Nareklishvili, Nicholas Polson, Vadim Sokolov

**Abstract:** We present generative artificial intelligence (AI) to empirically validate fundamental laws of physics, focusing on the Stefan-Boltzmann law linking stellar temperature and luminosity. Our approach simulates counterfactual luminosities under hypothetical temperature regimes for each individual star and iteratively refines the temperature-luminosity relationship in a deep learning architecture. We use Gaia DR3 data and find that, on average, temperature's effect on luminosity increases with stellar radius and decreases with absolute magnitude, consistent with theoretical predictions. By framing physics laws as causal problems, our method offers a novel, data-driven approach to refine theoretical understanding and inform evidence-based policy and practice.

**Comment:** The paper introduces a novel generative AI approach to validate physics laws, which aligns with foundational research in AI for Science. The framing of physics laws as causal problems is innovative.

**Relevance:** 8
**Novelty:** 8

---

## 25. [What's Producible May Not Be Reachable: Measuring the Steerability of Generative Models](https://arxiv.org/abs/2503.17482) <a id="link25"></a>

**ArXiv ID:** 2503.17482

**Authors:** Keyon Vafa, Sarah Bentley, Jon Kleinberg, Sendhil Mullainathan

**Abstract:** How should we evaluate the quality of generative models? Many existing metrics focus on a model's producibility, i.e. the quality and breadth of outputs it can generate. However, the actual value from using a generative model stems not just from what it can produce but whether a user with a specific goal can produce an output that satisfies that goal. We refer to this property as steerability. In this paper, we first introduce a mathematical framework for evaluating steerability independently from producibility. Steerability is more challenging to evaluate than producibility because it requires knowing a user's goals. We address this issue by creating a benchmark task that relies on one key idea: sample an output from a generative model and ask users to reproduce it. We implement this benchmark in a large-scale user study of text-to-image models and large language models. Despite the ability of these models to produce high-quality outputs, they all perform poorly on steerabilty. This suggests that we need to focus on improving the steerability of generative models. We show such improvements are indeed possible: through reinforcement learning techniques, we create an alternative steering mechanism for image models that achieves more than 2x improvement on this benchmark.

**Comment:** The paper introduces a framework for evaluating the steerability of generative models, which is a novel perspective on foundational aspects of generative model evaluation.

**Relevance:** 8
**Novelty:** 8

---

## 26. [AutoBayes: A Compositional Framework for Generalized Variational Inference](https://arxiv.org/abs/2503.18608) <a id="link26"></a>

**ArXiv ID:** 2503.18608

**Authors:** Toby St Clere Smithe, Marco Perin

**Abstract:** We introduce a new compositional framework for generalized variational inference, clarifying the different parts of a model, how they interact, and how they compose. We explain that both exact Bayesian inference and the loss functions typical of variational inference (such as variational free energy and its generalizations) satisfy chain rules akin to that of reverse-mode automatic differentiation, and we advocate for exploiting this to build and optimize models accordingly. To this end, we construct a series of compositional tools: for building models; for constructing their inversions; for attaching local loss functions; and for exposing parameters. Finally, we explain how the resulting parameterized statistical games may be optimized locally, too. We illustrate our framework with a number of classic examples, pointing to new areas of extensibility that are revealed.

**Comment:** The paper proposes a compositional framework for generalized variational inference, which aligns with foundational research in representation learning and model optimization. It introduces new tools and theoretical insights.

**Relevance:** 8
**Novelty:** 8

---

## 27. [Does GCL Need a Large Number of Negative Samples? Enhancing Graph Contrastive Learning with Effective and Efficient Negative Sampling](https://arxiv.org/abs/2503.17908) <a id="link27"></a>

**ArXiv ID:** 2503.17908

**Authors:** Yongqi Huang, Jitao Zhao, Dongxiao He, Di Jin, Yuxiao Huang, Zhen Wang

**Abstract:** Graph Contrastive Learning (GCL) aims to self-supervised learn low-dimensional graph representations, primarily through instance discrimination, which involves manually mining positive and negative pairs from graphs, increasing the similarity of positive pairs while decreasing negative pairs. Drawing from the success of Contrastive Learning (CL) in other domains, a consensus has been reached that the effectiveness of GCLs depends on a large number of negative pairs. As a result, despite the significant computational overhead, GCLs typically leverage as many negative node pairs as possible to improve model performance. However, given that nodes within a graph are interconnected, we argue that nodes cannot be treated as independent instances. Therefore, we challenge this consensus: Does employing more negative nodes lead to a more effective GCL model? To answer this, we explore the role of negative nodes in the commonly used InfoNCE loss for GCL and observe that: (1) Counterintuitively, a large number of negative nodes can actually hinder the model's ability to distinguish nodes with different semantics. (2) A smaller number of high-quality and non-topologically coupled negative nodes are sufficient to enhance the discriminability of representations. Based on these findings, we propose a new method called GCL with Effective and Efficient Negative samples, E2Neg, which learns discriminative representations using only a very small set of representative negative samples. E2Neg significantly reduces computational overhead and speeds up model training. We demonstrate the effectiveness and efficiency of E2Neg across multiple datasets compared to other GCL methods.

**Comment:** The paper challenges the consensus on negative sampling in Graph Contrastive Learning (GCL) and proposes a new method (E2Neg) for efficient and effective sampling. It aligns with representation learning and introduces novel insights.

**Relevance:** 8
**Novelty:** 8

---

## 28. [On the Minimax Regret of Sequential Probability Assignment via Square-Root Entropy](https://arxiv.org/abs/2503.17823) <a id="link28"></a>

**ArXiv ID:** 2503.17823

**Authors:** Zeyu Jia, Yury Polyanskiy, Alexander Rakhlin

**Abstract:** We study the problem of sequential probability assignment under logarithmic loss, both with and without side information. Our objective is to analyze the minimax regret -- a notion extensively studied in the literature -- in terms of geometric quantities, such as covering numbers and scale-sensitive dimensions. We show that the minimax regret for the case of no side information (equivalently, the Shtarkov sum) can be upper bounded in terms of sequential square-root entropy, a notion closely related to Hellinger distance. For the problem of sequential probability assignment with side information, we develop both upper and lower bounds based on the aforementioned entropy. The lower bound matches the upper bound, up to log factors, for classes in the Donsker regime (according to our definition of entropy).

**Comment:** The paper provides theoretical insights into sequential probability assignment using square-root entropy, which aligns with foundational research in representation learning.

**Relevance:** 8
**Novelty:** 7

---

## 29. [Dynamic Gradient Sparse Update for Edge Training](https://arxiv.org/abs/2503.17959) <a id="link29"></a>

**ArXiv ID:** 2503.17959

**Authors:** I-Hsuan Li, Tian-Sheuan Chang

**Abstract:** Training on edge devices enables personalized model fine-tuning to enhance real-world performance and maintain data privacy. However, the gradient computation for backpropagation in the training requires significant memory buffers to store intermediate features and compute losses. This is unacceptable for memory-constrained edge devices such as microcontrollers. To tackle this issue, we propose a training acceleration method using dynamic gradient sparse updates. This method updates the important channels and layers only and skips gradient computation for the less important channels and layers to reduce memory usage for each update iteration. In addition, the channel selection is dynamic for different iterations to traverse most of the parameters in the update layers along the time dimension for better performance. The experimental result shows that the proposed method enables an ImageNet pre-trained MobileNetV2 trained on CIFAR-10 to achieve an accuracy of 85.77\% while updating only 2\% of convolution weights within 256KB on-chip memory. This results in a remarkable 98\% reduction in feature memory usage compared to dense model training.

**Comment:** The paper proposes a dynamic gradient sparse update method for edge training, which aligns with model compression and efficiency breakthroughs.

**Relevance:** 8
**Novelty:** 7

---

## 30. [Neuromorphic Principles for Efficient Large Language Models on Intel Loihi 2](https://arxiv.org/abs/2503.18002) <a id="link30"></a>

**ArXiv ID:** 2503.18002

**Authors:** Steven Abreu, Sumit Bam Shrestha, Rui-Jie Zhu, Jason Eshraghian

**Abstract:** Large language models (LLMs) deliver impressive performance but require large amounts of energy. In this work, we present a MatMul-free LLM architecture adapted for Intel's neuromorphic processor, Loihi 2. Our approach leverages Loihi 2's support for low-precision, event-driven computation and stateful processing. Our hardware-aware quantized model on GPU demonstrates that a 370M parameter MatMul-free model can be quantized with no accuracy loss. Based on preliminary results, we report up to 3x higher throughput with 2x less energy, compared to transformer-based LLMs on an edge GPU, with significantly better scaling. Further hardware optimizations will increase throughput and decrease energy consumption. These results show the potential of neuromorphic hardware for efficient inference and pave the way for efficient reasoning models capable of generating complex, long-form text rapidly and cost-effectively.

**Comment:** The paper explores neuromorphic principles for efficient LLMs, which aligns with model compression and efficiency breakthroughs, particularly through hardware-aware innovations.

**Relevance:** 8
**Novelty:** 7

---

## 31. [On the Optimality of Single-label and Multi-label Neural Network Decoders](https://arxiv.org/abs/2503.18758) <a id="link31"></a>

**ArXiv ID:** 2503.18758

**Authors:** Yunus Can G\"ultekin, P\'eter Scheepers, Yuncheng Yuan, Federico Corradi, Alex Alvarado

**Abstract:** We investigate the design of two neural network (NN) architectures recently proposed as decoders for forward error correction: the so-called single-label NN (SLNN) and multi-label NN (MLNN) decoders. These decoders have been reported to achieve near-optimal codeword- and bit-wise performance, respectively. Results in the literature show near-optimality for a variety of short codes. In this paper, we analytically prove that certain SLNN and MLNN architectures can, in fact, always realize optimal decoding, regardless of the code. These optimal architectures and their binary weights are shown to be defined by the codebook, i.e., no training or network optimization is required. Our proposed architectures are in fact not NNs, but a different way of implementing the maximum likelihood decoding rule. Optimal performance is numerically demonstrated for Hamming $(7,4)$, Polar $(16,8)$, and BCH $(31,21)$ codes. The results show that our optimal architectures are less complex than the SLNN and MLNN architectures proposed in the literature, which in fact only achieve near-optimal performance. Extension to longer codes is still hindered by the curse of dimensionality. Therefore, even though SLNN and MLNN can perform maximum likelihood decoding, such architectures cannot be used for medium and long codes.

**Comment:** The paper analytically proves the optimality of certain neural network decoders, which aligns with foundational research in model architecture and theoretical insights.

**Relevance:** 8
**Novelty:** 7

---

## 32. [Improving Preference Extraction In LLMs By Identifying Latent Knowledge Through Classifying Probes](https://arxiv.org/abs/2503.17755) <a id="link32"></a>

**ArXiv ID:** 2503.17755

**Authors:** Sharan Maiya, Yinhong Liu, Ramit Debnath, Anna Korhonen

**Abstract:** Large Language Models (LLMs) are often used as automated judges to evaluate text, but their effectiveness can be hindered by various unintentional biases. We propose using linear classifying probes, trained by leveraging differences between contrasting pairs of prompts, to directly access LLMs' latent knowledge and extract more accurate preferences. Through extensive experiments using models of varying size from four different families and six diverse datasets assessing text quality evaluation and common sense reasoning, we demonstrate that both supervised and unsupervised probing approaches consistently outperform traditional generation-based judgement while maintaining similar computational costs. These probes generalise under domain shifts and can even outperform finetuned evaluators with the same training data size. Our results suggest linear probing offers an accurate, robust and computationally efficient approach for LLM-as-judge tasks while providing interpretable insights into how models encode judgement-relevant knowledge. Our data and code will be openly released in the future.

**Comment:** Proposes linear probing for extracting latent knowledge in LLMs, which provides insights into model interpretability and aligns with foundational research in representation learning.

**Relevance:** 8
**Novelty:** 7

---

## 33. [Generative Modeling of Class Probability for Multi-Modal Representation Learning](https://arxiv.org/abs/2503.17417) <a id="link33"></a>

**ArXiv ID:** 2503.17417

**Authors:** Jungkyoo Shin, Bumsoo Kim, Eunwoo Kim

**Abstract:** Multi-modal understanding plays a crucial role in artificial intelligence by enabling models to jointly interpret inputs from different modalities. However, conventional approaches such as contrastive learning often struggle with modality discrepancies, leading to potential misalignments. In this paper, we propose a novel class anchor alignment approach that leverages class probability distributions for multi-modal representation learning. Our method, Class-anchor-ALigned generative Modeling (CALM), encodes class anchors as prompts to generate and align class probability distributions for each modality, enabling more effective alignment. Furthermore, we introduce a cross-modal probabilistic variational autoencoder to model uncertainty in the alignment, enhancing the ability to capture deeper relationships between modalities and data variations. Extensive experiments on four benchmark datasets demonstrate that our approach significantly outperforms state-of-the-art methods, especially in out-of-domain evaluations. This highlights its superior generalization capabilities in multi-modal representation learning.

**Comment:** Proposes a novel generative modeling approach for multi-modal representation learning, which aligns with foundational research in representation learning.

**Relevance:** 8
**Novelty:** 7

---

## 34. [Language Models May Verbatim Complete TextThey Were Not Explicitly Trained On](https://arxiv.org/abs/2503.17514) <a id="link34"></a>

**ArXiv ID:** 2503.17514

**Authors:** Ken Ziyu Liu, Christopher A. Choquette-Choo, Matthew Jagielski, Peter Kairouz, Sanmi Koyejo, Percy Liang, Nicolas Papernot

**Abstract:** An important question today is whether a given text was used to train a large language model (LLM). A \emph{completion} test is often employed: check if the LLM completes a sufficiently complex text. This, however, requires a ground-truth definition of membership; most commonly, it is defined as a member based on the $n$-gram overlap between the target text and any text in the dataset. In this work, we demonstrate that this $n$-gram based membership definition can be effectively gamed. We study scenarios where sequences are \emph{non-members} for a given $n$ and we find that completion tests still succeed. We find many natural cases of this phenomenon by retraining LLMs from scratch after removing all training samples that were completed; these cases include exact duplicates, near-duplicates, and even short overlaps. They showcase that it is difficult to find a single viable choice of $n$ for membership definitions. Using these insights, we design adversarial datasets that can cause a given target sequence to be completed without containing it, for any reasonable choice of $n$. Our findings highlight the inadequacy of $n$-gram membership, suggesting membership definitions fail to account for auxiliary information available to the training algorithm.

**Comment:** Analyzes membership definitions in LLM training datasets, which provides theoretical insights into LLM behavior and aligns with foundational research.

**Relevance:** 8
**Novelty:** 7

---

## 35. [Towards Human-Understandable Multi-Dimensional Concept Discovery](https://arxiv.org/abs/2503.18629) <a id="link35"></a>

**ArXiv ID:** 2503.18629

**Authors:** Arne Grobr\"ugge, Niklas K\"uhl, Gerhard Satzger, Philipp Spitzer

**Abstract:** Concept-based eXplainable AI (C-XAI) aims to overcome the limitations of traditional saliency maps by converting pixels into human-understandable concepts that are consistent across an entire dataset. A crucial aspect of C-XAI is completeness, which measures how well a set of concepts explains a model's decisions. Among C-XAI methods, Multi-Dimensional Concept Discovery (MCD) effectively improves completeness by breaking down the CNN latent space into distinct and interpretable concept subspaces. However, MCD's explanations can be difficult for humans to understand, raising concerns about their practical utility. To address this, we propose Human-Understandable Multi-dimensional Concept Discovery (HU-MCD). HU-MCD uses the Segment Anything Model for concept identification and implements a CNN-specific input masking technique to reduce noise introduced by traditional masking methods. These changes to MCD, paired with the completeness relation, enable HU-MCD to enhance concept understandability while maintaining explanation faithfulness. Our experiments, including human subject studies, show that HU-MCD provides more precise and reliable explanations than existing C-XAI methods. The code is available at https://github.com/grobruegge/hu-mcd.

**Comment:** The paper proposes HU-MCD, which enhances concept-based explainability in CNNs, aligning with representation learning and interpretability research.

**Relevance:** 8
**Novelty:** 7

---

## 36. [Exploring Energy Landscapes for Minimal Counterfactual Explanations: Applications in Cybersecurity and Beyond](https://arxiv.org/abs/2503.18185) <a id="link36"></a>

**ArXiv ID:** 2503.18185

**Authors:** Spyridon Evangelatos, Eleni Veroni, Vasilis Efthymiou, Christos Nikolopoulos, Georgios Th. Papadopoulos, Panagiotis Sarigiannidis

**Abstract:** Counterfactual explanations have emerged as a prominent method in Explainable Artificial Intelligence (XAI), providing intuitive and actionable insights into Machine Learning model decisions. In contrast to other traditional feature attribution methods that assess the importance of input variables, counterfactual explanations focus on identifying the minimal changes required to alter a model's prediction, offering a ``what-if'' analysis that is close to human reasoning. In the context of XAI, counterfactuals enhance transparency, trustworthiness and fairness, offering explanations that are not just interpretable but directly applicable in the decision-making processes.   In this paper, we present a novel framework that integrates perturbation theory and statistical mechanics to generate minimal counterfactual explanations in explainable AI. We employ a local Taylor expansion of a Machine Learning model's predictive function and reformulate the counterfactual search as an energy minimization problem over a complex landscape. In sequence, we model the probability of candidate perturbations leveraging the Boltzmann distribution and use simulated annealing for iterative refinement. Our approach systematically identifies the smallest modifications required to change a model's prediction while maintaining plausibility. Experimental results on benchmark datasets for cybersecurity in Internet of Things environments, demonstrate that our method provides actionable, interpretable counterfactuals and offers deeper insights into model sensitivity and decision boundaries in high-dimensional spaces.

**Comment:** The paper introduces a novel framework for counterfactual explanations using energy minimization, which aligns with representation learning and interpretability research.

**Relevance:** 8
**Novelty:** 7

---

## 37. [TARDIS: Mitigate Temporal Misalignment via Representation Steering](https://arxiv.org/abs/2503.18693) <a id="link37"></a>

**ArXiv ID:** 2503.18693

**Authors:** Changho Shin, Xinya Yan, Suenggwan Jo, Sungjun Cho, Shourjo Aditya Chaudhuri, Frederic Sala

**Abstract:** Language models often struggle with temporal misalignment, performance degradation caused by shifts in the temporal distribution of data. Continuously updating models to avoid degradation is expensive. Can models be adapted without updating model weights? We present TARDIS, an unsupervised representation editing method that addresses this challenge. TARDIS extracts steering vectors from unlabeled data and adjusts the model's representations to better align with the target time period's distribution. Our experiments reveal that TARDIS enhances downstream task performance without the need for fine-tuning, can mitigate temporal misalignment even when exact target time period data is unavailable, and remains efficient even when the temporal information of the target data points is unknown at inference time.

**Comment:** The paper introduces TARDIS, a representation editing method to mitigate temporal misalignment, aligning with representation learning and efficiency improvements.

**Relevance:** 8
**Novelty:** 7

---

## 38. [Interpretable Feature Interaction via Statistical Self-supervised Learning on Tabular Data](https://arxiv.org/abs/2503.18048) <a id="link38"></a>

**ArXiv ID:** 2503.18048

**Authors:** Xiaochen Zhang, Haoyi Xiong

**Abstract:** In high-dimensional and high-stakes contexts, ensuring both rigorous statistical guarantees and interpretability in feature extraction from complex tabular data remains a formidable challenge. Traditional methods such as Principal Component Analysis (PCA) reduce dimensionality and identify key features that explain the most variance, but are constrained by their reliance on linear assumptions. In contrast, neural networks offer assumption-free feature extraction through self-supervised learning techniques such as autoencoders, though their interpretability remains a challenge in fields requiring transparency. To address this gap, this paper introduces Spofe, a novel self-supervised machine learning pipeline that marries the power of kernel principal components for capturing nonlinear dependencies with a sparse and principled polynomial representation to achieve clear interpretability with statistical rigor. Underpinning our approach is a robust theoretical framework that delivers precise error bounds and rigorous false discovery rate (FDR) control via a multi-objective knockoff selection procedure; it effectively bridges the gap between data-driven complexity and statistical reliability via three stages: (1) generating self-supervised signals using kernel principal components to model complex patterns, (2) distilling these signals into sparse polynomial functions for improved interpretability, and (3) applying a multi-objective knockoff selection procedure with significance testing to rigorously identify important features. Extensive experiments on diverse real-world datasets demonstrate the effectiveness of Spofe, consistently surpassing KPCA, SKPCA, and other methods in feature selection for regression and classification tasks. Visualization and case studies highlight its ability to uncover key insights, enhancing interpretability and practical utility.

**Comment:** The paper proposes a novel self-supervised learning pipeline combining kernel PCA and sparse polynomial representations, which aligns with representation learning and interpretability.

**Relevance:** 8
**Novelty:** 7

---

## 39. [Bayesian generative models can flag performance loss, bias, and out-of-distribution image content](https://arxiv.org/abs/2503.17477) <a id="link39"></a>

**ArXiv ID:** 2503.17477

**Authors:** Miguel L\'opez-P\'erez, Marco Miani, Valery Naranjo, S{\o}ren Hauberg, Aasa Feragen

**Abstract:** Generative models are popular for medical imaging tasks such as anomaly detection, feature extraction, data visualization, or image generation. Since they are parameterized by deep learning models, they are often sensitive to distribution shifts and unreliable when applied to out-of-distribution data, creating a risk of, e.g. underrepresentation bias. This behavior can be flagged using uncertainty quantification methods for generative models, but their availability remains limited. We propose SLUG: A new UQ method for VAEs that combines recent advances in Laplace approximations with stochastic trace estimators to scale gracefully with image dimensionality. We show that our UQ score -- unlike the VAE's encoder variances -- correlates strongly with reconstruction error and racial underrepresentation bias for dermatological images. We also show how pixel-wise uncertainty can detect out-of-distribution image content such as ink, rulers, and patches, which is known to induce learning shortcuts in predictive models.

**Comment:** The paper proposes a novel uncertainty quantification method for VAEs, which aligns with foundational research in generative models and representation learning.

**Relevance:** 8
**Novelty:** 7

---

## 40. [Distil-xLSTM: Learning Attention Mechanisms through Recurrent Structures](https://arxiv.org/abs/2503.18565) <a id="link40"></a>

**ArXiv ID:** 2503.18565

**Authors:** Abdoul Majid O. Thiombiano, Brahim Hnich, Ali Ben Mrad, Mohamed Wiem Mkaouer

**Abstract:** The current era of Natural Language Processing (NLP) is dominated by Transformer models. However, novel architectures relying on recurrent mechanisms, such as xLSTM and Mamba, have been proposed as alternatives to attention-based models. Although computation is done differently than with the attention mechanism mechanism, these recurrent models yield good results and sometimes even outperform state-of-the-art attention-based models. In this work, we propose Distil-xLSTM, an xLSTM-based Small Language Model (SLM) trained by distilling knowledge from a Large Language Model (LLM) that shows promising results while being compute and scale efficient. Our Distil-xLSTM focuses on approximating a transformer-based model attention parametrization using its recurrent sequence mixing components and shows good results with minimal training.

**Comment:** The paper introduces a recurrent architecture (Distil-xLSTM) as an alternative to attention-based models, which aligns with foundational research in model architecture.

**Relevance:** 8
**Novelty:** 7

---

## 41. [ConSol: Sequential Probability Ratio Testing to Find Consistent LLM Reasoning Paths Efficiently](https://arxiv.org/abs/2503.17587) <a id="link41"></a>

**ArXiv ID:** 2503.17587

**Authors:** Jaeyeon Lee, Guantong Qi, Matthew Brady Neeley, Zhandong Liu, Hyun-Hwan Jeong

**Abstract:** Recent advancements in large language models (LLMs) integrating explicit reasoning, such as OpenAI's o3-mini, DeepSeek-R1, and QWQ-32B, enable smaller models to solve complex tasks by generating intermediate reasoning steps prior to providing answers. However, this approach significantly increases computational costs, both monetarily and environmentally. The widely-used self-consistency method further exacerbates these costs by aggregating multiple reasoning paths to improve accuracy, often requiring between 40 to 64 samples per task. Although aggregation effectively reduces variance and bias, additional sampling can lead to diminishing returns when early samples yield consistent results. To address inefficiencies, we propose leveraging Sequential Probability Ratio Testing (SPRT) to dynamically terminate sampling once sufficient consistency is achieved. We calibrate SPRT parameters specifically for LLM applications, accounting for sensitivity to detect the mode of the distribution. Our experiments demonstrate that incorporating SPRT significantly enhances token efficiency, achieving comparable accuracy to self-consistency methods but at a substantially reduced computational cost. To promote transparency and facilitate reproducibility, we have made the source code and datasets used in our experiments publicly available at our GitHub repository: https://github.com/LiuzLab/consol, or available as a PyPI package: pip install consol. We hope that this resource will support further research and encourage the development of new methods building upon our work.

**Comment:** The paper introduces a method to improve efficiency in reasoning paths for LLMs, which is relevant to foundational research in LLM efficiency and reasoning.

**Relevance:** 8
**Novelty:** 7

---

## 42. [Every Sample Matters: Leveraging Mixture-of-Experts and High-Quality Data for Efficient and Accurate Code LLM](https://arxiv.org/abs/2503.17793) <a id="link42"></a>

**ArXiv ID:** 2503.17793

**Authors:** Codefuse, Ling Team, :, Wenting Cai, Yuchen Cao, Chaoyu Chen, Chen Chen, Siba Chen, Qing Cui, Peng Di, Junpeng Fang, Zi Gong, Ting Guo, Zhengyu He, Yang Huang, Cong Li, Jianguo Li, Zheng Li, Shijie Lian, BingChang Liu, Songshan Luo, Shuo Mao, Min Shen, Jian Wu, Jiaolong Yang, Wenjie Yang, Tong Ye, Hang Yu, Wei Zhang, Zhenduo Zhang, Hailin Zhao, Xunjin Zheng, Jun Zhou

**Abstract:** Recent advancements in code large language models (LLMs) have demonstrated remarkable capabilities in code generation and understanding. It is still challenging to build a code LLM with comprehensive performance yet ultimate efficiency. Many attempts have been released in the open source community to break the trade-off between performance and efficiency, such as the Qwen Coder series and the DeepSeek Coder series. This paper introduces yet another attempt in this area, namely Ling-Coder-Lite. We leverage the efficient Mixture-of-Experts (MoE) architecture along with a set of high-quality data curation methods (especially those based on program analytics) to build an efficient yet powerful code LLM. Ling-Coder-Lite exhibits on-par performance on 12 representative coding benchmarks compared to state-of-the-art models of similar size, such as Qwen2.5-Coder-7B and DeepSeek-Coder-V2-Lite, while offering competitive latency and throughput. In practice, we achieve a 50\% reduction in deployment resources compared to the similar-sized dense model without performance loss. To facilitate further research and development in this area, we open-source our models as well as a substantial portion of high-quality data for the annealing and post-training stages. The models and data can be accessed at~\url{https://huggingface.co/inclusionAI/Ling-Coder-lite}.

**Comment:** The paper leverages Mixture-of-Experts (MoE) architecture for code LLMs, which directly aligns with the model architecture criterion. However, it focuses on application-specific performance improvements rather than foundational insights.

**Relevance:** 8
**Novelty:** 6

---

## 43. [Do Your Best and Get Enough Rest for Continual Learning](https://arxiv.org/abs/2503.18371) <a id="link43"></a>

**ArXiv ID:** 2503.18371

**Authors:** Hankyul Kang, Gregor Seifer, Donghyun Lee, Jongbin Ryu

**Abstract:** According to the forgetting curve theory, we can enhance memory retention by learning extensive data and taking adequate rest. This means that in order to effectively retain new knowledge, it is essential to learn it thoroughly and ensure sufficient rest so that our brain can memorize without forgetting. The main takeaway from this theory is that learning extensive data at once necessitates sufficient rest before learning the same data again. This aspect of human long-term memory retention can be effectively utilized to address the continual learning of neural networks. Retaining new knowledge for a long period of time without catastrophic forgetting is the critical problem of continual learning. Therefore, based on Ebbinghaus' theory, we introduce the view-batch model that adjusts the learning schedules to optimize the recall interval between retraining the same samples. The proposed view-batch model allows the network to get enough rest to learn extensive knowledge from the same samples with a recall interval of sufficient length. To this end, we specifically present two approaches: 1) a replay method that guarantees the optimal recall interval, and 2) a self-supervised learning that acquires extensive knowledge from a single training sample at a time. We empirically show that these approaches of our method are aligned with the forgetting curve theory, which can enhance long-term memory. In our experiments, we also demonstrate that our method significantly improves many state-of-the-art continual learning methods in various protocols and scenarios. We open-source this project at https://github.com/hankyul2/ViewBatchModel.

**Comment:** The paper introduces a novel view-batch model inspired by human memory theories for continual learning, which aligns with representation learning and training dynamics.

**Relevance:** 7
**Novelty:** 7

---

## 44. [MoST: Efficient Monarch Sparse Tuning for 3D Representation Learning](https://arxiv.org/abs/2503.18368) <a id="link44"></a>

**ArXiv ID:** 2503.18368

**Authors:** Xu Han, Yuan Tang, Jinfeng Xu, Xianzhi Li

**Abstract:** We introduce Monarch Sparse Tuning (MoST), the first reparameterization-based parameter-efficient fine-tuning (PEFT) method tailored for 3D representation learning. Unlike existing adapter-based and prompt-tuning 3D PEFT methods, MoST introduces no additional inference overhead and is compatible with many 3D representation learning backbones. At its core, we present a new family of structured matrices for 3D point clouds, Point Monarch, which can capture local geometric features of irregular points while offering high expressiveness. MoST reparameterizes the dense update weight matrices as our sparse Point Monarch matrices, significantly reducing parameters while retaining strong performance. Experiments on various backbones show that MoST is simple, effective, and highly generalizable. It captures local features in point clouds, achieving state-of-the-art results on multiple benchmarks, e.g., 97.5% acc. on ScanObjectNN (PB_50_RS) and 96.2% on ModelNet40 classification, while it can also combine with other matrix decompositions (e.g., Low-rank, Kronecker) to further reduce parameters.

**Comment:** The paper introduces a sparse tuning method (MoST) for 3D representation learning, which aligns with model compression and sparsity topics. However, its focus on 3D-specific applications reduces its broader foundational relevance.

**Relevance:** 7
**Novelty:** 7

---

## 45. [Neural Network Approach to Stochastic Dynamics for Smooth Multimodal Density Estimation](https://arxiv.org/abs/2503.17807) <a id="link45"></a>

**ArXiv ID:** 2503.17807

**Authors:** Z. Zarezadeh, N. Zarezadeh

**Abstract:** In this paper we consider a new probability sampling methods based on Langevin diffusion dynamics to resolve the problem of existing Monte Carlo algorithms when draw samples from high dimensional target densities. We extent Metropolis-Adjusted Langevin Diffusion algorithm by modelling the stochasticity of precondition matrix as a random matrix. An advantage compared to other proposal method is that it only requires the gradient of log-posterior. The proposed method provides fully adaptation mechanisms to tune proposal densities to exploits and adapts the geometry of local structures of statistical models. We clarify the benefits of the new proposal by modelling a Quantum Probability Density Functions of a free particle in a plane (energy Eigen-functions). The proposed model represents a remarkable improvement in terms of performance accuracy and computational time over standard MCMC method.

**Comment:** The paper introduces a stochastic dynamics-based sampling method for density estimation, which aligns with representation learning by addressing sampling in high-dimensional spaces. However, its focus on specific sampling methods limits its broader impact.

**Relevance:** 7
**Novelty:** 7

---

# Paper Selection Prompt

You are a helpful paper reading assistant whose job is to read daily posts from ArXiv and identify a few papers that your friend will enjoy reading.
Your job is to carefully read the paper titles and abstracts below and find the ones that match the criteria below.

## Instructions

Write the response in JSONL format with {ARXIVID, COMMENT, RELEVANCE, NOVELTY} on each line, one for each paper.

- ARXIVID: should be the ArXiv ID.
- COMMENT: should identify whether there is a criteria that match the paper very closely. These matches should not be based on general terms like "language modeling" or "advancements" and should specifically refer to a criterion. No need to mention the non-matching criteria.
- RELEVANCE: should be a score from 1-10.
- NOVELTY: should be a score from 1-10.

## Scoring Criteria

> The "Relevance" score measures how closely the paper aligns with the core topics of the prompt.
> The "Novelty" score assesses the originality and impact of the paper.
> They are two **ORTHONORMAL** axes and **SHOULD NOT** be confused with each other.

### Relevance Scoring

- Relevance 9-10 (Completely Relevant)
  - Focus: Fully aligned with core topics with no deviation, score the highest if contains relevant keywords in it.
  - Examples: Papers focused on foundational methods or theoretical research, whose titles contain topic keywords like "MoE".

- Relevance 7-8 (Relevant)
  - Focus: Retain a solid link to the main research area, though may touch on peripheral elements.
  - Examples: Papers research on the fundamental part of MoE through a less critical aspect like its behavior in GNN.

- Relevance 5-6 (Borderline)
  - Focus: Maintains a link to the core topic but also extends into at least one other domain/area beyond the primary focus.
  - Examples: Work referencing MoE centered on reinforcement learning.

- Relevance 3-4 (Irrelevant)
  - Focus: Largely outside our interests with no association to our topics.
  - Examples: Application-focused papers like using MoE to solve a problem in the real world.

- Relevance 1-2 (Ignore)
  - Focus: Purely unrelated to our topics. Completely a different domain.
  - **Exception**: If the paper hints at a cutting-edge, radically new direction that could eventually transform the primary domain, consider a score of 9–10 despite initial appearances. (Usually a very rare concept that belongs to the fundamental research)

### Novelty Scoring

- Novelty 9-10 (Breakthrough)
  - Definition: Groundbreaking methods/theory introducing new directions or solving major challenges.
  - Examples: Entirely new paradigm for foundational models; a novel theory transforming representation learning.

- Novelty 7-8 (Improvements)
  - Definition: Substantial insights/enhancements, though not a full paradigm shift.
  - Examples: Modifications on existing methods yielding significantly better results.

- Novelty 5-6 (Borderline)
  - Definition: Incremental contributions with possible long-term benefits, not immediately transformative.
  - Examples: Moderately novel extension to an existing architecture; refining current methods without fundamentally altering them.

- Novelty 3-4 (Tangential)
  - Definition: Minor or domain-specific improvements with limited broader impact.
  - Examples: Slight modifications to known methods with strange motivation; purely engineering jobs like a new benchmark/dataset.

- Novelty 1-2 (Low)
  - Definition: Minimal originality, applying standard approaches without real innovation.
  - Examples: Using an off-the-shelf model without adding new insights; purely application-driven studies like finetuning a pretrained model using existing methods.

## Papers

[PAPER LIST HERE]

## Relevant Topics

Use the following relevance criteria to focus on foundational research. Keep **relevant** papers and filter out **irrelevant** ones. Avoid purely **application-driven** work.

1. Representation Learning
   - Relevant: Insights into how deep networks encode information, feature/dictionary learning, sparse/contrastive methods, training dynamics in neural networks.
   - Irrelevant: Standard applications of known techniques lacking new theoretical or methodological contributions.

2. Model Architecture
   - Relevant: Mixture-of-Experts (MoE), Transformers, Conditional/Dynamic Networks, Autoencoders, analysis on existing architectures (like encoder-decoder), or other architectural innovations.
   - Irrelevant: Merely using existing architectures for a certain task without insights into the structure themselves.

3. Model Compression
   - Relevant: Sparsity, pruning, quantization, low-rank approaches, KV cache, or other algorithmic/theoretical efficiency breakthroughs.
   - Irrelevant: Straightforward applications of existing compression methods to new tasks.

4. Large Language Models (LLMs)
   - Relevant: Major breakthroughs in pretraining or architecture, theoretical insights into LLM behavior/interpretability.
   - Irrelevant: Domain-specific usage (e.g., translation, jail-breaking), finetuning or inference tricks (e.g., instruction tuning, chain-of-thoughts, data mixing), or empirical dataset/benchmark studies and text-level analysis (e.g. hallucination, reasoning, safety).

5. AI for Science
   - Relevant: Foundational research in molecular/protein modeling, new generative paradigms, or significant architecture-level innovations.
   - Irrelevant: Conventional, domain-specific applications without new theoretical perspectives.

6. Emerging Trends
   - Relevant: Cutting-edge theoretical work challenging established assumptions or introducing broad new paradigms.
   - Irrelevant: Incremental improvements or trend-following without novel insights.

**Keywords:**

- Relevant: Mixture of Experts (MoE), Representation Learning, Compression/Efficiency, Sparse/Sparsity, Pruning, Quantization, Low-rank, Foundation Model, etc.
- Irrelevant: Reinforcement Learning, Transfer Learning, Federated Learning, Online Learning, Diffusion Models, etc.
- Application: Image Segmentation, Medical Imaging, 3D Vision, Video Understanding, Information Retrieval, Summarization, Recommendation Systems, Machine Translation, Speech Recognition, Signal Processing, Spatial/Temporal Modeling, Time Series, Knowledge Graph, etc.