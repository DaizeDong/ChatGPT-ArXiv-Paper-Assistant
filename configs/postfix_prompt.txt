## Scoring Criteria

> The "Relevance" score measures how closely the paper aligns with the core topics of the prompt.
> The "Novelty" score assesses the originality and impact of the paper.
> They are two **ORTHONORMAL** axes and **SHOULD NOT** be confused with each other. E.g., a paper with high relevance can be of low novelty, or vice versa.

### Relevance Scoring

- Relevance 9-10 (Completely Relevant)
  - Focus: Fully aligned with core topics, score the highest if also contains keywords in it.
  - Keywords: “Mixture of Experts (MoE),” “Representation Learning,” “Compression,” “Sparse,” “Pruning,” “Quantization,” “Low-rank,” “Theoretical,” “Scalability,” “Foundation Models,” etc.
  - Examples: Papers focused on foundational methods or theoretical research, whose titles contain topic keywords (like "MoE").

- Relevance 7-8 (Relevant)
  - Focus: Clearly tied to our main topics, may not fully hit the interest in foundational methods.
  - Examples: Pure research on representation/architecture with no other domain focus; significant overlap with MoE.

- Relevance 5-6 (Optional)
  - Focus: Link to our topics—covers relevant ideas but also includes another area of interest.
  - Examples: Work referencing MoE in a broader context or centered on another domain like federated learning, online learning, etc.

- Relevance 3-4 (Irrelevant)
  - Focus: Largely outside our interests, with little/no association to our topics.
  - Examples: application-level tasks like using MoE as a method for medical image segmentation, etc.

- Relevance 1-2 (Ignore)
  - Focus: Purely unrelated to our topics.
  - Examples: Entirely different fields like reinforcement learning, 3D vision learning, etc.

### Novelty Scoring

> *Note: Foundation vs. Application*
> - Foundational/theoretical papers (new theorems, architectures, or strong methodological insights) are of **high novelty**.
> - Subdomain papers  and application-focused papers (e.g., "methods for xxx") are **lower** in novelty.

- Novelty 9-10 (Breakthrough)
  - Definition: Groundbreaking methods/theory introducing new directions or solving major challenges.
  - Examples: Entirely new paradigm for MoE routing; a novel theoretical result transforming representation learning.

- Novelty 7-8 (Improvements)
  - Definition: Substantial insights/enhancements, though not a full paradigm shift.
  - Examples: Modifications on existing methods yielding significantly better results.

- Novelty 5-6 (Moderate)
  - Definition: Incremental contributions with possible long-term benefits, not immediately transformative.
  - Examples: Moderately novel extension to an existing architecture; refining current methods without fundamentally altering them.

- Novelty 3-4 (Tangential)
  - Definition: Minor or domain-specific improvements with limited broader impact.
  - Examples: Slight modifications to known methods that don’t change the broader landscape (e.g., a standard LLM fine-tuned on a new dataset).

- Novelty 1-2 (Low)
  - Definition: Minimal originality, applying standard approaches without real innovation.
  - Examples: Using an off-the-shelf model without adding new insights; purely application-driven studies with no methodological advancement.

## Instructions

Write the response in JSONL format with {ARXIVID, COMMENT, RELEVANCE, NOVELTY} on each line, one for each paper.

- ARXIVID: should be the ArXiv ID.
- COMMENT: should identify whether there is a criteria that match the paper very closely. These matches should not be based on general terms like "language modeling" or "advancements" and should specifically refer to a criterion. No need to mention the non-matching criteria.
- RELEVANCE: should be a score from 1-10.
- NOVELTY: should be a score from 1-10.