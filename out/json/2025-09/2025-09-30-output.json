{
    "2509.23202": {
        "SCORE": 19,
        "ARXIVID": "2509.23202",
        "COMMENT": "Compression/Efficiency: FP4 post-training quantization tailored to MXFP4/NVFP4 (MR-GPTQ) with format-specific algorithms and high-performance GPU kernels for LLM inference.",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "Vage Egiazarian",
            "Roberto L. Castro",
            "Denis Kuznedelev",
            "Andrei Panferov",
            "Eldar Kurtic",
            "Shubhra Pandit",
            "Alexandre Marques",
            "Mark Kurtz",
            "Saleh Ashkboos",
            "Torsten Hoefler",
            "Dan Alistarh"
        ],
        "title": "Bridging the Gap Between Promise and Performance for Microscaling FP4 Quantization",
        "abstract": "The recent hardware-accelerated microscaling 4-bit floating-point formats such as MXFP4 and NVFP4, supported on NVIDIA and AMD GPUs, promise to revolutionize large language model (LLM) inference. Yet, their practical benefits remain unproven. We present the first comprehensive study of MXFP4 and NVFP4 for post-training quantization, revealing gaps between their promise and real-world performance. Our analysis shows that state-of-the-art methods struggle with FP4, due to two key issues: (1) NVFP4's small group size provably neutralizes traditional outlier mitigation techniques; (2) MXFP4's power-of-two scale quantization severely degrades accuracy due to high induced error. To bridge this gap, we introduce Micro-Rotated-GPTQ (MR-GPTQ), a variant of the classic GPTQ quantization algorithm that tailors the quantization process to FP4's unique properties, by using block-wise Hadamard transforms and format-specific optimizations. We support our proposal with a set of high-performance GPU kernels that enable the MR-GPTQ format with negligible overhead, by rotation fusion into the weights, and fast online computation of the activations. This leads to speedups vs. FP16 of up to 3.6x layer-wise, and 2.2x end-to-end on NVIDIA B200, and of 6x layer-wise and 4x end-to-end on RTX5090. Our extensive empirical evaluation demonstrates that MR-GPTQ matches or outperforms state-of-the-art accuracy, significantly boosting MXFP4, to the point where it nears that of NVFP4. We conclude that, while FP4 is not an automatic upgrade over INT4, format-specialized methods like MR-GPTQ can unlock a new frontier of accuracy-performance trade-offs.",
        "arxiv_id": "2509.23202"
    },
    "2509.24914": {
        "SCORE": 19,
        "ARXIVID": "2509.24914",
        "COMMENT": "Architecture analysis: theoretical study of single-head attention\u2019s inductive bias and spectral properties, revealing implicit low-rank regularization via weight decay.",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "Fabrizio Boncoraglio",
            "Vittorio Erba",
            "Emanuele Troiani",
            "Florent Krzakala",
            "Lenka Zdeborov\\'a"
        ],
        "title": "Inductive Bias and Spectral Properties of Single-Head Attention in High Dimensions",
        "abstract": "We study empirical risk minimization in a single-head tied-attention layer trained on synthetic high-dimensional sequence tasks, given by the recently introduced attention-indexed model. Using tools from random matrix theory, spin-glass physics, and approximate message passing, we derive sharp asymptotics for training and test errors, locate interpolation and recovery thresholds, and characterize the limiting spectral distribution of the learned weights. Weight decay induces an implicit nuclear-norm regularization, favoring low-rank query and key matrices. Leveraging this, we compare the standard factorized training of query and key matrices with a direct parameterization in which their product is trained element-wise, revealing the inductive bias introduced by the factorized form. Remarkably, the predicted spectral distribution echoes empirical trends reported in large-scale transformers, offering a theoretical perspective consistent with these phenomena.",
        "arxiv_id": "2509.24914"
    },
    "2509.24125": {
        "SCORE": 19,
        "ARXIVID": "2509.24125",
        "COMMENT": "Model Architecture Theory: impossibility result for inverse permutation learning in decoder-only Transformers; shows fixes via encoder-decoder or scratch tokens.",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "Rohan Alur",
            "Chris Hays",
            "Manish Raghavan",
            "Devavrat Shah"
        ],
        "title": "The Impossibility of Inverse Permutation Learning in Transformer Models",
        "abstract": "In this technical note, we study the problem of inverse permutation learning in decoder-only transformers. Given a permutation and a string to which that permutation has been applied, the model is tasked with producing the original (``canonical'') string. We argue that this task models a natural robustness property across a variety of reasoning tasks, including long-context retrieval, multiple choice QA and in-context learning. Our primary contribution is an impossibility result: we show that an arbitrary depth, decoder-only transformer cannot learn this task. This result concerns the expressive capacity of decoder-only transformer models and is agnostic to training dynamics or sample complexity. We give a pair of alternative constructions under which inverse permutation learning is feasible. The first of these highlights the fundamental role of the causal attention mask, and reveals a gap between the expressivity of encoder-decoder transformers and the more popular decoder-only architecture. The latter result is more surprising: we show that simply padding the input with ``scratch tokens\" yields a construction under which inverse permutation learning is possible. We conjecture that this may suggest an alternative mechanism by which chain-of-thought prompting or, more generally, intermediate ``thinking'' tokens can enable reasoning in large language models, even when these tokens encode no meaningful semantic information (e.g., the results of intermediate computations).",
        "arxiv_id": "2509.24125"
    },
    "2509.23436": {
        "SCORE": 19,
        "ARXIVID": "2509.23436",
        "COMMENT": "Efficiency: linear-time attention via low-rank entropic optimal transport with provably doubly-stochastic maps and O(nr) compute.",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "Ashkan Shahbazi",
            "Chayne Thrash",
            "Yikun Bai",
            "Keaton Hamm",
            "Navid NaderiAlizadeh",
            "Soheil Kolouri"
        ],
        "title": "LOTFormer: Doubly-Stochastic Linear Attention via Low-Rank Optimal Transport",
        "abstract": "Transformers have proven highly effective across a wide range of modalities. However, the quadratic complexity of the standard softmax attention mechanism poses a fundamental barrier to scaling them to long context windows. A large body of work addresses this with linear attention, which reformulates attention as a kernel function and approximates it with finite feature maps to achieve linear-time computation. Orthogonal to computational scaling, most attention mechanisms -- both quadratic and linear -- produce row-normalized maps that can over-focus on a few tokens, degrading robustness and information flow. Enforcing doubly-stochastic attention alleviates this by balancing token participation across rows and columns, but existing doubly-stochastic attention mechanisms typically introduce substantial overhead, undermining scalability. We propose LOTFormer, a principled attention mechanism that is simultaneously linear-time and doubly-stochastic. Our approach exploits the connection between attention maps and transportation plans between query and key measures. The central idea is to constrain the transport plan to be low-rank by conditioning it on a learnable pivot measure with small support. Concretely, we solve two entropic optimal transport problems (queries $\\to$ pivot and pivot $\\to$ keys) and compose them into a conditional (glued) coupling. This yields an attention matrix that is provably doubly-stochastic, has rank at most $r \\ll n$, and applies to values in $O(nr)$ time without forming the full $n \\times n$ map. The pivot locations and masses are learned end-to-end. Empirically, LOTFormer achieves state-of-the-art results on the Long Range Arena benchmark, surpassing prior linear and transport-based attention methods in both accuracy and efficiency.",
        "arxiv_id": "2509.23436"
    },
    "2509.25149": {
        "SCORE": 19,
        "ARXIVID": "2509.25149",
        "COMMENT": "Model Compression and Efficiency + High Performance Computing: stable 4-bit (NVFP4) LLM pretraining using RHT, 2D quantization, stochastic rounding, and selective high-precision layers.",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "NVIDIA",
            "Felix Abecassis",
            "Anjulie Agrusa",
            "Dong Ahn",
            "Jonah Alben",
            "Stefania Alborghetti",
            "Michael Andersch",
            "Sivakumar Arayandi",
            "Alexis Bjorlin",
            "Aaron Blakeman",
            "Evan Briones",
            "Ian Buck",
            "Bryan Catanzaro",
            "Jinhang Choi",
            "Mike Chrzanowski",
            "Eric Chung",
            "Victor Cui",
            "Steve Dai",
            "Bita Darvish Rouhani",
            "Carlo del Mundo",
            "Deena Donia",
            "Burc Eryilmaz",
            "Henry Estela",
            "Abhinav Goel",
            "Oleg Goncharov",
            "Yugi Guvvala",
            "Robert Hesse",
            "Russell Hewett",
            "Herbert Hum",
            "Ujval Kapasi",
            "Brucek Khailany",
            "Mikail Khona",
            "Nick Knight",
            "Alex Kondratenko",
            "Ronny Krashinsky",
            "Ben Lanir",
            "Simon Layton",
            "Michael Lightstone",
            "Daniel Lo",
            "Paulius Micikevicius",
            "Asit Mishra",
            "Tim Moon",
            "Deepak Narayanan",
            "Chao Ni",
            "Abhijit Paithankar",
            "Satish Pasumarthi",
            "Ankit Patel",
            "Mostofa Patwary",
            "Ashwin Poojary",
            "Gargi Prasad",
            "Sweta Priyadarshi",
            "Yigong Qin",
            "Xiaowei Ren",
            "Oleg Rybakov",
            "Charbel Sakr",
            "Sanjeev Satheesh",
            "Stas Sergienko",
            "Pasha Shamis",
            "Kirthi Shankar",
            "Nishant Sharma",
            "Mohammad Shoeybi",
            "Michael Siu",
            "Misha Smelyanskiy",
            "Darko Stosic",
            "Dusan Stosic",
            "Bor-Yiing Su",
            "Frank Sun",
            "Nima Tajbakhsh",
            "Shelby Thomas",
            "Przemek Tredak",
            "Evgeny Tsykunov",
            "Gandhi Vaithilingam",
            "Aditya Vavre",
            "Rangharajan Venkatesan",
            "Roger Waleffe",
            "Qiyu Wan",
            "Hexin Wang",
            "Mengdi Wang",
            "Lizzie Wei",
            "Hao Wu",
            "Evan Wu",
            "Keith Wyss",
            "Ning Xu",
            "Jinze Xue",
            "Charlene Yang",
            "Yujia Zhai",
            "Ruoxi Zhang",
            "Jingyang Zhu",
            "Zhongbo Zhu"
        ],
        "title": "Pretraining Large Language Models with NVFP4",
        "abstract": "Large Language Models (LLMs) today are powerful problem solvers across many domains, and they continue to get stronger as they scale in model size, training set size, and training set quality, as shown by extensive research and experimentation across the industry. Training a frontier model today requires on the order of tens to hundreds of yottaflops, which is a massive investment of time, compute, and energy. Improving pretraining efficiency is therefore essential to enable the next generation of even more capable LLMs. While 8-bit floating point (FP8) training is now widely adopted, transitioning to even narrower precision, such as 4-bit floating point (FP4), could unlock additional improvements in computational speed and resource utilization. However, quantization at this level poses challenges to training stability, convergence, and implementation, notably for large-scale models trained on long token horizons.   In this study, we introduce a novel approach for stable and accurate training of large language models (LLMs) using the NVFP4 format. Our method integrates Random Hadamard transforms (RHT) to bound block-level outliers, employs a two-dimensional quantization scheme for consistent representations across both the forward and backward passes, utilizes stochastic rounding for unbiased gradient estimation, and incorporates selective high-precision layers. We validate our approach by training a 12-billion-parameter model on 10 trillion tokens -- the longest publicly documented training run in 4-bit precision to date. Our results show that the model trained with our NVFP4-based pretraining technique achieves training loss and downstream task accuracies comparable to an FP8 baseline. These findings highlight that NVFP4, when combined with our training approach, represents a major step forward in narrow-precision LLM training algorithms.",
        "arxiv_id": "2509.25149"
    },
    "2509.24093": {
        "SCORE": 19,
        "ARXIVID": "2509.24093",
        "COMMENT": "Matches Model Architecture and Efficiency: O(N log N) global equivariant attention via Clebsch\u2013Gordan convolution supporting high-order features.",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "Owen Lewis Howell",
            "Linfeng Zhao",
            "Xupeng Zhu",
            "Yaoyao Qian",
            "Haojie Huang",
            "Lingfeng Sun",
            "Wil Thomason",
            "Robert Platt",
            "Robin Walters"
        ],
        "title": "Clebsch-Gordan Transformer: Fast and Global Equivariant Attention",
        "abstract": "The global attention mechanism is one of the keys to the success of transformer architecture, but it incurs quadratic computational costs in relation to the number of tokens. On the other hand, equivariant models, which leverage the underlying geometric structures of problem instance, often achieve superior accuracy in physical, biochemical, computer vision, and robotic tasks, at the cost of additional compute requirements. As a result, existing equivariant transformers only support low-order equivariant features and local context windows, limiting their expressiveness and performance. This work proposes Clebsch-Gordan Transformer, achieving efficient global attention by a novel Clebsch-Gordon Convolution on $\\SO(3)$ irreducible representations. Our method enables equivariant modeling of features at all orders while achieving ${O}(N \\log N)$ input token complexity. Additionally, the proposed method scales well with high-order irreducible features, by exploiting the sparsity of the Clebsch-Gordon matrix. Lastly, we also incorporate optional token permutation equivariance through either weight sharing or data augmentation. We benchmark our method on a diverse set of benchmarks including n-body simulation, QM9, ModelNet point cloud classification and a robotic grasping dataset, showing clear gains over existing equivariant transformers in GPU memory size, speed, and accuracy.",
        "arxiv_id": "2509.24093"
    },
    "2509.22840": {
        "SCORE": 19,
        "ARXIVID": "2509.22840",
        "COMMENT": "Matches Model Architecture (theory): capacity scaling law for self-attention and principled multi-head budget allocation.",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "Micah Adler"
        ],
        "title": "On the Capacity of Self-Attention",
        "abstract": "While self-attention is known to learn relations among tokens, we lack a formal understanding of its capacity: how many distinct relations can a single layer reliably recover for a given budget?   To formalize this, we introduce Relational Graph Recognition (RGR), where the key-query channel represents a graph on $m$ items with $m'$ directed edges, and, given a context of items, must recover the neighbors of each item. We measure resources by the total key dimension $D_K = h\\,d_k$. Within this framework, we analytically derive a capacity scaling law and validate it empirically. We show that $D_K = \\Theta(m' \\log m' / d_{\\text{model}})$ is both necessary (information-theoretic lower bound) and sufficient (explicit construction) in a broad class of graphs to recover $m'$ relations. This scaling law directly leads to a new, capacity-based rationale for multi-head attention that applies even when each item only attends to a single target. When embeddings are uncompressed ($m = d_{\\text{model}}$) and the graph is a permutation, a single head suffices. However, compression ($m > d_{\\text{model}}$) forces relations into overlapping subspaces, creating interference that a single large head cannot disentangle. Our analysis shows that allocating a fixed $D_K$ across many small heads mitigates this interference, increasing the number of recoverable relations. Controlled single-layer experiments mirror the theory, revealing a sharp performance threshold that matches the predicted capacity scaling and confirms the benefit of distributing $D_K$ across multiple heads.   Altogether, these results provide a concrete scaling law for self-attention capacity and a principled design rule for allocating key-query budget across heads.",
        "arxiv_id": "2509.22840"
    },
    "2509.23024": {
        "SCORE": 19,
        "ARXIVID": "2509.23024",
        "COMMENT": "Matches Representation Learning criterion: spectral geometry (effective rank, eigenspectrum decay) tracing phases from pretraining to post-training.",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "Melody Zixuan Li",
            "Kumar Krishna Agrawal",
            "Arna Ghosh",
            "Komal Kumar Teru",
            "Adam Santoro",
            "Guillaume Lajoie",
            "Blake A. Richards"
        ],
        "title": "Tracing the Representation Geometry of Language Models from Pretraining to Post-training",
        "abstract": "Standard training metrics like loss fail to explain the emergence of complex capabilities in large language models. We take a spectral approach to investigate the geometry of learned representations across pretraining and post-training, measuring effective rank (RankMe) and eigenspectrum decay ($\\alpha$-ReQ). With OLMo (1B-7B) and Pythia (160M-12B) models, we uncover a consistent non-monotonic sequence of three geometric phases during autoregressive pretraining. The initial \"warmup\" phase exhibits rapid representational collapse. This is followed by an \"entropy-seeking\" phase, where the manifold's dimensionality expands substantially, coinciding with peak n-gram memorization. Subsequently, a \"compression-seeking\" phase imposes anisotropic consolidation, selectively preserving variance along dominant eigendirections while contracting others, a transition marked with significant improvement in downstream task performance. We show these phases can emerge from a fundamental interplay of cross-entropy optimization under skewed token frequencies and representational bottlenecks ($d \\ll |V|$). Post-training further transforms geometry: SFT and DPO drive \"entropy-seeking\" dynamics to integrate specific instructional or preferential data, improving in-distribution performance while degrading out-of-distribution robustness. Conversely, RLVR induces \"compression-seeking\", enhancing reward alignment but reducing generation diversity.",
        "arxiv_id": "2509.23024"
    },
    "2509.23678": {
        "SCORE": 19,
        "ARXIVID": "2509.23678",
        "COMMENT": "Matches Mixture-of-Experts + Scaling Laws: comprehensive MoE scaling law over key factors (N, Na, G, S, D) with optimal configuration guidance.",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "Guoliang Zhao",
            "Yuhan Fu",
            "Shuaipeng Li",
            "Xingwu Sun",
            "Ruobing Xie",
            "An Wang",
            "Weidong Han",
            "Zhen Yang",
            "Weixuan Sun",
            "Yudong Zhang",
            "Cheng-zhong Xu",
            "Di Wang",
            "Jie Jiang"
        ],
        "title": "Towards a Comprehensive Scaling Law of Mixture-of-Experts",
        "abstract": "Mixture-of-Experts (MoE) models have become the consensus approach for enabling parameter-efficient scaling and cost-effective deployment in large language models. However, existing scaling laws for dense models are inapplicable to MoE models, which stems from three critical challenges: the multiplicity of influencing factors, their intricate coupling relationships and the non-monotonic nature of their performance impacts. They collectively necessitate a fine-grained investigation into MoE-specific scaling laws. In this work, we perform a systematic decomposition of MoE settings, identifying five key factors that influence model performance from both size and structural perspectives (data size ($D$), total model size ($N$), activated model size ($N_a$), number of active experts ($G$) and the ratio of shared experts ($S$)). Specifically, we design $446$ controlled experiments to characterize their marginal effects, ultimately constructing a comprehensive and precise joint MoE scaling law that considers all essential factors. Furthermore, we derive the theoretically optimal and practically efficiency-aware optimal configurations for $G$, $S$ and $N_a/N$ with detailed analyses. Our results demonstrate that the optimal settings for $G$ and $S$ are independent of both the model architecture and data size. With the scaling of $N$, the optimal activation parameter ratio of $N_a/N$ becomes sparser. Our proposed MoE scaling law could function as an accurate and insightful guidance to facilitate future MoE model design and training.",
        "arxiv_id": "2509.23678"
    },
    "2509.22935": {
        "SCORE": 18,
        "ARXIVID": "2509.22935",
        "COMMENT": "Model Compression and Efficiency: compute allocation scaling law for QAT vs FP, tokens-per-parameter-byte predictor, and fused cooldown+QAT for efficient quantized training.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Aleksandr Dremov",
            "David Grangier",
            "Angelos Katharopoulos",
            "Awni Hannun"
        ],
        "title": "Compute-Optimal Quantization-Aware Training",
        "abstract": "Quantization-aware training (QAT) is a leading technique for improving the accuracy of quantized neural networks. Previous work has shown that decomposing training into a full-precision (FP) phase followed by a QAT phase yields superior accuracy compared to QAT alone. However, the optimal allocation of compute between the FP and QAT phases remains unclear. We conduct extensive experiments with various compute budgets, QAT bit widths, and model sizes from 86.0M to 2.2B to investigate how different QAT durations impact final performance. We demonstrate that, contrary to previous findings, the loss-optimal ratio of QAT to FP training increases with the total amount of compute. Moreover, the optimal fraction can be accurately predicted for a wide range of model sizes and quantization widths using the tokens-per-parameter-byte statistic. From experimental data, we derive a loss scaling law that predicts both optimal QAT ratios and final model performance across different QAT/FP compute allocation strategies and QAT bit widths. We use the scaling law to make further predictions, which we verify experimentally, including which QAT bit width is optimal under a given memory constraint and how QAT accuracy with different bit widths compares to full-precision model accuracy. Additionally, we propose a novel cooldown and QAT fusion approach that performs learning rate decay jointly with quantization-aware training, eliminating redundant full-precision model updates and achieving significant compute savings. These findings provide practical insights into efficient QAT planning and enable the training of higher-quality quantized models with the same compute budget.",
        "arxiv_id": "2509.22935"
    },
    "2509.24745": {
        "SCORE": 18,
        "ARXIVID": "2509.24745",
        "COMMENT": "Compression/Efficiency: guided sparse attention via representative heads with dynamic budgets, yielding training-free acceleration of attention and prefill.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Yixuan Wang",
            "Huang He",
            "Siqi Bao",
            "Hua Wu",
            "Haifeng Wang",
            "Qingfu Zhu",
            "Wanxiang Che"
        ],
        "title": "ProxyAttn: Guided Sparse Attention via Representative Heads",
        "abstract": "The quadratic complexity of attention mechanisms limits the efficiency of Large Language Models (LLMs) on long-text tasks. Recently, methods that dynamically estimate block importance have enabled efficient block sparse attention, leading to significant acceleration in long-text pre-filling of LLMs. However, their coarse-grained estimation inevitably leads to performance degradation at high sparsity rates. In this work, we propose ProxyAttn, a training-free sparse attention algorithm that achieves more precise block estimation by compressing the dimension of attention heads. Based on our observation of the similarity among multiple attention heads, we use the scores of pooled representative heads to approximate the scores for all heads. To account for the varying sparsity among heads, we also propose a block-aware dynamic budget estimation method. By combining the scores from representative proxy heads with multi-head dynamic budgets, we achieve a more fine-grained block importance evaluation at low computational cost. Experiments on a variety of mainstream models and extensive benchmarks confirm the underlying similarity among attention heads. Leveraging a fine-grained estimation, the proposed method achieves substantial gains in performance and efficiency compared to existing methods. More precisely, ProxyAttn can achieve up to 10.3x attention acceleration and 2.4x prefilling acceleration without significant performance loss. Our code is available at https://github.com/wyxstriker/ProxyAttn.",
        "arxiv_id": "2509.24745"
    },
    "2509.23898": {
        "SCORE": 18,
        "ARXIVID": "2509.23898",
        "COMMENT": "Compression/Efficiency: differentiable structured sparsity via D-Gating, theoretically equivalent to non-smooth group penalties with improved optimization dynamics.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Chris Kolb",
            "Laetitia Frost",
            "Bernd Bischl",
            "David R\\\"ugamer"
        ],
        "title": "Differentiable Sparsity via $D$-Gating: Simple and Versatile Structured Penalization",
        "abstract": "Structured sparsity regularization offers a principled way to compact neural networks, but its non-differentiability breaks compatibility with conventional stochastic gradient descent and requires either specialized optimizers or additional post-hoc pruning without formal guarantees. In this work, we propose $D$-Gating, a fully differentiable structured overparameterization that splits each group of weights into a primary weight vector and multiple scalar gating factors. We prove that any local minimum under $D$-Gating is also a local minimum using non-smooth structured $L_{2,2/D}$ penalization, and further show that the $D$-Gating objective converges at least exponentially fast to the $L_{2,2/D}$-regularized loss in the gradient flow limit. Together, our results show that $D$-Gating is theoretically equivalent to solving the original group sparsity problem, yet induces distinct learning dynamics that evolve from a non-sparse regime into sparse optimization. We validate our theory across vision, language, and tabular tasks, where $D$-Gating consistently delivers strong performance-sparsity tradeoffs and outperforms both direct optimization of structured penalties and conventional pruning baselines.",
        "arxiv_id": "2509.23898"
    },
    "2509.22944": {
        "SCORE": 18,
        "ARXIVID": "2509.22944",
        "COMMENT": "Compression/Efficiency: calibration-free low-precision LLM quantization using Sinkhorn-normalized per-row/column scaling to reduce matrix imbalance.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Lorenz K. M\\\"uller",
            "Philippe Bich",
            "Jiawei Zhuang",
            "Ahmet \\c{C}elik",
            "Luca Benfenati",
            "Lukas Cavigelli"
        ],
        "title": "SINQ: Sinkhorn-Normalized Quantization for Calibration-Free Low-Precision LLM Weights",
        "abstract": "Post-training quantization has emerged as the most widely used strategy for deploying large language models at low precision. Still, current methods show perplexity degradation at bit-widths less than or equal to 4, partly because representing outliers causes precision issues in parameters that share the same scales as these outliers. This problem is especially pronounced for calibration-free, uniform quantization methods. We introduce SINQ to augment existing post-training quantizers with an additional second-axis scale factor and a fast Sinkhorn-Knopp-style algorithm that finds scales to normalize per-row and per-column variances, thereby minimizing a novel per-matrix proxy target for quantization: the matrix imbalance. Our method has no interactions between layers and can be trivially applied to new architectures to quantize any linear layers. We evaluate our method on the Qwen3 model family and DeepSeek-V2.5. SINQ improves WikiText2 and C4 perplexity significantly against uncalibrated uniform quantization baselines and can be further enhanced by combining it with calibration and non-uniform quantization levels. Code to reproduce the results of this work and to easily quantize models using SINQ is available at https://github.com/huawei-csl/SINQ.",
        "arxiv_id": "2509.22944"
    },
    "2509.23809": {
        "SCORE": 18,
        "ARXIVID": "2509.23809",
        "COMMENT": "Compression/Efficiency: ternary quantization for LLMs with trapping-free optimization by repurposing deadzone-trapped weights as dynamic biases.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Hong Huang",
            "Decheng Wu",
            "Rui Cen",
            "Guanghua Yu",
            "Zonghang Li",
            "Kai Liu",
            "Jianchen Zhu",
            "Peng Chen",
            "Xue Liu",
            "Dapeng Wu"
        ],
        "title": "Tequila: Trapping-free Ternary Quantization for Large Language Models",
        "abstract": "Quantization techniques are essential for the deployment of Large Language Models (LLMs) on edge devices. However, prevailing methods often rely on mixed-precision multiplication that lacks efficient hardware support, making it not feasible. Ternary weight quantization addresses this by constraining weights to {-1, 0, 1}, replacing expensive multiplications with hardware-efficient additions. However, such aggressive compression leads to significant accuracy degradation, even after costly quantization-aware training with massive data. We identify the core issue as deadzone trapping: a large number of weights are trapped at the deadzone boundary. This occurs because these weights receive only noisy, uninformative gradients, preventing stable escape from the deadzone and severely impeding model capacity and optimization. To address this issue, we propose Tequila, a trapping-free quantization optimization method that reactivates deadzone-trapped weights by repurposing them as dynamic biases. This allows the repurposed weights to provide a continuous signal in the forward pass and, critically, receive direct, meaningful gradient signals during backpropagation, thereby enhancing model capacity and optimization with nearly zero inference overhead. Extensive evaluations demonstrate that Tequila outperforms state-of-the-art (SOTA) ternary quantization methods across five benchmarks. Specifically, on the ARC benchmark, it achieves >4% accuracy gain over the SOTA baseline, nearly matching full-precision performance (within <1% gap) with a 3.0x inference speedup. Consequently, Tequila offers a highly practical and efficient implementation for the deployment of advanced LLMs in resource-constrained environments. The code is available at https://github.com/Tencent/AngelSlim.",
        "arxiv_id": "2509.23809"
    },
    "2509.24663": {
        "SCORE": 18,
        "ARXIVID": "2509.24663",
        "COMMENT": "Trainable sparse attention with dense\u2013sparse switch; parameter reuse enables seamless short-to-long adaptation and 4x speedups.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Weilin Zhao",
            "Zihan Zhou",
            "Zhou Su",
            "Chaojun Xiao",
            "Yuxuan Li",
            "Yanghao Li",
            "Yudi Zhang",
            "Weilun Zhao",
            "Zhen Li",
            "Yuxiang Huang",
            "Ao Sun",
            "Xu Han",
            "Zhiyuan Liu"
        ],
        "title": "InfLLM-V2: Dense-Sparse Switchable Attention for Seamless Short-to-Long Adaptation",
        "abstract": "Long-sequence processing is a critical capability for modern large language models. However, the self-attention mechanism in the standard Transformer architecture faces severe computational and memory bottlenecks when processing long sequences. While trainable sparse attention methods offer a promising solution, existing approaches such as NSA introduce excessive extra parameters and disrupt the conventional \\textit{pretrain-on-short, finetune-on-long} workflow, resulting in slow convergence and difficulty in acceleration. To overcome these limitations, we introduce dense-sparse switchable attention framework, termed as InfLLM-V2. InfLLM-V2 is a trainable sparse attention that seamlessly adapts models from short to long sequences. Specifically, InfLLM-V2 reuses dense attention parameters through parameter-free architecture modification, maintaining consistency between short and long sequence processing. Additionally, InfLLM-V2 ensures computational efficiency across all sequence lengths, by using dense attention for short inputs and smoothly transitioning to sparse attention for long sequences. To achieve practical acceleration, we further introduce an efficient implementation of InfLLM-V2 that significantly reduces the computational overhead. Our experiments on long-context understanding and chain-of-thought reasoning demonstrate that InfLLM-V2 is 4$\\times$ faster than dense attention while retaining 98.1% and 99.7% of the performance, respectively. Based on the InfLLM-V2 framework, we have trained and open-sourced MiniCPM4.1 (https://huggingface.co/openbmb/MiniCPM4.1-8B), a hybrid reasoning model, providing a reproducible implementation for the research community.",
        "arxiv_id": "2509.24663"
    },
    "2509.23012": {
        "SCORE": 18,
        "ARXIVID": "2509.23012",
        "COMMENT": "Model Architecture and Efficiency (MoE): PHDS enables flexible runtime sparsity (dial k) from a single MoE checkpoint via lightweight SFT, giving controllable accuracy/latency trade-offs.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Lauren. A Hannah",
            "Soheil Zibakhsh",
            "Kumari Nishu",
            "Arnav Kundu",
            "Mohammad Samragh Razlighi",
            "Mehrdad Farajtabar",
            "Minsik Cho"
        ],
        "title": "MoE-PHDS: One MoE checkpoint for flexible runtime sparsity",
        "abstract": "Sparse Mixtures of Experts (MoEs) are typically trained to operate at a fixed sparsity level, e.g. $k$ in a top-$k$ gating function. This global sparsity level determines an operating point on the accuracy/latency curve; currently, meeting multiple efficiency targets means training and maintaining multiple models. This practice complicates serving, increases training and maintenance costs, and limits flexibility in meeting diverse latency, efficiency, and energy requirements. We show that pretrained MoEs are more robust to runtime sparsity shifts than commonly assumed, and introduce MoE-PHDS ({\\bf P}ost {\\bf H}oc {\\bf D}eclared {\\bf S}parsity), a lightweight SFT method that turns a single checkpoint into a global sparsity control surface. PHDS mixes training across sparsity levels and anchors with a short curriculum at high sparsity, requiring no architectural changes. The result is predictable accuracy/latency tradeoffs from one model: practitioners can ``dial $k$'' at inference time without swapping checkpoints, changing architecture, or relying on token-level heuristics. Experiments on OLMoE-1B-7B-0125, Qwen1.5-MoE-A2.7B, and proprietary models fit on multiple operating points show that PHDS matches or exceeds well-specified oracle models, improves cross-sparsity agreement by up to 22\\% vs. well-specified oracle models, and enables simplified, flexible runtime MoE deployment by making global sparsity a first-class serving primitive.",
        "arxiv_id": "2509.23012"
    },
    "2509.23410": {
        "SCORE": 18,
        "ARXIVID": "2509.23410",
        "COMMENT": "Compression and Efficiency: hybrid tile-level sparsity mixing dense and 2:4 patterns via learnable masks for LLMs, enabling controllable sparsity with practical speedups.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Younes Hourri",
            "Mohammad Mozaffari",
            "Maryam Mehri Dehnavi"
        ],
        "title": "PATCH: Learnable Tile-level Hybrid Sparsity for LLMs",
        "abstract": "Large language models (LLMs) deliver impressive performance but incur prohibitive memory and compute costs at deployment. Model pruning is an effective way to reduce these overheads, yet existing approaches face challenges: unstructured sparsity, where nonzeros can appear anywhere, preserves accuracy but yields irregular access patterns that prevent GPU acceleration, while semi-structured 2:4 sparsity is hardware-friendly but enforces a rigid 50% pattern that degrades model quality. To bridge this gap, we introduce PATCH, a hybrid sparsity framework that enables a continuous sparsity ratio between 0% and 50%. PATCH partitions weight matrices into tiles, assigning each tile to be either dense or 2:4 sparse via a learnable mask selection mechanism. This design provides fine-grained control over accuracy-acceleration tradeoffs and supports non-uniform sparsity across layers, leading to superior overall quality. Across models from 0.5B to 8B parameters, PATCH consistently narrows the gap to dense accuracy while delivering practical speedups. For instance, on LLaMA-2 7B with an A6000 GPU, PATCH achieves 1.18x-1.38x end-to-end speedup over dense baselines while improving accuracy by 0.37%-2.96% compared to the state-of-the-art 2:4 pruning method, MaskLLM.",
        "arxiv_id": "2509.23410"
    },
    "2509.23638": {
        "SCORE": 18,
        "ARXIVID": "2509.23638",
        "COMMENT": "Matches Model Compression and Efficiency + High-Performance Systems for MoE inference: prediction-driven expert prefetching/scheduling (LLaPor, PreSched, AsyncIO) to overcome PCIe/memory bottlenecks.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Enda Yu",
            "Zhaoning Zhang",
            "Dezun Dong",
            "Yongwei Wu",
            "Xiangke Liao"
        ],
        "title": "PreScope: Unleashing the Power of Prefetching for Resource-Constrained MoE Inference",
        "abstract": "Mixture-of-Experts (MoE) models face memory and PCIe latency bottlenecks when deployed on commodity hardware. Offloading expert weights to CPU memory results in PCIe transfer latency that exceeds GPU computation by several folds. We present PreScope, a prediction-driven expert scheduling system that addresses three key challenges: inaccurate activation prediction, PCIe bandwidth competition, and cross-device scheduling complexity. Our solution includes: 1) Learnable Layer-Aware Predictor (LLaPor) that captures layer-specific expert activation patterns; 2) Prefetch-Aware Cross-Layer Scheduling (PreSched) that generates globally optimal plans balancing prefetching costs and loading overhead; 3) Asynchronous I/O Optimizer (AsyncIO) that decouples I/O from computation, eliminating waiting bubbles. PreScope achieves 141% higher throughput and 74.6% lower latency than state-of-the-art solutions.",
        "arxiv_id": "2509.23638"
    },
    "2509.22849": {
        "SCORE": 18,
        "ARXIVID": "2509.22849",
        "COMMENT": "Foundational theory/verification: strongest parameterized hardness results for properties of ReLU networks (e.g., positivity, Lipschitz), directly analyzing neural network architecture capabilities.",
        "RELEVANCE": 9,
        "NOVELTY": 9,
        "authors": [
            "Vincent Froese",
            "Moritz Grillo",
            "Christoph Hertrich",
            "Moritz Stargalla"
        ],
        "title": "Parameterized Hardness of Zonotope Containment and Neural Network Verification",
        "abstract": "Neural networks with ReLU activations are a widely used model in machine learning. It is thus important to have a profound understanding of the properties of the functions computed by such networks. Recently, there has been increasing interest in the (parameterized) computational complexity of determining these properties. In this work, we close several gaps and resolve an open problem posted by Froese et al. [COLT '25] regarding the parameterized complexity of various problems related to network verification. In particular, we prove that deciding positivity (and thus surjectivity) of a function $f\\colon\\mathbb{R}^d\\to\\mathbb{R}$ computed by a 2-layer ReLU network is W[1]-hard when parameterized by $d$. This result also implies that zonotope (non-)containment is W[1]-hard with respect to $d$, a problem that is of independent interest in computational geometry, control theory, and robotics. Moreover, we show that approximating the maximum within any multiplicative factor in 2-layer ReLU networks, computing the $L_p$-Lipschitz constant for $p\\in(0,\\infty]$ in 2-layer networks, and approximating the $L_p$-Lipschitz constant in 3-layer networks are NP-hard and W[1]-hard with respect to $d$. Notably, our hardness results are the strongest known so far and imply that the naive enumeration-based methods for solving these fundamental problems are all essentially optimal under the Exponential Time Hypothesis.",
        "arxiv_id": "2509.22849"
    },
    "2509.22745": {
        "SCORE": 17,
        "ARXIVID": "2509.22745",
        "COMMENT": "Model Architecture (MoE): safety-preserving fine-tuning by aligning MoE routing weights to prevent harmful expert drift (safety routing alignment).",
        "RELEVANCE": 10,
        "NOVELTY": 7,
        "authors": [
            "Jaehan Kim",
            "Minkyoo Song",
            "Seungwon Shin",
            "Sooel Son"
        ],
        "title": "Defending MoE LLMs against Harmful Fine-Tuning via Safety Routing Alignment",
        "abstract": "Recent large language models (LLMs) have increasingly adopted the Mixture-of-Experts (MoE) architecture for efficiency. MoE-based LLMs heavily depend on a superficial safety mechanism in which harmful inputs are routed safety-critical experts. However, our analysis reveals that routing decisions for harmful inputs drift significantly after fine-tuning, exposing a critical vulnerability to harmful fine-tuning (HFT) attacks. Existing defenses, primarily designed for monolithic LLMs, are less effective for MoE LLMs as they fail to prevent drift in harmful input routing. To address this limitation, we propose SafeMoE, a safe fine-tuning method tailored to MoE LLMs. SafeMoE directly mitigates routing drift by penalizing the gap between the routing weights of a fine-tuned model and those of the initial safety-aligned model, thereby preserving the safety-aligned routing of harmful inputs to safety-critical experts. Experiments on open-source MoE LLMs ranging from 7B to 141B parameters demonstrate that SafeMoE effectively mitigates HFT attacks, reducing the harmfulness score of OLMoE from 62.0 to 5.0, for example, while maintaining task utility within 1% degradation and incurring only 2% overhead. It significantly outperforms state-of-the-art defense methods for safeguarding LLM fine-tuning and remains effective in recent large-scale MoE LLMs such as gpt-oss and Llama 4. Our implementation is available at https://anonymous.4open.science/r/SafeMoE.",
        "arxiv_id": "2509.22745"
    },
    "2509.24389": {
        "SCORE": 17,
        "ARXIVID": "2509.24389",
        "COMMENT": "Mixture-of-Experts (sparse MoE) architecture for diffusion language models; efficient inference with few active parameters.",
        "RELEVANCE": 10,
        "NOVELTY": 7,
        "authors": [
            "Fengqi Zhu",
            "Zebin You",
            "Yipeng Xing",
            "Zenan Huang",
            "Lin Liu",
            "Yihong Zhuang",
            "Guoshan Lu",
            "Kangyu Wang",
            "Xudong Wang",
            "Lanning Wei",
            "Hongrui Guo",
            "Jiaqi Hu",
            "Wentao Ye",
            "Tieyuan Chen",
            "Chenchen Li",
            "Chengfu Tang",
            "Haibo Feng",
            "Jun Hu",
            "Jun Zhou",
            "Xiaolu Zhang",
            "Zhenzhong Lan",
            "Junbo Zhao",
            "Da Zheng",
            "Chongxuan Li",
            "Jianguo Li",
            "Ji-Rong Wen"
        ],
        "title": "LLaDA-MoE: A Sparse MoE Diffusion Language Model",
        "abstract": "We introduce LLaDA-MoE, a large language diffusion model with the Mixture-of-Experts (MoE) architecture, trained from scratch on approximately 20T tokens. LLaDA-MoE achieves competitive performance with significantly reduced computational overhead by maintaining a 7B-parameter capacity while activating only 1.4B parameters during inference. Our empirical evaluation reveals that LLaDA-MoE achieves state-of-the-art performance among diffusion language models with larger parameters, surpassing previous diffusion language models LLaDA, LLaDA 1.5, and Dream across multiple benchmarks. The instruct-tuned model LLaDA-MoE-7B-A1B-Instruct demonstrates capabilities comparable to Qwen2.5-3B-Instruct in knowledge understanding, code generation, mathematical reasoning, agent and alignment tasks, despite using fewer active parameters. Our results show that integrating a sparse MoE architecture into the training objective of masked diffusion language models still brings out MoE's strengths under efficient inference with few active parameters, and opens ample room for further exploration of diffusion language models. LLaDA-MoE models are available at Huggingface.",
        "arxiv_id": "2509.24389"
    },
    "2509.24222": {
        "SCORE": 17,
        "ARXIVID": "2509.24222",
        "COMMENT": "Model Architecture (MoE): Mixture-of-Experts Transformer to scale capacity via routing specialized subnetworks; decoupled time/frequency streams and topological embeddings.",
        "RELEVANCE": 10,
        "NOVELTY": 7,
        "authors": [
            "Zhisheng Chen",
            "Yingwei Zhang",
            "Qizhen Lan",
            "Tianyu Liu",
            "Huacan Wang",
            "Yi Ding",
            "Ziyu Jia",
            "Ronghao Chen",
            "Kun Wang",
            "Xinliang Zhou"
        ],
        "title": "Uni-NTFM: A Unified Foundation Model for EEG Signal Representation Learning",
        "abstract": "Foundation models pretrained on various and unlabeled data have demonstrated significant success in natural language and vision, but their application to electroencephalography (EEG) remains challenged due to the signal's unique properties. Existing brain foundation models that inherit architectures designed for text or images lead to three limitations in pre-training: 1) conflating time-domain waveform patterns with frequency-domain rhythmic features in a single processing stream, 2) ignoring the critical spatial topology of electrodes with different standards, and 3) reliance on the inflexible, dense network to process functionally distinct EEG patterns. To address these challenges, we introduce the Unified Neural Topological Foundation Model (Uni-NTFM), which is designed based on neuroscience principles to produce universal and interpretable representations. Uni-NTFM integrates three core innovations: 1) a decoupled architecture parallelly encodes time, frequency, and raw signal representations before performing cross-domain feature integration; 2) a topological embedding mechanism to unify electrodes from different international standards and generate structured input sequences for brain regions; and 3) a Mixture-of-Experts neural Transformer that efficiently scales model capacity by routing signal patterns to specialized subnetworks. The largest model, Uni-NTFM$_{large}$, has a record-breaking 1.9B parameters and was pretrained on over 28,000 hours of diverse EEG data via a dual-domain masked reconstruction objective. Uni-NTFM significantly outperforms existing task-specific methods and foundation models across nine distinct downstream tasks under both linear probing and fine-tuning settings, demonstrating a superior ability to learn universal representations of brain activity.",
        "arxiv_id": "2509.24222"
    },
    "2509.23933": {
        "SCORE": 17,
        "ARXIVID": "2509.23933",
        "COMMENT": "Model Architecture (MoE): introduces an internal metric to analyze routing and expert/neuron utilization, revealing specialization and training dynamics.",
        "RELEVANCE": 10,
        "NOVELTY": 7,
        "authors": [
            "Jiahao Ying",
            "Mingbao Lin",
            "Qianru Sun",
            "Yixin Cao"
        ],
        "title": "Beyond Benchmarks: Understanding Mixture-of-Experts Models through Internal Mechanisms",
        "abstract": "Mixture-of-Experts (MoE) architectures have emerged as a promising direction, offering efficiency and scalability by activating only a subset of parameters during inference. However, current research remains largely performance-centric, with limited understanding of its internal mechanisms, thereby constraining broader progress. In this work, we use an internal metric to investigate the mechanisms of MoE architecture by explicitly incorporating routing mechanisms and analyzing expert-level behaviors. Through systematic analyses of a wide range of publicly available MoE models, we uncover several findings: (1) neuron utilization decreases as models evolve, reflecting stronger generalization; (2) training exhibits a dynamic trajectory, where benchmark performance alone provides limited signal while MUI reveals deeper insights; (3) task completion emerges from collaborative contributions of multiple experts, with shared experts driving concentration; and (4) activation patterns at the neuron level provide a fine-grained proxy for data diversity. Together, these results demonstrate the potential of MUI as a complementary indicator to benchmark performance, offering new insights into the capacity, dynamics, and specialization of MoE models. Our project can be found at https://yingjiahao14.github.io/MoE-MUI/.",
        "arxiv_id": "2509.23933"
    },
    "2509.23173": {
        "SCORE": 17,
        "ARXIVID": "2509.23173",
        "COMMENT": "PEFT/Model Architecture: frequency-adaptive adapters for Fourier operator models with theory on LoRA\u2019s approximation limits vs adapters; parameter-efficient fine-tuning advances.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Hangwei Zhang",
            "Chun Kang",
            "Yan Wang",
            "Difan Zou"
        ],
        "title": "F-Adapter: Frequency-Adaptive Parameter-Efficient Fine-Tuning in Scientific Machine Learning",
        "abstract": "Parameter-efficient fine-tuning (PEFT) of powerful pre-trained models for complex downstream tasks has proven effective in vision and language processing, yet this paradigm remains unexplored in scientific machine learning, where the objective is to model complex physical systems. We conduct the first systematic study of PEFT for pre-trained Large Operator Models (LOMs) obtained by scaling variants of Fourier Neural Operator. First, we observe that the widely used Low-Rank Adaptation (LoRA) yields markedly poorer performance on LOMs than Adapter tuning. Then, we further theoretically establish that stacked LoRA incurs a depth-amplified lower bound on approximation error within Fourier layers, whereas adapters retain universal approximation capacity and, by concentrating parameters on energy-dominant low-frequency modes, attain exponentially decaying error with bottleneck width in the Fourier domain. Motivated by the robust empirical gains of adapters and by our theoretical characterization of PDE solutions as spectrally sparse, we introduce Frequency-Adaptive Adapter (F-Adapter). F-Adapter allocates adapter capacity based on spectral complexity, assigning higher-dimension modules to low-frequency components and lower-dimension modules to high-frequency components. Our F-Adapters establish state-of-the-art (SOTA) results on multiple challenging 3D Navier-Stokes benchmarks, markedly enhancing both generalization and spectral fidelity over LoRA and other PEFT techniques commonly used in LLMs. To the best of our knowledge, this work is the first to explore PEFT for scientific machine-learning and establishes F-Adapter as an effective paradigm for this domain.",
        "arxiv_id": "2509.23173"
    },
    "2509.23736": {
        "SCORE": 17,
        "ARXIVID": "2509.23736",
        "COMMENT": "Model Architecture: multi-scale ViT-based tokenizer with scale-causal attention, improving latent representation quality for reconstruction/generation.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Cong Chen",
            "Ziyuan Huang",
            "Cheng Zou",
            "Muzhi Zhu",
            "Kaixiang Ji",
            "Jiajia Liu",
            "Jingdong Chen",
            "Hao Chen",
            "Chunhua Shen"
        ],
        "title": "HieraTok: Multi-Scale Visual Tokenizer Improves Image Reconstruction and Generation",
        "abstract": "In this work, we present HieraTok, a novel multi-scale Vision Transformer (ViT)-based tokenizer that overcomes the inherent limitation of modeling single-scale representations. This is realized through two key designs: (1) multi-scale downsampling applied to the token map generated by the tokenizer encoder, producing a sequence of multi-scale tokens, and (2) a scale-causal attention mechanism that enables the progressive flow of information from low-resolution global semantic features to high-resolution structural details. Coupling these designs, HieraTok achieves significant improvements in both image reconstruction and generation tasks. Under identical settings, the multi-scale visual tokenizer outperforms its single-scale counterpart by a 27.2\\% improvement in rFID ($1.47 \\rightarrow 1.07$). When integrated into downstream generation frameworks, it achieves a $1.38\\times$ faster convergence rate and an 18.9\\% boost in gFID ($16.4 \\rightarrow 13.3$), which may be attributed to the smoother and more uniformly distributed latent space. Furthermore, by scaling up the tokenizer's training, we demonstrate its potential by a sota rFID of 0.45 and a gFID of 1.82 among ViT tokenizers. To the best of our knowledge, we are the first to introduce multi-scale ViT-based tokenizer in image reconstruction and image generation. We hope our findings and designs advance the ViT-based tokenizers in visual generation tasks.",
        "arxiv_id": "2509.23736"
    },
    "2509.23722": {
        "SCORE": 17,
        "ARXIVID": "2509.23722",
        "COMMENT": "High Performance Computing: adaptive pipeline parallelism co-optimizing partition, placement, and scheduling guided by a performance model.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Jihu Guo",
            "Tenghui Ma",
            "Wei Gao",
            "Peng Sun",
            "Jiaxing Li",
            "Xun Chen",
            "Yuyang Jin",
            "Dahua Lin"
        ],
        "title": "AdaPtis: Reducing Pipeline Bubbles with Adaptive Pipeline Parallelism on Heterogeneous Models",
        "abstract": "Pipeline parallelism is widely used to train large language models (LLMs). However, increasing heterogeneity in model architectures exacerbates pipeline bubbles, thereby reducing training efficiency. Existing approaches overlook the co-optimization of model partition, model placement, and workload scheduling, resulting in limited efficiency improvement or even performance degradation. To respond, we propose AdaPtis, an LLM training system that supports adaptive pipeline parallelism. First, we develop a pipeline performance model to accurately estimate training throughput. Second, AdaPtis jointly optimizes model partition, model placement, and workload scheduling policies guided by this performance model. Third, we design a unified pipeline executor that efficiently supports the execution of diverse pipeline strategies. Extensive experiments show that AdaPtis achieves an average speedup of 1.42x (up to 2.14x) over Megatron-LM I-1F1B across various LLM architectures and scales.",
        "arxiv_id": "2509.23722"
    },
    "2509.24832": {
        "SCORE": 17,
        "ARXIVID": "2509.24832",
        "COMMENT": "Efficiency/Cache: semantic KV-cache sharing across prompts using token-level LSH and RoPE-aware matching to reduce memory and computation.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Xinye Zhao",
            "Spyridon Mastorakis"
        ],
        "title": "SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts via Token-Level LSH Matching",
        "abstract": "As large language models (LLMs) continue to scale, the memory footprint of key-value (KV) caches during inference has become a significant bottleneck. Existing approaches primarily focus on compressing KV caches within a single prompt or reusing shared prefixes or frequently ocurred text segments across prompts. However, such strategies are limited in scenarios where prompts are semantically similar but lexically different, which frequently occurs in tasks such as multi-document summarization and conversational agents. We propose \\textit{SemShareKV}, a KV cache sharing and compression framework that accelerates LLM inference by reusing KVCache in semantically similar prompts. Instead of relying on exact token matches, SemShareKV applies fuzzy token matching using locality-sensitive hashing (LSH) on token embeddings and incorporates Rotary Position Embedding (RoPE) to better preserve positional information. By selectively reusing relevant key-value pairs from a reference prompt's cache, SemShareKV reduces redundant computation while maintaining output quality. Experiments on diverse summarization datasets show up to 6.25$\\times$ speedup and 42\\% lower GPU memory usage with 5k tokens input, with negligible quality degradation. These results highlight the potential of semantic-aware cache sharing for efficient LLM inference.",
        "arxiv_id": "2509.24832"
    },
    "2509.24552": {
        "SCORE": 17,
        "ARXIVID": "2509.24552",
        "COMMENT": "Model Architecture: hybrid sliding-window attention + xLSTM with stochastic window-size training; insight that short windows strengthen long-term memory.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Lo\\\"ic Cabannes",
            "Maximilian Beck",
            "Gergely Szilvasy",
            "Matthijs Douze",
            "Maria Lomeli",
            "Jade Copet",
            "Pierre-Emmanuel Mazar\\'e",
            "Gabriel Synnaeve",
            "Herv\\'e J\\'egou"
        ],
        "title": "Short window attention enables long-term memorization",
        "abstract": "Recent works show that hybrid architectures combining sliding window softmax attention layers with linear recurrent neural network (RNN) layers outperform both of these architectures taken separately. However, the impact of the window length and the interplay between softmax attention and linear RNN layers remain under-studied. In this work, we introduce SWAX, a hybrid architecture consisting of sliding-window attention and xLSTM linear RNN layers.   A counter-intuitive finding with SWAX is that larger sliding windows do not improve the long-context performance. In fact, short window attention encourages the model to better train the long-term memory of the xLSTM, by relying less on the softmax attention mechanism for long context-retrieval.   The issue with small sliding windows is that they are detrimental for short-context tasks, which could be solved with information from moderately larger sliding windows otherwise. Therefore, we train SWAX by stochastically changing the sliding window size, forcing the model to leverage both a longer context window and the xLSTM memory. SWAX trained with stochastic window sizes significantly outperforms regular window attention both on short and long-context problems.",
        "arxiv_id": "2509.24552"
    },
    "2509.25020": {
        "SCORE": 17,
        "ARXIVID": "2509.25020",
        "COMMENT": "Model Architecture/Training Dynamics: replaces token CoT with a latent Markov chain of continuous thoughts and variational training for faster, decoupled reasoning.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Jiayu Liu",
            "Zhenya Huang",
            "Anya Sims",
            "Enhong Chen",
            "Yee Whye Teh",
            "Ning Miao"
        ],
        "title": "MARCOS: Deep Thinking by Markov Chain of Continuous Thoughts",
        "abstract": "The current paradigm for reasoning in large language models (LLMs) involves models \"thinking out loud\" via a sequence of tokens, known as chain-of-thought (CoT). This approach, while effective, has several significant drawbacks. Firstly, inference requires autoregressive generation of often thousands of CoT tokens, which is slow and computationally expensive. Secondly, it constrains reasoning to the discrete space of tokens, creating an information bottleneck across reasoning steps. Thirdly, it fundamentally entangles reasoning with token generation, forcing LLMs to \"think while speaking,\" which causes potentially short-sighted reasoning. In light of these limitations, we re-imagine reasoning in LLMs and present a new paradigm: MARCOS. In our approach, rather than autoregressively generating tokens, we model reasoning as a hidden Markov chain of continuous, high-dimensional \"thoughts\". Each reasoning step involves a transition of the internal thoughts, where explicit reasoning steps (which may consist of hundreds of tokens) serve as observable variables, which are windows to peek into the implicit thoughts. Since this latent process is incompatible with the standard supervised learning, we further propose a two-phase variational training scheme. Our experiments on three benchmarks demonstrate that MARCOS outperforms existing continuous reasoning methods and, for the first time, achieves performance comparable to token-based CoT, even surpassing it by 4.7% on GSM8K with up to 15.7x speedup in inference. Beyond this, MARCOS offers additional advantages, such as step-level instead of token-level control over randomness, opening significant opportunities for reinforcement learning and reasoning in LLMs.",
        "arxiv_id": "2509.25020"
    },
    "2509.23779": {
        "SCORE": 17,
        "ARXIVID": "2509.23779",
        "COMMENT": "Model Architecture/Training Dynamics: theoretical analysis showing Mamba performs online gradient descent for in-context linear regression.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Jiarui Jiang",
            "Wei Huang",
            "Miao Zhang",
            "Taiji Suzuki",
            "Liqiang Nie"
        ],
        "title": "Trained Mamba Emulates Online Gradient Descent in In-Context Linear Regression",
        "abstract": "State-space models (SSMs), particularly Mamba, emerge as an efficient Transformer alternative with linear complexity for long-sequence modeling. Recent empirical works demonstrate Mamba's in-context learning (ICL) capabilities competitive with Transformers, a critical capacity for large foundation models. However, theoretical understanding of Mamba's ICL remains limited, restricting deeper insights into its underlying mechanisms. Even fundamental tasks such as linear regression ICL, widely studied as a standard theoretical benchmark for Transformers, have not been thoroughly analyzed in the context of Mamba. To address this gap, we study the training dynamics of Mamba on the linear regression ICL task. By developing novel techniques tackling non-convex optimization with gradient descent related to Mamba's structure, we establish an exponential convergence rate to ICL solution, and derive a loss bound that is comparable to Transformer's. Importantly, our results reveal that Mamba can perform a variant of \\textit{online gradient descent} to learn the latent function in context. This mechanism is different from that of Transformer, which is typically understood to achieve ICL through gradient descent emulation. The theoretical results are verified by experimental simulation.",
        "arxiv_id": "2509.23779"
    },
    "2509.25040": {
        "SCORE": 17,
        "ARXIVID": "2509.25040",
        "COMMENT": "Transformer Theory: mean-field multiscale analysis of token dynamics through depth (fast/intermediate/slow phases).",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Giuseppe Bruno",
            "Federico Pasqualotto",
            "Andrea Agazzi"
        ],
        "title": "A multiscale analysis of mean-field transformers in the moderate interaction regime",
        "abstract": "In this paper, we study the evolution of tokens through the depth of encoder-only transformer models at inference time by modeling them as a system of particles interacting in a mean-field way and studying the corresponding dynamics. More specifically, we consider this problem in the moderate interaction regime, where the number $N$ of tokens is large and the inverse temperature parameter $\\beta$ of the model scales together with $N$. In this regime, the dynamics of the system displays a multiscale behavior: a fast phase, where the token empirical measure collapses on a low-dimensional space, an intermediate phase, where the measure further collapses into clusters, and a slow one, where such clusters sequentially merge into a single one. We provide a rigorous characterization of the limiting dynamics in each of these phases and prove convergence in the above mentioned limit, exemplifying our results with some simulations.",
        "arxiv_id": "2509.25040"
    },
    "2509.25087": {
        "SCORE": 17,
        "ARXIVID": "2509.25087",
        "COMMENT": "HPC/Training Efficiency: loss-curve collapse as a signature of optimal scaling; enables early diagnostics and hyperparameter tuning for LLM families.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Shane Bergsma",
            "Bin Claire Zhang",
            "Nolan Dey",
            "Shaheer Muhammad",
            "Gurpreet Gosal",
            "Joel Hestness"
        ],
        "title": "Scaling with Collapse: Efficient and Predictable Training of LLM Families",
        "abstract": "Effective LLM training relies on *consistency*, meaning that key quantities -- such as final losses and optimal hyperparameters -- scale predictably across model sizes. Qiu et al. (2025) recently showed that this consistency extends beyond scalars: whole training loss curves can *collapse* onto a universal trajectory after a simple normalization. What remains unclear is whether this phenomenon holds for LLM families trained under *practical scaling recipes*, where width, depth, learning rate, batch size, and weight decay are scaled jointly. We show that it does: loss curves collapse across scales precisely when optimization hyperparameters are set optimally for the given data budget, in accordance with recent empirical scaling laws. Collapse thus emerges as a signature of compute-efficient training. We demonstrate two applications at scale: (1) deviation-from-collapse provides a sensitive, early diagnostic of training pathologies, and (2) the predictability of collapsed curves enables early stopping in large-scale hyperparameter tuning. Finally, we train a competitive LLM family, *Celerity*, using these insights, highlighting collapse as an effective tool for developing efficient LLMs.",
        "arxiv_id": "2509.25087"
    },
    "2509.24510": {
        "SCORE": 17,
        "ARXIVID": "2509.24510",
        "COMMENT": "Representation Learning/Training Dynamics: theoretical framework explaining test-time training as specialization after generalization; supported by sparse autoencoder analyses.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Jonas H\\\"ubotter",
            "Patrik Wolf",
            "Alexander Shevchenko",
            "Dennis J\\\"uni",
            "Andreas Krause",
            "Gil Kur"
        ],
        "title": "Specialization after Generalization: Towards Understanding Test-Time Training in Foundation Models",
        "abstract": "Recent empirical studies have explored the idea of continuing to train a model at test-time for a given task, known as test-time training (TTT), and have found it to yield significant performance improvements. However, there is limited understanding of why and when TTT is effective. Earlier explanations mostly focused on the observation that TTT may help when applied to out-of-distribution adaptation or used with privileged data. However, the growing scale of foundation models with most test data being in-distribution questions these explanations. We instead posit that foundation models remain globally underparameterized, with TTT providing a mechanism for specialization after generalization, focusing capacity on concepts relevant to the test task. Specifically, under the linear representation hypothesis, we propose a model in which TTT achieves a substantially smaller in-distribution test error than global training. We empirically validate our model's key assumptions by training a sparse autoencoder on ImageNet, showing that semantically related data points are explained by only a few shared concepts. Finally, we perform scaling studies across image and language tasks that confirm the practical implications of our model, identifying the regimes where specialization is most effective.",
        "arxiv_id": "2509.24510"
    },
    "2509.24198": {
        "SCORE": 17,
        "ARXIVID": "2509.24198",
        "COMMENT": "Strong match to representation learning/interpretability: identifies a sparse mechanism (negative pre-activations in entangled neurons) underlying syntax processing with causal interventions.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Linghao Kong",
            "Angelina Ning",
            "Micah Adler",
            "Nir Shavit"
        ],
        "title": "Negative Pre-activations Differentiate Syntax",
        "abstract": "A recently discovered class of entangled neurons, known as Wasserstein neurons, is disproportionately critical in large language models despite constituting only a very small fraction of the network: their targeted removal collapses the model, consistent with their unique role in differentiating similar inputs. Interestingly, in Wasserstein neurons immediately preceding smooth activation functions, such differentiation manifests in the negative pre-activation space, especially in early layers. Pairs of similar inputs are driven to highly distinct negative values, and these pairs involve syntactic tokens such as determiners and prepositions. We show that this negative region is functional rather than simply favorable for optimization. A minimal, sign-specific intervention that zeroes only the negative pre-activations of a small subset of entangled neurons significantly weakens overall model function and disrupts grammatical behavior, while both random and perplexity-matched controls leave grammatical performance largely unchanged. Part of speech analysis localizes the excess surprisal to syntactic scaffolding tokens, and layer-specific interventions reveal that small local degradations accumulate across depth. Over training checkpoints, the same ablation impairs grammatical behavior as Wasserstein neurons emerge and stabilize. Together, these results identify negative differentiation in a sparse subset of entangled neurons as a crucial mechanism that language models rely on for syntax.",
        "arxiv_id": "2509.24198"
    },
    "2509.24472": {
        "SCORE": 17,
        "ARXIVID": "2509.24472",
        "COMMENT": "Model Architecture: principled permutation-equivariant Kolmogorov\u2013Arnold Networks via function sharing with theoretical expressivity guarantees and symmetry-aware design.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Ran Elbaz",
            "Guy Bar-Shalom",
            "Yam Eitan",
            "Fabrizio Frasca",
            "Haggai Maron"
        ],
        "title": "FS-KAN: Permutation Equivariant Kolmogorov-Arnold Networks via Function Sharing",
        "abstract": "Permutation equivariant neural networks employing parameter-sharing schemes have emerged as powerful models for leveraging a wide range of data symmetries, significantly enhancing the generalization and computational efficiency of the resulting models. Recently, Kolmogorov-Arnold Networks (KANs) have demonstrated promise through their improved interpretability and expressivity compared to traditional architectures based on MLPs. While equivariant KANs have been explored in recent literature for a few specific data types, a principled framework for applying them to data with permutation symmetries in a general context remains absent. This paper introduces Function Sharing KAN (FS-KAN), a principled approach to constructing equivariant and invariant KA layers for arbitrary permutation symmetry groups, unifying and significantly extending previous work in this domain. We derive the basic construction of these FS-KAN layers by generalizing parameter-sharing schemes to the Kolmogorov-Arnold setup and provide a theoretical analysis demonstrating that FS-KANs have the same expressive power as networks that use standard parameter-sharing layers, allowing us to transfer well-known and important expressivity results from parameter-sharing networks to FS-KANs. Empirical evaluations on multiple data types and symmetry groups show that FS-KANs exhibit superior data efficiency compared to standard parameter-sharing layers, by a wide margin in certain cases, while preserving the interpretability and adaptability of KANs, making them an excellent architecture choice in low-data regimes.",
        "arxiv_id": "2509.24472"
    },
    "2509.23085": {
        "SCORE": 17,
        "ARXIVID": "2509.23085",
        "COMMENT": "Model Architecture/Training Dynamics: closed-form signal-preserving weight initialization tailored to odd-sigmoid activations, enabling stable training without normalization.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Hyunwoo Lee",
            "Hayoung Choi",
            "Hyunju Kim"
        ],
        "title": "Signal Preserving Weight Initialization for Odd-Sigmoid Activations",
        "abstract": "Activation functions critically influence trainability and expressivity, and recent work has therefore explored a broad range of nonlinearities. However, activations and weight initialization are interdependent: without an appropriate initialization method, nonlinearities can cause saturation, variance collapse, and increased learning rate sensitivity. We address this by defining an odd sigmoid function class and, given any activation f in this class, proposing an initialization method tailored to f. The method selects a noise scale in closed form so that forward activations remain well dispersed up to a target layer, thereby avoiding collapse to zero or saturation. Empirically, the approach trains reliably without normalization layers, exhibits strong data efficiency, and enables learning for activations under which standard initialization methods (Xavier, He, Orthogonal) often do not converge reliably.",
        "arxiv_id": "2509.23085"
    },
    "2509.24653": {
        "SCORE": 17,
        "ARXIVID": "2509.24653",
        "COMMENT": "Representation Learning: identity supervision aligns latent geometry (implicit nuclear-norm regularization) to enable compositional reasoning (two-hop), with theory and scaling evidence.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Pengxiao Lin",
            "Zheng-An Chen",
            "Zhi-Qin John Xu"
        ],
        "title": "Identity Bridge: Enabling Implicit Reasoning via Shared Latent Memory",
        "abstract": "Despite remarkable advances, large language models often fail at compositional reasoning tasks, a phenomenon exemplified by the ``curse of two-hop reasoning''. This paper introduces the Identity Bridge, a simple yet powerful mechanism that resolves this compositionality gap by supervising the model on a zero-hop identity task. We demonstrate empirically that this addition enables models to successfully perform out-of-distribution two-hop reasoning, a task they otherwise completely fail. To explain this phenomenon, we provide a theoretical analysis using a simplified Emb-MLP model, proving that identity supervision reshapes the model's latent geometry. We show this alignment is induced by an implicit nuclear-norm regularization during optimization, which favors low-rank solutions that share structure across tasks. For complex tasks, we use small initialization or weight decay to enhance the regularization effect, which enhances the latent space alignment effect and slows down the generalization decay. Finally, we extend our investigation to large-scale models, observing that they still achieve two-hop reasoning through the latent memory, which provides crucial inspiration for enhancing their implicit reasoning abilities.",
        "arxiv_id": "2509.24653"
    },
    "2509.22868": {
        "SCORE": 17,
        "ARXIVID": "2509.22868",
        "COMMENT": "Representation Learning/Training Dynamics: NTK/GP-theoretic analysis of neighborhood sampling shows distinct posterior processes and errors versus full GNNs, clarifying systemic behaviors.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Zehao Niu",
            "Mihai Anitescu",
            "Jie Chen"
        ],
        "title": "Neighborhood Sampling Does Not Learn the Same Graph Neural Network",
        "abstract": "Neighborhood sampling is an important ingredient in the training of large-scale graph neural networks. It suppresses the exponential growth of the neighborhood size across network layers and maintains feasible memory consumption and time costs. While it becomes a standard implementation in practice, its systemic behaviors are less understood. We conduct a theoretical analysis by using the tool of neural tangent kernels, which characterize the (analogous) training dynamics of neural networks based on their infinitely wide counterparts -- Gaussian processes (GPs). We study several established neighborhood sampling approaches and the corresponding posterior GP. With limited samples, the posteriors are all different, although they converge to the same one as the sample size increases. Moreover, the posterior covariance, which lower-bounds the mean squared prediction error, is uncomparable, aligning with observations that no sampling approach dominates.",
        "arxiv_id": "2509.22868"
    },
    "2509.24218": {
        "SCORE": 17,
        "ARXIVID": "2509.24218",
        "COMMENT": "HPC/Optimization for large-scale training: Column-Normalized Adam improves spectral conditioning while retaining per-coordinate adaptivity, yielding 2\u20132.5x faster LLM pretraining.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Junjie Wang",
            "Pan Zhou",
            "Yiming Dong",
            "Huan Li",
            "Jia Li",
            "Xun Zhou",
            "Qicheng Lao",
            "Cong Fang",
            "Zhouchen Lin"
        ],
        "title": "Conda: Column-Normalized Adam for Training Large Language Models Faster",
        "abstract": "Large language models (LLMs) have demonstrated impressive generalization and emergent capabilities, yet their pre-training remains computationally expensive and sensitive to optimization dynamics. While Adam-based optimizers offer fast convergence by adapting learning rates coordinate-wise, recent studies reveal that their updates often suffer from poor spectral conditioning and low-rank structures, hindering efficiency. Muon addresses this issue via global spectral normalization but lacks the per-coordinate adaptivity of Adam. In this work, we propose \\textbf{Column-Normalized Adam (Conda)}, a novel optimizer that bridges the strengths of both approaches. Conda projects updates into an orthogonal subspace and applies column-wise second moment normalization based on the projected gradients, thereby achieving both improved spectral conditioning and maintaining coordinate-wise adaptivity. This design alleviates the spectral pathologies of Adam while preserving its fast convergence behavior. Extensive experiments on the LLaMA and GPT-2 series show that Conda consistently outperforms AdamW, Muon, and other baselines in pre-training. Remarkably, on the LLaMA series, \\textbf{Conda achieves $2{\\sim}2.5\\times$ the convergence speed of AdamW, measured in both training steps and training time.} Further ablations demonstrate its robustness under diverse training setups. These results collectively highlight Conda as an effective and broadly applicable optimizer for large-scale LLM training. The code is released on https://github.com/jie040109/Conda",
        "arxiv_id": "2509.24218"
    },
    "2509.23474": {
        "SCORE": 17,
        "ARXIVID": "2509.23474",
        "COMMENT": "Architecture/representation theory: statistical learning guarantees for group-invariant neural networks in the Barron framework; analyses of approximation and estimation errors.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Yahong Yang",
            "Wei Zhu"
        ],
        "title": "Statistical Learning Guarantees for Group-Invariant Barron Functions",
        "abstract": "We investigate the generalization error of group-invariant neural networks within the Barron framework. Our analysis shows that incorporating group-invariant structures introduces a group-dependent factor $\\delta_{G,\\Gamma,\\sigma} \\le 1$ into the approximation rate. When this factor is small, group invariance yields substantial improvements in approximation accuracy. On the estimation side, we establish that the Rademacher complexity of the group-invariant class is no larger than that of the non-invariant counterpart, implying that the estimation error remains unaffected by the incorporation of symmetry. Consequently, the generalization error can improve significantly when learning functions with inherent group symmetries. We further provide illustrative examples demonstrating both favorable cases, where $\\delta_{G,\\Gamma,\\sigma}\\approx |G|^{-1}$, and unfavorable ones, where $\\delta_{G,\\Gamma,\\sigma}\\approx 1$. Overall, our results offer a rigorous theoretical foundation showing that encoding group-invariant structures in neural networks leads to clear statistical advantages for symmetric target functions.",
        "arxiv_id": "2509.23474"
    },
    "2509.24882": {
        "SCORE": 17,
        "ARXIVID": "2509.24882",
        "COMMENT": "Matches Representation Learning criterion: theoretical scaling laws and spectral analysis linking weight spectra to generalization.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Leonardo Defilippis",
            "Yizhou Xu",
            "Julius Girardin",
            "Emanuele Troiani",
            "Vittorio Erba",
            "Lenka Zdeborov\\'a",
            "Bruno Loureiro",
            "Florent Krzakala"
        ],
        "title": "Scaling Laws and Spectra of Shallow Neural Networks in the Feature Learning Regime",
        "abstract": "Neural scaling laws underlie many of the recent advances in deep learning, yet their theoretical understanding remains largely confined to linear models. In this work, we present a systematic analysis of scaling laws for quadratic and diagonal neural networks in the feature learning regime. Leveraging connections with matrix compressed sensing and LASSO, we derive a detailed phase diagram for the scaling exponents of the excess risk as a function of sample complexity and weight decay. This analysis uncovers crossovers between distinct scaling regimes and plateau behaviors, mirroring phenomena widely reported in the empirical neural scaling literature. Furthermore, we establish a precise link between these regimes and the spectral properties of the trained network weights, which we characterize in detail. As a consequence, we provide a theoretical validation of recent empirical observations connecting the emergence of power-law tails in the weight spectrum with network generalization performance, yielding an interpretation from first principles.",
        "arxiv_id": "2509.24882"
    },
    "2509.24006": {
        "SCORE": 17,
        "ARXIVID": "2509.24006",
        "COMMENT": "Compression/Efficiency and Transformer Attention: Sparse-Linear Attention fuses sparse and linear attention with a custom GPU kernel, drastically reducing attention cost with minimal quality loss.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Jintao Zhang",
            "Haoxu Wang",
            "Kai Jiang",
            "Shuo Yang",
            "Kaiwen Zheng",
            "Haocheng Xi",
            "Ziteng Wang",
            "Hongzhou Zhu",
            "Min Zhao",
            "Ion Stoica",
            "Joseph E. Gonzalez",
            "Jun Zhu",
            "Jianfei Chen"
        ],
        "title": "SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention",
        "abstract": "In Diffusion Transformer (DiT) models, particularly for video generation, attention latency is a major bottleneck due to the long sequence length and the quadratic complexity. We find that attention weights can be separated into two parts: a small fraction of large weights with high rank and the remaining weights with very low rank. This naturally suggests applying sparse acceleration to the first part and low-rank acceleration to the second. Based on this finding, we propose SLA (Sparse-Linear Attention), a trainable attention method that fuses sparse and linear attention to accelerate diffusion models. SLA classifies attention weights into critical, marginal, and negligible categories, applying O(N^2) attention to critical weights, O(N) attention to marginal weights, and skipping negligible ones. SLA combines these computations into a single GPU kernel and supports both forward and backward passes. With only a few fine-tuning steps using SLA, DiT models achieve a 20x reduction in attention computation, resulting in significant acceleration without loss of generation quality. Experiments show that SLA reduces attention computation by 95% without degrading end-to-end generation quality, outperforming baseline methods. In addition, we implement an efficient GPU kernel for SLA, which yields a 13.7x speedup in attention computation and a 2.2x end-to-end speedup in video generation on Wan2.1-1.3B.",
        "arxiv_id": "2509.24006"
    },
    "2509.23365": {
        "SCORE": 17,
        "ARXIVID": "2509.23365",
        "COMMENT": "Representation Learning/Training Dynamics: theory of how superposition emerges in continuous chain-of-thought within transformers through two-stage training dynamics.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Hanlin Zhu",
            "Shibo Hao",
            "Zhiting Hu",
            "Jiantao Jiao",
            "Stuart Russell",
            "Yuandong Tian"
        ],
        "title": "Emergence of Superposition: Unveiling the Training Dynamics of Chain of Continuous Thought",
        "abstract": "Previous work shows that the chain of continuous thought (continuous CoT) improves the reasoning capability of large language models (LLMs) by enabling implicit parallel thinking, and a subsequent work provided theoretical insight by showing that a two-layer transformer equipped with continuous CoT can efficiently solve directed graph reachability by maintaining a superposition of multiple reasoning traces in the continuous thought. However, it remains unclear how the superposition mechanism is naturally learned from gradient-based training methods. To fill this gap, we theoretically analyze the training dynamics of a simplified two-layer transformer on the directed graph reachability problem to unveil how the superposition mechanism emerges during training in two training stages -- (i) a thought-generation stage that autoregressively expands the continuous thought, and (ii) a prediction stage that converts the thought into the final answer. Our analysis reveals that during training using continuous thought, the index-matching logit, an important quantity which reflects the strength of the model's local search ability, will first increase and then remain bounded under mild assumptions. The bounded index-matching logit effectively balances exploration and exploitation during the reasoning process: the model will exploit local problem structures to identify plausible search traces, and assign comparable weights to multiple such traces to explore when it is uncertain about which solution is correct, which results in superposition. Our experimental results tracking the growth of logits further validate our theory.",
        "arxiv_id": "2509.23365"
    },
    "2509.23323": {
        "SCORE": 17,
        "ARXIVID": "2509.23323",
        "COMMENT": "Matches Representation Learning/Identifiable Interpretability: identifiable temporal-instantaneous causal representation framework scaling to LLM activations with theoretical guarantees.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Xiangchen Song",
            "Jiaqi Sun",
            "Zijian Li",
            "Yujia Zheng",
            "Kun Zhang"
        ],
        "title": "LLM Interpretability with Identifiable Temporal-Instantaneous Representation",
        "abstract": "Despite Large Language Models' remarkable capabilities, understanding their internal representations remains challenging. Mechanistic interpretability tools such as sparse autoencoders (SAEs) were developed to extract interpretable features from LLMs but lack temporal dependency modeling, instantaneous relation representation, and more importantly theoretical guarantees, undermining both the theoretical foundations and the practical confidence necessary for subsequent analyses. While causal representation learning (CRL) offers theoretically grounded approaches for uncovering latent concepts, existing methods cannot scale to LLMs' rich conceptual space due to inefficient computation. To bridge the gap, we introduce an identifiable temporal causal representation learning framework specifically designed for LLMs' high-dimensional concept space, capturing both time-delayed and instantaneous causal relations. Our approach provides theoretical guarantees and demonstrates efficacy on synthetic datasets scaled to match real-world complexity. By extending SAE techniques with our temporal causal framework, we successfully discover meaningful concept relationships in LLM activations. Our findings show that modeling both temporal and instantaneous conceptual relationships advances the interpretability of LLMs.",
        "arxiv_id": "2509.23323"
    },
    "2509.25153": {
        "SCORE": 17,
        "ARXIVID": "2509.25153",
        "COMMENT": "Matches Architecture/Training Dynamics: theoretical analysis of single-layer attention\u2019s adaptive token selection and learnability in high dimensions.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Nicholas Barnfield",
            "Hugo Cui",
            "Yue M. Lu"
        ],
        "title": "High-Dimensional Analysis of Single-Layer Attention for Sparse-Token Classification",
        "abstract": "When and how can an attention mechanism learn to selectively attend to informative tokens, thereby enabling detection of weak, rare, and sparsely located features? We address these questions theoretically in a sparse-token classification model in which positive samples embed a weak signal vector in a randomly chosen subset of tokens, whereas negative samples are pure noise. In the long-sequence limit, we show that a simple single-layer attention classifier can in principle achieve vanishing test error when the signal strength grows only logarithmically in the sequence length $L$, whereas linear classifiers require $\\sqrt{L}$ scaling. Moving from representational power to learnability, we study training at finite $L$ in a high-dimensional regime, where sample size and embedding dimension grow proportionally. We prove that just two gradient updates suffice for the query weight vector of the attention classifier to acquire a nontrivial alignment with the hidden signal, inducing an attention map that selectively amplifies informative tokens. We further derive an exact asymptotic expression for the test error and training loss of the trained attention-based classifier, and quantify its capacity -- the largest dataset size that is typically perfectly separable -- thereby explaining the advantage of adaptive token selection over nonadaptive linear baselines.",
        "arxiv_id": "2509.25153"
    },
    "2509.24416": {
        "SCORE": 16,
        "ARXIVID": "2509.24416",
        "COMMENT": "Compression/Efficiency: PTQ for Diffusion Transformers with cross-block calibration, orthogonal-based smoothing via Hadamard transforms, and cross-layer parameter search.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Kai Liu",
            "Shaoqiu Zhang",
            "Linghe Kong",
            "Yulun Zhang"
        ],
        "title": "CLQ: Cross-Layer Guided Orthogonal-based Quantization for Diffusion Transformers",
        "abstract": "Visual generation quality has been greatly promoted with the rapid advances in diffusion transformers (DiTs), which is attributed to the scaling of model size and complexity. However, these attributions also hinder the practical deployment of DiTs on edge devices, limiting their development and application. Serve as an efficient model compression technique, model post-training quantization (PTQ) can reduce the memory consumption and speed up the inference, with inevitable performance degradation. To alleviate the degradation, we propose CLQ, a cross-layer guided orthogonal-based quantization method for DiTs. To be specific, CLQ consists of three key designs. First, we observe that the calibration data used by most of the PTQ methods can not honestly represent the distribution of the activations. Therefore, we propose cross-block calibration (CBC) to obtain accurate calibration data, with which the quantization can be better guided. Second, we propose orthogonal-based smoothing (OBS), which quantifies the outlier score of each channel and leverages block Hadamard matrix to smooth the outliers with negligible overhead. Third, we propose cross-layer parameter searching (CLPS) to search. We evaluate CLQ with both image generation and video generation models and successfully compress the model into W4A4 with negligible degradation in visual quality and metrics. CLQ achieves 3.98x memory saving and 3.95x speedup. Our code is available at \\hyperlink{https://github.com/Kai-Liu001/CLQ}{https://github.com/Kai-Liu001/CLQ}.",
        "arxiv_id": "2509.24416"
    },
    "2506.22694": {
        "SCORE": 16,
        "ARXIVID": "2506.22694",
        "COMMENT": "Compression/Efficiency: vocabulary pruning of the drafter LM head for speculative decoding to reduce memory-bound drafting latency.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Raghavv Goel",
            "Sudhanshu Agrawal",
            "Mukul Gagrani",
            "Junyoung Park",
            "Yifan Zao",
            "He Zhang",
            "Tian Liu",
            "Yiping Yang",
            "Xin Yuan",
            "Jiuyan Lu",
            "Chris Lott",
            "Mingu Lee"
        ],
        "title": "VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs",
        "abstract": "In this paper, we introduce a simple training-free technique to improve the performance of drafter-based speculative decoding (SpD) methods that incorporates language modeling head (LM head) during drafting process. A drafter-based speculative decoding leverages one or more smaller language models, a.k.a. drafters or draft models, to sample a draft sequence or tree consisting of multiple tokens, followed by verification by a base LLM, a target model, accepting a subset as its valid generation. As it is usually considered that the speculative decoding requires one-to-one mapping between vocabularies of the target model and the draft model, it has been natural to share the vocabulary between them, or even share the LM head as in EAGLE or Medusa. We first identify that this draft token sampling scheme inherently contains an unnecessary inference overhead in drafting, especially for some target LLMs with very large vocabularies. Then, we propose a simple technique, VocabTrim, to mitigate the drafting overhead to improve the generation speed in memory-bound environment. VocabTrim reconstructs the drafter LM head to contain only a limited set of tokens, selected by the most frequently sampled from the vocabulary of the target model. While limiting the vocabulary in drafting slightly degrades the acceptance rate, it significantly reduces the drafting latency in memory-bound process which is often the case on edge devices, resulting in higher memory-bound speed up (MBSU). We show that our method can boost the memory-bound speed-up for Llama-3 models on Spec-Bench, specifically by 16% for Llama-3.2-3B-Instruct.",
        "arxiv_id": "2506.22694"
    },
    "2509.24238": {
        "SCORE": 16,
        "ARXIVID": "2509.24238",
        "COMMENT": "Conditional/Dynamic Networks: instance-adaptive test-time compute via a controller that applies latent steering and learned halting without changing backbone.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Yixin He",
            "Lumingyuan Tang"
        ],
        "title": "Learning to Ponder: Adaptive Reasoning in Latent Space",
        "abstract": "Test-time compute has emerged as a key paradigm for enhancing LLM reasoning, yet prevailing approaches like Best-of-N and majority voting apply uniform depth across inputs, wasting computation on simple queries while potentially under-thinking complex ones. We present FR-Ponder, a single-graph, backbone-training-free framework that allocates instance-adaptive reasoning compute via latent steering. A less than 1M-param controller observes hidden states and decides to halt or apply a small ponder step by adding a pre-computed steering vector to frozen representations. Our method extracts the latent steering vector associated with deeper reasoning outputs and direct IO from LLM and re-applies it through a tunable scaling factor, allowing the model to adapt its reasoning depth to the complexity of each input. To balance performance and computational cost, we employ Group Relative Policy Optimization (GRPO) as a reward signal to adaptively regulate reasoning depth, achieving task accuracy while mitigating overreasoning. Through curriculum learning and careful reward engineering, FR-Ponder learns calibrated compute allocation correlated with problem difficulty. On GSM8K and MATH500, FR-Ponder improves the compute-accuracy frontier, delivering lower FLOPs with better matched accuracy and comparing favorably to early-exit baselines, without modifying backbone weights. Analyses visualize interpretable steering directions and show learned compute allocation correlates with problem difficulty.",
        "arxiv_id": "2509.24238"
    },
    "2509.24935": {
        "SCORE": 16,
        "ARXIVID": "2509.24935",
        "COMMENT": "Model Architecture/Scalability: purely transformer-based GANs trained in VAE latent space with scale-friendly stabilization (intermediate supervision, width-aware LR).",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Sangeek Hyun",
            "MinKyu Lee",
            "Jae-Pil Heo"
        ],
        "title": "Scalable GANs with Transformers",
        "abstract": "Scalability has driven recent advances in generative modeling, yet its principles remain underexplored for adversarial learning. We investigate the scalability of Generative Adversarial Networks (GANs) through two design choices that have proven to be effective in other types of generative models: training in a compact Variational Autoencoder latent space and adopting purely transformer-based generators and discriminators. Training in latent space enables efficient computation while preserving perceptual fidelity, and this efficiency pairs naturally with plain transformers, whose performance scales with computational budget. Building on these choices, we analyze failure modes that emerge when naively scaling GANs. Specifically, we find issues as underutilization of early layers in the generator and optimization instability as the network scales. Accordingly, we provide simple and scale-friendly solutions as lightweight intermediate supervision and width-aware learning-rate adjustment. Our experiments show that GAT, a purely transformer-based and latent-space GANs, can be easily trained reliably across a wide range of capacities (S through XL). Moreover, GAT-XL/2 achieves state-of-the-art single-step, class-conditional generation performance (FID of 2.96) on ImageNet-256 in just 40 epochs, 6x fewer epochs than strong baselines.",
        "arxiv_id": "2509.24935"
    },
    "2509.23106": {
        "SCORE": 16,
        "ARXIVID": "2509.23106",
        "COMMENT": "Model Compression/Efficiency: 8-bit quantization of Muon optimizer states (blockwise, linear/dynamic) with robustness analysis, yielding large memory savings.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Aman Gupta",
            "Rafael Celente",
            "Abhishek Shivanna",
            "D. T. Braithwaite",
            "Gregory Dexter",
            "Shao Tang",
            "Hiroto Udagawa",
            "Daniel Silva",
            "Rohan Ramanath",
            "S. Sathiya Keerthi"
        ],
        "title": "Effective Quantization of Muon Optimizer States",
        "abstract": "The Muon optimizer, based on matrix orthogonalization, has recently shown faster convergence and up to 2x computational efficiency over AdamW in LLM pretraining. Like AdamW, Muon is stateful, requiring storage of both model weights and accumulated gradients. While 8-bit AdamW variants mitigate this overhead using blockwise quantization, they are typically stable only under dynamic quantization - which improves stability on linear quantization for extreme values. In this paper, we introduce the 8-bit Muon optimizer using blockwise quantization, supporting both linear and dynamic schemes. We demonstrate that 8-bit Muon maintains stability under both, while delivering $\\sim$74\\% reduction in memory footprint compared to full-precision Muon. In extensive experiments, 8-bit Muon closely matches the performance of Muon while outperforming AdamW and 8-bit AdamW in pre-training a 1.6B model on 4B FineWeb tokens. It also shows competitive results when fine-tuning the Llama 3.2 3B model on post-training data. We also provide a theoretical perspective to help explain this robustness under quantization.",
        "arxiv_id": "2509.23106"
    },
    "2509.23684": {
        "SCORE": 16,
        "ARXIVID": "2509.23684",
        "COMMENT": "Representation Learning: mechanistic interpretability of Transformer MLPs via coalitional game theory to reveal synergistic neuron groups.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Tanya Chowdhury",
            "Atharva Nijasure",
            "Yair Zick",
            "James Allan"
        ],
        "title": "Hedonic Neurons: A Mechanistic Mapping of Latent Coalitions in Transformer MLPs",
        "abstract": "Fine-tuned Large Language Models (LLMs) encode rich task-specific features, but the form of these representations, especially within MLP layers, remains unclear. Empirical inspection of LoRA updates shows that new features concentrate in mid-layer MLPs, yet the scale of these layers obscures meaningful structure. Prior probing suggests that statistical priors may strengthen, split, or vanish across depth, motivating the need to study how neurons work together rather than in isolation.   We introduce a mechanistic interpretability framework based on coalitional game theory, where neurons mimic agents in a hedonic game whose preferences capture their synergistic contributions to layer-local computations. Using top-responsive utilities and the PAC-Top-Cover algorithm, we extract stable coalitions of neurons: groups whose joint ablation has non-additive effects. We then track their transitions across layers as persistence, splitting, merging, or disappearance.   Applied to LLaMA, Mistral, and Pythia rerankers fine-tuned on scalar IR tasks, our method finds coalitions with consistently higher synergy than clustering baselines. By revealing how neurons cooperate to encode features, hedonic coalitions uncover higher-order structure beyond disentanglement and yield computational units that are functionally important, interpretable, and predictive across domains.",
        "arxiv_id": "2509.23684"
    },
    "2509.24808": {
        "SCORE": 16,
        "ARXIVID": "2509.24808",
        "COMMENT": "Representation Learning: input-level mechanistic explanations via sparse \u201cquery circuits\u201d within the model, with a fidelity metric (NDF) and efficient discovery.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Tung-Yu Wu",
            "Fazl Barez"
        ],
        "title": "Query Circuits: Explaining How Language Models Answer User Prompts",
        "abstract": "Explaining why a language model produces a particular output requires local, input-level explanations. Existing methods uncover global capability circuits (e.g., indirect object identification), but not why the model answers a specific input query in a particular way. We introduce query circuits, which directly trace the information flow inside a model that maps a specific input to the output. Unlike surrogate-based approaches (e.g., sparse autoencoders), query circuits are identified within the model itself, resulting in more faithful and computationally accessible explanations. To make query circuits practical, we address two challenges. First, we introduce Normalized Deviation Faithfulness (NDF), a robust metric to evaluate how well a discovered circuit recovers the model's decision for a specific input, and is broadly applicable to circuit discovery beyond our setting. Second, we develop sampling-based methods to efficiently identify circuits that are sparse yet faithfully describe the model's behavior. Across benchmarks (IOI, arithmetic, MMLU, and ARC), we find that there exist extremely sparse query circuits within the model that can recover much of its performance on single queries. For example, a circuit covering only 1.3% of model connections can recover about 60% of performance on an MMLU questions. Overall, query circuits provide a step towards faithful, scalable explanations of how language models process individual inputs.",
        "arxiv_id": "2509.24808"
    },
    "2509.24066": {
        "SCORE": 16,
        "ARXIVID": "2509.24066",
        "COMMENT": "Compression/Efficiency: analyzes pruning-at-initialization for pre-trained models with second-order perspective; shows cross-task transferability.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Leonardo Iurada",
            "Beatrice Occhiena",
            "Tatiana Tommasi"
        ],
        "title": "A Second-Order Perspective on Pruning at Initialization and Knowledge Transfer",
        "abstract": "The widespread availability of pre-trained vision models has enabled numerous deep learning applications through their transferable representations. However, their computational and storage costs often limit practical deployment. Pruning-at-Initialization has emerged as a promising approach to compress models before training, enabling efficient task-specific adaptation. While conventional wisdom suggests that effective pruning requires task-specific data, this creates a challenge when downstream tasks are unknown in advance. In this paper, we investigate how data influences the pruning of pre-trained vision models. Surprisingly, pruning on one task retains the model's zero-shot performance also on unseen tasks. Furthermore, fine-tuning these pruned models not only improves performance on original seen tasks but can recover held-out tasks' performance. We attribute this phenomenon to the favorable loss landscapes induced by extensive pre-training on large-scale datasets.",
        "arxiv_id": "2509.24066"
    },
    "2509.23178": {
        "SCORE": 16,
        "ARXIVID": "2509.23178",
        "COMMENT": "Theoretical analysis of Transformer reasoning limits via information propagation rules (steps scale with number of layers).",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Tian Qin",
            "Yuhan Chen",
            "Zhiwei Wang",
            "Zhi-Qin John Xu"
        ],
        "title": "Limit Analysis for Symbolic Multi-step Reasoning Tasks with Information Propagation Rules Based on Transformers",
        "abstract": "Transformers are able to perform reasoning tasks, however the intrinsic mechanism remains widely open. In this paper we propose a set of information propagation rules based on Transformers and utilize symbolic reasoning tasks to theoretically analyze the limit reasoning steps. We show that the limit number of reasoning steps is between $O(3^{L-1})$ and $O(2^{L-1})$ for a model with $L$ attention layers in a single-pass.",
        "arxiv_id": "2509.23178"
    },
    "2509.23717": {
        "SCORE": 16,
        "ARXIVID": "2509.23717",
        "COMMENT": "Representation learning/interpretability: introduces sensitivity metric and evaluation protocol for Sparse Autoencoder features.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Claire Tian",
            "Katherine Tian",
            "Nathan Hu"
        ],
        "title": "Measuring Sparse Autoencoder Feature Sensitivity",
        "abstract": "Sparse Autoencoder (SAE) features have become essential tools for mechanistic interpretability research. SAE features are typically characterized by examining their activating examples, which are often \"monosemantic\" and align with human interpretable concepts. However, these examples don't reveal feature sensitivity: how reliably a feature activates on texts similar to its activating examples. In this work, we develop a scalable method to evaluate feature sensitivity. Our approach avoids the need to generate natural language descriptions for features; instead we use language models to generate text with the same semantic properties as a feature's activating examples. We then test whether the feature activates on these generated texts. We demonstrate that sensitivity measures a new facet of feature quality and find that many interpretable features have poor sensitivity. Human evaluation confirms that when features fail to activate on our generated text, that text genuinely resembles the original activating examples. Lastly, we study feature sensitivity at the SAE level and observe that average feature sensitivity declines with increasing SAE width across 7 SAE variants. Our work establishes feature sensitivity as a new dimension for evaluating both individual features and SAE architectures.",
        "arxiv_id": "2509.23717"
    },
    "2509.23729": {
        "SCORE": 16,
        "ARXIVID": "2509.23729",
        "COMMENT": "Model Compression and Efficiency: ultra-low-bit (<4-bit) layerwise quantization for MLLMs with selective per-layer PTQ based on activation entropy.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Shubhang Bhatnagar",
            "Andy Xu",
            "Kar-Han Tan",
            "Narendra Ahuja"
        ],
        "title": "LUQ: Layerwise Ultra-Low Bit Quantization for Multimodal Large Language Models",
        "abstract": "Large Language Models (LLMs) with multimodal capabilities have revolutionized vision-language tasks, but their deployment often requires huge memory and computational resources. While post-training quantization (PTQ) has successfully compressed language models to as low as 1-bit precision without significant performance loss, its effectiveness for multimodal LLMs (MLLMs) remains relatively unexplored. In this paper, we present the first study on ultra-low bit (<4-bit) quantization for multimodal LLMs. Our analysis reveals that multimodal tokens and intermediate layer activations produced by them exhibit significantly higher statistical variance and entropy compared to text tokens, making them less tolerant to ultra-low bit quantization. However, the activation distributions of multimodal tokens varies significantly over different layers, with some layers having lower entropy activation distributions. We empirically show that such layers in these models can better tolerate ultra-low bit quantization. Building on these insights, we propose a novel strategy for MLLM quantization, LUQ: Layerwise Ultra-Low Bit Quantization, which selectively applies ultra-low bit quantization to layers that are more resilient to it. Additionally, we also show that using a mix of multimodal tokens (image and text) for PTQ boosts VQA performance in the ultra-low bit regime. We evaluate our method on LLaVA-1.5 and Qwen-2.5-VL across 9 popular VQA benchmarks. The resulting LUQ models use 40% and 31% less memory than their 4-bit counterparts, respectively, while exhibiting a performance degradation of less than 10% on the MME benchmark.",
        "arxiv_id": "2509.23729"
    },
    "2509.23472": {
        "SCORE": 16,
        "ARXIVID": "2509.23472",
        "COMMENT": "Model Compression and Efficiency: low-rank activation compression during fine-tuning with a novel sampling-based orthogonal decomposition for low-rank matrices.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Jiang-Xin Shi",
            "Wen-Da Wei",
            "Jin-Fei Qi",
            "Xuanyu Chen",
            "Tong Wei",
            "Yu-Feng Li"
        ],
        "title": "Memory-Efficient Fine-Tuning via Low-Rank Activation Compression",
        "abstract": "The parameter-efficient fine-tuning paradigm has garnered significant attention with the advancement of foundation models. Although numerous methods have been proposed to reduce the number of trainable parameters, their substantial memory overhead remains a critical bottleneck that hinders practical deployment. In this paper, we observe that model activations constitute a major source of memory consumption, especially under large batch sizes and long context lengths; however, the rank of the activations remains consistently low. Motivated by this insight, we propose a memory-efficient fine-tuning approach Low-Rank Activation Compression (LoRAct). Unlike prior work, LoRAct provides a more flexible and versatile compressing strategy that can be applied online during the forward pass without the need for any calibration data. Moreover, LoRAct incorporates a novel sampling-based orthogonal decomposition algorithm specifically designed for low-rank matrices, offering improved computational efficiency and a tighter error bound compared to the widely used RSVD. Experiments on both vision and language tasks demonstrate the effectiveness of LoRAct. Notably, LoRAct further reduces activation memory by approximately 80% in comparison with the widely adopted LoRA method, while maintaining competitive performance. The source code is available at https://github.com/shijxcs/meft.",
        "arxiv_id": "2509.23472"
    },
    "2509.23928": {
        "SCORE": 16,
        "ARXIVID": "2509.23928",
        "COMMENT": "Model Compression/Efficiency: removes visual tokens from the drafter and reuses target hidden states to shorten prefill and speed up speculative decoding in VLMs while preserving quality.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Zhinan Xie",
            "Peisong Wang",
            "Jian Cheng"
        ],
        "title": "HiViS: Hiding Visual Tokens from the Drafter for Speculative Decoding in Vision-Language Models",
        "abstract": "Speculative decoding is an effective approach for accelerating inference in Large Language models (LLMs), but its adaptation to Vision-Language models (VLMs) remains challenging for additional visual tokens in multimodal inputs. First, owing to the fact that the drafter and the target VLM may derived from different families, the semantic representations of visual tokens in the target VLM are misaligned with those in the drafter, introducing bias into the KV-cache during the prefill stage. Second, the large number of visual tokens substantially slows down the drafter's self-attention during the decoding stage. We propose Hiding Visual Tokens from the Drafter for Speculative Decoding in Vision-Language Models (HiViS), an explicit-implicit input decomposition framework that alleviates the above inefficiency. All visual tokens are removed from the drafter's input, retaining only textual tokens as explicit inputs, while directly reusing the target VLM's corresponding last-layer hidden states as implicit visual information without additional processing. To train the drafter efficiently, we introduces multi-step self-feedback training strategy with dynamic data selection and sequential embedding supervision to simulate reasoning during training. Our approach compresses the prefill sequence length of the drafter to only 0.7%-1.3% of the target VLM's input, while maintaining lossless generation quality. Extensive experiments across diverse models and tasks demonstrate up to 2.65x speedup, confirming the effectiveness of HiViS in accelerating VLM inference.",
        "arxiv_id": "2509.23928"
    },
    "2509.22931": {
        "SCORE": 16,
        "ARXIVID": "2509.22931",
        "COMMENT": "Model Compression/Efficiency + Representation Learning: monotonic MLP head imposes functional constraints to learn ultra-compact, robust embeddings across domains.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Shreyas Gokhale"
        ],
        "title": "MonoCon: A general framework for learning ultra-compact high-fidelity representations using monotonicity constraints",
        "abstract": "Learning high-quality, robust, efficient, and disentangled representations is a central challenge in artificial intelligence (AI). Deep metric learning frameworks tackle this challenge primarily using architectural and optimization constraints. Here, we introduce a third approach that instead relies on $\\textit{functional}$ constraints. Specifically, we present MonoCon, a simple framework that uses a small monotonic multi-layer perceptron (MLP) head attached to any pre-trained encoder. Due to co-adaptation between encoder and head guided by contrastive loss and monotonicity constraints, MonoCon learns robust, disentangled, and highly compact embeddings at a practically negligible performance cost. On the CIFAR-100 image classification task, MonoCon yields representations that are nearly 9x more compact and 1.5x more robust than the fine-tuned encoder baseline, while retaining 99\\% of the baseline's 5-NN classification accuracy. We also report a 3.4x more compact and 1.4x more robust representation on an SNLI sentence similarity task for a marginal reduction in the STSb score, establishing MonoCon as a general domain-agnostic framework. Crucially, these robust, ultra-compact representations learned via functional constraints offer a unified solution to critical challenges in disparate contexts ranging from edge computing to cloud-scale retrieval.",
        "arxiv_id": "2509.22931"
    },
    "2509.23500": {
        "SCORE": 16,
        "ARXIVID": "2509.23500",
        "COMMENT": "Compression/Efficiency: systematic analysis of optimizer\u2013quantization interactions (PTQ/QAT), with scaling laws and optimizer comparisons (e.g., Shampoo) for quantized training.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Georgios Vlassis",
            "Saleh Ashkboos",
            "Alexandra Volkova",
            "Torsten Hoefler",
            "Dan Alistarh"
        ],
        "title": "Beyond Outliers: A Study of Optimizers Under Quantization",
        "abstract": "As new optimizers gain traction and model quantization becomes standard for efficient deployment, a key question arises: how does the choice of optimizer affect model performance in the presence of quantization? Despite progress in both areas, systematic evidence on optimizer-quantization interactions remains limited. To fill this gap, we study the impact of optimizer choice on model robustness under quantization, considering both post-training quantization (PTQ), and quantization-aware training (QAT). We first train full-precision models, ranging from 50M to 1.5B parameters, with six optimizers, to explore the hyperparameter landscape, and establish well-tuned baselines. We then apply PTQ to evaluate how model performance degrades when trained with different optimizers. We find that outlier-related metrics, such as the max-to-mean ratio (MMR) and Kurtosis, fail to predict the PTQ performance across different optimizers. We show analytically that this is due to the MMR capturing only isolated layer errors, while ignoring how quantization errors accumulate and propagate through the network. To study the QAT degradation, we train quantized models from scratch and compare them to our original-precision baselines. We find that optimizers performing well in the original pretraining setup may not remain optimal under QAT, and that models trained with Shampoo show the lowest accuracy degradation. Finally, we derive scaling laws for quantization-aware training under different optimizers, showing that Shampoo achieves the highest parameter efficiency of all tested optimizers.",
        "arxiv_id": "2509.23500"
    },
    "2509.22832": {
        "SCORE": 16,
        "ARXIVID": "2509.22832",
        "COMMENT": "High Performance Computing: fine-grained, hardware-aware GPU performance modeling for distributed LLM training across parallelism strategies; systems-level innovation enabling planning without on-cluster runs.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Biyao Zhang",
            "Mingkai Zheng",
            "Debargha Ganguly",
            "Xuecen Zhang",
            "Vikash Singh",
            "Vipin Chaudhary",
            "Zhao Zhang"
        ],
        "title": "Efficient Fine-Grained GPU Performance Modeling for Distributed Deep Learning of LLM",
        "abstract": "Training Large Language Models(LLMs) is one of the most compute-intensive tasks in high-performance computing. Predicting end-to-end training time for multi-billion parameter models distributed across hundreds of GPUs remains challenging due to complex interactions between transformer components, parallelism strategies(data, model, pipeline, tensor), and multi-tier communication. Learned models require costly sampling, while analytical models often struggle with real-world network and hardware complexities. We address this by decomposing LLMs into core computational primitives and modeling them with: (1) operator-level decomposition for fine-grained analysis; (2) lightweight sampling based hardware-aware prediction models for key operations; (3) an end-to-end prediction system integrating these components across complex parallelization strategies. Crucially, our methodology has been validated on two large-scale HPC systems. Our framework achieves low average prediction errors-4.98\\% on Perlmutter(A100) and 9.38\\% on Vista(GH200)-for models up to 20B parameters across 128 GPUs. Importantly, it runs entirely on CPUs, enabling rapid iteration over hardware configurations and training strategies without costly on-cluster experimentation.",
        "arxiv_id": "2509.22832"
    },
    "2509.23162": {
        "SCORE": 16,
        "ARXIVID": "2509.23162",
        "COMMENT": "Model Architecture + Representation Learning: extends dense associative memory to the Bures\u2013Wasserstein space with storage/retrieval theory, a foundational representational framework.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Chandan Tankala",
            "Krishnakumar Balasubramanian"
        ],
        "title": "Dense associative memory on the Bures-Wasserstein space",
        "abstract": "Dense associative memories (DAMs) store and retrieve patterns via energy-functional fixed points, but existing models are limited to vector representations. We extend DAMs to probability distributions equipped with the 2-Wasserstein distance, focusing mainly on the Bures-Wasserstein class of Gaussian densities. Our framework defines a log-sum-exp energy over stored distributions and a retrieval dynamics aggregating optimal transport maps in a Gibbs-weighted manner. Stationary points correspond to self-consistent Wasserstein barycenters, generalizing classical DAM fixed points. We prove exponential storage capacity, provide quantitative retrieval guarantees under Wasserstein perturbations, and validate the model on synthetic and real-world distributional tasks. This work elevates associative memory from vectors to full distributions, bridging classical DAMs with modern generative modeling and enabling distributional storage and retrieval in memory-augmented learning.",
        "arxiv_id": "2509.23162"
    },
    "2509.24496": {
        "SCORE": 16,
        "ARXIVID": "2509.24496",
        "COMMENT": "Representation Learning: defines a low-dimensional, bi-Lipschitz functional representation (LLM DNA) of model behavior with theoretical guarantees and a training-free extraction pipeline.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Zhaomin Wu",
            "Haodong Zhao",
            "Ziyang Wang",
            "Jizhou Guo",
            "Qian Wang",
            "Bingsheng He"
        ],
        "title": "LLM DNA: Tracing Model Evolution via Functional Representations",
        "abstract": "The explosive growth of large language models (LLMs) has created a vast but opaque landscape: millions of models exist, yet their evolutionary relationships through fine-tuning, distillation, or adaptation are often undocumented or unclear, complicating LLM management. Existing methods are limited by task specificity, fixed model sets, or strict assumptions about tokenizers or architectures. Inspired by biological DNA, we address these limitations by mathematically defining LLM DNA as a low-dimensional, bi-Lipschitz representation of functional behavior. We prove that LLM DNA satisfies inheritance and genetic determinism properties and establish the existence of DNA. Building on this theory, we derive a general, scalable, training-free pipeline for DNA extraction. In experiments across 305 LLMs, DNA aligns with prior studies on limited subsets and achieves superior or competitive performance on specific tasks. Beyond these tasks, DNA comparisons uncover previously undocumented relationships among LLMs. We further construct the evolutionary tree of LLMs using phylogenetic algorithms, which align with shifts from encoder-decoder to decoder-only architectures, reflect temporal progression, and reveal distinct evolutionary speeds across LLM families.",
        "arxiv_id": "2509.24496"
    },
    "2509.23822": {
        "SCORE": 16,
        "ARXIVID": "2509.23822",
        "COMMENT": "Model Architecture/Equivariance: group-conditioned flow matching with efficient group-averaged equivariant vector fields enforcing space-group/Wyckoff constraints.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Omri Puny",
            "Yaron Lipman",
            "Benjamin Kurt Miller"
        ],
        "title": "Space Group Conditional Flow Matching",
        "abstract": "Inorganic crystals are periodic, highly-symmetric arrangements of atoms in three-dimensional space. Their structures are constrained by the symmetry operations of a crystallographic \\emph{space group} and restricted to lie in specific affine subspaces known as \\emph{Wyckoff positions}. The frequency an atom appears in the crystal and its rough positioning are determined by its Wyckoff position. Most generative models that predict atomic coordinates overlook these symmetry constraints, leading to unrealistically high populations of proposed crystals exhibiting limited symmetry. We introduce Space Group Conditional Flow Matching, a novel generative framework that samples significantly closer to the target population of highly-symmetric, stable crystals. We achieve this by conditioning the entire generation process on a given space group and set of Wyckoff positions; specifically, we define a conditionally symmetric noise base distribution and a group-conditioned, equivariant, parametric vector field that restricts the motion of atoms to their initial Wyckoff position. Our form of group-conditioned equivariance is achieved using an efficient reformulation of \\emph{group averaging} tailored for symmetric crystals. Importantly, it reduces the computational overhead of symmetrization to a negligible level. We achieve state of the art results on crystal structure prediction and de novo generation benchmarks. We also perform relevant ablations.",
        "arxiv_id": "2509.23822"
    },
    "2509.23800": {
        "SCORE": 16,
        "ARXIVID": "2509.23800",
        "COMMENT": "Representation Learning/Control: introduces surrogate Euclidean latent spaces extracted from generative models for constraint-aware optimization without retraining.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Samuel Willis",
            "Alexandru I. Stere",
            "Dragos D. Margineantu",
            "Henry T. Oldroyd",
            "John A. Fozard",
            "Carl Henrik Ek",
            "Henry Moss",
            "Erik Bodin"
        ],
        "title": "Define latent spaces by example: optimisation over the outputs of generative models",
        "abstract": "Modern generative AI models such as diffusion and flow matching can sample from rich data distributions, but many downstream tasks -- such as experimental design or creative content generation -- require a higher level of control than unconstrained sampling. The challenge is to efficiently identify outputs that are both probable under the model and satisfy task-specific constraints. We address this by introducing surrogate latent spaces: non-parametric, low-dimensional Euclidean embeddings that can be extracted from any generative model without additional training. The axes in the Euclidean space can be defined via examples, providing a simple and interpretable approach to define custom latent spaces that both express intended features and are convenient to use in downstream tasks. The representation is Euclidean and has controllable dimensionality, permitting direct application of standard optimisation algorithms to traverse the outputs of generative models. Our approach is architecture-agnostic, incurs almost no additional computational cost, and generalises across modalities, including images, audio, videos, and structured objects like proteins.",
        "arxiv_id": "2509.23800"
    },
    "2509.24007": {
        "SCORE": 16,
        "ARXIVID": "2509.24007",
        "COMMENT": "Model architecture and efficiency: introduces Next Sequence Prediction and SDLM to retrofit AR LMs for diffusion-style inference while preserving KV-cache compatibility and adaptive decoding.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Yangzhou Liu",
            "Yue Cao",
            "Hao Li",
            "Gen Luo",
            "Zhe Chen",
            "Weiyun Wang",
            "Xiaobo Liang",
            "Biqing Qi",
            "Lijun Wu",
            "Changyao Tian",
            "Yanting Zhang",
            "Yuqiang Li",
            "Tong Lu",
            "Yu Qiao",
            "Jifeng Dai",
            "Wenhai Wang"
        ],
        "title": "Sequential Diffusion Language Models",
        "abstract": "Diffusion language models (DLMs) have strong theoretical efficiency but are limited by fixed-length decoding and incompatibility with key-value (KV) caches. Block diffusion mitigates these issues, yet still enforces a fixed block size and requires expensive training. We introduce Next Sequence Prediction (NSP), which unifies next-token and next-block prediction, enabling the model to adaptively determine the generation length at each step. When the length is fixed to 1, NSP reduces to standard next-token prediction. Building on NSP, we propose Sequential Diffusion Language Model (SDLM), which can retrofit pre-trained autoregressive language models (ALMs) at minimal cost. Specifically, SDLM performs diffusion inference within fixed-size mask blocks, but dynamically decodes consecutive subsequences based on model confidence, thereby preserving KV-cache compatibility and improving robustness to varying uncertainty and semantics across the sequence. Experiments show that SDLM matches or surpasses strong autoregressive baselines using only 3.5M training samples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the SDLM-32B model delivers even more pronounced efficiency gains, demonstrating the strong scalability potential of our modeling paradigm. Project page and codes: https://github.com/OpenGVLab/SDLM",
        "arxiv_id": "2509.24007"
    },
    "2509.24335": {
        "SCORE": 16,
        "ARXIVID": "2509.24335",
        "COMMENT": "Model Architecture/Training Dynamics: stabilizes continuous-token AR generation via hyperspherical VAE latents to prevent variance collapse, improving AR image models.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Guolin Ke",
            "Hui Xue"
        ],
        "title": "Hyperspherical Latents Improve Continuous-Token Autoregressive Generation",
        "abstract": "Autoregressive (AR) models are promising for image generation, yet continuous-token AR variants often trail latent diffusion and masked-generation models. The core issue is heterogeneous variance in VAE latents, which is amplified during AR decoding, especially under classifier-free guidance (CFG), and can cause variance collapse. We propose SphereAR to address this issue. Its core design is to constrain all AR inputs and outputs -- including after CFG -- to lie on a fixed-radius hypersphere (constant $\\ell_2$ norm), leveraging hyperspherical VAEs. Our theoretical analysis shows that hyperspherical constraint removes the scale component (the primary cause of variance collapse), thereby stabilizing AR decoding. Empirically, on ImageNet generation, SphereAR-H (943M) sets a new state of the art for AR models, achieving FID 1.34. Even at smaller scales, SphereAR-L (479M) reaches FID 1.54 and SphereAR-B (208M) reaches 1.92, matching or surpassing much larger baselines such as MAR-H (943M, 1.55) and VAR-d30 (2B, 1.92). To our knowledge, this is the first time a pure next-token AR image generator with raster order surpasses diffusion and masked-generation models at comparable parameter scales.",
        "arxiv_id": "2509.24335"
    },
    "2509.24372": {
        "SCORE": 16,
        "ARXIVID": "2509.24372",
        "COMMENT": "Matches high-performance training algorithms: scales Evolution Strategies to full-parameter LLM fine-tuning at billion-parameter scale, offering a systems-level alternative to RL.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Xin Qiu",
            "Yulu Gan",
            "Conor F. Hayes",
            "Qiyao Liang",
            "Elliot Meyerson",
            "Babak Hodjat",
            "Risto Miikkulainen"
        ],
        "title": "Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning",
        "abstract": "Fine-tuning pre-trained large language models (LLMs) for down-stream tasks is a critical step in the AI deployment pipeline. Reinforcement learning (RL) is arguably the most prominent fine-tuning method, contributing to the birth of many state-of-the-art LLMs. In contrast, evolution strategies (ES), which once showed comparable performance to RL on models with a few million parameters, was neglected due to the pessimistic perception of its scalability to larger models. In this work, we report the first successful attempt to scale up ES for fine-tuning the full parameters of LLMs, showing the surprising fact that ES can search efficiently over billions of parameters and outperform existing RL fine-tuning methods in multiple respects, including sample efficiency, tolerance to long-horizon rewards, robustness to different base LLMs, less tendency to reward hacking, and more stable performance across runs. It therefore serves as a basis to unlock a new direction in LLM fine-tuning beyond what current RL techniques provide. The source codes are provided at: https://github.com/VsonicV/es-fine-tuning-paper.",
        "arxiv_id": "2509.24372"
    },
    "2509.24291": {
        "SCORE": 16,
        "ARXIVID": "2509.24291",
        "COMMENT": "Matches Representation Learning: generative iterative refinement for contrastive sentence embeddings using autoregressive LLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Yu-Che Tsai",
            "Kuan-Yu Chen",
            "Yuan-Chi Li",
            "Yuan-Hao Chen",
            "Ching-Yu Tsai",
            "Shou-De Lin"
        ],
        "title": "Let LLMs Speak Embedding Languages: Generative Text Embeddings via Iterative Contrastive Refinement",
        "abstract": "Existing large language model (LLM)-based embeddings typically adopt an encoder-only paradigm, treating LLMs as static feature extractors and overlooking their core generative strengths. We introduce GIRCSE (Generative Iterative Refinement for Contrastive Sentence Embeddings), a novel framework that leverages autoregressive generation to iteratively refine semantic representations. By producing sequences of soft tokens optimized under contrastive objective, GIRCSE captures latent concepts and implicit semantics that encoder-only methods often miss. To guide this process, we propose an Iterative Contrastive Refinement (ICR) objective that encourages each refinement step to yield better representations. Extensive experiments show that GIRCSE outperforms strong LLM-based embedding baselines on the MTEB benchmark and instruction-following tasks. Moreover, GIRCSE exhibits an emergent test-time scaling property: generating more tokens at inference steadily improves embedding quality. Our results establish generative iterative refinement as a new paradigm for representation learning.",
        "arxiv_id": "2509.24291"
    },
    "2509.25176": {
        "SCORE": 16,
        "ARXIVID": "2509.25176",
        "COMMENT": "Model Compression/Efficiency: interleaved compression\u2013expansion of rollout length in RL training to reduce redundant tokens and improve the performance\u2013efficiency Pareto for LRMs.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Haoming Wen",
            "Yushi Bai",
            "Juanzi Li",
            "Jie Tang"
        ],
        "title": "SIRI: Scaling Iterative Reinforcement Learning with Interleaved Compression",
        "abstract": "We introduce SIRI, Scaling Iterative Reinforcement Learning with Interleaved Compression, a simple yet effective RL approach for Large Reasoning Models (LRMs) that enables more efficient and accurate reasoning. Existing studies have observed repetitive thinking patterns in LRMs, and attempts to reduce them often come at the cost of performance. In this paper, we show that this trade-off can be overcome through a training regime that iteratively alternates between compressing and expanding the reasoning budget, by dynamically adjusting the maximum rollout length during training. The compression phase cuts the rollout length, forcing the model to make precise and valuable decisions within a limited context, which effectively reduces redundant tokens and increases reasoning density. The expansion phase then relaxes the length limit, providing space for the model to explore and plan in long-horizon settings. Remarkably, we find that after each compression-expansion cycle, the model's performance improves even as its output length decreases, steadily pushing it closer to the Pareto frontier in the performance-efficiency trade-off. Training on DeepSeek-R1-Distill-Qwen-1.5B, SIRI-low improves performance on AIME24 by 43.2% while reducing token usage by 46.9% after three iterations, and SIRI-high achieves the highest accuracy compared to all other methods (Figure 1). Our findings shed light on the potential of periodically oscillating the LRM's output truncation length during training to dynamically balance exploration and efficiency in reasoning, converging towards an optimal \"sweet spot\" between the two. Our models are publicly available.",
        "arxiv_id": "2509.25176"
    },
    "2509.24317": {
        "SCORE": 16,
        "ARXIVID": "2509.24317",
        "COMMENT": "Representation Learning + Efficiency: SALT replaces EMA with a frozen teacher for masked-latent prediction, improving compute-efficiency and scalability in video SSL.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Xianhang Li",
            "Chen Huang",
            "Chun-Liang Li",
            "Eran Malach",
            "Josh Susskind",
            "Vimal Thilak",
            "Etai Littwin"
        ],
        "title": "Rethinking JEPA: Compute-Efficient Video SSL with Frozen Teachers",
        "abstract": "Video Joint Embedding Predictive Architectures (V-JEPA) learn generalizable off-the-shelf video representation by predicting masked regions in latent space with an exponential moving average (EMA)-updated teacher. While EMA prevents representation collapse, it complicates scalable model selection and couples teacher and student architectures. We revisit masked-latent prediction and show that a frozen teacher suffices. Concretely, we (i) train a target encoder with a simple pixel-reconstruction objective under V-JEPA masking, then (ii) freeze it and train a student to predict the teacher's latents on masked regions. This leads to a two-stage, unregularized scheme that we refer to as SALT (Static-teacher Asymmetric Latent Training). SALT decouples optimization into pixel reconstruction (teacher) and masked latent prediction (student), increasing transparency, efficiency, and scalability while preserving the ability of representation to generalize under frozen evaluation. Empirically, our student models outperform recently proposed V-JEPA 2 encoders under frozen backbone evaluation across diverse benchmarks. They are also more compute-optimal: at matched pretraining FLOPs, our method achieves higher probing accuracy, and its scaling curves dominate V-JEPA's accuracy-FLOPs Pareto frontier. Finally, we find that student quality is remarkably robust to teacher quality: high-performing students emerge even with small, sub-optimal teachers. This points to a compute budget allocation that should overwhelmingly favor the student. These results position SALT as a simple, scalable, and compute-efficient alternative to EMA-based self-distillation for video representation learning.",
        "arxiv_id": "2509.24317"
    },
    "2509.24886": {
        "SCORE": 16,
        "ARXIVID": "2509.24886",
        "COMMENT": "Model Architecture/Equivariance: adaptive canonicalization ensures continuity and symmetry with universal approximation; addresses eigenbasis/rotation ambiguities in geometric networks.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Ya-Wei Eileen Lin",
            "Ron Levie"
        ],
        "title": "Adaptive Canonicalization with Application to Invariant Anisotropic Geometric Networks",
        "abstract": "Canonicalization is a widely used strategy in equivariant machine learning, enforcing symmetry in neural networks by mapping each input to a standard form. Yet, it often introduces discontinuities that can affect stability during training, limit generalization, and complicate universal approximation theorems. In this paper, we address this by introducing \\emph{adaptive canonicalization}, a general framework in which the canonicalization depends both on the input and the network. Specifically, we present the adaptive canonicalization based on prior maximization, where the standard form of the input is chosen to maximize the predictive confidence of the network. We prove that this construction yields continuous and symmetry-respecting models that admit universal approximation properties.   We propose two applications of our setting: (i) resolving eigenbasis ambiguities in spectral graph neural networks, and (ii) handling rotational symmetries in point clouds. We empirically validate our methods on molecular and protein classification, as well as point cloud classification tasks. Our adaptive canonicalization outperforms the three other common solutions to equivariant machine learning: data augmentation, standard canonicalization, and equivariant architectures.",
        "arxiv_id": "2509.24886"
    },
    "2509.24244": {
        "SCORE": 16,
        "ARXIVID": "2509.24244",
        "COMMENT": "Matches Efficiency/Scaling Laws: empirical and theoretical scaling laws for model merging, enabling compute-efficient composition of specialists as an alternative to multitask training.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Yuanyi Wang",
            "Yanggan Gu",
            "Yiming Zhang",
            "Qi Zhou",
            "Zhaoyi Yan",
            "Congkai Xie",
            "Xinyao Wang",
            "Jianbo Yuan",
            "Hongxia Yang"
        ],
        "title": "Model Merging Scaling Laws in Large Language Models",
        "abstract": "We study empirical scaling laws for language model merging measured by cross-entropy. Despite its wide practical use, merging lacks a quantitative rule that predicts returns as we add experts or scale the model size. We identify a compact power law that links model size and expert number: the size-dependent floor decreases with model capacity, while the merging tail exhibits clear diminishing returns in the number of experts. The law holds in-domain and cross-domain, tightly fits measured curves across diverse architectures and methods (Average, TA, TIES, DARE), and explains two robust regularities: most gains arrive early, and variability shrinks as more experts are included. Building on this, we present a simple theory that explains why gains fall roughly as 1/k and links the floor and tail to properties of the base model and the diversity across domains. This law enables predictive planning: estimate how many experts are needed to reach a target loss, decide when to stop adding experts, and trade off scaling the base model versus adding experts under a fixed budget--turning merging from heuristic practice into a computationally efficient, planable alternative to multitask training. This suggests a scaling principle for distributed generative AI: predictable gains can be achieved by composing specialists, offering a complementary path toward AGI-level systems.",
        "arxiv_id": "2509.24244"
    },
    "2509.24716": {
        "SCORE": 16,
        "ARXIVID": "2509.24716",
        "COMMENT": "Matches Autoencoders/Discrete Representation Learning: new policy-search-based training for discrete VAEs avoiding reparameterization, with scaling to high-dimensional data.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Michael Drolet",
            "Firas Al-Hafez",
            "Aditya Bhatt",
            "Jan Peters",
            "Oleg Arenz"
        ],
        "title": "Discrete Variational Autoencoding via Policy Search",
        "abstract": "Discrete latent bottlenecks in variational autoencoders (VAEs) offer high bit efficiency and can be modeled with autoregressive discrete distributions, enabling parameter-efficient multimodal search with transformers. However, discrete random variables do not allow for exact differentiable parameterization; therefore, discrete VAEs typically rely on approximations, such as Gumbel-Softmax reparameterization or straight-through gradient estimates, or employ high-variance gradient-free methods such as REINFORCE that have had limited success on high-dimensional tasks such as image reconstruction. Inspired by popular techniques in policy search, we propose a training framework for discrete VAEs that leverages the natural gradient of a non-parametric encoder to update the parametric encoder without requiring reparameterization. Our method, combined with automatic step size adaptation and a transformer-based encoder, scales to challenging datasets such as ImageNet and outperforms both approximate reparameterization methods and quantization-based discrete autoencoders in reconstructing high-dimensional data from compact latent spaces, achieving a 20% improvement on FID Score for ImageNet 256.",
        "arxiv_id": "2509.24716"
    },
    "2509.23050": {
        "SCORE": 15,
        "ARXIVID": "2509.23050",
        "COMMENT": "Representation Learning: layer-wise chain-of-embedding analysis revealing a Visual Integration Point and TVI metric to quantify language prior in LVLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Lin Long",
            "Changdae Oh",
            "Seongheon Park",
            "Yixuan Li"
        ],
        "title": "Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding",
        "abstract": "Large vision-language models (LVLMs) achieve strong performance on multimodal tasks, yet they often default to their language prior (LP) -- memorized textual patterns from pre-training while under-utilizing visual evidence. Prior analyses of LP mostly rely on input-output probing, which fails to reveal the internal mechanisms governing when and how vision influences model behavior. To address this gap, we present the first systematic analysis of language prior through the lens of chain-of-embedding, which examines the layer-wise representation dynamics within LVLMs. Our analysis reveals a universal phenomenon: each model exhibits a Visual Integration Point (VIP), a critical layer at which visual information begins to meaningfully reshape hidden representations and influence decoding. Building on this observation, we introduce the Total Visual Integration (TVI) estimator, which aggregates representation distance beyond the VIP to quantify how strongly visual query influences response generation. Across 54 model-dataset combinations spanning 9 contemporary LVLMs and 6 benchmarks, we demonstrate that VIP consistently emerges, and that TVI reliably predicts the strength of language prior. This offers a principled toolkit for diagnosing and understanding language prior in LVLMs.",
        "arxiv_id": "2509.23050"
    },
    "2509.24365": {
        "SCORE": 15,
        "ARXIVID": "2509.24365",
        "COMMENT": "Model Architecture: a two-end-separated, middle-shared transformer to mitigate modality gradient conflicts in unified multimodal AR models.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Jitai Hao",
            "Hao Liu",
            "Xinyan Xiao",
            "Qiang Huang",
            "Jun Yu"
        ],
        "title": "Uni-X: Mitigating Modality Conflict with a Two-End-Separated Architecture for Unified Multimodal Models",
        "abstract": "Unified Multimodal Models (UMMs) built on shared autoregressive (AR) transformers are attractive for their architectural simplicity. However, we identify a critical limitation: when trained on multimodal inputs, modality-shared transformers suffer from severe gradient conflicts between vision and text, particularly in shallow and deep layers. We trace this issue to the fundamentally different low-level statistical properties of images and text, while noting that conflicts diminish in middle layers where representations become more abstract and semantically aligned. To overcome this challenge, we propose Uni-X, a two-end-separated, middle-shared architecture. Uni-X dedicates its initial and final layers to modality-specific processing, while maintaining shared parameters in the middle layers for high-level semantic fusion. This X-shaped design not only eliminates gradient conflicts at both ends but also further alleviates residual conflicts in the shared layers. Extensive experiments validate the effectiveness of Uni-X. Under identical training conditions, Uni-X achieves superior training efficiency compared to strong baselines. When scaled to 3B parameters with larger training data, Uni-X matches or surpasses 7B AR-based UMMs, achieving a GenEval score of 82 for image generation alongside strong performance in text and vision understanding tasks. These results establish Uni-X as a parameter-efficient and scalable foundation for future unified multimodal modeling. Our code is available at https://github.com/CURRENTF/Uni-X",
        "arxiv_id": "2509.24365"
    },
    "2509.24319": {
        "SCORE": 15,
        "ARXIVID": "2509.24319",
        "COMMENT": "Representation Learning/Mechanistic interpretability: identifies and contrasts intrinsic vs prompted value vectors/neurons in LLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Jongwook Han",
            "Jongwon Lim",
            "Injin Kong",
            "Yohan Jo"
        ],
        "title": "Dual Mechanisms of Value Expression: Intrinsic vs. Prompted Values in LLMs",
        "abstract": "Large language models (LLMs) can express different values in two distinct ways: (1) intrinsic expression, reflecting the model's inherent values learned during training, and (2) prompted expression, elicited by explicit prompts. Given their widespread use in value alignment and persona steering, it is paramount to clearly understand their underlying mechanisms, particularly whether they mostly overlap (as one might expect) or rely on substantially different mechanisms, but this remains largely understudied. We analyze this at the mechanistic level using two approaches: (1) value vectors, feature directions representing value mechanisms extracted from the residual stream, and (2) value neurons, MLP neurons that contribute to value expressions. We demonstrate that intrinsic and prompted value mechanisms partly share common components that are crucial for inducing value expression, but also possess unique elements that manifest in different ways. As a result, these mechanisms lead to different degrees of value steerability (prompted > intrinsic) and response diversity (intrinsic > prompted). In particular, components unique to the intrinsic mechanism seem to promote lexical diversity in responses, whereas those specific to the prompted mechanism primarily strengthen instruction following, taking effect even in distant tasks like jailbreaking.",
        "arxiv_id": "2509.24319"
    },
    "2509.23186": {
        "SCORE": 15,
        "ARXIVID": "2509.23186",
        "COMMENT": "Representation learning/training dynamics: theoretical analysis of multi-token prediction in Transformers for transitive relations, plus architectural tweaks (NTI, transformer-based transfer layer).",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Qimin Zhong",
            "Hao Liao",
            "Siwei Wang",
            "Mingyang Zhou",
            "Xiaoqun Wu",
            "Rui Mao",
            "Wei Chen"
        ],
        "title": "Understanding and Enhancing the Planning Capability of Language Models via Multi-Token Prediction",
        "abstract": "Large Language Models (LLMs) have achieved impressive performance across diverse tasks but continue to struggle with learning transitive relations, a cornerstone for complex planning. To address this issue, we investigate the Multi-Token Prediction (MTP) paradigm and its impact to transitive relation learning. We theoretically analyze the MTP paradigm using a Transformer architecture composed of a shared output head and a transfer layer. Our analysis reveals that the transfer layer gradually learns the multi-step adjacency information, which in turn enables the backbone model to capture unobserved transitive reachability relations beyond those directly present in the training data, albeit with some inevitable noise in adjacency estimation. Building on this foundation, we propose two strategies to enhance the transfer layer and overall learning quality: Next-Token Injection (NTI) and a Transformer-based transfer layer. Our experiments on both synthetic graphs and the Blocksworld planning benchmark validate our theoretical findings and demonstrate that the improvements significantly enhance the model's path-planning capability. These findings deepen our understanding of how Transformers with MTP learn in complex planning tasks, and provide practical strategies to overcome the transitivity bottleneck, paving the way toward structurally aware and general-purpose planning models.",
        "arxiv_id": "2509.23186"
    },
    "2509.22755": {
        "SCORE": 15,
        "ARXIVID": "2509.22755",
        "COMMENT": "Representation Learning/Interpretability: probabilistic theory of Concept Activation Vectors with variance analysis and adversarial vulnerability.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Ekkehard Schnoor",
            "Malik Tiomoko",
            "Jawher Said",
            "Alex Jung",
            "Wojciech Samek"
        ],
        "title": "Concept activation vectors: a unifying view and adversarial attacks",
        "abstract": "Concept Activation Vectors (CAVs) are a tool from explainable AI, offering a promising approach for understanding how human-understandable concepts are encoded in a model's latent spaces. They are computed from hidden-layer activations of inputs belonging either to a concept class or to non-concept examples. Adopting a probabilistic perspective, the distribution of the (non-)concept inputs induces a distribution over the CAV, making it a random vector in the latent space. This enables us to derive mean and covariance for different types of CAVs, leading to a unified theoretical view. This probabilistic perspective also reveals a potential vulnerability: CAVs can strongly depend on the rather arbitrary non-concept distribution, a factor largely overlooked in prior work. We illustrate this with a simple yet effective adversarial attack, underscoring the need for a more systematic study.",
        "arxiv_id": "2509.22755"
    },
    "2509.23667": {
        "SCORE": 15,
        "ARXIVID": "2509.23667",
        "COMMENT": "Model compression/distillation: theoretical and empirical evidence that Align->KD outperforms KD->Align due to recall constraints.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Sungmin Cha",
            "Kyunghyun Cho"
        ],
        "title": "Why Alignment Must Precede Distillation: A Minimal Working Explanation",
        "abstract": "For efficiency, preference alignment is often performed on compact, knowledge-distilled (KD) models. We argue this common practice introduces a significant limitation by overlooking a key property of the alignment's reference model: its distributional recall. We show that the standard KD -> Align workflow diminishes the model's capacity to align rare yet desirable behaviors, even under strong preference signals. We instead demonstrate that reversing the pipeline (i.e., Align -> KD) is essential: alignment must first be performed on a high-recall reference before distillation. Our contributions are threefold. First, we provide a minimal working explanation of how the reference model constrains preference alignment objectives at a fundamental level. Second, we validate this theory in a controllable Mixture-of-Gaussians experiment, where low-recall anchoring consistently results in suboptimal model performance. Finally, we demonstrate that the same phenomenon holds in LLM alignment with the SmolLM2 family: models aligned after KD fail to effectively align target behaviors, resulting in substantially lower reward and target precision. In contrast, our proposed Align -> KD pipeline robustly aligns these behaviors, yielding models with superior target-oriented metrics and lower variance. Together, these results establish reference-model recall as a first-order design choice in alignment, offering a clear principle: alignment must precede distillation.",
        "arxiv_id": "2509.23667"
    },
    "2509.24012": {
        "SCORE": 15,
        "ARXIVID": "2509.24012",
        "COMMENT": "Scaling laws for generative evaluations (pass@k), connecting compute, params\u00d7tokens, and gold-solution likelihoods.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Rylan Schaeffer",
            "Noam Levi",
            "Brando Miranda",
            "Sanmi Koyejo"
        ],
        "title": "Pretraining Scaling Laws for Generative Evaluations of Language Models",
        "abstract": "Neural scaling laws have played a central role in modern machine learning, driving the field's ever-expanding scaling of parameters, data and compute. While much research has gone into fitting scaling laws and predicting performance on pretraining losses and on discriminative evaluations such as multiple-choice question-answering, comparatively little research has been done on fitting scaling laws and predicting performance on generative evaluations such as mathematical problem-solving or software engineering. We propose and evaluate three different pretraining scaling laws for fitting pass-at-$k$ on generative evaluations and for predicting pass-at-$k$ of the most expensive model using the performance of cheaper models. Our three scaling laws differ in the covariates used: (1) compute, (2) model parameters and tokens, (3) log likelihoods of gold reference solutions. We make four main contributions: (1) We show how generative evaluations offer new hyperparameters (in our setting, $k$) that researchers can use to control the scaling laws parameters and the predictability of performance. (2) In terms of scaling law parameters, we find that the compute scaling law and parameters\\,+\\,tokens scaling law stabilize for the last ~$1.5{-}2.5$ orders of magnitude, whereas the gold reference likelihood scaling law stabilizes for the last ~$5$ orders of magnitude. (3) In terms of predictive performance, we find all three scaling laws perform comparably, although the compute scaling law predicts slightly worse for small $k$ and the log likelihoods of gold reference solutions predicts slightly worse for large $k$. (4) We establish a theoretical connection that the compute scaling law emerges as the compute-optimal envelope of the parameters-and-tokens scaling law. Our framework provides researchers and practitioners with insights and methodologies to forecast generative performance.",
        "arxiv_id": "2509.24012"
    },
    "2509.24868": {
        "SCORE": 15,
        "ARXIVID": "2509.24868",
        "COMMENT": "Model Architecture/Efficiency: dual spectral and image branches with bandwise fusion for globally coupled neural operators, reducing parameters and improving throughput vs attention.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Jiayi Li",
            "Flora D. Salim"
        ],
        "title": "DRIFT-Net: A Spectral--Coupled Neural Operator for PDEs Learning",
        "abstract": "Learning PDE dynamics with neural solvers can significantly improve wall-clock efficiency and accuracy compared with classical numerical solvers. In recent years, foundation models for PDEs have largely adopted multi-scale windowed self-attention, with the scOT backbone in \\textsc{Poseidon} serving as a representative example.   However, because of their locality, truly globally consistent spectral coupling can only be propagated gradually through deep stacking and window shifting. This weakens global coupling and leads to error accumulation and drift during closed-loop rollouts. To address this, we propose \\textbf{DRIFT-Net}. It employs a dual-branch design comprising a spectral branch and an image branch. The spectral branch is responsible for capturing global, large-scale low-frequency information, whereas the image branch focuses on local details and nonstationary structures. Specifically, we first perform controlled, lightweight mixing within the low-frequency range. Then we fuse the spectral and image paths at each layer via bandwise weighting, which avoids the width inflation and training instability caused by naive concatenation. The fused result is transformed back into the spatial domain and added to the image branch, thereby preserving both global structure and high-frequency details across scales. Compared with strong attention-based baselines, DRIFT-Net achieves lower error and higher throughput with fewer parameters under identical training settings and budget. On Navier--Stokes benchmarks, the relative $L_{1}$ error is reduced by 7\\%--54\\%, the parameter count decreases by about 15\\%, and the throughput remains higher than scOT. Ablation studies and theoretical analyses further demonstrate the stability and effectiveness of this design. The code is available at https://github.com/cruiseresearchgroup/DRIFT-Net.",
        "arxiv_id": "2509.24868"
    },
    "2509.24168": {
        "SCORE": 15,
        "ARXIVID": "2509.24168",
        "COMMENT": "Matches model architecture and representation learning: proposes a new autoencoder design with asymmetric global/local geometric constraints, with theory.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Qipeng Zhan",
            "Zhuoping Zhou",
            "Zexuan Wang",
            "Li Shen"
        ],
        "title": "Multi-Scale Geometric Autoencoder",
        "abstract": "Autoencoders have emerged as powerful models for visualization and dimensionality reduction based on the fundamental assumption that high-dimensional data is generated from a low-dimensional manifold. A critical challenge in autoencoder design is to preserve the geometric structure of data in the latent space, with existing approaches typically focusing on either global or local geometric properties separately. Global approaches often encounter errors in distance approximation that accumulate, while local methods frequently converge to suboptimal solutions that distort large-scale relationships. We propose Multi-Scale Geometric Autoencoder (MAE), which introduces an asymmetric architecture that simultaneously preserves both scales of the geometric structure by applying global distance constraints to the encoder and local geometric constraints to the decoder. Through theoretical analysis, we establish that this asymmetric design aligns naturally with the distinct roles of the encoder and decoder components. Our comprehensive experiments on both synthetic manifolds and real-world datasets demonstrate that MAE consistently outperforms existing methods across various evaluation metrics.",
        "arxiv_id": "2509.24168"
    },
    "2509.23893": {
        "SCORE": 15,
        "ARXIVID": "2509.23893",
        "COMMENT": "Matches training dynamics for continual learning: proposes dynamic orthogonal gradient constraints to mitigate catastrophic forgetting in LLM fine-tuning.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Zhixin Zhang",
            "Zeming Wei",
            "Meng Sun"
        ],
        "title": "Dynamic Orthogonal Continual Fine-tuning for Mitigating Catastrophic Forgettings",
        "abstract": "Catastrophic forgetting remains a critical challenge in continual learning for large language models (LLMs), where models struggle to retain performance on historical tasks when fine-tuning on new sequential data without access to past datasets. In this paper, we first reveal that the drift of functional directions during the fine-tuning process is a key reason why existing regularization-based methods fail in long-term LLM continual learning. To address this, we propose Dynamic Orthogonal Continual (DOC) fine-tuning, a novel approach that tracks the drift of these functional directions and dynamically updates them during the fine-tuning process. Furthermore, by adjusting the gradients of new task parameters to be orthogonal to the tracked historical function directions, our method mitigates interference between new and old tasks. Extensive experiments on various LLM continual learning benchmarks demonstrate that this approach outperforms prior methods, effectively reducing catastrophic forgetting and providing a robust tool for continuous LLM fine-tuning. Our code is available at https://github.com/meloxxxxxx/DOC.",
        "arxiv_id": "2509.23893"
    },
    "2509.23597": {
        "SCORE": 15,
        "ARXIVID": "2509.23597",
        "COMMENT": "Matches compression/efficiency and representation learning theory: analyzes linear forecasting via characteristic roots and proposes low-rank (rank reduction) and a new Root Purge regularization.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Zheng Wang",
            "Kaixuan Zhang",
            "Wanfang Chen",
            "Xiaonan Lu",
            "Longyuan Li",
            "Tobias Schlagenhauf"
        ],
        "title": "Characteristic Root Analysis and Regularization for Linear Time Series Forecasting",
        "abstract": "Time series forecasting remains a critical challenge across numerous domains, yet the effectiveness of complex models often varies unpredictably across datasets. Recent studies highlight the surprising competitiveness of simple linear models, suggesting that their robustness and interpretability warrant deeper theoretical investigation. This paper presents a systematic study of linear models for time series forecasting, with a focus on the role of characteristic roots in temporal dynamics. We begin by analyzing the noise-free setting, where we show that characteristic roots govern long-term behavior and explain how design choices such as instance normalization and channel independence affect model capabilities. We then extend our analysis to the noisy regime, revealing that models tend to produce spurious roots. This leads to the identification of a key data-scaling property: mitigating the influence of noise requires disproportionately large training data, highlighting the need for structural regularization. To address these challenges, we propose two complementary strategies for robust root restructuring. The first uses rank reduction techniques, including Reduced-Rank Regression and Direct Weight Rank Reduction, to recover the low-dimensional latent dynamics. The second, a novel adaptive method called Root Purge, encourages the model to learn a noise-suppressing null space during training. Extensive experiments on standard benchmarks demonstrate the effectiveness of both approaches, validating our theoretical insights and achieving state-of-the-art results in several settings. Our findings underscore the potential of integrating classical theories for linear systems with modern learning techniques to build robust, interpretable, and data-efficient forecasting models.",
        "arxiv_id": "2509.23597"
    },
    "2509.23324": {
        "SCORE": 15,
        "ARXIVID": "2509.23324",
        "COMMENT": "High Performance Computing: systems-level mobile NPU inference with hardware-aware tile quantization and LUT-based ops enabling test-time compute scaling.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Zixu Hao",
            "Jianyu Wei",
            "Tuowei Wang",
            "Minxing Huang",
            "Huiqiang Jiang",
            "Shiqi Jiang",
            "Ting Cao",
            "Ju Ren"
        ],
        "title": "Scaling LLM Test-Time Compute with Mobile NPU on Smartphones",
        "abstract": "Deploying Large Language Models (LLMs) on mobile devices faces the challenge of insufficient performance in smaller models and excessive resource consumption in larger ones. This paper highlights that mobile Neural Processing Units (NPUs) have underutilized computational resources, particularly their matrix multiplication units, during typical LLM inference. To leverage this wasted compute capacity, we propose applying parallel test-time scaling techniques on mobile NPUs to enhance the performance of smaller LLMs. However, this approach confronts inherent NPU challenges, including inadequate hardware support for fine-grained quantization and low efficiency in general-purpose computations. To overcome these, we introduce two key techniques: a hardware-aware tile quantization scheme that aligns group quantization with NPU memory access patterns, and efficient LUT-based replacements for complex operations such as Softmax and dequantization. We design and implement an end-to-end inference system that leverages the NPU's compute capability to support test-time scaling on Qualcomm Snapdragon platforms. Experiments show our approach brings significant speedups: up to 19.0 for mixed-precision GEMM and 2.2 for Softmax. More importantly, we demonstrate that smaller models using test-time scaling can match or exceed the accuracy of larger models, achieving a new performance-cost Pareto frontier.",
        "arxiv_id": "2509.23324"
    },
    "2509.24248": {
        "SCORE": 15,
        "ARXIVID": "2509.24248",
        "COMMENT": "Model Compression and Efficiency: speculative early-exit using a lightweight draft model\u2019s hidden states to signal stopping, reducing latency/length without probes.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Rubing Yang",
            "Huajun Bai",
            "Song Liu",
            "Guanghua Yu",
            "Runzhi Fan",
            "Yanbin Dang",
            "Jiejing Zhang",
            "Kai Liu",
            "Jianchen Zhu",
            "Peng Chen"
        ],
        "title": "SpecExit: Accelerating Large Reasoning Model via Speculative Exit",
        "abstract": "Despite their strong performance on reasoning tasks, large reasoning models (LRMs) often suffer from overthinking, producing unnecessarily long outputs and incurring high end-to-end latency, a significant limitation to their real-world deployment. To address overthinking, early-exit mechanisms have been proposed to terminate reasoning before typical completion, showing that this approach can effectively shorten generation length with minimal impact on accuracy. However, their reliance on probing mechanisms introduces a detection overhead that limits their end-to-end latency gains and compromises their generalizability across diverse problems. Inspired by the use of hidden states in speculative decoding, we propose SpecExit, a novel framework that predicts both future tokens and an early-exit signal directly from a lightweight draft model without probing overhead. Our method offers significant improvements, reducing average generation length by 66\\% and achieving a 2.5x speedup in end-to-end latency compared to the speculative decoding baseline, without compromising accuracy. Our method leverages the inherent signals from hidden states to provide effective early-exit signals, suggesting broader use of hidden states for efficient reasoning. Our code is available at https://github.com/Tencent/AngelSlim.",
        "arxiv_id": "2509.24248"
    },
    "2509.23982": {
        "SCORE": 15,
        "ARXIVID": "2509.23982",
        "COMMENT": "Matches Efficiency/Representation Learning: training-free residual-stream steering vectors for preference alignment applied at inference time.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Lucio La Cava",
            "Andrea Tagarelli"
        ],
        "title": "Toward Preference-aligned Large Language Models via Residual-based Model Steering",
        "abstract": "Preference alignment is a critical step in making Large Language Models (LLMs) useful and aligned with (human) preferences. Existing approaches such as Reinforcement Learning from Human Feedback or Direct Preference Optimization typically require curated data and expensive optimization over billions of parameters, and eventually lead to persistent task-specific models. In this work, we introduce Preference alignment of Large Language Models via Residual Steering (PaLRS), a training-free method that exploits preference signals encoded in the residual streams of LLMs. From as few as one hundred preference pairs, PaLRS extracts lightweight, plug-and-play steering vectors that can be applied at inference time to push models toward preferred behaviors. We evaluate PaLRS on various small-to-medium-scale open-source LLMs, showing that PaLRS-aligned models achieve consistent gains on mathematical reasoning and code generation benchmarks while preserving baseline general-purpose performance. Moreover, when compared to DPO-aligned models, they perform better with huge time savings. Our findings highlight that PaLRS offers an effective, much more efficient and flexible alternative to standard preference optimization pipelines, offering a training-free, plug-and-play mechanism for alignment with minimal data.",
        "arxiv_id": "2509.23982"
    },
    "2509.23068": {
        "SCORE": 15,
        "ARXIVID": "2509.23068",
        "COMMENT": "Matches Model Architecture and Sparsity: sparse deep additive model with structured regularization and explicit interaction modeling for interpretability.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Yi-Ting Hung",
            "Li-Hsiang Lin",
            "Vince D. Calhoun"
        ],
        "title": "Sparse Deep Additive Model with Interactions: Enhancing Interpretability and Predictability",
        "abstract": "Recent advances in deep learning highlight the need for personalized models that can learn from small or moderate samples, handle high dimensional features, and remain interpretable. To address this challenge, we propose the Sparse Deep Additive Model with Interactions (SDAMI), a framework that combines sparsity driven feature selection with deep subnetworks for flexible function approximation. Unlike conventional deep learning models, which often function as black boxes, SDAMI explicitly disentangles main effects and interaction effects to enhance interpretability. At the same time, its deep additive structure achieves higher predictive accuracy than classical additive models. Central to SDAMI is the concept of an Effect Footprint, which assumes that higher order interactions project marginally onto main effects. Guided by this principle, SDAMI adopts a two stage strategy: first, identify strong main effects that implicitly carry information about important interactions. second, exploit this information through structured regularization such as group lasso to distinguish genuine main effects from interaction effects. For each selected main effect, SDAMI constructs a dedicated subnetwork, enabling nonlinear function approximation while preserving interpretability and providing a structured foundation for modeling interactions. Extensive simulations with comparisons confirm SDAMI$'$s ability to recover effect structures across diverse scenarios, while applications in reliability analysis, neuroscience, and medical diagnostics further demonstrate its versatility in addressing real-world high-dimensional modeling challenges.",
        "arxiv_id": "2509.23068"
    },
    "2509.24431": {
        "SCORE": 15,
        "ARXIVID": "2509.24431",
        "COMMENT": "Matches Compression/Efficiency and Representation Learning: semantic compression of multimodal embeddings via modality-gap reduction and centroiding.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Eleonora Grassucci",
            "Giordano Cicchetti",
            "Aurelio Uncini",
            "Danilo Comminiello"
        ],
        "title": "Semantic Compression via Multimodal Representation Learning",
        "abstract": "Multimodal representation learning produces high-dimensional embeddings that align diverse modalities in a shared latent space. While this enables strong generalization, it also introduces scalability challenges, both in terms of storage and downstream processing. A key open problem is how to achieve semantic compression, reducing the memory footprint of multimodal embeddings while preserving their ability to represent shared semantic content across modalities. In this paper, we prove a strong connection between reducing the modality gap, which is the residual separation of embeddings from different modalities, and the feasibility of post-training semantic compression. When the gap is sufficiently reduced, embeddings from different modalities but expressing the same semantics share a common portion of the space. Therefore, their centroid is a faithful representation of such a semantic concept. This enables replacing multiple embeddings with a single centroid, yielding significant memory savings. We propose a novel approach for semantic compression grounded on the latter intuition, operating directly on pretrained encoders. We demonstrate its effectiveness across diverse large-scale multimodal downstream tasks. Our results highlight that modality alignment is a key enabler for semantic compression, showing that the proposed approach achieves significant compression without sacrificing performance.",
        "arxiv_id": "2509.24431"
    },
    "2509.23373": {
        "SCORE": 15,
        "ARXIVID": "2509.23373",
        "COMMENT": "Matches Representation Learning: graph consistency regularization aligning feature-similarity graphs with class-aware prediction graphs via parameter-free layers.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Xi Ding",
            "Lei Wang",
            "Piotr Koniusz",
            "Yongsheng Gao"
        ],
        "title": "Graph Your Own Prompt",
        "abstract": "We propose Graph Consistency Regularization (GCR), a novel framework that injects relational graph structures, derived from model predictions, into the learning process to promote class-aware, semantically meaningful feature representations. Functioning as a form of self-prompting, GCR enables the model to refine its internal structure using its own outputs. While deep networks learn rich representations, these often capture noisy inter-class similarities that contradict the model's predicted semantics. GCR addresses this issue by introducing parameter-free Graph Consistency Layers (GCLs) at arbitrary depths. Each GCL builds a batch-level feature similarity graph and aligns it with a global, class-aware masked prediction graph, derived by modulating softmax prediction similarities with intra-class indicators. This alignment enforces that feature-level relationships reflect class-consistent prediction behavior, acting as a semantic regularizer throughout the network. Unlike prior work, GCR introduces a multi-layer, cross-space graph alignment mechanism with adaptive weighting, where layer importance is learned from graph discrepancy magnitudes. This allows the model to prioritize semantically reliable layers and suppress noisy ones, enhancing feature quality without modifying the architecture or training procedure. GCR is model-agnostic, lightweight, and improves semantic structure across various networks and datasets. Experiments show that GCR promotes cleaner feature structure, stronger intra-class cohesion, and improved generalization, offering a new perspective on learning from prediction structure. [Project website](https://darcyddx.github.io/gcr/) [Code](https://github.com/Darcyddx/graph-prompt)",
        "arxiv_id": "2509.23373"
    },
    "2509.22947": {
        "SCORE": 15,
        "ARXIVID": "2509.22947",
        "COMMENT": "Representation Learning: mechanism-focused study of induction circuits under iso-FLOPs with targeted synthetic curricula and head-level telemetry/ablations informing ICL dynamics.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Mohammed Sabry",
            "Anya Belz"
        ],
        "title": "What Matters More For In-Context Learning under Matched Compute Budgets: Pretraining on Natural Text or Incorporating Targeted Synthetic Examples?",
        "abstract": "Does explicitly exercising the induction circuit during pretraining improve in-context learning (ICL), or is natural text sufficient when compute is held constant (iso-FLOPs)? To test whether targeted synthetic data can accelerate induction-head emergence and enhance ICL, we introduce Bi-Induct, a lightweight curriculum that injects forward-copy (Induction), backward-copy (Anti), or a balanced mix into the pretraining stream. We train models from 0.13B to 1B parameters under iso-FLOPs, evaluating (i) few-shot ICL benchmarks, (ii) head-level telemetry, and (iii) held-out language modeling perplexity. Our findings challenge the assumption that early induction circuit activation directly improves ICL. While Bi-Induct accelerates induction-head emergence at small scales, this does not consistently yield stronger generalization. On standard LM benchmarks, Bi-Induct matches natural-only training; on function-style ICL probes, the 1B natural-only performs best. Stress tests (e.g., label permutation, HITS@1 vs. HITS@3, 1 vs. 10 shots) preserve these trends. Telemetry shows larger natural-only models develop broader, earlier induction heads without explicit induction patterns. Anti-induction data fails to elicit meaningful activation. Perplexity penalties from synthetic data shrink with scale, suggesting larger models can absorb non-natural patterns with minimal cost. Crucially, ablating the top 2% of induction heads degrades ICL more than random ablations, especially for natural-only models, indicating more centralized, load-bearing circuits. Bi-Induct variants exhibit more redundant induction activity, implying different circuit utilization. Overall, inducing activation is not sufficient: ICL gains depend on these circuits becoming functionally necessary. These results underscore mechanism-aware pretraining diagnostics and data mixtures that foster load-bearing, not merely present, structure.",
        "arxiv_id": "2509.22947"
    },
    "2509.23655": {
        "SCORE": 15,
        "ARXIVID": "2509.23655",
        "COMMENT": "Model Compression/Efficiency: object\u2013agent-centric visual tokenization drastically reduces visual tokens for VLA training while retaining performance.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Rokas Bendikas",
            "Daniel Dijkman",
            "Markus Peschl",
            "Sanjay Haresh",
            "Pietro Mazzaglia"
        ],
        "title": "Focusing on What Matters: Object-Agent-centric Tokenization for Vision Language Action models",
        "abstract": "Vision-Language-Action (VLA) models offer a pivotal approach to learning robotic manipulation at scale by repurposing large pre-trained Vision-Language-Models (VLM) to output robotic actions. However, adapting VLMs for robotic domains comes with an unnecessarily high computational cost, which we attribute to the tokenization scheme of visual inputs. In this work, we aim to enable efficient VLA training by proposing Oat-VLA, an Object-Agent-centric Tokenization for VLAs. Building on the insights of object-centric representation learning, our method introduces an inductive bias towards scene objects and the agent's own visual information. As a result, we find that Oat-VLA can drastically reduce the number of visual tokens to just a few tokens without sacrificing performance. We reveal that Oat-VLA converges at least twice as fast as OpenVLA on the LIBERO suite, as well as outperform OpenVLA in diverse real-world pick and place tasks.",
        "arxiv_id": "2509.23655"
    },
    "2509.23666": {
        "SCORE": 15,
        "ARXIVID": "2509.23666",
        "COMMENT": "Model Compression/Efficiency: adaptive early-exit thresholding via bandits with risk guarantees for reliable speed\u2013accuracy trade-offs under distribution shift.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Divya Jyoti Bajpai",
            "Manjesh Kumar Hanawal"
        ],
        "title": "Beyond Greedy Exits: Improved Early Exit Decisions for Risk Control and Reliability",
        "abstract": "Early-Exit Deep Neural Networks enable adaptive inference by allowing prediction at intermediary layers, significantly reducing computational costs and latency. Most of the early exit strategies greedily exit a sample at an intermediary layer if the confidence in class prediction exceeds a predefined threshold that is set using a static validation set. This is problematic as the model might be overconfident in a wrong class. Also, they are not robust to distribution shifts encountered in deployment, which can undermine model trustworthiness and accuracy. To address these challenges, we propose UAT that adapts the threshold for exit decisions using a Multi-Armed Bandit framework, enabling online, unsupervised adjustment of exit decisions. UAT makes decisions based on a new reward function that assesses predictive certainty and its reliability to balance computational efficiency and prediction quality while penalizing unnecessary late exits. We provide guarantees on risk achieved by UAT and validate its performance on diverse tasks spanning vision-language understanding, text generation, and classification. Our framework demonstrates consistent improvements in speedup (1.70-2.10x) with a minimal performance drop (<2%) as compared to full model performance. Our source code is available at https://github.com/Div290/UAT.",
        "arxiv_id": "2509.23666"
    },
    "2509.23027": {
        "SCORE": 15,
        "ARXIVID": "2509.23027",
        "COMMENT": "Representation Learning: theoretical framework linking catastrophic interference to identifiability of shared latent variables; method to learn shared representations to mitigate forgetting.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Yuke Li",
            "Yujia Zheng",
            "Tianyi Xiong",
            "Zhenyi Wang",
            "Heng Huang"
        ],
        "title": "Understanding Catastrophic Interference On the Identifibility of Latent Representations",
        "abstract": "Catastrophic interference, also known as catastrophic forgetting, is a fundamental challenge in machine learning, where a trained learning model progressively loses performance on previously learned tasks when adapting to new ones. In this paper, we aim to better understand and model the catastrophic interference problem from a latent representation learning point of view, and propose a novel theoretical framework that formulates catastrophic interference as an identification problem. Our analysis demonstrates that the forgetting phenomenon can be quantified by the distance between partial-task aware (PTA) and all-task aware (ATA) setups. Building upon recent advances in identifiability theory, we prove that this distance can be minimized through identification of shared latent variables between these setups. When learning, we propose our method \\ourmeos with two-stage training strategy: First, we employ maximum likelihood estimation to learn the latent representations from both PTA and ATA configurations. Subsequently, we optimize the KL divergence to identify and learn the shared latent variables. Through theoretical guarantee and empirical validations, we establish that identifying and learning these shared representations can effectively mitigate catastrophic interference in machine learning systems. Our approach provides both theoretical guarantees and practical performance improvements across both synthetic and benchmark datasets.",
        "arxiv_id": "2509.23027"
    },
    "2509.24122": {
        "SCORE": 15,
        "ARXIVID": "2509.24122",
        "COMMENT": "Model Architecture: extended Echo State Networks with matrix-gated composite activations and a dual-stream design for long-range memory with constant memory/compute.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Hongbo Liu",
            "Jia Xu"
        ],
        "title": "Echo Flow Networks",
        "abstract": "At the heart of time-series forecasting (TSF) lies a fundamental challenge: how can models efficiently and effectively capture long-range temporal dependencies across ever-growing sequences? While deep learning has brought notable progress, conventional architectures often face a trade-off between computational complexity and their ability to retain accumulative information over extended horizons.   Echo State Networks (ESNs), a class of reservoir computing models, have recently regained attention for their exceptional efficiency, offering constant memory usage and per-step training complexity regardless of input length. This makes them particularly attractive for modeling extremely long-term event history in TSF. However, traditional ESNs fall short of state-of-the-art performance due to their limited nonlinear capacity, which constrains both their expressiveness and stability.   We introduce Echo Flow Networks (EFNs), a framework composed of a group of extended Echo State Networks (X-ESNs) with MLP readouts, enhanced by our novel Matrix-Gated Composite Random Activation (MCRA), which enables complex, neuron-specific temporal dynamics, significantly expanding the network's representational capacity without compromising computational efficiency. In addition, we propose a dual-stream architecture in which recent input history dynamically selects signature reservoir features from an infinite-horizon memory, leading to improved prediction accuracy and long-term stability.   Extensive evaluations on five benchmarks demonstrate that EFNs achieve up to 4x faster training and 3x smaller model size compared to leading methods like PatchTST, reducing forecasting error from 43% to 35%, a 20% relative improvement. One instantiation of our framework, EchoFormer, consistently achieves new state-of-the-art performance across five benchmark datasets: ETTh, ETTm, DMV, Weather, and Air Quality.",
        "arxiv_id": "2509.24122"
    },
    "2509.24072": {
        "SCORE": 15,
        "ARXIVID": "2509.24072",
        "COMMENT": "Representation Learning: uncovers latent Grounding IDs induced by external cues, explaining improved multimodal binding and attention via causal/representational analyses.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Hosein Hasani",
            "Amirmohammad Izadi",
            "Fatemeh Askari",
            "Mobin Bagherian",
            "Sadegh Mohammadian",
            "Mohammad Izadi",
            "Mahdieh Soleymani Baghshah"
        ],
        "title": "Uncovering Grounding IDs: How External Cues Shape Multi-Modal Binding",
        "abstract": "Large vision-language models (LVLMs) show strong performance across multimodal benchmarks but remain limited in structured reasoning and precise grounding. Recent work has demonstrated that adding simple visual structures, such as partitions and annotations, improves accuracy, yet the internal mechanisms underlying these gains remain unclear. We investigate this phenomenon and propose the concept of Grounding IDs, latent identifiers induced by external cues that bind objects to their designated partitions across modalities. Through representation analysis, we find that these identifiers emerge as robust within-partition alignment in embedding space and reduce the modality gap between image and text. Causal interventions further confirm that these identifiers mediate binding between objects and symbolic cues. We show that Grounding IDs strengthen attention between related components, which in turn improves cross-modal grounding and reduces hallucinations. Taken together, our results identify Grounding IDs as a key symbolic mechanism explaining how external cues enhance multimodal binding, offering both interpretability and practical improvements in robustness.",
        "arxiv_id": "2509.24072"
    },
    "2509.22889": {
        "SCORE": 15,
        "ARXIVID": "2509.22889",
        "COMMENT": "Model Architecture: Convolutional Set Transformer operates directly on 3D image tensors to jointly perform feature extraction and set-context modeling.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Federico Chinello",
            "Giacomo Boracchi"
        ],
        "title": "Convolutional Set Transformer",
        "abstract": "We introduce the Convolutional Set Transformer (CST), a novel neural architecture designed to process image sets of arbitrary cardinality that are visually heterogeneous yet share high-level semantics - such as a common category, scene, or concept. Existing set-input networks, e.g., Deep Sets and Set Transformer, are limited to vector inputs and cannot directly handle 3D image tensors. As a result, they must be cascaded with a feature extractor, typically a CNN, which encodes images into embeddings before the set-input network can model inter-image relationships. In contrast, CST operates directly on 3D image tensors, performing feature extraction and contextual modeling simultaneously, thereby enabling synergies between the two processes. This design yields superior performance in tasks such as Set Classification and Set Anomaly Detection and further provides native compatibility with CNN explainability methods such as Grad-CAM, unlike competing approaches that remain opaque. Finally, we show that CSTs can be pre-trained on large-scale datasets and subsequently adapted to new domains and tasks through standard Transfer Learning schemes. To support further research, we release CST-15, a CST backbone pre-trained on ImageNet (https://github.com/chinefed/convolutional-set-transformer).",
        "arxiv_id": "2509.22889"
    },
    "2509.23587": {
        "SCORE": 15,
        "ARXIVID": "2509.23587",
        "COMMENT": "Matches Compression/Efficiency criterion: sketching method to jointly estimate low-rank plus diagonal structure of large operators (e.g., Hessians).",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Andres Fernandez",
            "Felix Dangel",
            "Philipp Hennig",
            "Frank Schneider"
        ],
        "title": "Sketching Low-Rank Plus Diagonal Matrices",
        "abstract": "Many relevant machine learning and scientific computing tasks involve high-dimensional linear operators accessible only via costly matrix-vector products. In this context, recent advances in sketched methods have enabled the construction of *either* low-rank *or* diagonal approximations from few matrix-vector products. This provides great speedup and scalability, but approximation errors arise due to the assumed simpler structure. This work introduces SKETCHLORD, a method that simultaneously estimates both low-rank *and* diagonal components, targeting the broader class of Low-Rank *plus* Diagonal (LoRD) linear operators. We demonstrate theoretically and empirically that this joint estimation is superior also to any sequential variant (diagonal-then-low-rank or low-rank-then-diagonal). Then, we cast SKETCHLORD as a convex optimization problem, leading to a scalable algorithm. Comprehensive experiments on synthetic (approximate) LoRD matrices confirm SKETCHLORD's performance in accurately recovering these structures. This positions it as a valuable addition to the structured approximation toolkit, particularly when high-fidelity approximations are desired for large-scale operators, such as the deep learning Hessian.",
        "arxiv_id": "2509.23587"
    },
    "2509.24467": {
        "SCORE": 15,
        "ARXIVID": "2509.24467",
        "COMMENT": "Matches Representation Learning criterion: scalable kernel-based representation learning via Nystr\u00f6m approximation with interpretability.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Maedeh Zarvandi",
            "Michael Timothy",
            "Theresa Wasserer",
            "Debarghya Ghoshdastidar"
        ],
        "title": "Interpretable Kernel Representation Learning at Scale: A Unified Framework Utilizing Nystr\\\"om Approximation",
        "abstract": "Kernel methods provide a theoretically grounded framework for non-linear and non-parametric learning, with strong analytic foundations and statistical guarantees. Yet, their scalability has long been limited by prohibitive time and memory costs. While progress has been made in scaling kernel regression, no framework exists for scalable kernel-based representation learning, restricting their use in the era of foundation models where representations are learned from massive unlabeled data. We introduce KREPES -- a unified, scalable framework for kernel-based representation learning via Nystr\\\"om approximation. KREPES accommodates a wide range of unsupervised and self-supervised losses, and experiments on large image and tabular datasets demonstrate its efficiency. Crucially, KREPES enables principled interpretability of the learned representations, an immediate benefit over deep models, which we substantiate through dedicated analysis.",
        "arxiv_id": "2509.24467"
    },
    "2509.23103": {
        "SCORE": 15,
        "ARXIVID": "2509.23103",
        "COMMENT": "Matches Compression/Efficiency and Model Architecture: multiplication-avoiding network via Hadamard transforms and SRAM in-memory computing.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Emadeldeen Hamdan",
            "Ahmet Enis Cetin"
        ],
        "title": "HTMA-Net: Towards Multiplication-Avoiding Neural Networks via Hadamard Transform and In-Memory Computing",
        "abstract": "Reducing the cost of multiplications is critical for efficient deep neural network deployment, especially in energy-constrained edge devices. In this work, we introduce HTMA-Net, a novel framework that integrates the Hadamard Transform (HT) with multiplication-avoiding (MA) SRAM-based in-memory computing to reduce arithmetic complexity while maintaining accuracy. Unlike prior methods that only target multiplications in convolutional layers or focus solely on in-memory acceleration, HTMA-Net selectively replaces intermediate convolutions with Hybrid Hadamard-based transform layers whose internal convolutions are implemented via multiplication-avoiding in-memory operations. We evaluate HTMA-Net on ResNet-18 using CIFAR-10, CIFAR-100, and Tiny ImageNet, and provide a detailed comparison against regular, MF-only, and HT-only variants. Results show that HTMA-Net eliminates up to 52\\% of multiplications compared to baseline ResNet-18, ResNet-20, and ResNet-50 models, while achieving comparable accuracy in evaluation and significantly reducing computational complexity and the number of parameters. Our results demonstrate that combining structured Hadamard transform layers with SRAM-based in-memory computing multiplication-avoiding operators is a promising path towards efficient deep learning architectures.",
        "arxiv_id": "2509.23103"
    },
    "2509.24728": {
        "SCORE": 15,
        "ARXIVID": "2509.24728",
        "COMMENT": "Model Architecture/Representation Learning: proposes a replacement for softmax (catnat) via hierarchical binary splits with information-geometric justification (diagonal Fisher), improving gradient-based learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Alessandro Manenti",
            "Cesare Alippi"
        ],
        "title": "Beyond Softmax: A Natural Parameterization for Categorical Random Variables",
        "abstract": "Latent categorical variables are frequently found in deep learning architectures. They can model actions in discrete reinforcement-learning environments, represent categories in latent-variable models, or express relations in graph neural networks. Despite their widespread use, their discrete nature poses significant challenges to gradient-descent learning algorithms. While a substantial body of work has offered improved gradient estimation techniques, we take a complementary approach. Specifically, we: 1) revisit the ubiquitous $\\textit{softmax}$ function and demonstrate its limitations from an information-geometric perspective; 2) replace the $\\textit{softmax}$ with the $\\textit{catnat}$ function, a function composed of a sequence of hierarchical binary splits; we prove that this choice offers significant advantages to gradient descent due to the resulting diagonal Fisher Information Matrix. A rich set of experiments - including graph structure learning, variational autoencoders, and reinforcement learning - empirically show that the proposed function improves the learning efficiency and yields models characterized by consistently higher test performance. $\\textit{Catnat}$ is simple to implement and seamlessly integrates into existing codebases. Moreover, it remains compatible with standard training stabilization techniques and, as such, offers a better alternative to the $\\textit{softmax}$ function.",
        "arxiv_id": "2509.24728"
    },
    "2509.24957": {
        "SCORE": 15,
        "ARXIVID": "2509.24957",
        "COMMENT": "High Performance Computing/Systems: intra-request branch orchestration using activation-based linear probes to reduce inference cost/latency for multi-branch reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Weifan Jiang",
            "Rana Shahout",
            "Yilun Du",
            "Michael Mitzenmacher",
            "Minlan Yu"
        ],
        "title": "Intra-request branch orchestration for efficient LLM reasoning",
        "abstract": "Large Language Models (LLMs) increasingly rely on inference-time reasoning algorithms such as chain-of-thought and multi-branch reasoning to improve accuracy on complex tasks. These methods, however, substantially increase token usage and per-request latency. Prior work has largely focused on reducing token usage, often at the expense of accuracy, while overlooking other latency factors. We present DUCHESS, an LLM serving system that reduces cost and latency without sacrificing accuracy through intra-request branch orchestration guided by predictions. DUCHESS employs a lightweight linear probing model over LLM layer activations to estimate branch correctness, and its orchestration policy decides whether to terminate, duplicate, or continue a branch. When handling multiple requests, DUCHESS further reduces latency by prioritizing easier reasoning tasks when complexity can be estimated from the prompt. Experiments on three reasoning benchmarks show that DUCHESS consistently improves the token-accuracy Pareto frontier, reducing token usage by 42-63% at matched accuracy compared to self-consistency. In serving with vLLM, DUCHESS reduces mean, median, and tail latencies by 57-81%, 58-85%, and 52-84% with First-Come-First-Served scheduling, and achieves additional gains under difficulty-aware scheduling at higher request rates.",
        "arxiv_id": "2509.24957"
    },
    "2509.24483": {
        "SCORE": 15,
        "ARXIVID": "2509.24483",
        "COMMENT": "Model Architecture (MoE): sparse mixture of prompt experts with gating for continual learning, mitigating interference while keeping efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Minh Le",
            "Bao-Ngoc Dao",
            "Huy Nguyen",
            "Quyen Tran",
            "Anh Nguyen",
            "Nhat Ho"
        ],
        "title": "One-Prompt Strikes Back: Sparse Mixture of Experts for Prompt-based Continual Learning",
        "abstract": "Prompt-based methods have recently gained prominence in Continual Learning (CL) due to their strong performance and memory efficiency. A prevalent strategy in this paradigm assigns a dedicated subset of prompts to each task, which, while effective, incurs substantial computational overhead and causes memory requirements to scale linearly with the number of tasks. Conversely, approaches employing a single shared prompt across tasks offer greater efficiency but often suffer from degraded performance due to knowledge interference. To reconcile this trade-off, we propose SMoPE, a novel framework that integrates the benefits of both task-specific and shared prompt strategies. Inspired by recent findings on the relationship between Prefix Tuning and Mixture of Experts (MoE), SMoPE organizes a shared prompt into multiple \"prompt experts\" within a sparse MoE architecture. For each input, only a select subset of relevant experts is activated, effectively mitigating interference. To facilitate expert selection, we introduce a prompt-attention score aggregation mechanism that computes a unified proxy score for each expert, enabling dynamic and sparse activation. Additionally, we propose an adaptive noise mechanism to encourage balanced expert utilization while preserving knowledge from prior tasks. To further enhance expert specialization, we design a prototype-based loss function that leverages prefix keys as implicit memory representations. Extensive experiments across multiple CL benchmarks demonstrate that SMoPE consistently outperforms task-specific prompt methods and achieves performance competitive with state-of-the-art approaches, all while significantly reducing parameter counts and computational costs.",
        "arxiv_id": "2509.24483"
    },
    "2509.25045": {
        "SCORE": 15,
        "ARXIVID": "2509.25045",
        "COMMENT": "Matches Representation Learning/Interpretability: proposes a VSA-based probing method to decode structured concepts from LLM residual stream, addressing SAE/DLA limitations.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Marco Bronzini",
            "Carlo Nicolini",
            "Bruno Lepri",
            "Jacopo Staiano",
            "Andrea Passerini"
        ],
        "title": "Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic Architectures",
        "abstract": "Despite their capabilities, Large Language Models (LLMs) remain opaque with limited understanding of their internal representations. Current interpretability methods, such as direct logit attribution (DLA) and sparse autoencoders (SAEs), provide restricted insight due to limitations such as the model's output vocabulary or unclear feature names. This work introduces Hyperdimensional Probe, a novel paradigm for decoding information from the LLM vector space. It combines ideas from symbolic representations and neural probing to project the model's residual stream into interpretable concepts via Vector Symbolic Architectures (VSAs). This probe combines the strengths of SAEs and conventional probes while overcoming their key limitations. We validate our decoding paradigm with controlled input-completion tasks, probing the model's final state before next-token prediction on inputs spanning syntactic pattern recognition, key-value associations, and abstract inference. We further assess it in a question-answering setting, examining the state of the model both before and after text generation. Our experiments show that our probe reliably extracts meaningful concepts across varied LLMs, embedding sizes, and input domains, also helping identify LLM failures. Our work advances information decoding in LLM vector space, enabling extracting more informative, interpretable, and structured features from neural representations.",
        "arxiv_id": "2509.25045"
    },
    "2509.24425": {
        "SCORE": 15,
        "ARXIVID": "2509.24425",
        "COMMENT": "Model Architecture + Compression/Efficiency: proposes a binary hyperdimensional Transformer (binarization/quantization) with theoretical analysis of information distortion for efficient sequence modeling.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Jingtao Zhang",
            "Yi Liu",
            "Qi Shen",
            "Changhong Wang"
        ],
        "title": "BiHDTrans: binary hyperdimensional transformer for efficient multivariate time series classification",
        "abstract": "The proliferation of Internet-of-Things (IoT) devices has led to an unprecedented volume of multivariate time series (MTS) data, requiring efficient and accurate processing for timely decision-making in resource-constrained edge environments. Hyperdimensional (HD) computing, with its inherent efficiency and parallelizability, has shown promise in classification tasks but struggles to capture complex temporal patterns, while Transformers excel at sequence modeling but incur high computational and memory overhead. We introduce BiHDTrans, an efficient neurosymbolic binary hyperdimensional Transformer that integrates self-attention into the HD computing paradigm, unifying the representational efficiency of HD computing with the temporal modeling power of Transformers. Empirically, BiHDTrans outperforms state-of-the-art (SOTA) HD computing models by at least 14.47% and achieves 6.67% higher accuracy on average than SOTA binary Transformers. With hardware acceleration on FPGA, our pipelined implementation leverages the independent and identically distributed properties of high-dimensional representations, delivering 39.4 times lower inference latency than SOTA binary Transformers. Theoretical analysis shows that binarizing in holographic high-dimensional space incurs significantly less information distortion than directly binarizing neural networks, explaining BiHDTrans's superior accuracy. Furthermore, dimensionality experiments confirm that BiHDTrans remains competitive even with a 64% reduction in hyperspace dimensionality, surpassing SOTA binary Transformers by 1-2% in accuracy with 4.4 times less model size, as well as further reducing the latency by 49.8% compare to the full-dimensional baseline. Together, these contributions bridge the gap between the expressiveness of Transformers and the efficiency of HD computing, enabling accurate, scalable, and low-latency MTS classification.",
        "arxiv_id": "2509.24425"
    },
    "2509.23886": {
        "SCORE": 15,
        "ARXIVID": "2509.23886",
        "COMMENT": "Representation Learning/Training dynamics: mechanistic analysis of subliminal learning in distillation, identifying divergence tokens and early-layer roles.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Simon Schrodi",
            "Elias Kempf",
            "Fazl Barez",
            "Thomas Brox"
        ],
        "title": "Towards Understanding Subliminal Learning: When and How Hidden Biases Transfer",
        "abstract": "Language models can transfer hidden biases during distillation. For example, a teacher that \"likes owls\" can make its student \"like owls\" too, even when the training data consists only of lists of numbers. This surprising phenomenon is called subliminal learning. Subliminal learning can be expected under soft distillation, where the student is trained on the teacher's full next-token distribution. But the fact that this also occurs under hard distillation-where the student only sees sampled tokens-raises a deeper question: when and how does subliminal learning actually occur? We answer this question through controlled experiments and mechanistic analysis. Our results show that subliminal learning does not need (global) token entanglement or logit leakage. Instead, it comes down to a small set of divergence tokens-rare cases where teachers with different biases would predict different tokens. Masking out these tokens mostly removes the hidden bias transfer. Mechanistically, divergence tokens reveal that early layers are critical. Surprisingly, finetuning even a single such early layer is sufficient for subliminal learning. Finally, we find that subliminal learning is fragile. Even small changes, like paraphrasing prompts, are usually sufficient to suppress it.",
        "arxiv_id": "2509.23886"
    }
}